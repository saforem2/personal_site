---
title: "ğŸ‹ <code>ezpz</code> @ ALCF"
categories:
  - ezpz
  - ML
  - ai4science
toc: true
date: 2024-08-23
date-modified: last-modified
lightbox: auto
tbl-cap-location: bottom
citation:
   author: Sam Foreman
   type: article-journal
   title: "ğŸ‹ ezpz at ALCF"
   url: https://samforeman.me/posts/ezpz-at-alcf
open-graph:
  title: "ğŸ‹ ezpz @ ALCF"
  description: "ğŸ‹ `ezpz` at ALCF"
  image: "./assets/thumbnail.png"
twitter-card:
  site: "saforem2"
  creator: "saforem2"
  title: "ğŸ‹ ezpz @ ALCF"
  image: "./assets/thumbnail.png"
  description: "ğŸ‹ `ezpz` at ALCF"
  issued: 2022-10-01
format:
  html: default
  # gfm: default

# csl: iso690-numeric-en.csl
# code-tools: true
# filters:
#   - collapse-social-embeds
# format:
#   # gfm:
#   #   output_file: "ezpz-at-alcf.md"
#   html: default
#   # revealjs:
#   #   slide-url: https://samforeman.me/posts/ezpz-at-alcf/slides.html

  # revealjs:
  #   width: 1024
  #   height: 768
  #   max-scale: 4.0
  #   min-scale: 0.1
  #   # title: "AuroraGPT"
  #   pdf-separate-fragments: true
  #   center: false
  #   footer: "[samforeman.me/talks/AuroraGPT/alcf-hpc-workshop-2024/slides](https://samforeman.me/talks/AuroraGPT/alcf-hpc-workshop-2024/slides)"
  #   slide-url: https://samforeman.me/talks/AuroraGPT/alcf-hpc-workshop-2024/slides.html
  #   template-partials:
  #     - title-slide.html
  #   title-slide-attributes:
  #     # data-background-iframe: "file:///iframes/center-of-universe/index.html"
  #     # data-background-image: "https://raw.githubusercontent.com/saforem3/llm-lunch-talk/refs/heads/main/docs/assets/image2.png"
  #     data-background-size: cover
  #     data-background-iframe: https://emilhvitfeldt.github.io/quarto-iframe-examples/stars/index.html
  #     # background-color: dark
  #     # data-background-color: dark
  #   # shift-heading-level-by: -1
# twitter-card:                      
#     image: "./assets/thumbnail.png"
#     creator: "@saforem2"           
#     site: "@saforem2"              
# open-graph:                        
#     image: "./assets/thumbnail.png"
---

> _Work smarter, not harder_.

## ğŸ£ Getting Started

There are two main, distinct components of `ezpz`:

1. ğŸ [**Python Library**](#python-library)
1. ğŸš [**Shell Utilities**](#shell-utilities)

## ğŸš Shell Utilities

The Shell Utilities can be roughly broken up further into two main components

1. [Setup Environment](#setup-environment):
   1. [Setup Python](#setup-python):
      1. [Setup Conda](#setup-conda)
      1. [Setup Virtual Environment](#setup-virtual-environment)
   1. [Setup Job](#setup-job):

We provide a variety of helper functions designed to make your life easier
when working with job schedulers (e.g. `PBS Pro` @ ALCF or `slurm` elsewhere).

**All** of these functions are:

- located in [`utils.sh`](/src/ezpz/bin/utils.sh)
- prefixed with `ezpz_*` (e.g. `ezpz_setup_python`)[^completions]

To use these, we can `source` the file directly via:

```bash
export PBS_O_WORKDIR=$(pwd) # if on ALCF
source /dev/stdin <<< $(curl 'https://raw.githubusercontent.com/saforem2/ezpz/refs/heads/main/src/ezpz/bin/utils.sh')
```

### âš™ï¸ Setup Environment

We would like to write our application in such a way that it is able to take
full advantage of the resources allocated by the job scheduler.

That is to say, we want to have a single script with the ability to dynamically
`launch` python applications across any number of accelerators on any of the
systems under consideration.

In order to do this, there is some basic setup and information gathering that
needs to occur.

In particular, we need mechanisms for:

1. Setting up a python environment
1. Determining what system / machine we're on
   - \+ what job scheduler we're using (e.g. `PBS Pro` @ ALCF or `slurm`
   elsewhere)
1. Determining how many nodes have been allocated in the current job (`NHOSTS`)
   - \+ Determining how many accelerators exist on each of these nodes
   (`NGPU_PER_HOST`)

This allows us to calculate the total number of accelerators (GPUs) as:
$N_{\mathrm{GPU}} = N_{\mathrm{HOST}} \times n_{\mathrm{GPU}}$

where $n_{\mathrm{GPU}} = N_{\mathrm{GPU}} / N_{\mathrm{HOST}}$ is the number
of GPUs per host.

With this we have everything we need to build the appropriate {`mpi`{`run`,
`exec`}, `slurm`} command for launching our python application across them.

Now, there are a few functions in particular worth elaborating on.

::: {#tbl-shell-fns}

| &nbsp;                                                   | Function                     | Description                                                                               |
| :------------------------------------------------------ | :--------------------------  | :-----------                                                                              |
| [Setup Environment](#setup-environment)                  | `ezpz_setup_env`             | Wrapper around `ezpz_setup_python` `&&` `ezpz_setup_job`                                      |
| [Setup Job](#setup-job)                                  | `ezpz_setup_job`             | Determine {`NGPUS`, `NGPU_PER_HOST`, `NHOSTS`}, build `launch` command alias              |
| [Setup Python](#setup-python)                            | `ezpz_setup_python`          | Load python modules and activate virtual environment                                      |
| [Setup Conda](#setup-conda)                              | `ezpz_setup_conda`           | Identify appropriate `conda` module to load                                               |
| [Setup Virtual Environment](#setup-virtual-environment)  | `ezpz_setup_venv_from_conda` | From `${CONDA_NAME}`, build or activate the virtual env located in `venvs/${CONDA_NAME}/` |

: Shell Functions {.responsive .striped .hover}

:::

<!--1. Setup Environment: `ezpz_setup_env`.-->
<!--   Wrapper to chain the following two commands with `&&`:-->
<!--```bash-->
<!--git clone https://github.com/saforem2/ezpz deps/ezpz-->
<!--export PBS_O_WORKDIR=$(pwd)-->
<!--source deps/ezpz/src/ezpz/bin/utils.sh-->
<!--ezpz_setup_python-->
<!--ezpz_setup_job-->
<!--```-->
 <!-- 1. [Setup Pyhon](#setup-python): `ezpz_setup_python` -->
 <!-- 1. [Setup Job](#setup-job): `ezpz_setup_job` -->

::: {.callout-warning collapse=false title="Where am I?"}

_Some_ of the `ezpz_*` functions (e.g. `ezpz_setup_python`), will try to create
/ look for certain directories.

In an effort to be explicit, these directories will be defined **relative to**
a `WORKING_DIR` (e.g. `"${WORKING_DIR}/venvs/"`)

This `WORKING_DIR` will be assigned to the first non-zero match found below:

1. `PBS_O_WORKDIR`: If found in environment, paths will be relative to this
2. `SLURM_SUBMIT_DIR`: Next in line. If not @ ALCF, maybe using `slurm`...
3. `$(pwd)`: Otherwise, no worries. Use your _actual_ working directory.

:::

#### ğŸ› ï¸ Setup Python

```bash
ezpz_setup_python
```

This will:

1. Automatically load and activate `conda` using the `ezpz_setup_conda` function.

   How this is done, in practice, varies from machine to machine:

   - **ALCF**[^alcf]: Automatically load
     the most recent `conda` module and activate the base environment.

   - **Frontier**: Load the appropriate AMD modules (e.g. `rocm`,
     `RCCL`, etc.), and activate base `conda`

   - **Perlmutter**: Load the appropriate `pytorch` module and activate environment

   - **Unknown**: In this case, we will look for a `conda`, `mamba`, or
     `micromamba` executable, and if found, use that to activate the base
     environment.


   ::: {.callout-tip title="Using your own `conda`"}

   If you are already in a conda environment when calling
   `ezpz_setup_python` then it will try and use this instead.

   For example, if you have a custom `conda` env at `~/conda/envs/custom`, then
   this would bootstrap the `custom` conda environment and create the virtual
   env in `venvs/custom/`

   :::

[^alcf]: Any of {Aurora, Polaris, Sophia, Sunspot, Sirius}

2. Build (or activate, if found) a virtual environment on top of (the active) base `conda` environment.

   By default, it will try looking in:

   - `$PBS_O_WORKDIR`, otherwise
   - `${SLURM_SUBMIT_DIR}`, otherwise
   - `$(pwd)`

   for a nested folder named `"venvs/${CONDA_NAME}"`.

   If this doesn't exist, it will attempt to create a new virtual environment
   at this location using:

   ```bash
   python3 -m venv venvs/${CONDA_NAME} --system-site-packages
   ```

   (where we've pulled in the `--system-site-packages` from conda).

#### ğŸ§° Setup Job

```bash
ezpz_setup_job
```

<!-- Now, we are in a suitable python environment with the necessary pre-requisites -->
<!-- installed (e.g. `torch`, `mpi4py`, `deepspeed` (optional)). -->
<!---->
<!-- Next, we would like to build the appropriate command to launch `python3` -->
<!---->
<!-- across our accelerators. -->

Now that we are in a suitable python environment, we need to construct the
command that we will use to run python on each of our acceleratorss.

To do this, we need a few things:

1. What machine we're on (and what scheduler is it using i.e. {PBS, SLURM})
2. How many nodes are available in our active job
3. How many GPUs are on each of those nodes
4. What type of GPUs are they

With this information, we can then use `mpi{exec,run}` or `srun` to launch
python across all of our accelerators.

Again, how this is done will vary from machine to machine and will depend on
the job scheduler in use.

To identify where we are, we look at our `$(hostname)` and see if we're running
on one of the known machines:

- **ALCF**[^node-names]: Using PBS Pro via `qsub` and `mpiexec` / `mpirun`.
  - `x4*`: **Aurora**
  - **Aurora**: `x4*` (or `aurora*` on login nodes)
  - **Sunspot**: `x1*` (or `uan*`)
  - **Sophia**: `sophia-*`
  - **Polaris** / **Sirius**: `x3*`
    - to determine between the two, we look at `"${PBS_O_HOST}"`


[^node-names]: At ALCF, if our `$(hostname)` starts with `x*`, we're on a
    compute node.

- **OLCF**: Using Slurm via `sbatch` / `srun`.

  - `frontier*`: **Frontier**, using Slurm
  - `nid*`: Perlmutter, using Slurm


- Unknown machine: If `$(hostname)` does not match one of these patterns we
  assume that we are running on an unknown machine and will try to use `mpirun`
  as our generic launch command


  Once we have this, we can:

  1. Get `PBS_NODEFILE` from `$(hostname)`:

      - `ezpz_qsme_running`: For each (running) job owned by `${USER}`, print
      out both the jobid as well as a list of hosts the job is running on,
      e.g.:

        ```bash
        <jobid0> host00 host01 host02 host03 ...
        <jobid1> host10 host11 host12 host13 ...
        ...
        ```

      - `ezpz_get_pbs_nodefile_from_hostname`: Look for `$(hostname)` in the
        output from the above command to determine our `${PBS_JOBID}`.

        Once we've identified our `${PBS_JOBID}` we then know the location of
        our `${PBS_NODEFILE}` since they are named according to:

        ```bash
        jobid=$(ezpz_qsme_running | grep "$(hostname)" | awk '{print $1}')
        prefix=/var/spool/pbs/aux
        match=$(/bin/ls "${prefix}" | grep "${jobid}")
        hostfile="${prefix}/${match}"
        ```

  2. Identify number of available accelerators:


## ğŸ Python Library

To install[^require-venv]:

```bash
python3 -m pip install -e "git+https://github.com/saforem2/ezpz#egg=ezpz" --require-virtualenv
```

[^require-venv]: Note the `--require-virtualenv` isn't _strictly_ required, but
    I highly recommend to always try and work within a virtual environment,
    when possible.

<!--
- Clone repo[^location]:

   ```bash
   git clone https://github.com/saforem2/ezpz deps/ezpz
   ```

[^location]: _Where_, specifically, you clone it into (e.g. `deps/ezpz` here),
    isn't terribly important.

    I usually choose to nest it inside a `deps/` directory to avoid the case
    where you have installed `ezpz` as a python library but you've additionally got
    a folder named `ezpz` in your working directory.

    For example:

    ```bash
    $ cd /path/to/your/project
    $ PBS_O_WORKDIR=$(pwd) source /dev/stdin <<< $(curl 'https://raw.githubusercontent.com/saforem2/ezpz/refs/heads/main/src/ezpz/bin/utils.sh')
    $ ezpz_setup_env  # just chains ezpz_setup_python + ezpz_setup_job
    (venv) $ python3 -m pip install -e "git+https://github.com/saforem2/ezpz#egg=ezpz" --require-virtualenv
    ```

    ```bash
    $ git clone https://github.com/saforem2/ezpz
    $ python3 -m pip install -e ezpz
    ```
-->


- ğŸ“‚ [bin/](https://github.com/saforem2/ezpz/blob/main/src/ezpz/bin/):
  - [ezpz/bin/`utils.sh`](https://github.com/saforem2/ezpz/blob/main/src/ezpz/bin/utils.sh): Common utilities for EZPZ
- ğŸ“‚ [conf/](https://github.com/saforem2/ezpz/blob/main/src/ezpz/conf/):
  - âš™ï¸ [conf/`config.yaml`](https://github.com/saforem2/ezpz/blob/main/src/ezpz/conf/config.yaml):  Default `TrainConfig` object
  - âš™ï¸ [conf/`ds_config.json`](https://github.com/saforem2/ezpz/blob/main/src/ezpz/conf/ds_config.json): DeepSpeed configuration
- ğŸ“‚ [log/](https://github.com/saforem2/ezpz/blob/main/src/ezpz/log/): Logging configuration.
- ğŸ [`__about__.py`](https://github.com/saforem2/ezpz/blob/main/src/ezpz/__about__.py): Version information
- ğŸ [`__init__.py`](https://github.com/saforem2/ezpz/blob/main/src/ezpz/__init__.py): Main module
- ğŸ [`__main__.py`](https://github.com/saforem2/ezpz/blob/main/src/ezpz/__main__.py): Entry point
- ğŸ [`configs.py`](https://github.com/saforem2/ezpz/blob/main/src/ezpz/configs.py): Configuration module
- ğŸ[`cria.py`](https://github.com/saforem2/ezpz/blob/main/src/ezpz/create.py): Baby Llama
- ğŸ[**`dist.py`**](https://github.com/saforem2/ezpz/blob/main/src/ezpz/dist.py): Distributed training module
- ğŸ[`history.py`](https://github.com/saforem2/ezpz/blob/main/src/ezpz/history.py): History module
- ğŸ[`jobs.py`](https://github.com/saforem2/ezpz/blob/main/src/ezpz/jobs.py): Jobs module
- ğŸ[`model.py`](https://github.com/saforem2/ezpz/blob/main/src/ezpz/model.py): Model module
- ğŸ[`plot.py`](https://github.com/saforem2/ezpz/blob/main/src/ezpz/plot.py): Plot modul
- ğŸ[`profile.py`](https://github.com/saforem2/ezpz/blob/main/src/ezpz/profile.py): Profile module
- ğŸ[`runtime.py`](https://github.com/saforem2/ezpz/blob/main/src/ezpz/runtime.py): Runtime module
- ğŸ[`test.py`](https://github.com/saforem2/ezpz/blob/main/src/ezpz/test.py): Test module
- ğŸ[**`test_dist.py`**](https://github.com/saforem2/ezpz/blob/main/src/ezpz/test_dist.py): Distributed training test module
- ğŸ[`train.py`](https://github.com/saforem2/ezpz/blob/main/src/ezpz/train.py): train module
- ğŸ[`trainer.py`](https://github.com/saforem2/ezpz/blob/main/src/ezpz/trainer.py): trainer module
- ğŸ[`utils.py`](https://github.com/saforem2/ezpz/blob/main/src/ezpz/utils.py): utility module






```bash
ğŸ“‚ /ezpz/src/ezpz/
â”£â”â” ğŸ“‚ bin/
â”ƒ   â”£â”â” ğŸ“„ affinity.sh
â”ƒ   â”£â”â” ğŸ“„ getjobenv
â”ƒ   â”£â”â” ğŸ“„ savejobenv
â”ƒ   â”£â”â” ğŸ“„ saveslurmenv
â”ƒ   â”£â”â” ğŸ“„ setup.sh
â”ƒ   â”£â”â” ğŸ“„ train.sh
â”ƒ   â”—â”â” ğŸ“„ utils.sh
â”£â”â” ğŸ“‚ conf/
â”ƒ   â”£â”â” ğŸ“‚ hydra/
â”ƒ   â”ƒ   â”—â”â” ğŸ“‚ job_logging/
â”ƒ   â”ƒ       â”£â”â” âš™ï¸ colorlog1.yaml
â”ƒ   â”ƒ       â”£â”â” âš™ï¸ custom.yaml
â”ƒ   â”ƒ       â”—â”â” âš™ï¸ enrich.yaml
â”ƒ   â”£â”â” ğŸ“‚ logdir/
â”ƒ   â”ƒ   â”—â”â” âš™ï¸ default.yaml
â”ƒ   â”£â”â” âš™ï¸ config.yaml
â”ƒ   â”£â”â” ğŸ“„ ds_config.json
â”ƒ   â”—â”â” âš™ï¸ ds_config.yaml
â”£â”â” ğŸ“‚ log/
â”ƒ   â”£â”â” ğŸ“‚ conf/
â”ƒ   â”ƒ   â”—â”â” ğŸ“‚ hydra/
â”ƒ   â”ƒ       â”—â”â” ğŸ“‚ job_logging/
â”ƒ   â”ƒ           â”—â”â” âš™ï¸ enrich.yaml
â”ƒ   â”£â”â” ğŸ __init__.py
â”ƒ   â”£â”â” ğŸ __main__.py
â”ƒ   â”£â”â” ğŸ config.py
â”ƒ   â”£â”â” ğŸ console.py
â”ƒ   â”£â”â” ğŸ handler.py
â”ƒ   â”£â”â” ğŸ style.py
â”ƒ   â”£â”â” ğŸ test.py
â”ƒ   â”—â”â” ğŸ test_log.py
â”£â”â” ğŸ __about__.py
â”£â”â” ğŸ __init__.py
â”£â”â” ğŸ __main__.py
â”£â”â” ğŸ configs.py
â”£â”â” ğŸ cria.py
â”£â”â” ğŸ dist.py
â”£â”â” ğŸ history.py
â”£â”â” ğŸ jobs.py
â”£â”â” ğŸ loadjobenv.py
â”£â”â” ğŸ model.py
â”£â”â” ğŸ plot.py
â”£â”â” ğŸ profile.py
â”£â”â” ğŸ runtime.py
â”£â”â” ğŸ savejobenv.py
â”£â”â” ğŸ test.py
â”£â”â” ğŸ test_dist.py
â”£â”â” ğŸ train.py
â”£â”â” ğŸ trainer.py
â”—â”â” ğŸ utils.py
```


   <!-- ```bash -->
   <!-- "${PBS_O_WORKDIR:-$(pwd)}/venvs/${CONDA_NAME}" -->
   <!-- ``` -->

   <!-- Ideally, following step 1., we are now inside a suitable `conda` -->
   <!-- environment. -->
   <!---->
   <!-- Next, we will look for a virtual environment located in: -->
   <!---->
   <!-- ```bash -->
   <!-- venvs/${CONDA_NAME} -->
   <!-- ``` -->
   <!---->
   <!-- If this exists, activate it and continue. -->
   <!-- Otherwise, we will create a new `venv` via: -->
   <!---->

<!-- `ezpz_setup_venv`:  -->

<!-- base `conda` environment to build a `venv` on top of -->
<!-- Build (or activate, if already exists) a virtual environment on top of this  -->

<!-- Place you inside a `venv/${CONDA_NAME}` (building it if needed)  -->
<!-- This is done in two parts. -->
<!---->
<!-- 1. `ezpz_setup_conda`: Will first attempt to load and activate a suitable -->
<!--   `conda` environment. -->


[^completions]: Plus this is useful for tab-completions in your shell, e.g.:

    ```bash
    $ ezpz_<TAB>
    ezpz_check_and_kill_if_running
    ezpz_get_dist_launch_cmd
    ezpz_get_job_env
    --More--
    ```

<!-- ```bash -->
<!-- $ ezpz_<TAB> -->
<!-- ezpz_check_and_kill_if_running -->
<!-- ezpz_get_dist_launch_cmd -->
<!-- ezpz_get_job_env -->
<!-- ezpz_get_jobenv_file -->
<!-- ezpz_get_jobid_from_hostname -->
<!-- ezpz_get_machine_name -->
<!-- ezpz_get_num_gpus_nvidia -->
<!-- ezpz_get_num_gpus_per_host -->
<!-- ezpz_get_num_gpus_total -->
<!-- ezpz_get_num_hosts -->
<!-- ezpz_get_num_xpus -->
<!-- ezpz_get_pbs_env -->
<!-- ezpz_get_pbs_nodefile_from_hostname -->
<!-- ezpz_get_scheduler_type -->
<!-- ezpz_get_shell_name -->
<!-- ezpz_get_slurm_env -->
<!-- ezpz_get_slurm_running_jobid -->
<!-- ezpz_get_slurm_running_nodelist -->
<!-- ezpz_get_tstamp -->
<!-- ezpz_getjobenv_main -->
<!-- ezpz_make_slurm_nodefile -->
<!-- ezpz_parse_hostfile -->
<!-- ezpz_print_hosts -->
<!-- ezpz_print_job_env -->
<!-- ezpz_qsme_running -->
<!-- ezpz_reset_pbs_vars -->
<!-- ezpz_save_deepspeed_env -->
<!-- ezpz_save_dotenv -->
<!-- ezpz_save_ds_env -->
<!-- ezpz_save_pbs_env -->
<!-- ezpz_save_slurm_env -->
<!-- ezpz_savejobenv_main -->
<!-- ezpz_setup_alcf -->
<!-- ezpz_setup_conda -->
<!-- ezpz_setup_conda_aurora -->
<!-- ezpz_setup_conda_frontier -->
<!-- ezpz_setup_conda_polaris -->
<!-- ezpz_setup_conda_sirius -->
<!-- ezpz_setup_conda_sophia -->
<!-- ezpz_setup_conda_sunspot -->
<!-- ezpz_setup_host -->
<!-- ezpz_setup_host_old -->
<!-- ezpz_setup_host_pbs -->
<!-- ezpz_setup_host_slurm -->
<!-- ezpz_setup_job -->
<!-- --More-- -->
<!-- ``` -->

<!--
One of the major frustrations I had when running and testing distributed
applications was the amount of overhead needed simply to "run" the set of
commands I wanted to.

We would like to write our applications in such a way that they are independent
of the machine, job scheduler and accelerator type used.

are completely
independent of the details of our job scheduler
scale
automatically with the resources allocated by our job scheduler.
<!--
In order to do this, we need a mechanism for determining the 
the amount of resources ava

In order to do this, there is some (relatively simple) setup and calculations
that must 

are able to run
on an arbitrary configuration of resources allocated
We provide a simple mechanism for dynamically setting up the runtime

environment when allocated a set of resources through a job scheduler.
our environment to run python applications across any number of GPUs.

environment to 
<!--
    on ALCF:
    setup conda + venv (creating if needed)
    from inside a {PBS, SLURM} job, this will:
    - do some sleuthing to determine specifics of active job 
    - number of nodes available
    - how many GPUs are on each of them
    - what type of GPUs they are
    - what machine am I on
    - etc.
    then, with this information, we can fill in the details of the
    {mpirun,  mpiexec, srun} command that we will use to launch our
    executable (`python3`, in this case) across all the accelerators in our job.
<!-- To setup Python, run: -->
<!---->
<!-- ```bash -->
<!-- ezpz_setup_python -->
<!-- ``` -->
<!---->
<!-- which will automagically âœ¨ set up and activate an appropriate virtual -->
<!-- environment. -->
<!---->
<!-- ::: -->
<!--
::: {.callout-important title="Shell Utilities" collapsed=false}
We provide a variety of helper functions designed to make your life easier.

They're:

- entirely self-contained in [**`utils.sh`**](/ezpz/src/ezpz/bin/utils.sh)
- all prefixed with `ezpz_*`

:::

To take advantage of these helper functions we need to `source` this
[`utils.sh`](/ezpz/src/ezpz/bin/utils.sh).

This can be done by:

```bash
PBS_O_WORKDIR=$(pwd) source /dev/stdin <<< $(curl 'https://raw.githubusercontent.com/saforem2/ezpz/refs/heads/main/src/ezpz/bin/utils.sh')
```
-->

