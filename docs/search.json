[
  {
    "objectID": "posts/ai-for-physics/diffusion/index.html",
    "href": "posts/ai-for-physics/diffusion/index.html",
    "title": "🎲 MCMC + Diffusion Sampling",
    "section": "",
    "text": "2D U(1)\nfrom l2hmc.configs import dict_to_list_of_overrides\n\nseed = np.random.randint(0, 2**32)\nconsole.print(f\"seed = {seed}\")\n\noverrides = {\n    \"seed\": f\"{seed}\",\n    \"precision\": \"float32\",\n    \"init_wandb\": False,\n    \"init_aim\": False,\n    \"use_wandb\": False,\n    \"dynamics\": {\n        \"latvolume\": [32, 32],\n        \"nleapfrog\": 10,\n        \"nchains\": 16,\n        \"eps\": 0.05,\n    },\n    \"network\": {\n        \"use_batch_norm\": False,\n    },\n    'annealing_schedule': {\n        'beta_init': 6.0,\n        'beta_final': 6.0,\n    },\n\n}\nOVERRIDES = dict_to_list_of_overrides(overrides)\nfrom pathlib import Path\nfrom l2hmc.common import get_timestamp\nfrom enrich.console import get_theme, Console\nconsole = Console(theme=get_theme())\n\nOUTDIR = Path(\n    'l2hmc-diffusion-2dU1'\n).joinpath(get_timestamp(\"%Y-%m-%d\"))\nOUTDIR.mkdir(exist_ok=True, parents=True)\nconsole.print(f\"OUTDIR: {OUTDIR}\")\n\ndate = get_timestamp('%Y-%m-%d')\nPLOTS_DIR = OUTDIR.joinpath('plots')\nPLOTS_DIR.mkdir(exist_ok=True, parents=True)\nconsole.print(f\"Saving figures to: {PLOTS_DIR}\")\n#os.environ['MASTER_PORT'] = '5436'\n\nexp = build_experiment(\n    overrides=[\n        *OVERRIDES,\n        'framework=pytorch',\n        'backend=DDP'\n    ]\n)\nstate = exp.trainer.dynamics.random_state(6.0)\nxdim = state.x.flatten().shape[0]\n\ndim = xdim\nlow_bound = (-np.pi) * np.ones(dim)\nhigh_bound = (np.pi) * np.ones(dim)\nsigma = 0.15\nretrains = 10\nsamples_per_retrain = 100\ndiffusion_prob = 0.1\nsns.set_context('notebook')\n\noutputs = {}\noutputs['hmc'] = exp.trainer.eval(\n    job_type='hmc',\n    beta=6.0,\n    nprint=100,\n    nchains=16,\n    eval_steps=1000\n)\n#hdset = exp.save_dataset(job_type='hmc', nchains=1)\n# %matplotlib inline\nfrom l2hmc.common import plot_dataset\nsns.set_context('notebook')\nhdataset = outputs['hmc']['history'].get_dataset()\nplot_dataset(hdataset, outdir=PLOTS_DIR, job_type='HMC')\nimport torch\n\ninitial_states = []\nstate_init = exp.trainer.dynamics.random_state(6.0)\nx = state_init.x\nbeta = state_init.beta\n\nNSAMPLES = 1000\nfor idx in range(NSAMPLES + int(0.1 * NSAMPLES)):\n    if idx % 100 == 0:\n        console.print(f\"step: {idx}\")\n    x, metrics = exp.trainer.hmc_step((x, beta))\n    if idx &gt; int((0.1 * NSAMPLES)):\n        initial_states.append(x)\n\ninitial_states = torch.stack(initial_states).squeeze()\ninitial_states_np = initial_states.detach().cpu().numpy()\ninitial_states_np.shape\nx_ = initial_states_np.reshape(-1, 16, 2, 32, 32)\ntmp_ = x_[:, 0, ...]\nconsole.print(f'{x_.shape}')\nconsole.print(f'{tmp_.shape}')\nfrom l2hmc.common import savefig\n\n#x_ = initial_states_np[:100].reshape(-1, 2, 32, 32)\ntmp_ = x_[:, 0, ...]\nfig, ax = plt.subplots()\nsns.kdeplot(\n    x=tmp_[-100:, 0].flatten(),\n    y=tmp_[-100:, 1].flatten(),\n    # ax=ax,\n    cmap='viridis',\n    # ax=axes[0],\n    # cmap=\"Blues\",\n    shade=False,\n    # bw_adjust=0.5,\n    thresh=0\n)\nax.set_xlim((-4, 4))\nax.set_ylim((-4, 4))\nsavefig(\n    f'hmc_samples-{NSAMPLES}',\n    Path(PLOTS_DIR),\n    tstamp=True,\n)\nclass Diffusion:\n    def __init__(\n            self,\n            noise_steps: int = 1000,\n            beta_start: float = 1e-4,\n            beta_end: float = 0.02,\n            nchannels: int = 2,\n            img_size: int = 256,\n            device: str = \"cuda\"\n    ):\n        self.noise_steps = noise_steps\n        self.beta_start = beta_start\n        self.beta_end = beta_end\n        self.img_size = img_size\n        self.device = device\n        self.nchannels = nchannels\n\n        self.beta = self.prepare_noise_schedule().to(device)\n        self.alpha = 1. - self.beta\n        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n\n    def prepare_noise_schedule(self):\n        return torch.linspace(self.beta_start, self.beta_end, self.noise_steps)\n\n    def noise_images(self, x, t):\n        sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:, None, None, None]\n        sqrt_one_minus_alpha_hat = torch.sqrt(\n            1 - self.alpha_hat[t]\n        )[:, None, None, None]\n        eps = torch.randn_like(x)\n        return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * eps, eps\n\n    def sample_timesteps(self, n):\n        return torch.randint(low=1, high=self.noise_steps, size=(n,))\n\n    def sample(self, model, n):\n        # console.print(f\"Sampling {n} new images....\")\n        model.eval()\n        with torch.no_grad():\n            x = torch.randn(\n                (n, self.nchannels, self.img_size, self.img_size)\n            ).to(self.device)\n            sample_bar = tqdm(\n                reversed(range(1, self.noise_steps)),\n                position=0,\n                total=self.noise_steps - 1,\n                dynamic_ncols=True,\n            )\n            for i in sample_bar:\n                t = (torch.ones(n) * i).long().to(self.device)\n                predicted_noise = model(x, t)\n                alpha = self.alpha[t][:, None, None, None]\n                alpha_hat = self.alpha_hat[t][:, None, None, None]\n                beta = self.beta[t][:, None, None, None]\n                if i &gt; 1:\n                    noise = torch.randn_like(x)\n                else:\n                    noise = torch.zeros_like(x)\n                x = (\n                    (1 / torch.sqrt(alpha))\n                    * (\n                        x \n                        - ((1 - alpha) / (torch.sqrt(1 - alpha_hat)))\n                        * predicted_noise\n                    ) \n                    + (torch.sqrt(beta) * noise)\n                )\n        model.train()\n        x = (x + np.pi) % (2 * np.pi) - np.pi\n        return x\ninitial_states.shape\nTrain Diffusion Model\nimport torchvision\nimport os\nimport random\nfrom pathlib import Path\nimport torch\nimport torchvision\nimport torchvision.transforms as T\nimport numpy as np\nfrom PIL import Image\n#from fastdownload import FastDownload\nfrom torch.utils.data import DataLoader\n\ndef save_images(images, path, **kwargs):\n    grid = torchvision.utils.make_grid(images, **kwargs)\n    ndarr = grid.permute(1, 2, 0).to('cpu').numpy()\n    im = Image.fromarray(ndarr)\n    im.save(path)\nBuild Diffusion Model with UNet Architecure\nfrom torch import nn\nfrom torch import optim\nimport torch.nn.functional as F\nfrom tqdm.auto import tqdm\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import TensorDataset\n\nfrom l2hmc.common import savefig\nfrom l2hmc.diffusion.modules import NoiseScheduler, UNet\nfrom l2hmc.diffusion import ddpm\n\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nconfig = {\n    'channels_in': 2,\n    'channels_out': 2,\n    'train_batch_size': 5,\n    'learning_rate': 0.001,\n    'num_epochs': 1,\n    'noise_steps': 100,\n    'beta': 6.0,\n    'img_size': 32,\n    'retrains': 10,\n    'samples_per_retrain': 500,\n    'diffusion_prob': 0.1,\n}\n\nmodel = UNet(c_in=2, c_out=2)\n\ndataset = TensorDataset(initial_states.reshape(-1, 2, 32, 32))\ndataloader = DataLoader(\n    dataset,\n    batch_size=config[\"train_batch_size\"],\n    shuffle=False,\n    drop_last=True\n)\n\n\noptimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'])\nmse = nn.MSELoss()\ndiffusion = Diffusion(\n    noise_steps=100,\n    img_size=32,\n    device=DEVICE,\n    nchannels=2,\n)\n#logger = SummaryWriter(os.path.join(\"runs\", args.run_name))\nl = len(dataloader)\n\nrun_name = 'diffusion2dU1'\nPerform initial training on HMC samples\nfrom torch import optim\ndevice = 'cpu'\n#dataloader = get_data(args)\n#model = UNet().to(device)\n\nsampled_images_history = []\n\nfor epoch in range(config['num_epochs']):\n    console.print(f\"Starting epoch {epoch}:\")\n    pbar = tqdm(dataloader)\n    for i, images in enumerate(pbar):\n        if isinstance(images, (tuple, list)) and len(images) == 1:\n            images = images[0]\n        t = diffusion.sample_timesteps(images.shape[0]).to(device)\n        x_t, noise = diffusion.noise_images(images, t)\n        predicted_noise = model(x_t, t)\n        loss = mse(noise, predicted_noise)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        pbar.set_postfix({'epoch': epoch, 'batch': i, 'MSE': loss.item()})\n    console.print(f'epoch: {epoch}, loss: {loss.item()}')\n    sampled_images = diffusion.sample(model, n=images.shape[0])\n    sampled_images_history.append(sampled_images)\n    sns.set_context('notebook')\n    #tmp = initial_states.reshape(-1, 2, 32, 32)\n    fig, ax = plt.subplots(ncols=2)\n    _ = ax[0].imshow(sampled_images[0, 0, :, :])\n    _ = ax[1].imshow(sampled_images[0, 1, :, :])\n    _ = ax[0].set_xticklabels([])\n    _ = ax[1].set_xticklabels([])\n    _ = ax[0].set_yticklabels([])\n    _ = ax[1].set_yticklabels([])\n    _ = ax[0].set_title(r\"$U_{0}$\", loc='center')\n    _ = ax[1].set_title(r\"$U_{1}$\", loc='center')\n    _ = fig.suptitle('Diffusion Samples', y=0.8)\n    plt.show()\n    savefig(fname=f'sampled_image_epoch{epoch}', outdir=PLOTS_DIR, tstamp=True)\n    MODEL_FILE = OUTDIR.joinpath(\"models\", f\"unet-diffusion-epoch{epoch}.pt\")\n    MODEL_FILE.parent.mkdir(exist_ok=True, parents=True)\n    console.print(f\"Saving model checkpoint to: {MODEL_FILE}\")\n    torch.save(model.state_dict(), MODEL_FILE)\nsns.set_context('notebook')\ntmp = initial_states.reshape(-1, 2, 32, 32)\nfig, ax = plt.subplots(ncols=2)\n_ = ax[0].imshow(tmp[0, 0, :, :])\n_ = ax[1].imshow(tmp[0, 1, :, :])\n_ = ax[0].set_title(r\"$U_{0}$\", loc='center')\n_ = ax[0].set_xticklabels([])\n_ = ax[1].set_xticklabels([])\n_ = ax[0].set_yticklabels([])\n_ = ax[1].set_yticklabels([])\n_ = ax[1].set_title(r\"$U_{1}$\", loc='center')\n_ = fig.suptitle('HMC Samples', y=0.8)\nsampled_images_history_ = torch.stack(sampled_images_history)\nsampled_images_history_.shape\n\ntorch.Size([1, 5, 2, 32, 32])\nsns.set_context('notebook')\nfig, ax = plt.subplots(ncols=2)\n_ = ax[0].imshow(sampled_images_history_[0][0][0])\n_ = ax[1].imshow(sampled_images_history_[0][0][1])\n_ = ax[0].set_xticklabels([])\n_ = ax[1].set_xticklabels([])\n_ = ax[0].set_yticklabels([])\n_ = ax[1].set_yticklabels([])\n_ = ax[0].set_title(r\"$U_{0}$\", loc='center')\n_ = ax[1].set_title(r\"$U_{1}$\", loc='center')\n_ = fig.suptitle('Diffusion Samples', y=0.85)\nfor idx in range(sampled_images_history_.shape[0]):\n    q = exp.trainer.lattice.charges(x=sampled_images_history_[idx])\n    console.print(f'{idx}: {q}')\nHMC Sampling with Diffusion\n#for retrain_iter in range(config['retrains']):\nstate = exp.trainer.dynamics.random_state(config['beta'])\nx = state.x\n\nhistories = {}\nsamples = []\nhmc_samples = []\ndiffusion_samples = []\n\nglobal_step = 0\nwatcher = {}\nupdate_types = []\ncombined_samples = {}\nglobal_step\nfor retrain_iter in range(2):\n    console.print(f'retrain_iter: {retrain_iter}')\n    ndiff_acc = 0\n    ndiff_proposed = 0\n    histories[retrain_iter] = {\n        'diffusion': [],\n        'hmc': [],\n    }\n    #for idx in range(config['samples_per_retrain']):\n    sbar = tqdm(range(10))\n    for idx in sbar:\n        t0_ = time.perf_counter()\n        if idx % 100 == 0:\n            console.print(f'sample idx: {idx}')\n        rand = np.random.uniform()\n        if (retrain_iter &gt;= 1) and rand &lt; diffusion_prob:\n            console.print(f'rand: {rand} &lt; {diffusion_prob}')\n            # Sample from diffusion model\n            x_ = diffusion.sample(model, n=x.shape[0])\n            ll_ = exp.trainer.dynamics.potential_energy(x_, config['beta'])\n            ll = exp.trainer.dynamics.potential_energy(x, config['beta'])\n            ratio = ll_ / ll\n            a = torch.min(torch.ones_like(ratio), ratio)\n            u = torch.rand(a.shape)\n            #u = np.random.uniform()\n            #for jdx in range(u.shape[0]):\n            #    if u[jdx] &lt; a[jdx]:\n            #        samples.append(x_[jdx])\n            #        diffusion_samples.append(x_[jdx])\n            #x = torch.where((u &lt; a), x_, x.reshape_as(x_)).reshape_as(x)\n            x = torch.where((u &lt; a)[:, None, None, None], x_, x.reshape_as(x_))\n            samples.append(x)\n            diffusion_samples.append(x)\n            combined_samples[global_step] = x\n            watcher[global_step] = 'diffusion'\n            #diffusion_samples.extend(x)\n            #samples.extend(x)\n            #ndiff_acc += \n            #if u &lt; a:\n            #    console.print('Accepted diffusion sample!')\n            #    console.print(f'{ndiff_acc} / {ndiff_proposed}')\n            #    ndiff_acc += 1\n            #    x = x_\n            #    diffusion_samples.append(x)\n            #    samples.append(x)\n        else:\n            # Oherwise, HMC\n            x, metrics = exp.trainer.hmc_step((x, config['beta']))\n            hmc_samples.append(x)\n            samples.append(x)\n            combined_samples[global_step] = x\n            watcher[global_step] = 'HMC'\n        smetrics = {\n            'idx': idx,\n            'global_step': global_step,\n            'dt': time.perf_counter() - t0_,\n        }\n        global_step += 1\n        #smetrics |= {\n        #    f'{k}': {torch.tensor(v).mean().item()} for k, v in metrics.items()\n        #}\n        sbar.set_postfix(smetrics)\n    # Train loop\n    dataset = TensorDataset(\n        torch.stack(hmc_samples).reshape(-1, 2, 32, 32)\n    )\n    dataloader = DataLoader(\n        dataset,\n        shuffle=False,\n        drop_last=True,\n        batch_size=config[\"train_batch_size\"],\n    )\n    pbar = tqdm(dataloader)\n    for i, batch in enumerate(pbar):\n        if i == 0:\n            console.print('Retraining...')\n        if isinstance(batch, (tuple, list)) and len(batch) == 1:\n            batch, = batch\n        batch = batch.reshape(-1, 2, 32, 32)\n        t0 = time.time()\n        t = diffusion.sample_timesteps(batch.shape[0]).to(device)\n        x_t, noise = diffusion.noise_images(batch, t)\n        predicted_noise = model(x_t, t)\n        loss = mse(noise, predicted_noise)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        t1 = time.time()\n        pbar.set_postfix(\n            {\n                'global_step': global_step,\n                'retrain_iter': retrain_iter,\n                'batch': i,\n                'dt': t1 - t0,\n                'MSE': loss.item()\n            }\n        )\nconsole.print('\\n'.join([f\"{i.shape}\" for i in samples[:100]]))\nsamples_ = torch.stack([i.reshape(-1, 2, 32, 32) for i in samples])\nsamples_.shape\n\ntorch.Size([30, 16, 2, 32, 32])\nlen(hmc_samples)\nlen(diffusion_samples)\n\n2\nhmc_samples_ = torch.stack([i.reshape(-1, 2, 32, 32) for i in hmc_samples])\ndiffusion_samples_ = torch.stack(\n    [i.reshape(-1, 2, 32, 32) for i in diffusion_samples]\n)\nhmc_samples_.shape\n\ntorch.Size([28, 16, 2, 32, 32])\ndiffusion_samples_.shape\nsamples_.shape\ndef calc_plaqs(x):\n    return torch.stack([\n        exp.trainer.lattice.plaqs(\n            x[:, idx]\n        ) for idx in range(x.shape[1])\n    ], -1)\n\ndef calc_intQ(x):\n    return torch.stack([\n        exp.trainer.lattice.int_charges(\n            x[:, idx]\n        ) for idx in range(x.shape[1])\n    ], -1)\n    \ndef calc_sinQ(x):\n    return torch.stack([\n        exp.trainer.lattice.sin_charges(\n            x[:, idx]\n        ) for idx in range(x.shape[1])\n    ], -1)\nsamples_init_ = initial_states.reshape(-1, initial_states.shape[1], 2, 32, 32)\nsamples_init_.shape\nmetrics_init_ = {\n    'plaqs': calc_plaqs(samples_init_),\n    'intQ': calc_intQ(samples_init_),\n    'sinQ': calc_sinQ(samples_init_)\n}\n    \nmetrics_ = {\n    'plaqs': calc_plaqs(samples_),\n    'intQ': calc_intQ(samples_),\n    'sinQ': calc_sinQ(samples_)\n}\n\nmetrics_hmc_ = {\n    'plaqs': calc_plaqs(hmc_samples_),\n    'intQ': calc_intQ(hmc_samples_),\n    'sinQ': calc_sinQ(hmc_samples_)\n}\n\nmetrics_diffusion_ = {\n    'plaqs': calc_plaqs(diffusion_samples_),\n    'intQ': calc_intQ(diffusion_samples_),\n    'sinQ': calc_sinQ(diffusion_samples_)\n}\nmetrics_['plaqs'].shape\nconsole.print('\\n'.join([f\"{k}: {v}\" for k, v in watcher.items()]))\nfig, ax = plt.subplots()\n\n_ = ax.plot(metrics_['plaqs'][:, 0], label='Combined')\n_ = ax.plot(metrics_hmc_['plaqs'][:, 0], label='HMC')\n_ = ax.plot(metrics_diffusion_['plaqs'][:, 0], label='Diffusion')\n#_ = ax.plot(metrics_hmc1['plaqs'], label='HMC 1')\n#_ = ax.plot(metrics_diff_['plaqs'], label='Diffusion')\n_ = ax.legend(loc='upper left', bbox_to_anchor=(1.05, 1.00))\nfig, ax = plt.subplots(ncols=3, figsize=(14, 4))\nfor idx, (key, val) in enumerate(metrics_init_.items()):\n    _ = ax[idx].plot(val[:, 0], label='HMC (Initial Samples)')\n    _ = ax[idx].set_ylabel(key, loc='center')\n    _ = ax[idx].set_xlabel('Draw', loc='center')\n    #_ = ax[idx].legend(loc='best', frameon=True, edgecolor=\"#838383\")\n\n_ = fig.suptitle(f\"Initial HMC States\", y=0.92)\nfig, ax = plt.subplots(ncols=3, figsize=(14, 4))\nfor idx, (key, val) in enumerate(metrics_.items()):\n    _ = ax[idx].plot(val[:, 0], label='Combined')\n    _ = ax[idx].set_ylabel(key, loc='center')\n    _ = ax[idx].set_xlabel('Draw', loc='center')\n    #_ = ax[idx].legend(loc='best', frameon=True, edgecolor=\"#838383\")\n\n_ = fig.suptitle(f\"Combined Samples\", y=0.92)\nfig, ax = plt.subplots(ncols=3, figsize=(14, 4))\nfor idx, (key, val) in enumerate(metrics_hmc_.items()):\n    _ = ax[idx].plot(val[:, 0], label='HMC')\n    _ = ax[idx].set_ylabel(key, loc='center')\n    _ = ax[idx].set_xlabel('Draw', loc='center')\n_ = fig.suptitle(f\"Generated HMC States\", y=0.92)\nfig, ax = plt.subplots(ncols=3, figsize=(14, 4))\nfor idx, (key, val) in enumerate(metrics_diffusion_.items()):\n    _ = ax[idx].plot(val[:, 0], label='Diffusion')\n    _ = ax[idx].set_ylabel(key, loc='center')\n    _ = ax[idx].set_xlabel('Draw', loc='center')\n_ = fig.suptitle(f\"Generated Diffusion States\", y=0.92)\nfrom l2hmc.lattice.u1.pytorch.lattice import plaq_exact\nplaq_exact(torch.tensor(6.0))\nfig, ax = plt.subplots()\n#_ = plt.hist(metrics_['intQ'].flatten(), color='C0', alpha=0.6, label='Combined', edgecolor='none')\n_ = ax.hist(\n    metrics_diffusion_['intQ'].flatten(),\n    color='C0',\n    alpha=0.6,\n    edgecolor='none',\n    label='Diffusion',\n    density=True,\n)\n_ = ax.hist(\n    metrics_hmc_['intQ'].flatten(),\n    color='C1',\n    alpha=0.6,\n    edgecolor='none',\n    label='HMC',\n    density=True,\n)\n_ = ax.legend(loc='best', frameon=True, edgecolor='#666666')\n_ = ax.set_xlabel(r\"$Q$\", loc='center')\n_ = ax.set_title('Topological Charge ($Q$) Distribution', loc='center')\nfig, ax = plt.subplots()\n_ = plt.plot(metrics_['plaqs'][:, 0], color='C0', label='Diffusion')\n_ = plt.plot(metrics_hmc_['plaqs'][:, 0], color='C1', label='HMC')\n_ = ax.legend(loc='best', frameon=True, edgecolor='#666666', ncols=2)\n_ = ax.set_ylabel(r\"$\\left\\langle U_{\\mu\\nu}\\right\\rangle $\", loc='center')\n_ = ax.set_xlabel(f\"Draw\", loc='center')\nwloops = {\n    'hmc': [\n        exp.trainer.lattice.wilson_loops(i) for i in hmc_samples_\n    ],\n    'diffusion': [\n        exp.trainer.lattice.wilson_loops(i) for i in diffusion_samples_\n    ],\n}\n\nplaqs = {\n    'hmc': [\n        exp.trainer.lattice.plaqs(i) for i in hmc_samples_\n    ],\n    'diffusion': [\n        exp.trainer.lattice.plaqs(i) for i in diffusion_samples_\n    ],\n}\nwlhmc = torch.stack(wloops['hmc']).squeeze()\nwldiff = torch.stack(wloops['diffusion']).squeeze()\nwlhmc.shape\n_ = plt.tight_layout()\nfor idx in range(2):\n    fig, ax = plt.subplots(ncols=2)\n    _ = ax[0].imshow(wlhmc[idx, 0])\n    _ = ax[0].set_title(\"HMC\", loc='center')\n    _ = ax[1].imshow(wldiff[idx, 0])\n    _ = ax[1].set_title(\"Diffusion\", loc='center')\n    _ = fig.suptitle(r\"$U_{\\mu\\nu}$\", y=0.8)\n    for ax_ in ax:\n        _ = ax_.set_xticklabels([])\n        _ = ax_.set_yticklabels([])\nqhmc = metrics_hmc_['intQ']\nqdiff = metrics_diffusion_['intQ']\nqhmc.shape\nphmc = torch.stack(plaqs['hmc']).squeeze()\npdiff = torch.stack(plaqs['diffusion']).squeeze()\nphmc.shape\npdiff.shape\nfig, ax = plt.subplots()\n\n_ = ax.hist(\n    metrics_['plaqs'].flatten(),\n    color='C1',\n    histtype='step',\n    stacked=True,\n    density=True,\n    label='HMC',\n    linewidth=1.5\n)\n_ = ax.hist(\n    metrics_diffusion_['plaqs'].flatten(),\n    color='C0',\n    histtype='step',\n    stacked=True,\n    density=True,\n    label='Diffusion',\n    linewidth=1.5\n)\n_ = ax.hist(\n    metrics_hmc_['plaqs'].flatten(),\n    color='C2',\n    histtype='step',\n    stacked=True,\n    density=True,\n    label='Combined',\n    linewidth=1.5\n)\n_ = ax.set_xlabel(r\"$U_{\\mu\\nu}$\", loc='center')\n_ = ax.legend(\n    loc='upper left',\n    frameon=True,\n    #ncols=2,\n    bbox_to_anchor=(0.55, 1.00),\n    edgecolor=\"#838383\",\n)\n_ = ax.set_title('Plaquette Distribution', loc='center')\nfig, ax = plt.subplots()\n\n_ = ax.hist(\n    metrics_['intQ'].flatten(),\n    color='C1',\n    histtype='step',\n    stacked=True,\n    density=True,\n    label='HMC',\n    linewidth=1.5\n)\n_ = ax.hist(\n    metrics_diffusion_['intQ'].flatten(),\n    color='C0',\n    histtype='step',\n    stacked=True,\n    density=True,\n    label='Diffusion',\n    linewidth=1.5\n)\n_ = ax.hist(\n    metrics_hmc_['intQ'].flatten(),\n    color='C2',\n    histtype='step',\n    stacked=True,\n    density=True,\n    label='Combined',\n    linewidth=1.5\n)\n_ = ax.set_xlabel('$Q_{\\mathbb{Z}}$', loc='center')\n_ = ax.legend(\n    loc='upper left',\n    frameon=True,\n    #ncols=2,\n    bbox_to_anchor=(0.55, 1.00),\n    edgecolor=\"#838383\",\n)\n_ = ax.set_title('Charge Distribution', loc='center')\nglobal_step = 0\nframes = []\nlosses = []\nprint(\"Training model...\")\nfor epoch in range(config[\"num_epochs\"]):\n    model.train()\n    progress_bar = tqdm(total=len(dataloader))\n    progress_bar.set_description(f\"Epoch {epoch}\")\n    for step, batch in enumerate(dataloader):\n        t = diffusion.sample_timesteps(images.shape[0]).to(device)\n\n        noise = torch.randn(batch.shape)\n        timesteps = torch.randint(\n            0, noise_scheduler.num_timesteps, (batch.shape[0],)\n        ).long()\n\n        #noisy = noise_scheduler.add_noise(batch, noise, timesteps)\n        noisy = noise_scheduler.noise_images(batch, timesteps)\n        noise_pred = model(noisy, timesteps)\n        loss = F.mse_loss(noise_pred, noise)\n        loss.backward(loss)\n\n        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        optimizer.zero_grad()\n\n        progress_bar.update(1)\n        logs = {\"loss\": loss.detach().item(), \"step\": global_step}\n        losses.append(loss.detach().item())\n        progress_bar.set_postfix(**logs)\n        global_step += 1\n    progress_bar.close()\n\n    if epoch % config[\"save_images_step\"] == 0 or epoch == config[\"num_epochs\"] - 1:\n        # generate data with the model to later visualize the learning process\n        model.eval()\n        sample = torch.randn(config[\"eval_batch_size\"], 2)\n        timesteps = list(range(len(noise_scheduler)))[::-1]\n        for i, t in enumerate(tqdm(timesteps)):\n            t = torch.from_numpy(np.repeat(t, config[\"eval_batch_size\"])).long()\n            with torch.no_grad():\n                residual = model(sample, t)\n            sample = noise_scheduler.step(residual, t[0], sample)\n        frames.append(sample.numpy())\ndataset[6]\nlen(dataloader)\neval_batch_size = 10\nnum_timesteps = 50\nplot_step = 5\nnoise_scheduler = ddpm.NoiseScheduler(num_timesteps=num_timesteps)\nsample = torch.randn(eval_batch_size, 2)\ntimesteps = list(range(num_timesteps))[::-1]\nsamples = []\nsteps = []\n\nretrains = 10\ndiffusion_prob = 0.3\nsamples_per_retrain = 100\neval_batch_size = 10\nt = torch.from_numpy(np.repeat(timesteps[0], eval_batch_size)).long()\nwith torch.no_grad():\n    residual = model(sample, t)\nsample_ = noise_scheduler.step(residual, t[0], sample)\nsample.shape\nresidual.shape\nsample_.shape\ndiffusion_samples = []\nhmc_samples = []\nbeta = 1.\nfor retrain_iter in range(retrains):\n    console.print(f'retrain_iter: {retrain_iter}')\n    ndiff_acc = 0\n    ndiff_proposed = 0\n    for idx in range(samples_per_retrain):\n        console.print(f'sample idx: {idx}')\n        rand = np.random.uniform()\n        if rand &lt; diffusion_prob:\n            ndiff_proposed += 1\n            rand_pick = randrange(len(dataloader))\n            #theta_prime = dataset[rand_pick]\n            t = torch.from_numpy(np.repeat(t, eval_batch_size)).long()\n            with torch.no_grad():\n                residual = model(sample, t)\n            sample_ = noise_scheduler.step(residual, t[0], sample)\n            ratio = (\n                log_likelihood_2dU1(sample_, 2)\n                / log_likelihood_2dU1(sample, 2)\n            )\n            a = min(1, ratio)\n            u = np.random.uniform()\n            if u &lt; a:\n                ndiff_acc += 1\n                sample = sample_\n                diffusion_samples.append(sample)\n        else:\n            sample_, metrics = exp.trainer.hmc_step((sample_, beta))\n            hmc_samples.append(sample)\nfor i, t in enumerate(tqdm(timesteps)):\n    t = torch.from_numpy(np.repeat(t, eval_batch_size)).long()\n    with torch.no_grad():\n        residual = model(sample, t)\n    sample = noise_scheduler.step(residual, t[0], sample)\n    if (i + 1) % plot_step == 0:\n        samples.append(sample.numpy())\n        steps.append(i + 1)\nAlternate\ndiffusion_ = DiffusionAlt(img_size=64, device='cpu')\nimage = torch.rand(1, 2, 64, 64)\nt = diffusion_.sample_timesteps(image.shape[0]).to('cpu')\nunet(image, t)",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "⚛️ AI for Physics",
      "🎲 MCMC + Diffusion Sampling"
    ]
  },
  {
    "objectID": "posts/ai-for-physics/diffusion/index.html#denoising-diffusion-probabilistic-models",
    "href": "posts/ai-for-physics/diffusion/index.html#denoising-diffusion-probabilistic-models",
    "title": "🎲 MCMC + Diffusion Sampling",
    "section": "Denoising Diffusion Probabilistic Models",
    "text": "Denoising Diffusion Probabilistic Models",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "⚛️ AI for Physics",
      "🎲 MCMC + Diffusion Sampling"
    ]
  },
  {
    "objectID": "posts/ai-for-physics/diffusion/index.html#imports--setup",
    "href": "posts/ai-for-physics/diffusion/index.html#imports--setup",
    "title": "🎲 MCMC + Diffusion Sampling",
    "section": "Imports / Setup",
    "text": "Imports / Setup\n\nfrom __future__ import absolute_import, print_function, annotations, division\nfrom dataclasses import dataclass\n\nimport sys\nimport os\nimport math\nimport numpy as np\nimport scipy\nimport time\nfrom random import randrange\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n\nfrom ezpz.dist import setup_torch\n\nport = np.random.randint(5000, 6000)\nprint(f\"Using port: {port}\")\n\nRANK = setup_torch(\n    backend=\"DDP\",\n    port=f\"{port}\"\n)\n\n\nUsing port: 5561\n\nUsing DDP for distributed training\n\nGlobal Rank: 0 / 0\n\n\n\n%matplotlib inline\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n\nfrom l2hmc.main import build_experiment\nfrom l2hmc.utils.rich import get_console\nfrom l2hmc.utils.plot_helpers import set_plot_style\n\nimport opinionated\nfrom l2hmc.diffusion.diffusion import PureMH, MH_Diffusion\nfrom l2hmc.utils.plot_helpers import set_plot_style\n\nfrom pandas.io.formats import style\nimport scipy\nimport time\nfrom random import randrange\nfrom l2hmc.diffusion.diffusion import PureMH, MH_Diffusion\n\nset_plot_style()\nconsole = get_console()\nprint(console.is_jupyter)\nif console.is_jupyter:\n    console.is_jupyter = False\nprint(console.is_jupyter)\n\n\nUsing device: cpu\nFailed to download font: Source Sans Pro, skipping!\nFailed to download font: Titillium WebRoboto Condensed, skipping!\nTrue\nFalse\n\n\n\nplt.style.use(opinionated.STYLES['opinionated_min'])\nsns.set_context('notebook')",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "⚛️ AI for Physics",
      "🎲 MCMC + Diffusion Sampling"
    ]
  },
  {
    "objectID": "posts/ai-for-physics/diffusion/index.html#2d-u1",
    "href": "posts/ai-for-physics/diffusion/index.html#2d-u1",
    "title": "🎲 MCMC + Diffusion Sampling",
    "section": "2D U(1)",
    "text": "2D U(1)",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "⚛️ AI for Physics",
      "🎲 MCMC + Diffusion Sampling"
    ]
  },
  {
    "objectID": "posts/ai-for-physics/diffusion/index.html#train-diffusion-model",
    "href": "posts/ai-for-physics/diffusion/index.html#train-diffusion-model",
    "title": "🎲 MCMC + Diffusion Sampling",
    "section": "Train Diffusion Model",
    "text": "Train Diffusion Model",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "⚛️ AI for Physics",
      "🎲 MCMC + Diffusion Sampling"
    ]
  },
  {
    "objectID": "posts/ai-for-physics/diffusion/index.html#build-diffusion-model-with-unet-architecure",
    "href": "posts/ai-for-physics/diffusion/index.html#build-diffusion-model-with-unet-architecure",
    "title": "🎲 MCMC + Diffusion Sampling",
    "section": "Build Diffusion Model with UNet Architecure",
    "text": "Build Diffusion Model with UNet Architecure",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "⚛️ AI for Physics",
      "🎲 MCMC + Diffusion Sampling"
    ]
  },
  {
    "objectID": "posts/ai-for-physics/diffusion/index.html#hmc-sampling-with-diffusion",
    "href": "posts/ai-for-physics/diffusion/index.html#hmc-sampling-with-diffusion",
    "title": "🎲 MCMC + Diffusion Sampling",
    "section": "HMC Sampling with Diffusion",
    "text": "HMC Sampling with Diffusion",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "⚛️ AI for Physics",
      "🎲 MCMC + Diffusion Sampling"
    ]
  },
  {
    "objectID": "posts/ai-for-physics/diffusion/index.html#alternate",
    "href": "posts/ai-for-physics/diffusion/index.html#alternate",
    "title": "🎲 MCMC + Diffusion Sampling",
    "section": "Alternate",
    "text": "Alternate",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "⚛️ AI for Physics",
      "🎲 MCMC + Diffusion Sampling"
    ]
  },
  {
    "objectID": "posts/dope-slides/index.html#quarto-reveal.js",
    "href": "posts/dope-slides/index.html#quarto-reveal.js",
    "title": "💅 How to Make Dope Slides",
    "section": "Quarto 🤝 Reveal.js",
    "text": "Quarto 🤝 Reveal.js\nSo, after making a promise some time ago on twitter 1, and having many questions following my talk on Parallel Training Techniques last week, I’m finally getting around to writing this up.\nThe slides are written using Quarto, a flavor of Markdown, and uses the built-in Quarto + Reveal.js functionality.\nFor this post, I’ll focus on the slides I presented at last years Lattice 2023, shown below:\n\n\n\n\n\n\n\n🪧 MLMC: Machine Learning Monte Carlo @ Lattice 2023 [07/2023]\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\n🏃‍♂️ Follow Along…\n\n\n\n\n\nOnce you’ve Installed Quarto, you can build these slides yourself by:\n\ngit clone  saforem2/lattice23\ncd lattice23 && quarto preview\n\nThis will create a docs/ directory with the following structure:\n📂 docs/\n├── 📂 assets/\n├── 📂 css/\n├── 📄 index.html\n├── 📄 lattice23.md\n├── 📄 search.json\n└── 📂 site_libs/\nOnce you’ve created this, and the docs/index.html file looks how you want, you can add the docs/ directory to your GitHub repo:\n$ git add docs\n$ git commit -m 'Create site'\n$ git push\nOnce you’ve enabled the GitHub page, the site will be automatically built and updated alongside the repo.",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "💅 How to Make Dope Slides"
    ]
  },
  {
    "objectID": "posts/dope-slides/index.html#getting-started",
    "href": "posts/dope-slides/index.html#getting-started",
    "title": "💅 How to Make Dope Slides",
    "section": "Getting Started",
    "text": "Getting Started\nWhenever I give a talk, my workflow is typically:\n\nCreate new GitHub repo for it\nHunt down the GitHub repo from my last talk and2:\n$ cp -r old_talk/{_quarto.yml,index.qmd,references.bib,css/*} new_talk/\n\nHonestly, other than that, 90% of the work is done automatically by Quarto. The remaining 10% consists of figuring out why my css is broken (see CSS).\nThe best place to start for learning to make slides with Quarto and Reveal.js is the official documentation:\n\nQuarto / Presentations / Revealjs:\n\nReveal Basics\nPresenting Slides\nAdvanced Reveal\nReveal Themes\n\n\n\nThe slides are written in markdown Quarto (.qmd)3, a pandoc-compliant based markup language.\nFor a single slide deck, the content will be placed in index.qmd and our directory structure will look something like:\n\n📂 lattice23/\n├── 📂 assets/            # for images, etc.\n│   └── 🖼️ thumbnail.png  # can be used as social preview image\n├── 📂 css/\n│   ├── 📄 callouts.css\n│   ├── 📄 dark.scss\n│   └── 📄 default.css\n├── 🛠️ _quarto.yml        # Configuration goes here\n├── 📄 index.qmd          # Quarto document containing slides content\n└── 📜 references.bib     # BibTex references\n\nEquations are rendered using $ delimiters for inline math and $$ for display math4.\nWe can use Divs and Spans from Pandoc.\n\n&lt;span&gt;’s: are created by wrapping text in square brackets, and will be treated as a &lt;span&gt; with attributes if it is followed immediately by attributes, e.g.:\n\nExample: [This is *some text*]{.class key=\"val\"}\nidk what I’m doing really, so I mostly find myself doing things like [blue text]{style=\"color:#1E88E5;\"} which produces blue text.\n\n&lt;div&gt;’s: are created by wrapping text with a line consisting of at least three colons :::.\n\nExample:\n::: {#special .sidebar}\nHere is a paragraph.\n\nAnd another.\n:::\nWe can use either attributes in curly braces or a single unbraced word, which will be treated as a class name.\n\n\n\n\n🎁 Install Extensions\nFind the full list of available extensions at Quarto Extensions\nTo install various icon sets used in the example slides, we can install the following extensions:\n$ quarto install extension mcanouil/quarto-iconify      # https://icones.js.org/ [&lt;-- Contains rest of icon sets ??]\n$ quarto install extension shafayetShafee/bsicons       # bootstrap icons\n$ quarto install extension schochastics/academicicons   # OrcID, Google Scholar, ...\n$ quarto install extension quarto-ext/fontawesome       # Font Awesome icons\nnote that these aren’t necessary for functionality, but provide additional icons that I like to use 🤷🏻‍♂️",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "💅 How to Make Dope Slides"
    ]
  },
  {
    "objectID": "posts/dope-slides/index.html#metadata",
    "href": "posts/dope-slides/index.html#metadata",
    "title": "💅 How to Make Dope Slides",
    "section": "Metadata",
    "text": "Metadata\nThe first section of our index.qmd contains the YAML metadata for the Quarto document.\nExplicitly, we see this consists of:\n\n\nExpand for yaml\n\n---\nformat:\n  revealjs:\n    title-block-style: none\n    slide-number: c\n    title-slide-style: default\n    chalkboard:\n      buttons: false\n    auto-animate: true\n    reference-location: section\n    touch: true\n    pause: false\n    footnotes-hover: true\n    citations-hover: true\n    preview-links: true\n    controls-tutorial: true\n    controls: false\n    logo: \"https://raw.githubusercontent.com/saforem2/anl-job-talk/main/docs/assets/anl.svg\"\n    history: false\n    theme: [dark, css/dark.scss]\n    css: [css/default.css, css/callouts.css]\n    self-contained: false\n    embed-resources: false\n    self-contained-math: false\n    center: true\n    highlight-style: \"atom-one\"\n    default-image-extension: svg\n    code-line-numbers: true\n    code-overflow: scroll\n    html-math-method: katex\n    fig-align: center\n    mermaid:\n      theme: dark\n  gfm:\n    output-file: \"lattice23.md\"\n---\n\nThe complete list of Reveal.js options are listed, with descriptions at: Quarto – Revealjs Options",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "💅 How to Make Dope Slides"
    ]
  },
  {
    "objectID": "posts/dope-slides/index.html#title-slide",
    "href": "posts/dope-slides/index.html#title-slide",
    "title": "💅 How to Make Dope Slides",
    "section": "Title Slide",
    "text": "Title Slide\n\nStarting with the title slide5:\n\n\n\n\n\n\nFigure 1: Title Slide\n\n\n\n\nThe full slide contents are included below:\n\n\nExpand for quarto\n\n# {.title-slide .centeredslide background-iframe=\"https://saforem2.github.io/grid-worms-animation/\" loading=\"lazy\"}\n\n::: {style=\"background-color: rgba(22,22,22,0.75); border-radius: 10px; text-align:center; padding: 0px; padding-left: 1.5em; padding-right: 1.5em; max-width: min-content; min-width: max-content; margin-left: auto; margin-right: auto; padding-top: 0.2em; padding-bottom: 0.2em; line-height: 1.5em!important;\"}\n\n[MLMC: Machine Learning Monte Carlo]{.style=\"color:#939393; font-size:1.5em; font-weight:bold;}  \n[for Lattice Gauge Theory]{style=\"color:#777777; font-size:1.2em; font-weight: bold;\"}\n[&lt;br&gt;&nbsp;]{style=\"padding-bottom: 0.5rem;\"}  \n[](https://samforeman.me) Sam Foreman  \n[Xiao-Yong Jin, James C. Osborn]{.dim-text style=\"font-size:0.8em;\"}  \n[[[ `saforem2/`](https://github.com/saforem2/)]{style=\"border-bottom: 0.5px solid #00ccff;\"}`{`[[`lattice23`](https://github.com/saforem2/lattice23)]{style=\"border-bottom: 0.5px solid #00ccff;\"}, [[`l2hmc-qcd`](https://github.com/saforem2/l2hmc-qcd)]{style=\"border-bottom: 0.5px solid #00ccff;\"}`}`]{style=\"font-size:0.8em;\"}\n\n:::\n\n::: footer\n[2023-07-31 @ [Lattice 2023](https://indico.fnal.gov/event/57249/contributions/271305/)]{.dim-text style=\"text-align:left;'}\n:::\n\nFor the background, I made a simple animation  saforem2/grid-worms-animation that is hosted on GitHub pages as a simple html website\nThis static GitHub page is then used as an IFrame Background natively in Quarto with Reveal.js\nThis is as simple as:\n# {.title-slide .centeredslide background-iframe=\"https://saforem2.github.io/grid-worms-animation/\" loading=\"lazy\"}",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "💅 How to Make Dope Slides"
    ]
  },
  {
    "objectID": "posts/dope-slides/index.html#single-column-slides",
    "href": "posts/dope-slides/index.html#single-column-slides",
    "title": "💅 How to Make Dope Slides",
    "section": "Single-Column Slides",
    "text": "Single-Column Slides\nOther than the title slide, the remainder of the slides are all relatively straightforward to construct.\nFor single-column slides, constructing the content is as simple as writing it in Markdown:\n\nCodeSlide\n\n\n# Overview\n\n1. [Background: `{MCMC,HMC}`](#markov-chain-monte-carlo-mcmc)\n    - [Leapfrog Integrator](#leapfrog-integrator-hmc)\n    - [Issues with HMC](#sec-issues-with-hmc)\n    - [Can we do better?](#sec-can-we-do-better)\n\n2. [L2HMC: Generalizing MD](#sec-l2hmc)\n    - [4D $SU(3)$ Model](#sec-su3)\n    - [Results](#sec-results)\n3. [References](#sec-references)\n4. [Extras](#sec-extras)\n\n\n\n\n\n\n\n\n\nFigure 2: Overview Slide",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "💅 How to Make Dope Slides"
    ]
  },
  {
    "objectID": "posts/dope-slides/index.html#centered-slides",
    "href": "posts/dope-slides/index.html#centered-slides",
    "title": "💅 How to Make Dope Slides",
    "section": "Centered Slides",
    "text": "Centered Slides\nWe can center all the text on a slide by adding the {.centeredslide} class to the slide header, e.g.\n\nindex.qmdstyle.scss\n\n\n---\nformat:\n  revealjs:\n    theme: [style.scss]\n---\n\n# Title {.centeredslide}\n\n\n.centeredslide {\n  text-align: center;\n}",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "💅 How to Make Dope Slides"
    ]
  },
  {
    "objectID": "posts/dope-slides/index.html#multi-column-slides",
    "href": "posts/dope-slides/index.html#multi-column-slides",
    "title": "💅 How to Make Dope Slides",
    "section": "Multi-Column Slides",
    "text": "Multi-Column Slides\nSide-by-side content (either text or images)\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Markov Chain Monte Carlo\n\n\n\n\n\nThis slide is horizontally centered6 and the content consists of two rows split as follows:\n\n\n\n\n\n\nA\nB\n\n\nC\nC\n\n\n\n\n\nTable 1: Slide layout. First row split into two columns, second row spans full width.\n\n\n\n\nIn panel A, we have a ::: {.callout-note} block followed by a single list element containing a LaTeX equation.\nIn panel B we have a standard markdown image\n![](./asets/mcmc.png)\nIn panel C we have normal text + math with LaTeX7 syntax.\n\n\n\n\n\nNote that we additionally have a ::: footer element included at the bottom of the slide.\nThe code used to generate the slide above is included below:\n\n\nExpand forquarto\n\n# Markov Chain Monte Carlo (MCMC) {.centeredslide}\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n::: {.callout-note title=\"Goal\" style=\"text-align:left;!important\"}\nGenerate **independent** samples $\\{x_{i}\\}$, such that[^notation]\n$$\\{x_{i}\\} \\sim p(x) \\propto e^{-S(x)}$$\nwhere $S(x)$ is the _action_ (or potential energy)\n:::\n\n- Want to calculate observables $\\mathcal{O}$:  \n  $\\left\\langle \\mathcal{O}\\right\\rangle \\propto \\int \\left[\\mathcal{D}x\\right]\\hspace{4pt} {\\mathcal{O}(x)\\, p(x)}$\n\n:::\n\n::: {.column width=\"49%\"}\n![](https://raw.githubusercontent.com/saforem2/deep-fridays/main/assets/normal_distribution.dark.svg)\n:::\n\n::::\n\nIf these were [independent]{.style=\"color:#00CCFF;\"}, we could approximate:\n$\\left\\langle\\mathcal{O}\\right\\rangle \\simeq \\frac{1}{N}\\sum^{N}_{n=1}\\mathcal{O}(x_{n})$\n$$\\sigma_{\\mathcal{O}}^{2} = \\frac{1}{N}\\mathrm{Var}{\\left[\\mathcal{O} (x) \\right]}\\Longrightarrow\n\\sigma_{\\mathcal{O}} \\propto \\frac{1}{\\sqrt{N}}$$\n\n[^notation]: Here, $\\sim$ means \"is distributed according to\"\n\n::: footer\n[ `saforem2/lattice23`](https://saforem2.github.io/lattice23)\n:::",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "💅 How to Make Dope Slides"
    ]
  },
  {
    "objectID": "posts/dope-slides/index.html#css",
    "href": "posts/dope-slides/index.html#css",
    "title": "💅 How to Make Dope Slides",
    "section": "💅 CSS",
    "text": "💅 CSS\nMy web developer friend laughs at me, but when something is broken / doesn’t look right / I want it to look different, I:\n\nPull up Chrome Tools ( ⌘ + ⌥ + I )\nInspect element of interest ( ⌘ + ⇧ + C )\nMake changes to the CSS\nSave the new rule to my .scss file 🤷🏻‍♂️\n\nI’m guessing this might be obvious to some people, but it took me a while to figure out how things worked so maybe its helpful for others.\n\n\nExpand for css\n\n\n\n\n\n\n\nFigure 4: Example of selecting an element and making a change to the CSS.",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "💅 How to Make Dope Slides"
    ]
  },
  {
    "objectID": "posts/dope-slides/index.html#github-page",
    "href": "posts/dope-slides/index.html#github-page",
    "title": "💅 How to Make Dope Slides",
    "section": "📃 GitHub Page",
    "text": "📃 GitHub Page\nTo enable your GitHub page, you can do the following:\n\n\n\n\n\n\nFigure 5: Instructions for building a GitHub page using the docs/ directory off the main branch.\n\n\n\nIn this case, the repo is:\n saforem2/lattice23\nand the site is published at\nhttps://saforem2.github.io/lattice23",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "💅 How to Make Dope Slides"
    ]
  },
  {
    "objectID": "posts/dope-slides/index.html#references",
    "href": "posts/dope-slides/index.html#references",
    "title": "💅 How to Make Dope Slides",
    "section": "📓 References",
    "text": "📓 References\n\nReveal Themes\nUsing Pandoc fenced divs\nSlidecraft 101: Colors and Fonts\nBeautiful Reports and Presentations with Quarto\n Quarto Clean Theme\n\n\n\n\n\n\n\n❤️‍🩹 Status\n\n\n\n\n\n\n\nLast Updated: 08/13/2024 @ 11:48:37",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "💅 How to Make Dope Slides"
    ]
  },
  {
    "objectID": "posts/dope-slides/index.html#footnotes",
    "href": "posts/dope-slides/index.html#footnotes",
    "title": "💅 How to Make Dope Slides",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAnd countless other people IRL↩︎\nOne thing I’ve been meaning to do, is clean up all my css/* files and move them all to a single repository, but I’ll save that for another day.↩︎\nAn open-source scientific and technical publishing system↩︎\nEquations↩︎\nQuarto comes with lightbox support, so you can click on images to display them full screen.↩︎\nBy adding the {.centeredslide} class to the slide header↩︎\nText surrounded by $ will be rendered with LaTeX↩︎",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "💅 How to Make Dope Slides"
    ]
  },
  {
    "objectID": "posts/jupyter/index.html",
    "href": "posts/jupyter/index.html",
    "title": "📗 Jupyter",
    "section": "",
    "text": "CitationBibTeX citation:@online{foreman,\n  author = {Foreman, Sam},\n  title = {📗 {Jupyter}},\n  url = {https://samforeman.me},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nForeman, Sam. n.d. “📗 Jupyter.” https://samforeman.me."
  },
  {
    "objectID": "posts/ezpz-at-alcf/index.html#complete-example",
    "href": "posts/ezpz-at-alcf/index.html#complete-example",
    "title": "🍋 ezpz @ ALCF",
    "section": "Complete Example",
    "text": "Complete Example\n\nGist\n\n\n\n📦 Clone Repo(s)\n\nargonne-lcf/Megatron-DeepSpeed:\n$ git clone https://github.com/argonne-lcf/Megatron-DeepSpeed\nCloning into 'Megatron-DeepSpeed'...\nremote: Enumerating objects: 15538, done.\nremote: Counting objects: 100% (21/21), done.\nremote: Compressing objects: 100% (11/11), done.\nremote: Total 15538 (delta 10), reused 18 (delta 10), pack-reused 15517\nReceiving objects: 100% (15538/15538), 6.25 MiB | 32.32 MiB/s, done.\nResolving deltas: 100% (11482/11482), done.\nUpdating files: 100% (596/596), done.\nsaforem2/ezpz:\n$ cd Megatron-DeepSpeed\n$ git clone https://github.com/saforem2/ezpz deps/ezpz\nCloning into 'deps/ezpz'...\nremote: Enumerating objects: 2161, done.\nremote: Counting objects: 100% (390/390), done.\nremote: Compressing objects: 100% (181/181), done.\nremote: Total 2161 (delta 214), reused 285 (delta 151), pack-reused 1771\nReceiving objects: 100% (2161/2161), 4.28 MiB | 25.35 MiB/s, done.\nResolving deltas: 100% (1134/1134), done.\n\n\n\n🛜 Setup Job\n\nSource ezpz/bin/utils.sh:\n$ PBS_O_WORKDIR=$(pwd) source deps/ezpz/src/ezpz/bin/utils.sh\nUsing WORKING_DIR: /eagle/argonne_tpc/foremans/projects/argonne-lcf/tmp/2024-07-23-221253/Megatron-DeepSpeed\nezpz_setup_alcf:\n$ ezpz_setup_alcf\n[ezpz/bin/utils.sh]\n\n[2024-07-23-221417]\n    • USER=foremans\n    • MACHINE=polaris\n    • HOST=x3006c0s25b1n0\n\n[ezpz_get_pbs_env]: Caught 0 arguments\n    • hostfile: /var/spool/pbs/aux/2036165.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov\n    • jobenv_file: /home/foremans/.pbsenv\n\n[ezpz_setup_host]\n    • Using hostfile: /var/spool/pbs/aux/2036165.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov\n    • Found in environment:\n        • HOSTFILE: /var/spool/pbs/aux/2036165.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov\n        • Writing PBS vars to: /home/foremans/.pbsenv\n\n[ezpz_save_pbs_env]\n    • Setting:\n        • HOSTFILE: /var/spool/pbs/aux/2036165.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov\n        • JOBENV_FILE: /home/foremans/.pbsenv\n\n[HOSTS]\n    • [host:0] - x3006c0s25b1n0.hsn.cm.polaris.alcf.anl.gov\n\n[DIST INFO]\n    • HOSTFILE=/var/spool/pbs/aux/2036165.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov\n    • NHOSTS=1\n    • NGPU_PER_HOST=4\n    • NGPUS=4\n    • DIST_LAUNCH=mpiexec --verbose --envall -n 4 -ppn 4 --hostfile /var/spool/pbs/aux/2036165.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov --cpu-bind depth -d 16\n\n[LAUNCH]:\n    • To launch across all available GPUs, use: launch\n      launch = mpiexec --verbose --envall -n 4 -ppn 4 --hostfile /var/spool/pbs/aux/2036165.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov --cpu-bind depth -d 16\n\n\n\n🐍 Setup Python\n\nezpz_setup_python:\n$ ezpz_setup_python\nNo conda_prefix OR virtual_env found in environment...\nSetting up conda...\nLmod is automatically replacing \"nvhpc/23.9\" with \"gcc-native/12.3\".\nLmod is automatically replacing \"PrgEnv-nvhpc/8.5.0\" with \"PrgEnv-gnu/8.5.0\".\nDue to MODULEPATH changes, the following have been reloaded:\n  1) cray-mpich/8.1.28\nFound conda at: /soft/applications/conda/2024-04-29/mconda3\nNo VIRTUAL_ENV found in environment!\n    - Trying to setup from /soft/applications/conda/2024-04-29/mconda3\n    - Using VENV_DIR=/eagle/argonne_tpc/foremans/projects/argonne-lcf/tmp/2024-07-23-221253/Megatron-DeepSpeed/venvs/2024-04-29\n    - Creating a new virtual env on top of 2024-04-29 in /eagle/argonne_tpc/foremans/projects/argonne-lcf/tmp/2024-07-23-221253/Megatron-DeepSpeed/venvs/2024-04-29\n[python] Using /eagle/argonne_tpc/foremans/projects/argonne-lcf/tmp/2024-07-23-221253/Megatron-DeepSpeed/venvs/2024-04-29/bin/python3\n\n$ which python3\n/eagle/argonne_tpc/foremans/projects/argonne-lcf/tmp/2024-07-23-221253/Megatron-DeepSpeed/venvs/2024-04-29/bin/python3\n\n$ python3 -m pip install -e deps/ezpz --require-virtualenv\nLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\nObtaining file:///lus/eagle/projects/argonne_tpc/foremans/projects/argonne-lcf/tmp/2024-07-23-221253/Megatron-DeepSpeed/deps/ezpz\n  Installing build dependencies ... done\n  Checking if build backend supports build_editable ... done\n  Getting requirements to build editable ... done\n  Installing backend dependencies ... done\n  Preparing editable metadata (pyproject.toml) ... done\n\n\n\n📝 Test Setup\n\nezpz/test_dist.py\n$ launch python3 -m ezpz.test_dist\n\n\nOutput\n\n[2024-07-23 22:21:37.972869][INFO][__init__:156] - Setting logging level to 'INFO' on 'RANK == 0'\n[2024-07-23 22:21:37.975224][INFO][__init__:157] - Setting logging level to 'CRITICAL' on all others 'RANK != 0'\n[2024-07-23 22:21:37.975718][INFO][__init__:160] - To disable this behavior, and log from ALL ranks (not recommended), set: 'export LOG_FROM_ALL_RANKS=1'  in your environment, and re-run.\n[2024-07-23 22:21:39.790899][INFO][dist:358] - [device='cuda'][rank=1/3][local_rank=1/3][node=0/0]\n[2024-07-23 22:21:39.790850][INFO][dist:358] - [device='cuda'][rank=2/3][local_rank=2/3][node=0/0]\n[2024-07-23 22:21:39.791749][INFO][dist:358] - [device='cuda'][rank=3/3][local_rank=3/3][node=0/0]\n[2024-07-23 22:21:39.797666][INFO][dist:95] -\n\n[dist_info]:\n  • DEVICE=cuda\n  • DEVICE_ID=cuda:0\n  • DISTRIBUTED_BACKEND=nccl\n  • GPUS_PER_NODE=4\n  • HOSTS=['x3006c0s25b1n0.hsn.cm.polaris.alcf.anl.gov']\n  • HOSTFILE=/var/spool/pbs/aux/2036165.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov\n  • HOSTNAME=x3006c0s25b1n0.hsn.cm.polaris.alcf.anl.gov\n  • LOCAL_RANK=0\n  • MACHINE=Polaris\n  • NUM_NODES=1\n  • NGPUS=4\n  • NGPUS_AVAILABLE=4\n  • NODE_ID=0\n  • RANK=0\n  • SCHEDULER=PBS\n  • WORLD_SIZE_TOTAL=4\n  • WORLD_SIZE_IN_USE=4\n  • LAUNCH_CMD=mpiexec --verbose --envall -n 4 -ppn 4 --hostfile /var/spool/pbs/aux/2036165.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov --cpu-bind depth -d 16\n\n\n[2024-07-23 22:21:39.800519][INFO][dist:725] - [0/4] Using device='cuda' with backend='DDP' + 'nccl' for distributed training.\n[2024-07-23 22:21:39.805001][INFO][dist:358] - [device='cuda'][rank=0/3][local_rank=0/3][node=0/0]\n[2024-07-23 22:21:39.805513][WARNING][dist:364] - Using [4 / 4] available \"cuda\" devices !!\n[2024-07-23 22:21:39.806121][INFO][dist:95] -\n\n[timers_import]:\n  • os=1.062639057636261e-06\n  • logging=4.0046870708465576e-07\n  • typing=2.7157366275787354e-06\n  • pathlib=1.2516975402832031e-06\n  • ezpz=6.30505383014679e-07\n  • torch=2.555549144744873e-06\n  • torch_ddp=2.4745240807533264e-06\n  • wandb=6.44102692604065e-05\n  • total=7.550138980150223e-05\n\n\n[2024-07-23 22:21:39.807221][INFO][dist:95] -\n\n[CONFIG]:\n  • warmup=0\n  • log_freq=1\n  • batch_size=64\n  • input_size=128\n  • output_size=128\n  • dtype=torch.float32\n  • device=cuda\n  • world_size=4\n  • train_iters=100\n\n[2024-07-23 22:21:41.373173][INFO][test_dist:183] - model=Network(\n  (layers): Sequential(\n    (0): Linear(in_features=128, out_features=1024, bias=True)\n    (1): Linear(in_features=1024, out_features=512, bias=True)\n    (2): Linear(in_features=512, out_features=256, bias=True)\n    (3): Linear(in_features=256, out_features=128, bias=True)\n    (4): Linear(in_features=128, out_features=128, bias=True)\n  )\n)\n[2024-07-23 22:21:43.625040][INFO][test_dist:274] - iter=1, loss=2039.91, sps=2.252e+04, dt=0.00284196, dtf=0.0009801, dtb=0.001862\n[2024-07-23 22:21:43.628643][INFO][test_dist:274] - iter=2, loss=1424.54, sps=3.272e+04, dt=0.00195628, dtf=0.0005183, dtb=0.001438\n[2024-07-23 22:21:43.631833][INFO][test_dist:274] - iter=3, loss=1159.38, sps=3.331e+04, dt=0.00192147, dtf=0.0006139, dtb=0.001308\n[2024-07-23 22:21:43.634991][INFO][test_dist:274] - iter=4, loss=935.58, sps=3.343e+04, dt=0.00191451, dtf=0.0006113, dtb=0.001303\n[2024-07-23 22:21:43.638092][INFO][test_dist:274] - iter=5, loss=851.468, sps=3.483e+04, dt=0.00183759, dtf=0.0005938, dtb=0.001244\n[2024-07-23 22:21:43.641232][INFO][test_dist:274] - iter=6, loss=785.109, sps=3.409e+04, dt=0.00187757, dtf=0.0005972, dtb=0.00128\n[2024-07-23 22:21:43.644367][INFO][test_dist:274] - iter=7, loss=772.966, sps=3.417e+04, dt=0.00187292, dtf=0.0005868, dtb=0.001286\n[2024-07-23 22:21:43.647507][INFO][test_dist:274] - iter=8, loss=727.854, sps=3.411e+04, dt=0.00187638, dtf=0.0005814, dtb=0.001295\n[2024-07-23 22:21:43.650561][INFO][test_dist:274] - iter=9, loss=725.773, sps=3.546e+04, dt=0.00180485, dtf=0.0005975, dtb=0.001207\n[2024-07-23 22:21:43.653520][INFO][test_dist:274] - iter=10, loss=720.374, sps=3.785e+04, dt=0.00169078, dtf=0.0006108, dtb=0.00108\n[2024-07-23 22:21:43.656564][INFO][test_dist:274] - iter=11, loss=717.926, sps=3.602e+04, dt=0.00177678, dtf=0.0005694, dtb=0.001207\n[2024-07-23 22:21:43.659555][INFO][test_dist:274] - iter=12, loss=692.535, sps=3.682e+04, dt=0.00173814, dtf=0.0005557, dtb=0.001182\n[2024-07-23 22:21:43.662596][INFO][test_dist:274] - iter=13, loss=679.509, sps=3.67e+04, dt=0.00174377, dtf=0.0005531, dtb=0.001191\n[2024-07-23 22:21:43.665598][INFO][test_dist:274] - iter=14, loss=674.778, sps=3.727e+04, dt=0.00171741, dtf=0.0005496, dtb=0.001168\n[2024-07-23 22:21:43.668593][INFO][test_dist:274] - iter=15, loss=673.873, sps=3.666e+04, dt=0.00174556, dtf=0.0005708, dtb=0.001175\n[2024-07-23 22:21:43.671589][INFO][test_dist:274] - iter=16, loss=667.283, sps=3.694e+04, dt=0.00173238, dtf=0.0005453, dtb=0.001187\n[2024-07-23 22:21:43.674599][INFO][test_dist:274] - iter=17, loss=660.292, sps=3.646e+04, dt=0.00175558, dtf=0.0005538, dtb=0.001202\n[2024-07-23 22:21:43.677592][INFO][test_dist:274] - iter=18, loss=660.664, sps=3.696e+04, dt=0.00173169, dtf=0.0005441, dtb=0.001188\n[2024-07-23 22:21:43.680559][INFO][test_dist:274] - iter=19, loss=676.161, sps=3.709e+04, dt=0.00172556, dtf=0.0005668, dtb=0.001159\n[2024-07-23 22:21:43.683539][INFO][test_dist:274] - iter=20, loss=665.099, sps=3.702e+04, dt=0.0017287, dtf=0.0005281, dtb=0.001201\n[2024-07-23 22:21:43.686527][INFO][test_dist:274] - iter=21, loss=626.671, sps=3.7e+04, dt=0.00172989, dtf=0.0005279, dtb=0.001202\n[2024-07-23 22:21:43.689518][INFO][test_dist:274] - iter=22, loss=632.127, sps=3.702e+04, dt=0.00172883, dtf=0.0005085, dtb=0.00122\n[2024-07-23 22:21:43.692469][INFO][test_dist:274] - iter=23, loss=657.324, sps=3.755e+04, dt=0.00170436, dtf=0.0005164, dtb=0.001188\n[2024-07-23 22:21:43.695563][INFO][test_dist:274] - iter=24, loss=617.646, sps=3.558e+04, dt=0.00179856, dtf=0.0005767, dtb=0.001222\n[2024-07-23 22:21:43.698537][INFO][test_dist:274] - iter=25, loss=618.284, sps=3.705e+04, dt=0.00172744, dtf=0.0005522, dtb=0.001175\n[2024-07-23 22:21:43.701410][INFO][test_dist:274] - iter=26, loss=615.418, sps=3.961e+04, dt=0.00161577, dtf=0.0005298, dtb=0.001086\n[2024-07-23 22:21:43.704427][INFO][test_dist:274] - iter=27, loss=599.058, sps=3.648e+04, dt=0.00175461, dtf=0.0005156, dtb=0.001239\n[2024-07-23 22:21:43.707374][INFO][test_dist:274] - iter=28, loss=621.717, sps=3.778e+04, dt=0.00169387, dtf=0.0004899, dtb=0.001204\n[2024-07-23 22:21:43.710390][INFO][test_dist:274] - iter=29, loss=597.588, sps=3.623e+04, dt=0.00176654, dtf=0.0005663, dtb=0.0012\n[2024-07-23 22:21:43.713386][INFO][test_dist:274] - iter=30, loss=598.102, sps=3.71e+04, dt=0.00172484, dtf=0.0005497, dtb=0.001175\n[2024-07-23 22:21:43.716530][INFO][test_dist:274] - iter=31, loss=586.188, sps=3.357e+04, dt=0.00190664, dtf=0.0005618, dtb=0.001345\n[2024-07-23 22:21:43.719525][INFO][test_dist:274] - iter=32, loss=591.646, sps=3.672e+04, dt=0.00174293, dtf=0.000561, dtb=0.001182\n[2024-07-23 22:21:43.722513][INFO][test_dist:274] - iter=33, loss=574.161, sps=3.668e+04, dt=0.00174487, dtf=0.0005502, dtb=0.001195\n[2024-07-23 22:21:43.725524][INFO][test_dist:274] - iter=34, loss=586.41, sps=3.707e+04, dt=0.00172628, dtf=0.0005552, dtb=0.001171\n[2024-07-23 22:21:43.728594][INFO][test_dist:274] - iter=35, loss=574.43, sps=3.605e+04, dt=0.00177526, dtf=0.000576, dtb=0.001199\n[2024-07-23 22:21:43.731615][INFO][test_dist:274] - iter=36, loss=552.77, sps=3.642e+04, dt=0.00175741, dtf=0.0005588, dtb=0.001199\n[2024-07-23 22:21:43.734574][INFO][test_dist:274] - iter=37, loss=567.612, sps=3.748e+04, dt=0.00170768, dtf=0.0005318, dtb=0.001176\n[2024-07-23 22:21:43.737564][INFO][test_dist:274] - iter=38, loss=561.004, sps=3.706e+04, dt=0.00172686, dtf=0.0005489, dtb=0.001178\n[2024-07-23 22:21:43.740578][INFO][test_dist:274] - iter=39, loss=555.718, sps=3.645e+04, dt=0.00175567, dtf=0.0005662, dtb=0.001189\n[2024-07-23 22:21:43.743565][INFO][test_dist:274] - iter=40, loss=543.661, sps=3.708e+04, dt=0.00172613, dtf=0.0005363, dtb=0.00119\n[2024-07-23 22:21:43.746561][INFO][test_dist:274] - iter=41, loss=537.186, sps=3.691e+04, dt=0.00173373, dtf=0.0005346, dtb=0.001199\n[2024-07-23 22:21:43.749446][INFO][test_dist:274] - iter=42, loss=545.877, sps=3.998e+04, dt=0.00160083, dtf=0.000533, dtb=0.001068\n[2024-07-23 22:21:43.752446][INFO][test_dist:274] - iter=43, loss=546.533, sps=3.681e+04, dt=0.00173875, dtf=0.0005124, dtb=0.001226\n[2024-07-23 22:21:43.755384][INFO][test_dist:274] - iter=44, loss=545.989, sps=3.796e+04, dt=0.001686, dtf=0.0005054, dtb=0.001181\n[2024-07-23 22:21:43.758389][INFO][test_dist:274] - iter=45, loss=531.344, sps=3.667e+04, dt=0.00174516, dtf=0.0005569, dtb=0.001188\n[2024-07-23 22:21:43.761470][INFO][test_dist:274] - iter=46, loss=515.415, sps=3.69e+04, dt=0.00173432, dtf=0.000551, dtb=0.001183\n[2024-07-23 22:21:43.764494][INFO][test_dist:274] - iter=47, loss=523.498, sps=3.634e+04, dt=0.00176121, dtf=0.0005524, dtb=0.001209\n[2024-07-23 22:21:43.767522][INFO][test_dist:274] - iter=48, loss=515.942, sps=3.625e+04, dt=0.00176562, dtf=0.0005655, dtb=0.0012\n[2024-07-23 22:21:43.770555][INFO][test_dist:274] - iter=49, loss=527.433, sps=3.62e+04, dt=0.00176783, dtf=0.0005579, dtb=0.00121\n[2024-07-23 22:21:43.773467][INFO][test_dist:274] - iter=50, loss=520.038, sps=3.938e+04, dt=0.00162521, dtf=0.0005579, dtb=0.001067\n[2024-07-23 22:21:43.776470][INFO][test_dist:274] - iter=51, loss=507.743, sps=3.68e+04, dt=0.00173934, dtf=0.0005378, dtb=0.001202\n[2024-07-23 22:21:43.779466][INFO][test_dist:274] - iter=52, loss=505.372, sps=3.694e+04, dt=0.00173268, dtf=0.0005321, dtb=0.001201\n[2024-07-23 22:21:43.782434][INFO][test_dist:274] - iter=53, loss=505.824, sps=3.736e+04, dt=0.00171324, dtf=0.0005403, dtb=0.001173\n[2024-07-23 22:21:43.785426][INFO][test_dist:274] - iter=54, loss=498.697, sps=3.751e+04, dt=0.00170619, dtf=0.0005259, dtb=0.00118\n[2024-07-23 22:21:43.788396][INFO][test_dist:274] - iter=55, loss=492.434, sps=3.719e+04, dt=0.00172085, dtf=0.0005036, dtb=0.001217\n[2024-07-23 22:21:43.791354][INFO][test_dist:274] - iter=56, loss=486.032, sps=3.754e+04, dt=0.00170497, dtf=0.0005077, dtb=0.001197\n[2024-07-23 22:21:43.794333][INFO][test_dist:274] - iter=57, loss=487.687, sps=3.803e+04, dt=0.00168299, dtf=0.0005009, dtb=0.001182\n[2024-07-23 22:21:43.797238][INFO][test_dist:274] - iter=58, loss=481.011, sps=3.929e+04, dt=0.00162898, dtf=0.0005554, dtb=0.001074\n[2024-07-23 22:21:43.800237][INFO][test_dist:274] - iter=59, loss=478.058, sps=3.692e+04, dt=0.00173365, dtf=0.0005374, dtb=0.001196\n[2024-07-23 22:21:43.803250][INFO][test_dist:274] - iter=60, loss=476.983, sps=3.666e+04, dt=0.00174587, dtf=0.0005318, dtb=0.001214\n[2024-07-23 22:21:43.806222][INFO][test_dist:274] - iter=61, loss=468.415, sps=3.716e+04, dt=0.00172234, dtf=0.0005256, dtb=0.001197\n[2024-07-23 22:21:43.809230][INFO][test_dist:274] - iter=62, loss=461.661, sps=3.727e+04, dt=0.00171737, dtf=0.0005219, dtb=0.001195\n[2024-07-23 22:21:43.812204][INFO][test_dist:274] - iter=63, loss=465.746, sps=3.688e+04, dt=0.00173519, dtf=0.0005067, dtb=0.001228\n[2024-07-23 22:21:43.815192][INFO][test_dist:274] - iter=64, loss=470.95, sps=3.724e+04, dt=0.00171855, dtf=0.0004994, dtb=0.001219\n[2024-07-23 22:21:43.818155][INFO][test_dist:274] - iter=65, loss=463.301, sps=3.774e+04, dt=0.00169586, dtf=0.0005053, dtb=0.001191\n[2024-07-23 22:21:43.821161][INFO][test_dist:274] - iter=66, loss=450.195, sps=3.68e+04, dt=0.00173904, dtf=0.0005626, dtb=0.001176\n[2024-07-23 22:21:43.824143][INFO][test_dist:274] - iter=67, loss=449.097, sps=3.662e+04, dt=0.00174746, dtf=0.0005578, dtb=0.00119\n[2024-07-23 22:21:43.827103][INFO][test_dist:274] - iter=68, loss=447.465, sps=3.778e+04, dt=0.00169412, dtf=0.0005488, dtb=0.001145\n[2024-07-23 22:21:43.830071][INFO][test_dist:274] - iter=69, loss=444.676, sps=3.835e+04, dt=0.00166873, dtf=0.0005467, dtb=0.001122\n[2024-07-23 22:21:43.833030][INFO][test_dist:274] - iter=70, loss=429.532, sps=3.83e+04, dt=0.00167122, dtf=0.0005362, dtb=0.001135\n[2024-07-23 22:21:43.836024][INFO][test_dist:274] - iter=71, loss=437.085, sps=3.711e+04, dt=0.00172438, dtf=0.0005086, dtb=0.001216\n[2024-07-23 22:21:43.839009][INFO][test_dist:274] - iter=72, loss=436.272, sps=3.71e+04, dt=0.00172525, dtf=0.0005177, dtb=0.001208\n[2024-07-23 22:21:43.841920][INFO][test_dist:274] - iter=73, loss=430.464, sps=3.893e+04, dt=0.00164403, dtf=0.0004874, dtb=0.001157\n[2024-07-23 22:21:43.844806][INFO][test_dist:274] - iter=74, loss=426.483, sps=3.904e+04, dt=0.0016393, dtf=0.000449, dtb=0.00119\n[2024-07-23 22:21:43.847771][INFO][test_dist:274] - iter=75, loss=413.371, sps=3.75e+04, dt=0.0017066, dtf=0.0005185, dtb=0.001188\n[2024-07-23 22:21:43.850712][INFO][test_dist:274] - iter=76, loss=421.381, sps=3.77e+04, dt=0.00169769, dtf=0.000506, dtb=0.001192\n[2024-07-23 22:21:43.853587][INFO][test_dist:274] - iter=77, loss=415.112, sps=3.988e+04, dt=0.0016047, dtf=0.000537, dtb=0.001068\n[2024-07-23 22:21:43.856557][INFO][test_dist:274] - iter=78, loss=413.084, sps=3.729e+04, dt=0.0017161, dtf=0.0005459, dtb=0.00117\n[2024-07-23 22:21:43.859518][INFO][test_dist:274] - iter=79, loss=412.671, sps=3.761e+04, dt=0.00170149, dtf=0.0005066, dtb=0.001195\n[2024-07-23 22:21:43.862469][INFO][test_dist:274] - iter=80, loss=408.688, sps=3.776e+04, dt=0.00169481, dtf=0.0005446, dtb=0.00115\n[2024-07-23 22:21:43.865521][INFO][test_dist:274] - iter=81, loss=400.914, sps=3.674e+04, dt=0.00174196, dtf=0.0005528, dtb=0.001189\n[2024-07-23 22:21:43.868536][INFO][test_dist:274] - iter=82, loss=389.823, sps=3.655e+04, dt=0.00175112, dtf=0.000574, dtb=0.001177\n[2024-07-23 22:21:43.871531][INFO][test_dist:274] - iter=83, loss=399.073, sps=3.686e+04, dt=0.00173618, dtf=0.0005504, dtb=0.001186\n[2024-07-23 22:21:43.874511][INFO][test_dist:274] - iter=84, loss=385.773, sps=3.725e+04, dt=0.00171814, dtf=0.0005499, dtb=0.001168\n[2024-07-23 22:21:43.877492][INFO][test_dist:274] - iter=85, loss=400.61, sps=3.739e+04, dt=0.00171182, dtf=0.000546, dtb=0.001166\n[2024-07-23 22:21:43.880505][INFO][test_dist:274] - iter=86, loss=389.813, sps=3.673e+04, dt=0.00174226, dtf=0.0005734, dtb=0.001169\n[2024-07-23 22:21:43.883515][INFO][test_dist:274] - iter=87, loss=385.995, sps=3.694e+04, dt=0.00173256, dtf=0.0005296, dtb=0.001203\n[2024-07-23 22:21:43.886470][INFO][test_dist:274] - iter=88, loss=379.115, sps=3.774e+04, dt=0.00169591, dtf=0.0005467, dtb=0.001149\n[2024-07-23 22:21:43.889422][INFO][test_dist:274] - iter=89, loss=378.738, sps=3.798e+04, dt=0.00168494, dtf=0.0005276, dtb=0.001157\n[2024-07-23 22:21:43.892414][INFO][test_dist:274] - iter=90, loss=365.054, sps=3.675e+04, dt=0.00174164, dtf=0.000513, dtb=0.001229\n[2024-07-23 22:21:43.895367][INFO][test_dist:274] - iter=91, loss=380.372, sps=3.772e+04, dt=0.00169654, dtf=0.000495, dtb=0.001201\n[2024-07-23 22:21:43.898322][INFO][test_dist:274] - iter=92, loss=377.233, sps=3.852e+04, dt=0.00166155, dtf=0.000539, dtb=0.001123\n[2024-07-23 22:21:43.901288][INFO][test_dist:274] - iter=93, loss=366.226, sps=3.788e+04, dt=0.00168959, dtf=0.0005446, dtb=0.001145\n[2024-07-23 22:21:43.904284][INFO][test_dist:274] - iter=94, loss=366.221, sps=3.69e+04, dt=0.00173462, dtf=0.0005535, dtb=0.001181\n[2024-07-23 22:21:43.907289][INFO][test_dist:274] - iter=95, loss=366.673, sps=3.662e+04, dt=0.00174759, dtf=0.0005328, dtb=0.001215\n[2024-07-23 22:21:43.910260][INFO][test_dist:274] - iter=96, loss=362.985, sps=3.716e+04, dt=0.00172234, dtf=0.0005436, dtb=0.001179\n[2024-07-23 22:21:43.913277][INFO][test_dist:274] - iter=97, loss=349.768, sps=3.668e+04, dt=0.00174469, dtf=0.000529, dtb=0.001216\n[2024-07-23 22:21:43.916293][INFO][test_dist:274] - iter=98, loss=363.521, sps=3.675e+04, dt=0.0017416, dtf=0.0005412, dtb=0.0012\n[2024-07-23 22:21:43.919280][INFO][test_dist:274] - iter=99, loss=345.533, sps=3.717e+04, dt=0.00172205, dtf=0.0005134, dtb=0.001209\n                             train/dt [2024-07-23-222143]\n       │\n0.00284┤▘\n       │\n       │\n0.00264┤\n       │\n       │\n0.00243┤\n       │\n0.00222┤\n       │\n       │\n0.00201┤\n       │▗\n       │ ▝▘▗▗▖               ▝\n0.00181┤   ▘  ▖         ▗\n       │       ▘▄ ▖▄▖▄▗▖ ▗▗ ▘ ▗▄▝▖▗▗▖▖▖▗▗▘▀ ▄    ▗▗ ▗  ▄   ▖     ▗▗▖ ▖▖ ▖  ▄ ▖▖\n       │      ▝  ▝      ▘  ▝ ▘    ▘    ▖     ▝▘▀▗  ▘▘▝▘ ▘▄▝  ▘▘▝▘▘ ▝▝ ▝▗▝▗▘ ▝ ▝\n0.00160┤                  ▖          ▗     ▝     ▘          ▀ ▗\n       └┬─────────────────┬────────────────┬─────────────────┬────────────────┬─\n       1.0              25.5             50.0              74.5            99.0\ntrain/dt                                 iter\n[2024-07-23 22:21:43.943086][INFO][plot:156] - Appending plot to: /lus/eagle/projects/argonne_tpc/foremans/projects/argonne-lcf/tmp/2024-07-23-221253/Megatron-DeepSpeed/test-dist-plots/train/dt.txt\ntext saved in /lus/eagle/projects/argonne_tpc/foremans/projects/argonne-lcf/tmp/2024-07-23-221253/Megatron-DeepSpeed/test-dist-plots/train/dt.txt\n                             train/dtf [2024-07-23-222143]\n       ┌───────────────────────────────────────────────────────────────────────┐\n0.00098┤▘                                                                      │\n       │                                                                       │\n       │                                                                       │\n0.00089┤                                                                       │\n       │                                                                       │\n       │                                                                       │\n0.00080┤                                                                       │\n       │                                                                       │\n0.00071┤                                                                       │\n       │                                                                       │\n       │                                                                       │\n0.00063┤                                                                       │\n       │ ▝▘▄  ▞                                                                │\n       │    ▝▘ ▖  ▖  ▖  ▗   ▖   ▗  ▗      ▖                       ▗  ▖         │\n0.00054┤        ▀▝ ▞▖    ▝   ▀▝▀ ▘▝ ▖▄ ▝▝▘▝▝▖▗   ▚     ▀▘▄    ▗▗ ▞ ▀▗ ▗  ▗▖▚▗ ▖│\n       │▝            ▝▝▖▖ ▚       ▘   ▖▖    ▝ ▘▄  ▝▘▚ ▖   ▗▘ ▘▖ ▖     ▘▝▖    ▘▗│\n       │                   ▝                    ▝    ▝      ▘           ▝      │\n0.00045┤                                                    ▗                  │\n       └┬─────────────────┬────────────────┬─────────────────┬────────────────┬┘\n       1.0              25.5             50.0              74.5            99.0\ntrain/dtf                                iter\n[2024-07-23 22:21:43.952631][INFO][plot:156] - Appending plot to: /lus/eagle/projects/argonne_tpc/foremans/projects/argonne-lcf/tmp/2024-07-23-221253/Megatron-DeepSpeed/test-dist-plots/train/dtf.txt\ntext saved in /lus/eagle/projects/argonne_tpc/foremans/projects/argonne-lcf/tmp/2024-07-23-221253/Megatron-DeepSpeed/test-dist-plots/train/dtf.txt\n                             train/dtb [2024-07-23-222143]\n       ┌───────────────────────────────────────────────────────────────────────┐\n0.00186┤▘                                                                      │\n       │                                                                       │\n       │                                                                       │\n0.00173┤                                                                       │\n       │                                                                       │\n       │                                                                       │\n0.00160┤                                                                       │\n       │                                                                       │\n0.00146┤                                                                       │\n       │▗                                                                      │\n       │                                                                       │\n0.00133┤                     ▗                                                 │\n       │ ▝▖ ▗▖                                                                 │\n       │   ▞              ▗                                                    │\n0.00120┤      ▖▖   ▗ ▗▗▘▝  ▗▖  ▖▗▖   ▖▘  ▖▄ ▄  ▚ ▗▗▖▞▝    ▝▖    ▖     ▖ ▚  ▗ ▘▄│\n       │        ▀▗▘▘▘▖  ▘▝   ▘▝▝  ▀▝▘  ▀▝    ▝▘ ▝     ▘▀    ▞▘▘▝ ▞▝▚▗▖▗▗   ▘▝  │\n       │                                                ▘▞               ▗▘    │\n0.00107┤      ▝           ▘          ▗     ▗     ▖            ▗                │\n       └┬─────────────────┬────────────────┬─────────────────┬────────────────┬┘\n       1.0              25.5             50.0              74.5            99.0\ntrain/dtb                                iter\n[2024-07-23 22:21:43.962230][INFO][plot:156] - Appending plot to: /lus/eagle/projects/argonne_tpc/foremans/projects/argonne-lcf/tmp/2024-07-23-221253/Megatron-DeepSpeed/test-dist-plots/train/dtb.txt\ntext saved in /lus/eagle/projects/argonne_tpc/foremans/projects/argonne-lcf/tmp/2024-07-23-221253/Megatron-DeepSpeed/test-dist-plots/train/dtb.txt\n                            train/loss [2024-07-23-222143]\n       ┌────────────────────────────────────────────────────────────────────────┐\n 2039.9┤▘                                                                       │\n       │                                                                        │\n       │                                                                        │\n 1757.5┤                                                                        │\n       │                                                                        │\n       │                                                                        │\n 1475.1┤▗                                                                       │\n       │                                                                        │\n 1192.7┤                                                                        │\n       │ ▝                                                                      │\n       │                                                                        │\n  910.3┤  ▖                                                                     │\n       │   ▖                                                                    │\n       │   ▝▝▖▄▗                                                                │\n  627.9┤        ▘▀▘▀▝▘▚▗▖▄▖▗                                                    │\n       │                   ▘▝▘▀▝▘▚▝▄▗▖▄▗▖▄▗▖▖                                   │\n       │                                    ▝▘▀▝▘▀▝▘▚▖▚▗▖▄▗▖▄▗▗                 │\n  345.5┤                                                      ▘▝▘▀▝▘▀▝▀▝▘▞▝▖▄▗▖▄│\n       └┬─────────────────┬─────────────────┬────────────────┬─────────────────┬┘\n       1.0              25.5              50.0             74.5             99.0\ntrain/loss                               iter\n[2024-07-23 22:21:44.011096][INFO][plot:156] - Appending plot to: /lus/eagle/projects/argonne_tpc/foremans/projects/argonne-lcf/tmp/2024-07-23-221253/Megatron-DeepSpeed/test-dist-plots/train/loss.txt\ntext saved in /lus/eagle/projects/argonne_tpc/foremans/projects/argonne-lcf/tmp/2024-07-23-221253/Megatron-DeepSpeed/test-dist-plots/train/loss.txt\n                           train/iter [2024-07-23-222144]\n     ┌──────────────────────────────────────────────────────────────────────────┐\n 99.0┤                                                                      ▗▗▖▀│\n     │                                                                   ▄▝▘▘   │\n     │                                                              ▗▖▞▝▘       │\n 82.7┤                                                          ▄▗▘▀            │\n     │                                                      ▖▄▝▘                │\n     │                                                 ▗▗▖▀▝                    │\n 66.3┤                                              ▄▝▘▘                        │\n     │                                         ▗▖▞▝▘                            │\n 50.0┤                                     ▄▗▘▀                                 │\n     │                                 ▖▄▝▘                                     │\n     │                            ▗▗▖▀▝                                         │\n 33.7┤                         ▄▝▘▘                                             │\n     │                    ▗▖▞▝▘                                                 │\n     │                ▄▗▘▀                                                      │\n 17.3┤            ▖▄▝▘                                                          │\n     │       ▗▗▖▀▝                                                              │\n     │    ▄▝▘▘                                                                  │\n  1.0┤▖▞▝▘                                                                      │\n     └┬─────────────────┬──────────────────┬─────────────────┬─────────────────┬┘\n     1.0              25.5               50.0              74.5             99.0\ntrain/iter                              iter\n[2024-07-23 22:21:44.021040][INFO][plot:156] - Appending plot to: /lus/eagle/projects/argonne_tpc/foremans/projects/argonne-lcf/tmp/2024-07-23-221253/Megatron-DeepSpeed/test-dist-plots/train/iter.txt\ntext saved in /lus/eagle/projects/argonne_tpc/foremans/projects/argonne-lcf/tmp/2024-07-23-221253/Megatron-DeepSpeed/test-dist-plots/train/iter.txt\n                            train/sps [2024-07-23-222144]\n       ┌───────────────────────────────────────────────────────────────────────┐\n39979.2┤                  ▖          ▝     ▗     ▖            ▝                │\n       │                                                 ▄  ▀            ▗     │\n       │      ▝  ▗      ▖  ▝      ▖    ▘     ▗▖▗▝   ▖▗▘ ▘    ▖▖▗▖▘ ▗▗ ▝▝▗ ▘    │\n37069.3┤        ▚ ▖▚▘▀▝▘ ▝▗  ▘▗▞ ▖▝▗▘▘▘▗▝▖▖ ▀  ▘ ▝▗▘▝  ▚  ▝▘     ▝▗▘ ▖▘ ▘  ▚▝▖▀│\n       │      ▖▘        ▗   ▘   ▝         ▝                                    │\n       │   ▘                                                                   │\n34159.4┤ ▗▖▝▝▘               ▗                                                 │\n       │▗                                                                      │\n31249.4┤                                                                       │\n       │                                                                       │\n       │                                                                       │\n28339.5┤                                                                       │\n       │                                                                       │\n       │                                                                       │\n25429.6┤                                                                       │\n       │                                                                       │\n       │                                                                       │\n22519.7┤▖                                                                      │\n       └┬─────────────────┬────────────────┬─────────────────┬────────────────┬┘\n       1.0              25.5             50.0              74.5            99.0\ntrain/sps                                iter\n[2024-07-23 22:21:44.030585][INFO][plot:156] - Appending plot to: /lus/eagle/projects/argonne_tpc/foremans/projects/argonne-lcf/tmp/2024-07-23-221253/Megatron-DeepSpeed/test-dist-plots/train/sps.txt\ntext saved in /lus/eagle/projects/argonne_tpc/foremans/projects/argonne-lcf/tmp/2024-07-23-221253/Megatron-DeepSpeed/test-dist-plots/train/sps.txt\n\n\nPyInstrument Profile:\n\nRecorded: 22:21:41  Samples:  2223\nDuration: 2.668     CPU time: 2.406\nv4.6.2\n\nProgram: /lus/eagle/projects/argonne_tpc/foremans/projects/argonne-lcf/tmp/2024-07-23-221253/Megatron-DeepSpeed/deps/ezpz/src/ezpz/test_dist.py\n\n2.668 &lt;module&gt;  ezpz/test_dist.py:1\n└─ 2.667 main  ezpz/test_dist.py:217\n  ├─ 2.106 build_model_and_optimizer  ezpz/test_dist.py:171\n  │  └─ 2.092 Adam.__init__  torch/optim/adam.py:15\n  │        [147 frames hidden]  torch, transformers, jax, huggingface...\n  ├─ 0.183 _forward_step  ezpz/test_dist.py:231\n  │  ├─ 0.137 DistributedDataParallel._wrapped_call_impl  torch/nn/modules/module.py:1528\n  │  │     [6 frames hidden]  torch\n  │  │        0.123 Network._call_impl  torch/nn/modules/module.py:1534\n  │  │        └─ 0.123 Network.forward  ezpz/test_dist.py:164\n  │  │           └─ 0.123 Sequential._wrapped_call_impl  torch/nn/modules/module.py:1528\n  │  │                 [7 frames hidden]  torch, &lt;built-in&gt;\n  │  └─ 0.046 calc_loss  ezpz/test_dist.py:168\n  ├─ 0.164 _backward_step  ezpz/test_dist.py:236\n  │  ├─ 0.103 wrapper  torch/optim/optimizer.py:374\n  │  │     [5 frames hidden]  torch\n  │  └─ 0.060 Tensor.backward  torch/_tensor.py:466\n  │        [4 frames hidden]  torch, &lt;built-in&gt;\n  ├─ 0.113 tplot_dict  ezpz/plot.py:136\n  │  └─ 0.082 show  plotext/_core.py:292\n  │        [5 frames hidden]  plotext\n  └─ 0.099 Logger.info  logging/__init__.py:1479\n        [6 frames hidden]  logging, rich\n            0.099 RichHandler.emit  rich/logging.py:126\n            └─ 0.099 Console.print  ezpz/log/console.py:79\n              └─ 0.099 Console.print  rich/console.py:1624\n                    [5 frames hidden]  rich\n\n\n[2024-07-23 22:21:44.231519][INFO][profile:115] - Saving pyinstrument profile output to: /lus/eagle/projects/argonne_tpc/foremans/projects/argonne-lcf/tmp/2024-07-23-221253/Megatron-DeepSpeed/ezpz_pyinstrument_profiles\n[2024-07-23 22:21:44.232054][INFO][profile:123] - PyInstrument profile saved (as html) to:  /lus/eagle/projects/argonne_tpc/foremans/projects/argonne-lcf/tmp/2024-07-23-221253/Megatron-DeepSpeed/ezpz_pyinstrument_profiles/pyinstrument-profile-2024-07-23-222144.html\n[2024-07-23 22:21:44.232619][INFO][profile:131] - PyInstrument profile saved (as text) to:  /lus/eagle/projects/argonne_tpc/foremans/projects/argonne-lcf/tmp/2024-07-23-221253/Megatron-DeepSpeed/ezpz_pyinstrument_profiles/pyinstrument-profile-2024-07-23-222144.txt\n[2024-07-23 22:21:44.761876][INFO][profile:143] - Finished with pyinstrument profiler. Took: 2.66778s\n[2024-07-23 22:21:44.762534][INFO][test_dist:318] - [0] runtime=6.785542s\nezpz-test-dist.log lines 216-359/359 (END)\n\n\n\n\n\n\nDeprecated:\n\n\nDownload and source ezpz/bin/utils.sh:\nezpz_utils() {\n    fp=$(mktemp)\n    curl -Ls https://raw.githubusercontent.com/saforem2/ezpz/main/src/ezpz/bin/utils.sh &gt; $fp\n    source $fp\n}\nezpz_utils\nUse ezpz_setup_python to:\n\nSetup base conda environment\nCreate1 a virtual environment on top of the base conda environment2\n\n#[🌌][12:45:49 PM][foremans@x3006c0s13b0n0][~/tmp/foremans/2024-07-15-124441]\n$ PBS_O_WORKDIR=$(pwd) ezpz_utils && ezpz_setup_python && ezpz_setup_alcf \n\nUsing WORKING_DIR: /home/foremans/tmp/foremans/2024-07-15-124441\nNo conda_prefix OR virtual_env found in environment...\nSetting up conda...\n\nLmod is automatically replacing \"nvhpc/23.9\" with \"gcc-native/12.3\".\n\n\nLmod is automatically replacing \"PrgEnv-nvhpc/8.5.0\" with \"PrgEnv-gnu/8.5.0\".\n\n\nDue to MODULEPATH changes, the following have been reloaded:\n1) cray-mpich/8.1.28\n\nFound conda at: /soft/applications/conda/2024-04-29/mconda3\nNo VIRTUAL_ENV found in environment!\n  - Trying to setup from /soft/applications/conda/2024-04-29/mconda3\n  - Using VENV_DIR=/home/foremans/tmp/foremans/2024-07-15-124441/venvs/2024-04-29\n\n  - Creating a new virtual env on top of 2024-04-29 in /home/foremans/tmp/foremans/2024-07-15-124441/venvs/2024-04-29\n[python] Using /home/foremans/tmp/foremans/2024-07-15-124441/venvs/2024-04-29/bin/python3\n\n[ezpz/bin/utils.sh]\n\n[2024-07-15-124600]\n  • USER=foremans\n  • MACHINE=polaris\n  • HOST=x3006c0s13b0n0\n\n[ezpz_setup_host]\n  • Using hostfile: /var/spool/pbs/aux/2021158.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov\n  • Found in environment:\n      • HOSTFILE: /var/spool/pbs/aux/2021158.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov\n      • Writing PBS vars to: /home/foremans/.pbsenv\n\n[ezpz_save_pbs_env]\n  • Setting:\n      • HOSTFILE: /var/spool/pbs/aux/2021158.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov\n      • JOBENV_FILE: /home/foremans/.pbsenv\n\n[HOSTS]\n  • [host:0] - x3006c0s13b0n0.hsn.cm.polaris.alcf.anl.gov\n\n[DIST INFO]\n  • HOSTFILE=/var/spool/pbs/aux/2021158.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov\n  • NHOSTS=1\n  • NGPU_PER_HOST=4\n  • NGPUS=4\n  • DIST_LAUNCH=mpiexec --verbose --envall -n 4 -ppn 4 --hostfile /var/spool/pbs/aux/2021158.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov --cpu-bind depth -d 16\n\n[LAUNCH]:\n  • To launch across all available GPUs, use: launch\n    launch = mpiexec --verbose --envall -n 4 -ppn 4 --hostfile /var/spool/pbs/aux/2021158.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov --cpu-bind depth -d 16\n\n\nInstall ezpz into the virtual environment from 2.\npython3 -m pip install -e \"git+https://github.com/saforem2/ezpz#egg=ezpz\" --require-virtualenv\n\n\noutput\n\nLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\nObtaining ezpz from git+https://github.com/saforem2/ezpz#egg=ezpz\nCloning https://github.com/saforem2/ezpz to ./venvs/2024-04-29/src/ezpz\nRunning command git clone --filter=blob:none --quiet https://github.com/saforem2/ezpz /home/foremans/tmp/foremans/2024-07-15-124441/venvs/2024-04-29/src/ezpz\nResolved https://github.com/saforem2/ezpz to commit d8fabca03038db55a1dc490f801581e980f93a25\nInstalling build dependencies ... done\nChecking if build backend supports build_editable ... done\nGetting requirements to build editable ... done\nInstalling backend dependencies ... done\nPreparing editable metadata (pyproject.toml) ... done\nRequirement already satisfied: ambivalent in /home/foremans/.local/polaris/conda/2024-04-29/lib/python3.11/site-packages (from ezpz) (0.0.1)\nRequirement already satisfied: hydra-colorlog in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ezpz) (1.2.0)\nRequirement already satisfied: hydra-core in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ezpz) (1.3.2)\nRequirement already satisfied: ipython in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ezpz) (8.24.0)\nRequirement already satisfied: jax in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ezpz) (0.4.26)\nRequirement already satisfied: jaxlib in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ezpz) (0.4.26+cuda12.cudnn89)\nRequirement already satisfied: jaxtyping in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ezpz) (0.2.28)\nRequirement already satisfied: joblib in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ezpz) (1.4.0)\nRequirement already satisfied: ml-dtypes in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ezpz) (0.3.2)\nRequirement already satisfied: mpi4py in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ezpz) (3.1.6)\nRequirement already satisfied: omegaconf in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ezpz) (2.3.0)\nRequirement already satisfied: plotext in /home/foremans/.local/polaris/conda/2024-04-29/lib/python3.11/site-packages (from ezpz) (5.2.8)\nCollecting pyinstrument (from ezpz)\nDownloading pyinstrument-4.6.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\nRequirement already satisfied: rich in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ezpz) (13.7.1)\nRequirement already satisfied: seaborn in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ezpz) (0.13.2)\nRequirement already satisfied: sentencepiece in /home/foremans/.local/polaris/conda/2024-04-29/lib/python3.11/site-packages (from ezpz) (0.2.0)\nRequirement already satisfied: sh in /home/foremans/.local/polaris/conda/2024-04-29/lib/python3.11/site-packages (from ezpz) (2.0.6)\nRequirement already satisfied: tensorboard in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ezpz) (2.16.2)\nRequirement already satisfied: torch in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ezpz) (2.3.0)\nRequirement already satisfied: tqdm in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ezpz) (4.65.0)\nRequirement already satisfied: wandb in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ezpz) (0.16.6)\nRequirement already satisfied: xarray in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ezpz) (2024.3.0)\nRequirement already satisfied: colormaps in /home/foremans/.local/polaris/conda/2024-04-29/lib/python3.11/site-packages (from ambivalent-&gt;ezpz) (0.4.1)\nRequirement already satisfied: matplotlib in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ambivalent-&gt;ezpz) (3.8.4)\nRequirement already satisfied: requests in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ambivalent-&gt;ezpz) (2.31.0)\nRequirement already satisfied: colorlog in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from hydra-colorlog-&gt;ezpz) (6.8.2)\nRequirement already satisfied: antlr4-python3-runtime==4.9.* in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from hydra-core-&gt;ezpz) (4.9.3)\nRequirement already satisfied: packaging in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from hydra-core-&gt;ezpz) (24.0)\nRequirement already satisfied: PyYAML&gt;=5.1.0 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from omegaconf-&gt;ezpz) (6.0.1)\nRequirement already satisfied: decorator in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ipython-&gt;ezpz) (5.1.1)\nRequirement already satisfied: jedi&gt;=0.16 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ipython-&gt;ezpz) (0.19.1)\nRequirement already satisfied: matplotlib-inline in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ipython-&gt;ezpz) (0.1.7)\nRequirement already satisfied: prompt-toolkit&lt;3.1.0,&gt;=3.0.41 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ipython-&gt;ezpz) (3.0.43)\nRequirement already satisfied: pygments&gt;=2.4.0 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ipython-&gt;ezpz) (2.17.2)\nRequirement already satisfied: stack-data in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ipython-&gt;ezpz) (0.6.3)\nRequirement already satisfied: traitlets&gt;=5.13.0 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ipython-&gt;ezpz) (5.14.3)\nRequirement already satisfied: typing-extensions&gt;=4.6 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ipython-&gt;ezpz) (4.11.0)\nRequirement already satisfied: pexpect&gt;4.3 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ipython-&gt;ezpz) (4.9.0)\nRequirement already satisfied: numpy&gt;=1.22 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from jax-&gt;ezpz) (1.26.4)\nRequirement already satisfied: opt-einsum in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from jax-&gt;ezpz) (3.3.0)\nRequirement already satisfied: scipy&gt;=1.9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from jax-&gt;ezpz) (1.13.0)\nRequirement already satisfied: typeguard==2.13.3 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from jaxtyping-&gt;ezpz) (2.13.3)\nRequirement already satisfied: markdown-it-py&gt;=2.2.0 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from rich-&gt;ezpz) (3.0.0)\nRequirement already satisfied: pandas&gt;=1.2 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from seaborn-&gt;ezpz) (2.2.2)\nRequirement already satisfied: absl-py&gt;=0.4 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from tensorboard-&gt;ezpz) (2.1.0)\nRequirement already satisfied: grpcio&gt;=1.48.2 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from tensorboard-&gt;ezpz) (1.62.2)\nRequirement already satisfied: markdown&gt;=2.6.8 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from tensorboard-&gt;ezpz) (3.6)\nRequirement already satisfied: protobuf!=4.24.0,&gt;=3.19.6 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from tensorboard-&gt;ezpz) (3.20.3)\nRequirement already satisfied: setuptools&gt;=41.0.0 in ./venvs/2024-04-29/lib/python3.11/site-packages (from tensorboard-&gt;ezpz) (65.5.0)\nRequirement already satisfied: six&gt;1.9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from tensorboard-&gt;ezpz) (1.16.0)\nRequirement already satisfied: tensorboard-data-server&lt;0.8.0,&gt;=0.7.0 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from tensorboard-&gt;ezpz) (0.7.2)\nRequirement already satisfied: werkzeug&gt;=1.0.1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from tensorboard-&gt;ezpz) (3.0.2)\nRequirement already satisfied: filelock in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from torch-&gt;ezpz) (3.13.1)\nRequirement already satisfied: sympy in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from torch-&gt;ezpz) (1.12)\nRequirement already satisfied: networkx in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from torch-&gt;ezpz) (3.3)\nRequirement already satisfied: jinja2 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from torch-&gt;ezpz) (3.0.3)\nRequirement already satisfied: fsspec in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from torch-&gt;ezpz) (2024.3.1)\nRequirement already satisfied: Click!=8.0.0,&gt;=7.1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from wandb-&gt;ezpz) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,&gt;=1.0.0 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from wandb-&gt;ezpz) (3.1.43)\nRequirement already satisfied: psutil&gt;=5.0.0 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from wandb-&gt;ezpz) (5.9.8)\nRequirement already satisfied: sentry-sdk&gt;=1.0.0 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from wandb-&gt;ezpz) (2.0.1)\nRequirement already satisfied: docker-pycreds&gt;=0.4.0 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from wandb-&gt;ezpz) (0.4.0)\nRequirement already satisfied: setproctitle in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from wandb-&gt;ezpz) (1.3.3)\nRequirement already satisfied: appdirs&gt;=1.4.3 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from wandb-&gt;ezpz) (1.4.4)\nRequirement already satisfied: gitdb&lt;5,&gt;=4.0.1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from GitPython!=3.1.29,&gt;=1.0.0-&gt;wandb-&gt;ezpz) (4.0.11)\nRequirement already satisfied: parso&lt;0.9.0,&gt;=0.8.3 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from jedi&gt;=0.16-&gt;ipython-&gt;ezpz) (0.8.4)\nRequirement already satisfied: mdurl~=0.1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from markdown-it-py&gt;=2.2.0-&gt;rich-&gt;ezpz) (0.1.2)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from matplotlib-&gt;ambivalent-&gt;ezpz) (1.2.1)\nRequirement already satisfied: cycler&gt;=0.10 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from matplotlib-&gt;ambivalent-&gt;ezpz) (0.12.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from matplotlib-&gt;ambivalent-&gt;ezpz) (4.51.0)\nRequirement already satisfied: kiwisolver&gt;=1.3.1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from matplotlib-&gt;ambivalent-&gt;ezpz) (1.4.5)\nRequirement already satisfied: pillow&gt;=8 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from matplotlib-&gt;ambivalent-&gt;ezpz) (10.3.0)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from matplotlib-&gt;ambivalent-&gt;ezpz) (3.1.2)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from matplotlib-&gt;ambivalent-&gt;ezpz) (2.9.0.post0)\nRequirement already satisfied: pytz&gt;=2020.1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from pandas&gt;=1.2-&gt;seaborn-&gt;ezpz) (2024.1)\nRequirement already satisfied: tzdata&gt;=2022.7 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from pandas&gt;=1.2-&gt;seaborn-&gt;ezpz) (2024.1)\nRequirement already satisfied: ptyprocess&gt;=0.5 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from pexpect&gt;4.3-&gt;ipython-&gt;ezpz) (0.7.0)\nRequirement already satisfied: wcwidth in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from prompt-toolkit&lt;3.1.0,&gt;=3.0.41-&gt;ipython-&gt;ezpz) (0.2.13)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from requests-&gt;ambivalent-&gt;ezpz) (2.0.4)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from requests-&gt;ambivalent-&gt;ezpz) (3.4)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from requests-&gt;ambivalent-&gt;ezpz) (2.1.0)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from requests-&gt;ambivalent-&gt;ezpz) (2024.2.2)\nRequirement already satisfied: MarkupSafe&gt;=2.1.1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from werkzeug&gt;=1.0.1-&gt;tensorboard-&gt;ezpz) (2.1.3)\nRequirement already satisfied: executing&gt;=1.2.0 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from stack-data-&gt;ipython-&gt;ezpz) (2.0.1)\nRequirement already satisfied: asttokens&gt;=2.1.0 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from stack-data-&gt;ipython-&gt;ezpz) (2.4.1)\nRequirement already satisfied: pure-eval in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from stack-data-&gt;ipython-&gt;ezpz) (0.2.2)\nRequirement already satisfied: mpmath&gt;=0.19 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from sympy-&gt;torch-&gt;ezpz) (1.3.0)\nRequirement already satisfied: smmap&lt;6,&gt;=3.0.1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from gitdb&lt;5,&gt;=4.0.1-&gt;GitPython!=3.1.29,&gt;=1.0.0-&gt;wandb-&gt;ezpz) (5.0.1)\nDownloading pyinstrument-4.6.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (104 kB)\n ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 104.9/104.9 kB 3.1 MB/s eta 0:00:00\nBuilding wheels for collected packages: ezpz\nBuilding editable for ezpz (pyproject.toml) ... done\nCreated wheel for ezpz: filename=ezpz-0.1-py3-none-any.whl size=10104 sha256=f73fbc552c6192f2d1575c08528267c1c70bd4ed2eebab011c692b4cf66fd9cb\nStored in directory: /tmp/pip-ephem-wheel-cache-6xb0tqk8/wheels/b3/57/90/f3324177d75cbc607a034b5b8e66d5b3d35dcf087967430718\nSuccessfully built ezpz\nInstalling collected packages: pyinstrument, ezpz\nAttempting uninstall: ezpz\n  Found existing installation: ezpz 0.1\n  Not uninstalling ezpz at /home/foremans/.local/polaris/conda/2024-04-29/lib/python3.11/site-packages, outside environment /home/foremans/tmp/foremans/2024-07-15-124441/venvs/2024-04-29\n  Cant uninstall 'ezpz'. No files were found to uninstall.\nSuccessfully installed ezpz pyinstrument-4.6.2\n\n[notice] A new release of pip is available: 24.0 -&gt; 24.1.2\n[notice] To update, run: pip install --upgrade pip\n9.63s user 1.44s system 60% cpu 18.301s total\n\n\n#[🌌][12:44:34 PM][foremans@x3006c0s13b0n0][~/tmp]\n$ dname=$USER/$(tstamp) ; mkdir -p $dname && cd $dname\n\n\n#[🌌][12:44:45 PM][foremans@x3006c0s13b0n0][~/tmp/foremans/2024-07-15-124441]\n; ezpz_utils() { fp=$(mktemp) && curl -Ls https://raw.githubusercontent.com/saforem2/ezpz/main/src/ezpz/bin/utils.sh &gt; $fp && source $fp || exit }\n\n\n#[🌌][12:44:47 PM][foremans@x3006c0s13b0n0][~/tmp/foremans/2024-07-15-124441]\n$ PBS_O_WORKDIR=$(pwd) ezpz_utils\nUsing WORKING_DIR: /home/foremans/tmp/foremans/2024-07-15-124441\n\n\n#[🌌][12:45:49 PM][foremans@x3006c0s13b0n0][~/tmp/foremans/2024-07-15-124441]\n$ PBS_O_WORKDIR=$(pwd) ezpz_utils && ezpz_setup_python && ezpz_setup_alcf \n\nUsing WORKING_DIR: /home/foremans/tmp/foremans/2024-07-15-124441\nNo conda_prefix OR virtual_env found in environment...\nSetting up conda...\n\nLmod is automatically replacing \"nvhpc/23.9\" with \"gcc-native/12.3\".\n\n\nLmod is automatically replacing \"PrgEnv-nvhpc/8.5.0\" with \"PrgEnv-gnu/8.5.0\".\n\n\nDue to MODULEPATH changes, the following have been reloaded:\n  1) cray-mpich/8.1.28\n\nFound conda at: /soft/applications/conda/2024-04-29/mconda3\nNo VIRTUAL_ENV found in environment!\n    - Trying to setup from /soft/applications/conda/2024-04-29/mconda3\n    - Using VENV_DIR=/home/foremans/tmp/foremans/2024-07-15-124441/venvs/2024-04-29\n\n    - Creating a new virtual env on top of 2024-04-29 in /home/foremans/tmp/foremans/2024-07-15-124441/venvs/2024-04-29\n[python] Using /home/foremans/tmp/foremans/2024-07-15-124441/venvs/2024-04-29/bin/python3\n\n[ezpz/bin/utils.sh]\n\n[2024-07-15-124600]\n    • USER=foremans\n    • MACHINE=polaris\n    • HOST=x3006c0s13b0n0\n\n[ezpz_setup_host]\n    • Using hostfile: /var/spool/pbs/aux/2021158.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov\n    • Found in environment:\n        • HOSTFILE: /var/spool/pbs/aux/2021158.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov\n        • Writing PBS vars to: /home/foremans/.pbsenv\n\n[ezpz_save_pbs_env]\n    • Setting:\n        • HOSTFILE: /var/spool/pbs/aux/2021158.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov\n        • JOBENV_FILE: /home/foremans/.pbsenv\n\nalias LAUNCH='mpiexec --verbose --envall -n 4 -ppn 4 --hostfile /var/spool/pbs/aux/2021158.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov --cpu-bind depth -d 16'\n[HOSTS]\n    • [host:0] - x3006c0s13b0n0.hsn.cm.polaris.alcf.anl.gov\n\n[DIST INFO]\n    • HOSTFILE=/var/spool/pbs/aux/2021158.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov\n    • NHOSTS=1\n    • NGPU_PER_HOST=4\n    • NGPUS=4\n    • DIST_LAUNCH=mpiexec --verbose --envall -n 4 -ppn 4 --hostfile /var/spool/pbs/aux/2021158.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov --cpu-bind depth -d 16\n\n[LAUNCH]:\n    • To launch across all available GPUs, use: launch\n      launch = mpiexec --verbose --envall -n 4 -ppn 4 --hostfile /var/spool/pbs/aux/2021158.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov --cpu-bind depth -d 16\nLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\nObtaining ezpz from git+https://github.com/saforem2/ezpz#egg=ezpz\n  Cloning https://github.com/saforem2/ezpz to ./venvs/2024-04-29/src/ezpz\n  Running command git clone --filter=blob:none --quiet https://github.com/saforem2/ezpz /home/foremans/tmp/foremans/2024-07-15-124441/venvs/2024-04-29/src/ezpz\n  Resolved https://github.com/saforem2/ezpz to commit d8fabca03038db55a1dc490f801581e980f93a25\n  Installing build dependencies ... done\n  Checking if build backend supports build_editable ... done\n  Getting requirements to build editable ... done\n  Installing backend dependencies ... done\n  Preparing editable metadata (pyproject.toml) ... done\nRequirement already satisfied: ambivalent in /home/foremans/.local/polaris/conda/2024-04-29/lib/python3.11/site-packages (from ezpz) (0.0.1)\nRequirement already satisfied: hydra-colorlog in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ezpz) (1.2.0)\nRequirement already satisfied: hydra-core in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ezpz) (1.3.2)\nRequirement already satisfied: ipython in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ezpz) (8.24.0)\nRequirement already satisfied: jax in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ezpz) (0.4.26)\nRequirement already satisfied: jaxlib in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ezpz) (0.4.26+cuda12.cudnn89)\nRequirement already satisfied: jaxtyping in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ezpz) (0.2.28)\nRequirement already satisfied: joblib in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ezpz) (1.4.0)\nRequirement already satisfied: ml-dtypes in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ezpz) (0.3.2)\nRequirement already satisfied: mpi4py in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ezpz) (3.1.6)\nRequirement already satisfied: omegaconf in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ezpz) (2.3.0)\nRequirement already satisfied: plotext in /home/foremans/.local/polaris/conda/2024-04-29/lib/python3.11/site-packages (from ezpz) (5.2.8)\nCollecting pyinstrument (from ezpz)\n  Downloading pyinstrument-4.6.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\nRequirement already satisfied: rich in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ezpz) (13.7.1)\nRequirement already satisfied: seaborn in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ezpz) (0.13.2)\nRequirement already satisfied: sentencepiece in /home/foremans/.local/polaris/conda/2024-04-29/lib/python3.11/site-packages (from ezpz) (0.2.0)\nRequirement already satisfied: sh in /home/foremans/.local/polaris/conda/2024-04-29/lib/python3.11/site-packages (from ezpz) (2.0.6)\nRequirement already satisfied: tensorboard in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ezpz) (2.16.2)\nRequirement already satisfied: torch in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ezpz) (2.3.0)\nRequirement already satisfied: tqdm in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ezpz) (4.65.0)\nRequirement already satisfied: wandb in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ezpz) (0.16.6)\nRequirement already satisfied: xarray in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ezpz) (2024.3.0)\nRequirement already satisfied: colormaps in /home/foremans/.local/polaris/conda/2024-04-29/lib/python3.11/site-packages (from ambivalent-&gt;ezpz) (0.4.1)\nRequirement already satisfied: matplotlib in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ambivalent-&gt;ezpz) (3.8.4)\nRequirement already satisfied: requests in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ambivalent-&gt;ezpz) (2.31.0)\nRequirement already satisfied: colorlog in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from hydra-colorlog-&gt;ezpz) (6.8.2)\nRequirement already satisfied: antlr4-python3-runtime==4.9.* in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from hydra-core-&gt;ezpz) (4.9.3)\nRequirement already satisfied: packaging in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from hydra-core-&gt;ezpz) (24.0)\nRequirement already satisfied: PyYAML&gt;=5.1.0 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from omegaconf-&gt;ezpz) (6.0.1)\nRequirement already satisfied: decorator in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ipython-&gt;ezpz) (5.1.1)\nRequirement already satisfied: jedi&gt;=0.16 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ipython-&gt;ezpz) (0.19.1)\nRequirement already satisfied: matplotlib-inline in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ipython-&gt;ezpz) (0.1.7)\nRequirement already satisfied: prompt-toolkit&lt;3.1.0,&gt;=3.0.41 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ipython-&gt;ezpz) (3.0.43)\nRequirement already satisfied: pygments&gt;=2.4.0 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ipython-&gt;ezpz) (2.17.2)\nRequirement already satisfied: stack-data in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ipython-&gt;ezpz) (0.6.3)\nRequirement already satisfied: traitlets&gt;=5.13.0 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ipython-&gt;ezpz) (5.14.3)\nRequirement already satisfied: typing-extensions&gt;=4.6 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ipython-&gt;ezpz) (4.11.0)\nRequirement already satisfied: pexpect&gt;4.3 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from ipython-&gt;ezpz) (4.9.0)\nRequirement already satisfied: numpy&gt;=1.22 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from jax-&gt;ezpz) (1.26.4)\nRequirement already satisfied: opt-einsum in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from jax-&gt;ezpz) (3.3.0)\nRequirement already satisfied: scipy&gt;=1.9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from jax-&gt;ezpz) (1.13.0)\nRequirement already satisfied: typeguard==2.13.3 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from jaxtyping-&gt;ezpz) (2.13.3)\nRequirement already satisfied: markdown-it-py&gt;=2.2.0 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from rich-&gt;ezpz) (3.0.0)\nRequirement already satisfied: pandas&gt;=1.2 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from seaborn-&gt;ezpz) (2.2.2)\nRequirement already satisfied: absl-py&gt;=0.4 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from tensorboard-&gt;ezpz) (2.1.0)\nRequirement already satisfied: grpcio&gt;=1.48.2 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from tensorboard-&gt;ezpz) (1.62.2)\nRequirement already satisfied: markdown&gt;=2.6.8 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from tensorboard-&gt;ezpz) (3.6)\nRequirement already satisfied: protobuf!=4.24.0,&gt;=3.19.6 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from tensorboard-&gt;ezpz) (3.20.3)\nRequirement already satisfied: setuptools&gt;=41.0.0 in ./venvs/2024-04-29/lib/python3.11/site-packages (from tensorboard-&gt;ezpz) (65.5.0)\nRequirement already satisfied: six&gt;1.9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from tensorboard-&gt;ezpz) (1.16.0)\nRequirement already satisfied: tensorboard-data-server&lt;0.8.0,&gt;=0.7.0 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from tensorboard-&gt;ezpz) (0.7.2)\nRequirement already satisfied: werkzeug&gt;=1.0.1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from tensorboard-&gt;ezpz) (3.0.2)\nRequirement already satisfied: filelock in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from torch-&gt;ezpz) (3.13.1)\nRequirement already satisfied: sympy in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from torch-&gt;ezpz) (1.12)\nRequirement already satisfied: networkx in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from torch-&gt;ezpz) (3.3)\nRequirement already satisfied: jinja2 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from torch-&gt;ezpz) (3.0.3)\nRequirement already satisfied: fsspec in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from torch-&gt;ezpz) (2024.3.1)\nRequirement already satisfied: Click!=8.0.0,&gt;=7.1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from wandb-&gt;ezpz) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,&gt;=1.0.0 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from wandb-&gt;ezpz) (3.1.43)\nRequirement already satisfied: psutil&gt;=5.0.0 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from wandb-&gt;ezpz) (5.9.8)\nRequirement already satisfied: sentry-sdk&gt;=1.0.0 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from wandb-&gt;ezpz) (2.0.1)\nRequirement already satisfied: docker-pycreds&gt;=0.4.0 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from wandb-&gt;ezpz) (0.4.0)\nRequirement already satisfied: setproctitle in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from wandb-&gt;ezpz) (1.3.3)\nRequirement already satisfied: appdirs&gt;=1.4.3 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from wandb-&gt;ezpz) (1.4.4)\nRequirement already satisfied: gitdb&lt;5,&gt;=4.0.1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from GitPython!=3.1.29,&gt;=1.0.0-&gt;wandb-&gt;ezpz) (4.0.11)\nRequirement already satisfied: parso&lt;0.9.0,&gt;=0.8.3 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from jedi&gt;=0.16-&gt;ipython-&gt;ezpz) (0.8.4)\nRequirement already satisfied: mdurl~=0.1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from markdown-it-py&gt;=2.2.0-&gt;rich-&gt;ezpz) (0.1.2)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from matplotlib-&gt;ambivalent-&gt;ezpz) (1.2.1)\nRequirement already satisfied: cycler&gt;=0.10 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from matplotlib-&gt;ambivalent-&gt;ezpz) (0.12.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from matplotlib-&gt;ambivalent-&gt;ezpz) (4.51.0)\nRequirement already satisfied: kiwisolver&gt;=1.3.1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from matplotlib-&gt;ambivalent-&gt;ezpz) (1.4.5)\nRequirement already satisfied: pillow&gt;=8 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from matplotlib-&gt;ambivalent-&gt;ezpz) (10.3.0)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from matplotlib-&gt;ambivalent-&gt;ezpz) (3.1.2)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from matplotlib-&gt;ambivalent-&gt;ezpz) (2.9.0.post0)\nRequirement already satisfied: pytz&gt;=2020.1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from pandas&gt;=1.2-&gt;seaborn-&gt;ezpz) (2024.1)\nRequirement already satisfied: tzdata&gt;=2022.7 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from pandas&gt;=1.2-&gt;seaborn-&gt;ezpz) (2024.1)\nRequirement already satisfied: ptyprocess&gt;=0.5 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from pexpect&gt;4.3-&gt;ipython-&gt;ezpz) (0.7.0)\nRequirement already satisfied: wcwidth in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from prompt-toolkit&lt;3.1.0,&gt;=3.0.41-&gt;ipython-&gt;ezpz) (0.2.13)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from requests-&gt;ambivalent-&gt;ezpz) (2.0.4)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from requests-&gt;ambivalent-&gt;ezpz) (3.4)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from requests-&gt;ambivalent-&gt;ezpz) (2.1.0)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from requests-&gt;ambivalent-&gt;ezpz) (2024.2.2)\nRequirement already satisfied: MarkupSafe&gt;=2.1.1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from werkzeug&gt;=1.0.1-&gt;tensorboard-&gt;ezpz) (2.1.3)\nRequirement already satisfied: executing&gt;=1.2.0 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from stack-data-&gt;ipython-&gt;ezpz) (2.0.1)\nRequirement already satisfied: asttokens&gt;=2.1.0 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from stack-data-&gt;ipython-&gt;ezpz) (2.4.1)\nRequirement already satisfied: pure-eval in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from stack-data-&gt;ipython-&gt;ezpz) (0.2.2)\nRequirement already satisfied: mpmath&gt;=0.19 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from sympy-&gt;torch-&gt;ezpz) (1.3.0)\nRequirement already satisfied: smmap&lt;6,&gt;=3.0.1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (from gitdb&lt;5,&gt;=4.0.1-&gt;GitPython!=3.1.29,&gt;=1.0.0-&gt;wandb-&gt;ezpz) (5.0.1)\nDownloading pyinstrument-4.6.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (104 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 104.9/104.9 kB 3.1 MB/s eta 0:00:00\nBuilding wheels for collected packages: ezpz\n  Building editable for ezpz (pyproject.toml) ... done\n  Created wheel for ezpz: filename=ezpz-0.1-py3-none-any.whl size=10104 sha256=f73fbc552c6192f2d1575c08528267c1c70bd4ed2eebab011c692b4cf66fd9cb\n  Stored in directory: /tmp/pip-ephem-wheel-cache-6xb0tqk8/wheels/b3/57/90/f3324177d75cbc607a034b5b8e66d5b3d35dcf087967430718\nSuccessfully built ezpz\nInstalling collected packages: pyinstrument, ezpz\n  Attempting uninstall: ezpz\n    Found existing installation: ezpz 0.1\n    Not uninstalling ezpz at /home/foremans/.local/polaris/conda/2024-04-29/lib/python3.11/site-packages, outside environment /home/foremans/tmp/foremans/2024-07-15-124441/venvs/2024-04-29\n    Cant uninstall 'ezpz'. No files were found to uninstall.\nSuccessfully installed ezpz pyinstrument-4.6.2\n\n[notice] A new release of pip is available: 24.0 -&gt; 24.1.2\n[notice] To update, run: pip install --upgrade pip\n9.63s user 1.44s system 60% cpu 18.301s total\nConnected to tcp://x3006c0s13b0n0.hsn.cm.polaris.alcf.anl.gov:7919\nFound executable /home/foremans/tmp/foremans/2024-07-15-124441/venvs/2024-04-29/bin/python3\nLaunching application 5ccc89be-4289-49f5-8b4e-64104021b3c5\n[2024-07-15 12:46:24.601903][INFO][__init__:156] - Setting logging level to 'INFO' on 'RANK == 0'\n[2024-07-15 12:46:24.604160][INFO][__init__:157] - Setting logging level to 'CRITICAL' on all others 'RANK != 0'\n[2024-07-15 12:46:24.604624][INFO][__init__:160] - To disable this behavior, and log from ALL ranks (not recommended), set: 'export LOG_FROM_ALL_RANKS=1'  in your environment, and re-run.\nwandb: WARNING require() unsupported requirement: core\nwandb: ERROR Supported wandb.require() features can be found at: https://wandb.me/library-require\nwandb: WARNING require() unsupported requirement: core\nwandb: ERROR Supported wandb.require() features can be found at: https://wandb.me/library-require\nwandb: WARNING require() unsupported requirement: core\nwandb: ERROR Supported wandb.require() features can be found at: https://wandb.me/library-require\nwandb: WARNING require() unsupported requirement: core\nwandb: ERROR Supported wandb.require() features can be found at: https://wandb.me/library-require\n[2024-07-15 12:46:26.437667][INFO][dist:358] - [device='cuda'][rank=1/3][local_rank=1/3][node=0/0]\n[2024-07-15 12:46:26.437716][INFO][dist:358] - [device='cuda'][rank=3/3][local_rank=3/3][node=0/0]\n[2024-07-15 12:46:26.438619][INFO][dist:358] - [device='cuda'][rank=2/3][local_rank=2/3][node=0/0]\n[2024-07-15 12:46:26.444402][INFO][dist:95] -\n\n[dist_info]:\n  • DEVICE=cuda\n  • DEVICE_ID=cuda:0\n  • DISTRIBUTED_BACKEND=nccl\n  • GPUS_PER_NODE=4\n  • HOSTS=['x3006c0s13b0n0.hsn.cm.polaris.alcf.anl.gov']\n  • HOSTFILE=/var/spool/pbs/aux/2021158.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov\n  • HOSTNAME=x3006c0s13b0n0.hsn.cm.polaris.alcf.anl.gov\n  • LOCAL_RANK=0\n  • MACHINE=Polaris\n  • NUM_NODES=1\n  • NGPUS=4\n  • NGPUS_AVAILABLE=4\n  • NODE_ID=0\n  • RANK=0\n  • SCHEDULER=PBS\n  • WORLD_SIZE_TOTAL=4\n  • WORLD_SIZE_IN_USE=4\n  • LAUNCH_CMD=mpiexec --verbose --envall -n 4 -ppn 4 --hostfile /var/spool/pbs/aux/2021158.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov --cpu-bind depth -d 16\n\n\n[2024-07-15 12:46:26.447254][INFO][dist:725] - [0/4] Using device='cuda' with backend='DDP' + 'nccl' for distributed training.\n[2024-07-15 12:46:26.451604][INFO][dist:358] - [device='cuda'][rank=0/3][local_rank=0/3][node=0/0]\n[2024-07-15 12:46:26.452120][WARNING][dist:364] - Using [4 / 4] available \"cuda\" devices !!\n[2024-07-15 12:46:26.452676][INFO][dist:95] -\n\n[timers_import]:\n  • os=1.1026859283447266e-06\n  • logging=4.507601261138916e-07\n  • typing=2.9457733035087585e-06\n  • pathlib=1.2619420886039734e-06\n  • ezpz=6.109476089477539e-07\n  • torch=3.5976991057395935e-06\n  • torch_ddp=2.3636966943740845e-06\n  • wandb=6.36400654911995e-05\n  • total=7.597357034683228e-05\n\n\n[2024-07-15 12:46:26.453718][INFO][dist:95] -\n\n[CONFIG]:\n  • warmup=0\n  • log_freq=1\n  • batch_size=64\n  • input_size=128\n  • output_size=128\n  • dtype=torch.float32\n  • device=cuda\n  • world_size=4\n  • train_iters=100\n\n\n[2024-07-15 12:46:28.048558][INFO][test_dist:183] - model=Network(\n  (layers): Sequential(\n    (0): Linear(in_features=128, out_features=1024, bias=True)\n    (1): Linear(in_features=1024, out_features=512, bias=True)\n    (2): Linear(in_features=512, out_features=256, bias=True)\n    (3): Linear(in_features=256, out_features=128, bias=True)\n    (4): Linear(in_features=128, out_features=128, bias=True)\n  )\n)\n[2024-07-15 12:46:30.303706][INFO][test_dist:274] - iter=1, loss=1977.36, sps=1.829e+04, dt=0.00349948, dtf=0.00097, dtb=0.00253\n[2024-07-15 12:46:30.307445][INFO][test_dist:274] - iter=2, loss=1521.61, sps=3.179e+04, dt=0.00201315, dtf=0.0005266, dtb=0.001487\n[2024-07-15 12:46:30.310480][INFO][test_dist:274] - iter=3, loss=1101.45, sps=3.636e+04, dt=0.00176022, dtf=0.0005615, dtb=0.001199\n[2024-07-15 12:46:30.313460][INFO][test_dist:274] - iter=4, loss=907.963, sps=3.725e+04, dt=0.00171805, dtf=0.0005335, dtb=0.001185\n[2024-07-15 12:46:30.316456][INFO][test_dist:274] - iter=5, loss=842.827, sps=3.698e+04, dt=0.0017307, dtf=0.0005462, dtb=0.001184\n[2024-07-15 12:46:30.319577][INFO][test_dist:274] - iter=6, loss=781.918, sps=3.443e+04, dt=0.00185873, dtf=0.0006062, dtb=0.001253\n[2024-07-15 12:46:30.322847][INFO][test_dist:274] - iter=7, loss=778.317, sps=3.253e+04, dt=0.00196762, dtf=0.0005706, dtb=0.001397\n[2024-07-15 12:46:30.325813][INFO][test_dist:274] - iter=8, loss=746.629, sps=3.746e+04, dt=0.00170848, dtf=0.0005085, dtb=0.0012\n[2024-07-15 12:46:30.328695][INFO][test_dist:274] - iter=9, loss=736.385, sps=3.913e+04, dt=0.00163572, dtf=0.0004885, dtb=0.001147\n[2024-07-15 12:46:30.331672][INFO][test_dist:274] - iter=10, loss=733.067, sps=3.713e+04, dt=0.00172381, dtf=0.0004742, dtb=0.00125\n[2024-07-15 12:46:30.334841][INFO][test_dist:274] - iter=11, loss=729.301, sps=3.342e+04, dt=0.00191528, dtf=0.0003869, dtb=0.001528\n[2024-07-15 12:46:30.338105][INFO][test_dist:274] - iter=12, loss=701.243, sps=3.198e+04, dt=0.00200144, dtf=0.0004697, dtb=0.001532\n[2024-07-15 12:46:30.340953][INFO][test_dist:274] - iter=13, loss=681.563, sps=4.088e+04, dt=0.00156543, dtf=0.0004341, dtb=0.001131\n[2024-07-15 12:46:30.343843][INFO][test_dist:274] - iter=14, loss=685.259, sps=4.053e+04, dt=0.00157891, dtf=0.0004456, dtb=0.001133\n[2024-07-15 12:46:30.346744][INFO][test_dist:274] - iter=15, loss=678.99, sps=3.935e+04, dt=0.00162651, dtf=0.0004026, dtb=0.001224\n[2024-07-15 12:46:30.349674][INFO][test_dist:274] - iter=16, loss=672.654, sps=3.801e+04, dt=0.00168398, dtf=0.0004565, dtb=0.001227\n[2024-07-15 12:46:30.352400][INFO][test_dist:274] - iter=17, loss=660.552, sps=4.319e+04, dt=0.00148184, dtf=0.0003963, dtb=0.001085\n[2024-07-15 12:46:30.355521][INFO][test_dist:274] - iter=18, loss=640.175, sps=3.408e+04, dt=0.00187814, dtf=0.0005312, dtb=0.001347\n[2024-07-15 12:46:30.358530][INFO][test_dist:274] - iter=19, loss=645.41, sps=3.65e+04, dt=0.00175325, dtf=0.0004639, dtb=0.001289\n[2024-07-15 12:46:30.361515][INFO][test_dist:274] - iter=20, loss=649.203, sps=3.675e+04, dt=0.00174155, dtf=0.0004799, dtb=0.001262\n[2024-07-15 12:46:30.364256][INFO][test_dist:274] - iter=21, loss=633.891, sps=4.277e+04, dt=0.00149629, dtf=0.000416, dtb=0.00108\n[2024-07-15 12:46:30.367135][INFO][test_dist:274] - iter=22, loss=623.202, sps=3.892e+04, dt=0.00164444, dtf=0.0004509, dtb=0.001194\n[2024-07-15 12:46:30.370090][INFO][test_dist:274] - iter=23, loss=623.178, sps=3.771e+04, dt=0.00169695, dtf=0.0004252, dtb=0.001272\n[2024-07-15 12:46:30.373119][INFO][test_dist:274] - iter=24, loss=626.489, sps=3.69e+04, dt=0.00173444, dtf=0.0004551, dtb=0.001279\n[2024-07-15 12:46:30.375951][INFO][test_dist:274] - iter=25, loss=636.674, sps=4.089e+04, dt=0.0015651, dtf=0.0004223, dtb=0.001143\n[2024-07-15 12:46:30.378913][INFO][test_dist:274] - iter=26, loss=639.64, sps=3.758e+04, dt=0.00170305, dtf=0.0004532, dtb=0.00125\n[2024-07-15 12:46:30.381808][INFO][test_dist:274] - iter=27, loss=605.015, sps=3.874e+04, dt=0.00165192, dtf=0.0004257, dtb=0.001226\n[2024-07-15 12:46:30.384573][INFO][test_dist:274] - iter=28, loss=603.894, sps=4.244e+04, dt=0.00150807, dtf=0.0004296, dtb=0.001078\n[2024-07-15 12:46:30.387388][INFO][test_dist:274] - iter=29, loss=619.885, sps=4.03e+04, dt=0.00158808, dtf=0.0004196, dtb=0.001168\n[2024-07-15 12:46:30.390148][INFO][test_dist:274] - iter=30, loss=589.771, sps=4.21e+04, dt=0.0015203, dtf=0.0004438, dtb=0.001076\n[2024-07-15 12:46:30.392838][INFO][test_dist:274] - iter=31, loss=595.523, sps=4.381e+04, dt=0.001461, dtf=0.0004153, dtb=0.001046\n[2024-07-15 12:46:30.395622][INFO][test_dist:274] - iter=32, loss=605.537, sps=4.104e+04, dt=0.00155956, dtf=0.0004367, dtb=0.001123\n[2024-07-15 12:46:30.398489][INFO][test_dist:274] - iter=33, loss=586.025, sps=3.913e+04, dt=0.00163565, dtf=0.0003743, dtb=0.001261\n[2024-07-15 12:46:30.401355][INFO][test_dist:274] - iter=34, loss=577.14, sps=3.877e+04, dt=0.0016506, dtf=0.0004581, dtb=0.001192\n[2024-07-15 12:46:30.404092][INFO][test_dist:274] - iter=35, loss=568.886, sps=4.383e+04, dt=0.00146019, dtf=0.0003966, dtb=0.001064\n[2024-07-15 12:46:30.406843][INFO][test_dist:274] - iter=36, loss=567.26, sps=4.228e+04, dt=0.00151377, dtf=0.0004537, dtb=0.00106\n[2024-07-15 12:46:30.409591][INFO][test_dist:274] - iter=37, loss=574.633, sps=4.226e+04, dt=0.00151457, dtf=0.0003845, dtb=0.00113\n[2024-07-15 12:46:30.412347][INFO][test_dist:274] - iter=38, loss=557.928, sps=4.212e+04, dt=0.00151945, dtf=0.000457, dtb=0.001062\n[2024-07-15 12:46:30.415130][INFO][test_dist:274] - iter=39, loss=558.186, sps=4.135e+04, dt=0.00154767, dtf=0.0003975, dtb=0.00115\n[2024-07-15 12:46:30.417880][INFO][test_dist:274] - iter=40, loss=553.377, sps=4.233e+04, dt=0.00151185, dtf=0.0004556, dtb=0.001056\n[2024-07-15 12:46:30.420583][INFO][test_dist:274] - iter=41, loss=542.918, sps=4.394e+04, dt=0.00145645, dtf=0.0003928, dtb=0.001064\n[2024-07-15 12:46:30.423489][INFO][test_dist:274] - iter=42, loss=547.64, sps=3.827e+04, dt=0.00167242, dtf=0.0004762, dtb=0.001196\n[2024-07-15 12:46:30.426243][INFO][test_dist:274] - iter=43, loss=546.106, sps=4.22e+04, dt=0.00151648, dtf=0.0004665, dtb=0.00105\n[2024-07-15 12:46:30.428998][INFO][test_dist:274] - iter=44, loss=535.946, sps=4.209e+04, dt=0.0015204, dtf=0.0004679, dtb=0.001053\n[2024-07-15 12:46:30.431749][INFO][test_dist:274] - iter=45, loss=534.731, sps=4.324e+04, dt=0.00148002, dtf=0.0003821, dtb=0.001098\n[2024-07-15 12:46:30.434709][INFO][test_dist:274] - iter=46, loss=520.207, sps=3.74e+04, dt=0.00171109, dtf=0.0004486, dtb=0.001263\n[2024-07-15 12:46:30.437524][INFO][test_dist:274] - iter=47, loss=527.301, sps=4.191e+04, dt=0.00152713, dtf=0.0004943, dtb=0.001033\n[2024-07-15 12:46:30.440277][INFO][test_dist:274] - iter=48, loss=516.108, sps=4.279e+04, dt=0.00149561, dtf=0.0004418, dtb=0.001054\n[2024-07-15 12:46:30.443029][INFO][test_dist:274] - iter=49, loss=516.086, sps=4.231e+04, dt=0.00151262, dtf=0.0003899, dtb=0.001123\n[2024-07-15 12:46:30.445895][INFO][test_dist:274] - iter=50, loss=508.945, sps=3.922e+04, dt=0.00163198, dtf=0.0004333, dtb=0.001199\n[2024-07-15 12:46:30.448667][INFO][test_dist:274] - iter=51, loss=513.235, sps=4.163e+04, dt=0.00153721, dtf=0.0004863, dtb=0.001051\n[2024-07-15 12:46:30.451581][INFO][test_dist:274] - iter=52, loss=511.549, sps=3.822e+04, dt=0.00167444, dtf=0.0004698, dtb=0.001205\n[2024-07-15 12:46:30.454412][INFO][test_dist:274] - iter=53, loss=510.211, sps=4.042e+04, dt=0.00158328, dtf=0.0004193, dtb=0.001164\n[2024-07-15 12:46:30.457220][INFO][test_dist:274] - iter=54, loss=492.431, sps=4.151e+04, dt=0.00154189, dtf=0.0004324, dtb=0.001109\n[2024-07-15 12:46:30.460039][INFO][test_dist:274] - iter=55, loss=499.074, sps=4.121e+04, dt=0.00155284, dtf=0.0005013, dtb=0.001052\n[2024-07-15 12:46:30.462836][INFO][test_dist:274] - iter=56, loss=490.718, sps=4.108e+04, dt=0.0015581, dtf=0.0004388, dtb=0.001119\n[2024-07-15 12:46:30.465621][INFO][test_dist:274] - iter=57, loss=493.223, sps=4.157e+04, dt=0.00153946, dtf=0.0003867, dtb=0.001153\n[2024-07-15 12:46:30.468390][INFO][test_dist:274] - iter=58, loss=486.601, sps=4.256e+04, dt=0.00150368, dtf=0.0004462, dtb=0.001057\n[2024-07-15 12:46:30.471148][INFO][test_dist:274] - iter=59, loss=473.609, sps=4.326e+04, dt=0.00147947, dtf=0.0004221, dtb=0.001057\n[2024-07-15 12:46:30.473847][INFO][test_dist:274] - iter=60, loss=478.577, sps=4.378e+04, dt=0.00146188, dtf=0.0004284, dtb=0.001033\n[2024-07-15 12:46:30.476547][INFO][test_dist:274] - iter=61, loss=475.991, sps=4.387e+04, dt=0.00145878, dtf=0.0003923, dtb=0.001066\n[2024-07-15 12:46:30.479447][INFO][test_dist:274] - iter=62, loss=471.526, sps=3.859e+04, dt=0.00165854, dtf=0.0004523, dtb=0.001206\n[2024-07-15 12:46:30.482180][INFO][test_dist:274] - iter=63, loss=460.986, sps=4.277e+04, dt=0.00149626, dtf=0.0003914, dtb=0.001105\n[2024-07-15 12:46:30.484857][INFO][test_dist:274] - iter=64, loss=464.409, sps=4.404e+04, dt=0.00145311, dtf=0.0004247, dtb=0.001028\n[2024-07-15 12:46:30.487652][INFO][test_dist:274] - iter=65, loss=458.023, sps=4.104e+04, dt=0.00155961, dtf=0.0003934, dtb=0.001166\n[2024-07-15 12:46:30.490474][INFO][test_dist:274] - iter=66, loss=456.718, sps=4.013e+04, dt=0.00159499, dtf=0.000416, dtb=0.001179\n[2024-07-15 12:46:30.493187][INFO][test_dist:274] - iter=67, loss=451.993, sps=4.296e+04, dt=0.00148977, dtf=0.000395, dtb=0.001095\n[2024-07-15 12:46:30.495894][INFO][test_dist:274] - iter=68, loss=454.66, sps=4.4e+04, dt=0.0014545, dtf=0.000425, dtb=0.001029\n[2024-07-15 12:46:30.498664][INFO][test_dist:274] - iter=69, loss=451.727, sps=4.17e+04, dt=0.00153459, dtf=0.0003837, dtb=0.001151\n[2024-07-15 12:46:30.501502][INFO][test_dist:274] - iter=70, loss=440.922, sps=4.015e+04, dt=0.00159391, dtf=0.0004272, dtb=0.001167\n[2024-07-15 12:46:30.504271][INFO][test_dist:274] - iter=71, loss=442.788, sps=4.322e+04, dt=0.00148083, dtf=0.0004178, dtb=0.001063\n[2024-07-15 12:46:30.507000][INFO][test_dist:274] - iter=72, loss=439.069, sps=4.307e+04, dt=0.00148594, dtf=0.0004285, dtb=0.001057\n[2024-07-15 12:46:30.509755][INFO][test_dist:274] - iter=73, loss=430.236, sps=4.211e+04, dt=0.00151976, dtf=0.0003829, dtb=0.001137\n[2024-07-15 12:46:30.512494][INFO][test_dist:274] - iter=74, loss=428.951, sps=4.318e+04, dt=0.00148219, dtf=0.0004357, dtb=0.001046\n[2024-07-15 12:46:30.515244][INFO][test_dist:274] - iter=75, loss=430.417, sps=4.253e+04, dt=0.00150487, dtf=0.0004357, dtb=0.001069\n[2024-07-15 12:46:30.517921][INFO][test_dist:274] - iter=76, loss=416.647, sps=4.401e+04, dt=0.00145412, dtf=0.0004374, dtb=0.001017\n[2024-07-15 12:46:30.520610][INFO][test_dist:274] - iter=77, loss=422.518, sps=4.468e+04, dt=0.0014323, dtf=0.0003941, dtb=0.001038\n[2024-07-15 12:46:30.523343][INFO][test_dist:274] - iter=78, loss=412.028, sps=4.272e+04, dt=0.00149821, dtf=0.0004521, dtb=0.001046\n[2024-07-15 12:46:30.526016][INFO][test_dist:274] - iter=79, loss=406.225, sps=4.473e+04, dt=0.00143094, dtf=0.0003893, dtb=0.001042\n[2024-07-15 12:46:30.528730][INFO][test_dist:274] - iter=80, loss=402.887, sps=4.382e+04, dt=0.00146036, dtf=0.0004402, dtb=0.00102\n[2024-07-15 12:46:30.531536][INFO][test_dist:274] - iter=81, loss=397.311, sps=4.069e+04, dt=0.0015729, dtf=0.000404, dtb=0.001169\n[2024-07-15 12:46:30.534242][INFO][test_dist:274] - iter=82, loss=411.916, sps=4.39e+04, dt=0.00145795, dtf=0.0004351, dtb=0.001023\n[2024-07-15 12:46:30.537005][INFO][test_dist:274] - iter=83, loss=402.795, sps=4.366e+04, dt=0.00146599, dtf=0.0004252, dtb=0.001041\n[2024-07-15 12:46:30.539805][INFO][test_dist:274] - iter=84, loss=391.05, sps=4.1e+04, dt=0.00156095, dtf=0.0004323, dtb=0.001129\n[2024-07-15 12:46:30.542590][INFO][test_dist:274] - iter=85, loss=383.782, sps=4.118e+04, dt=0.00155416, dtf=0.000388, dtb=0.001166\n[2024-07-15 12:46:30.545290][INFO][test_dist:274] - iter=86, loss=399.543, sps=4.396e+04, dt=0.00145595, dtf=0.0004339, dtb=0.001022\n[2024-07-15 12:46:30.547992][INFO][test_dist:274] - iter=87, loss=379.003, sps=4.456e+04, dt=0.00143613, dtf=0.0004131, dtb=0.001023\n[2024-07-15 12:46:30.550837][INFO][test_dist:274] - iter=88, loss=372.048, sps=3.998e+04, dt=0.00160092, dtf=0.0004375, dtb=0.001163\n[2024-07-15 12:46:30.553641][INFO][test_dist:274] - iter=89, loss=376.187, sps=4.137e+04, dt=0.0015471, dtf=0.0004142, dtb=0.001133\n[2024-07-15 12:46:30.556354][INFO][test_dist:274] - iter=90, loss=372.281, sps=4.408e+04, dt=0.00145186, dtf=0.0004239, dtb=0.001028\n[2024-07-15 12:46:30.559101][INFO][test_dist:274] - iter=91, loss=370.701, sps=4.252e+04, dt=0.00150523, dtf=0.0004545, dtb=0.001051\n[2024-07-15 12:46:30.561884][INFO][test_dist:274] - iter=92, loss=356.074, sps=4.191e+04, dt=0.00152712, dtf=0.0004291, dtb=0.001098\n[2024-07-15 12:46:30.564556][INFO][test_dist:274] - iter=93, loss=360.663, sps=4.468e+04, dt=0.00143241, dtf=0.0003938, dtb=0.001039\n[2024-07-15 12:46:30.567287][INFO][test_dist:274] - iter=94, loss=374.599, sps=4.296e+04, dt=0.0014897, dtf=0.0004539, dtb=0.001036\n[2024-07-15 12:46:30.570088][INFO][test_dist:274] - iter=95, loss=364.476, sps=4.253e+04, dt=0.00150469, dtf=0.0004189, dtb=0.001086\n[2024-07-15 12:46:30.572767][INFO][test_dist:274] - iter=96, loss=358.992, sps=4.415e+04, dt=0.00144967, dtf=0.0004295, dtb=0.00102\n[2024-07-15 12:46:30.575567][INFO][test_dist:274] - iter=97, loss=354.959, sps=4.092e+04, dt=0.0015641, dtf=0.0004054, dtb=0.001159\n[2024-07-15 12:46:30.578287][INFO][test_dist:274] - iter=98, loss=345.644, sps=4.374e+04, dt=0.00146311, dtf=0.0004197, dtb=0.001043\n[2024-07-15 12:46:30.580996][INFO][test_dist:274] - iter=99, loss=348.209, sps=4.391e+04, dt=0.00145758, dtf=0.0004263, dtb=0.001031\n                             train/dt [2024-07-15-124630]\n       ┌───────────────────────────────────────────────────────────────────────┐\n0.00350┤▘                                                                      │\n       │                                                                       │\n       │                                                                       │\n0.00315┤                                                                       │\n       │                                                                       │\n       │                                                                       │\n0.00281┤                                                                       │\n       │                                                                       │\n0.00247┤                                                                       │\n       │                                                                       │\n       │                                                                       │\n0.00212┤                                                                       │\n       │▗       ▖                                                              │\n       │    ▝  ▖    ▖                                                          │\n0.00178┤ ▗ ▝                                                                   │\n       │  ▘▘ ▘▝    ▖ ▀ ▖▀ ▚    ▗     ▗  ▝   ▗       ▖                          │\n       │      ▘ ▗▝▘      ▗  ▘▖▗▘   ▗   ▖ ▖ ▝▖▝▖▄▗     ▖▘ ▞  ▖    ▗ ▗▗ ▝▗ ▗   ▖ │\n0.00143┤           ▝  ▝    ▝ ▝  ▗▘▀ ▘▖▘▝  ▀      ▀▝▖▝▗ ▝▖ ▝▘▝▘▄▝▖▖▗▘ ▖▖ ▞ ▖▀▗ ▚│\n       └┬─────────────────┬────────────────┬─────────────────┬────────────────┬┘\n       1.0              25.5             50.0              74.5            99.0\ntrain/dt                                 iter\n[2024-07-15 12:46:30.627053][INFO][plot:156] - Appending plot to: /home/foremans/tmp/foremans/2024-07-15-124441/test-dist-plots/train/dt.txt\ntext saved in /home/foremans/tmp/foremans/2024-07-15-124441/test-dist-plots/train/dt.txt\n                             train/dtf [2024-07-15-124630]\n       ┌───────────────────────────────────────────────────────────────────────┐\n0.00097┤▘                                                                      │\n       │                                                                       │\n       │                                                                       │\n0.00087┤                                                                       │\n       │                                                                       │\n       │                                                                       │\n0.00077┤                                                                       │\n       │                                                                       │\n0.00067┤                                                                       │\n       │                                                                       │\n       │   ▗                                                                   │\n0.00057┤    ▗                                                                  │\n       │ ▝ ▖                                                                   │\n       │▝ ▘  ▖      ▘                                                          │\n0.00047┤      ▚ ▖    ▗               ▗ ▖ ▘  ▚  ▘                               │\n       │        ▗▗ ▘ ▘ ▘▝ ▘  ▖▗▝ ▘▝ ▘ ▘ ▗ ▖    ▗ ▖  ▘       ▗▖▖▝ ▖▗  ▖▗ ▝  ▘   │\n       │          ▖   ▗ ▘▝▝▝▘▗             ▝ ▝▘  ▝▝  ▝ ▖▘▝▝▘     ▗ ▀  ▖▗▘▝ ▝▝▖▀│\n0.00037┤       ▘   ▝           ▖▝ ▘▝ ▘ ▗  ▝     ▝  ▘▝ ▘▝ ▘  ▘ ▝ ▘   ▝     ▘    │\n       └┬─────────────────┬────────────────┬─────────────────┬────────────────┬┘\n       1.0              25.5             50.0              74.5            99.0\ntrain/dtf                                iter\n[2024-07-15 12:46:30.638406][INFO][plot:156] - Appending plot to: /home/foremans/tmp/foremans/2024-07-15-124441/test-dist-plots/train/dtf.txt\ntext saved in /home/foremans/tmp/foremans/2024-07-15-124441/test-dist-plots/train/dtf.txt\n                             train/dtb [2024-07-15-124630]\n       ┌───────────────────────────────────────────────────────────────────────┐\n0.00253┤▘                                                                      │\n       │                                                                       │\n       │                                                                       │\n0.00228┤                                                                       │\n       │                                                                       │\n       │                                                                       │\n0.00203┤                                                                       │\n       │                                                                       │\n0.00177┤                                                                       │\n       │                                                                       │\n       │                                                                       │\n0.00152┤       ▖▖                                                              │\n       │▝                                                                      │\n       │    ▝       ▖                                                          │\n0.00127┤             ▄  ▄      ▖        ▗                                      │\n       │ ▗▖▞ ▖▝   ▘▘   ▖  ▀ ▖  ▗     ▗     ▗▗       ▖  ▖         ▗             │\n       │      ▘ ▝▝ ▗     ▝    ▗   ▘▝   ▗  ▗  ▝▖▗▝   ▗ ▘▗ ▀  ▘      ▝▝ ▝▝ ▗ ▗ ▘ │\n0.00102┤              ▝    ▝ ▀  ▝▘▝ ▘▘▘▘ ▖▘ ▘  ▘ ▀▗▘ ▗  ▖ ▝▘▝▘▄▝▘▖▗▘ ▖▖ ▞ ▘▖▗ ▚│\n       └┬─────────────────┬────────────────┬─────────────────┬────────────────┬┘\n       1.0              25.5             50.0              74.5            99.0\ntrain/dtb                                iter\n[2024-07-15 12:46:30.648950][INFO][plot:156] - Appending plot to: /home/foremans/tmp/foremans/2024-07-15-124441/test-dist-plots/train/dtb.txt\ntext saved in /home/foremans/tmp/foremans/2024-07-15-124441/test-dist-plots/train/dtb.txt\n                            train/loss [2024-07-15-124630]\n      ┌────────────────────────────────────────────────────────────────────────┐\n1977.4┤▘                                                                       │\n      │                                                                        │\n      │                                                                        │\n1705.4┤                                                                        │\n      │                                                                        │\n      │▝                                                                       │\n1433.5┤                                                                        │\n      │                                                                        │\n      │                                                                        │\n1161.5┤ ▗                                                                      │\n      │                                                                        │\n 889.5┤  ▖                                                                     │\n      │   ▘                                                                    │\n      │   ▝▝▘▄▗▖                                                               │\n 617.6┤         ▀▘▀▗▖▚▗▖▄▖▄▗ ▗                                                 │\n      │                     ▘▘▝▘▀▝▀▗▖▄▗▖▄▗▖▄▖▖                                 │\n      │                                      ▝▝▘▀▝▘▀▖▚▗▖▄▗▖▄▗▄▗                │\n 345.6┤                                                        ▘▀▝▘▀▝▀▝▘▀▗▖▚▗▖▄│\n      └┬─────────────────┬─────────────────┬────────────────┬─────────────────┬┘\n      1.0              25.5              50.0             74.5             99.0\ntrain/loss                               iter\n[2024-07-15 12:46:30.702717][INFO][plot:156] - Appending plot to: /home/foremans/tmp/foremans/2024-07-15-124441/test-dist-plots/train/loss.txt\ntext saved in /home/foremans/tmp/foremans/2024-07-15-124441/test-dist-plots/train/loss.txt\n                           train/iter [2024-07-15-124630]\n    ┌──────────────────────────────────────────────────────────────────────────┐\n99.0┤                                                                      ▗▗▖▀│\n    │                                                                   ▄▝▘▘   │\n    │                                                              ▗▖▞▝▘       │\n82.7┤                                                          ▄▗▘▀            │\n    │                                                      ▖▄▝▘                │\n    │                                                 ▗▗▖▀▝                    │\n66.3┤                                              ▄▝▘▘                        │\n    │                                         ▗▖▞▝▘                            │\n50.0┤                                     ▄▗▘▀                                 │\n    │                                 ▖▄▝▘                                     │\n    │                            ▗▗▖▀▝                                         │\n33.7┤                         ▄▝▘▘                                             │\n    │                    ▗▖▞▝▘                                                 │\n    │                ▄▗▘▀                                                      │\n17.3┤            ▖▄▝▘                                                          │\n    │       ▗▗▖▀▝                                                              │\n    │    ▄▝▘▘                                                                  │\n 1.0┤▖▞▝▘                                                                      │\n    └┬─────────────────┬──────────────────┬─────────────────┬─────────────────┬┘\n    1.0              25.5               50.0              74.5             99.0\ntrain/iter                              iter\n[2024-07-15 12:46:30.714042][INFO][plot:156] - Appending plot to: /home/foremans/tmp/foremans/2024-07-15-124441/test-dist-plots/train/iter.txt\ntext saved in /home/foremans/tmp/foremans/2024-07-15-124441/test-dist-plots/train/iter.txt\n                             train/sps [2024-07-15-124630]\n       ┌───────────────────────────────────────────────────────────────────────┐\n44725.9┤                     ▗  ▗    ▖            ▗▖ ▗  ▖     ▞ ▘▖▗▖ ▖▘ ▖ ▘ ▗ ▄│\n       │           ▝  ▗    ▗ ▖   ▖▄ ▖ ▖▞  ▄      ▞  ▗  ▝  ▝▘▞▖ ▗        ▗  ▚   │\n       │        ▗        ▗    ▗    ▝     ▘  ▘ ▘▄▝     ▖  ▘       ▗ ▗▗  ▝ ▝   ▖ │\n40319.7┤      ▖  ▝▖         ▘  ▖           ▗ ▝         ▘ ▝            ▝        │\n       │           ▖   ▘▖ ▞    ▝     ▗      ▗       ▘                          │\n       │ ▗▘▘ ▘▝      ▄  ▝               ▝                                      │\n35913.4┤                                                                       │\n       │   ▝   ▖    ▘                                                          │\n31507.2┤▗   ▝   ▖                                                              │\n       │                                                                       │\n       │                                                                       │\n27100.9┤                                                                       │\n       │                                                                       │\n       │                                                                       │\n22694.7┤                                                                       │\n       │                                                                       │\n       │                                                                       │\n18288.4┤▖                                                                      │\n       └┬─────────────────┬────────────────┬─────────────────┬────────────────┬┘\n       1.0              25.5             50.0              74.5            99.0\ntrain/sps                                iter\n[2024-07-15 12:46:30.724919][INFO][plot:156] - Appending plot to: /home/foremans/tmp/foremans/2024-07-15-124441/test-dist-plots/train/sps.txt\ntext saved in /home/foremans/tmp/foremans/2024-07-15-124441/test-dist-plots/train/sps.txt\n\n  _     ._   __/__   _ _  _  _ _/_   Recorded: 12:46:28  Samples:  2182\n /_//_/// /_\\ / //_// / //_'/ //     Duration: 2.688     CPU time: 2.356\n/   _/                      v4.6.2\n\nProgram: /home/foremans/tmp/foremans/2024-07-15-124441/venvs/2024-04-29/src/ezpz/src/ezpz/test_dist.py\n\n2.688 &lt;module&gt;  ezpz/test_dist.py:1\n└─ 2.687 main  ezpz/test_dist.py:217\n   ├─ 2.104 build_model_and_optimizer  ezpz/test_dist.py:171\n   │  └─ 2.089 Adam.__init__  torch/optim/adam.py:15\n   │        [142 frames hidden]  torch, transformers, jax, huggingface...\n   ├─ 0.199 _backward_step  ezpz/test_dist.py:236\n   │  ├─ 0.104 Tensor.backward  torch/_tensor.py:466\n   │  │     [4 frames hidden]  torch, &lt;built-in&gt;\n   │  └─ 0.094 wrapper  torch/optim/optimizer.py:374\n   │        [6 frames hidden]  torch, &lt;built-in&gt;\n   ├─ 0.145 tplot_dict  ezpz/plot.py:136\n   │  ├─ 0.085 show  plotext/_core.py:292\n   │  │     [5 frames hidden]  plotext\n   │  └─ 0.031 &lt;module&gt;  plotext/__init__.py:1\n   ├─ 0.136 _forward_step  ezpz/test_dist.py:231\n   │  ├─ 0.084 DistributedDataParallel._wrapped_call_impl  torch/nn/modules/module.py:1528\n   │  │     [6 frames hidden]  torch\n   │  │        0.071 Network._call_impl  torch/nn/modules/module.py:1534\n   │  │        └─ 0.071 Network.forward  ezpz/test_dist.py:164\n   │  │           └─ 0.071 Sequential._wrapped_call_impl  torch/nn/modules/module.py:1528\n   │  │                 [7 frames hidden]  torch, &lt;built-in&gt;\n   │  └─ 0.052 calc_loss  ezpz/test_dist.py:168\n   └─ 0.100 Logger.info  logging/__init__.py:1479\n         [6 frames hidden]  logging, rich\n            0.100 RichHandler.emit  rich/logging.py:126\n            └─ 0.098 Console.print  ezpz/log/console.py:79\n               └─ 0.098 Console.print  rich/console.py:1624\n                     [4 frames hidden]  rich\n\n\n[2024-07-15 12:46:30.921061][INFO][profile:115] - Saving pyinstrument profile output to: /home/foremans/tmp/foremans/2024-07-15-124441/ezpz_pyinstrument_profiles\n[2024-07-15 12:46:30.921555][INFO][profile:123] - PyInstrument profile saved (as html) to:  /home/foremans/tmp/foremans/2024-07-15-124441/ezpz_pyinstrument_profiles/pyinstrument-profile-2024-07-15-124630.html\n[2024-07-15 12:46:30.922034][INFO][profile:131] - PyInstrument profile saved (as text) to:  /home/foremans/tmp/foremans/2024-07-15-124441/ezpz_pyinstrument_profiles/pyinstrument-profile-2024-07-15-124630.txt\n[2024-07-15 12:46:31.432464][INFO][profile:143] - Finished with pyinstrument profiler. Took: 2.68764s\n[2024-07-15 12:46:31.433122][INFO][test_dist:318] - [0] runtime=6.820802s\nApplication 5ccc89be resources: utime=21s stime=21s maxrss=1383056KB inblock=8080 oublock=3456 minflt=659443 majflt=896 nvcsw=192077 nivcsw=672014",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "🍋 <code>ezpz</code> @ ALCF"
    ]
  },
  {
    "objectID": "posts/ezpz-at-alcf/index.html#footnotes",
    "href": "posts/ezpz-at-alcf/index.html#footnotes",
    "title": "🍋 ezpz @ ALCF",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf necessary, otherwise activate if already exists↩︎\nNote that the virtual environment will be created at ./venvs/${CONDA_NAME}, where ${CONDA_NAME} will match the prefix of the active conda environment↩︎",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "🍋 <code>ezpz</code> @ ALCF"
    ]
  },
  {
    "objectID": "posts/ai-for-physics/l2hmc-qcd/4dSU3/index.html#imports",
    "href": "posts/ai-for-physics/l2hmc-qcd/4dSU3/index.html#imports",
    "title": "🔳 l2hmc-qcd Example: 4D SU(3)",
    "section": "Imports",
    "text": "Imports\n# %load_ext autoreload\n# %autoreload 2\n# %matplotlib inline\nimport os\nfrom pathlib import Path\nfrom typing import Optional\nimport logging\n\nimport matplotlib_inline\nimport matplotlib.pyplot as plt\n\nimport torch\nimport yaml\n\nimport ambivalent\n\nimport numpy as np\n\nimport lovely_tensors as lt\nlt.monkey_patch()\n\nmatplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n\nos.environ['CUDA_VISIBLE_DEVICES'] = '6'\nos.environ['COLORTERM'] = 'truecolor;'\nport = np.random.randint(5000, 6000)\nos.environ['MASTER_PORT'] = f\"{port}\"\n\nlog_config = get_logging_config()\nlog = logging.getLogger('name')\nlog.setLevel(\"INFO\")\n\nfrom l2hmc.utils.dist import setup_torch\nRANK = setup_torch(precision='float64', backend='DDP', seed=4351)\n\nplt.style.use(ambivalent.STYLES[\"ambivalent\"])\nplt.rcParams['figure.figsize'] = plt.rcParamsDefault['figure.figsize']\n\n\n\n\n\n\n\n\n\n\noutput:\n\n\n\n\n\n\n[Sams-MacBook-Pro.local:56401] shmem: mmap: an error occurred while determining whether or not /var/folders/53/5t2nv83136j76rld14vgfh2h0000gq/T//ompi.Sams-MacBook-Pro.503/jf.0/226820096/sm_segment.Sams-MacBook-Pro.503.d850000.0 could be created.\n[2024-07-04 11:44:34][INFO][init:136] - Setting logging level to ‘INFO’ on ‘RANK == 0’\n [2024-07-04 11:44:34][INFO][init:137] - Setting logging level to ‘CRITICAL’ on ‘RANK != 0’ [2024-07-04 11:44:34][INFO][init:138] - To disable this behavior, and log from ALL ranks (not recommended), set: ‘export LOG_FROM_ALL_RANKS=1’ in your environment, and re-run. Using device: cpu [2024-07-04 11:44:38,445] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to mps (auto detect) W0704 11:44:39.287000 8461388800 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs. [2024-07-04 11:44:39][WARNING][dist:332] - Setting default dtype: float64 \n\n\n\n\ndef savefig(fig: plt.Figure, fname: str, outdir: os.PathLike):\n    pngfile = Path(outdir).joinpath(f\"pngs/{fname}.png\")\n    svgfile = Path(outdir).joinpath(f\"svgs/{fname}.svg\")\n    pngfile.parent.mkdir(exist_ok=True, parents=True)\n    svgfile.parent.mkdir(exist_ok=True, parents=True)\n    fig.savefig(svgfile, transparent=True, bbox_inches='tight')\n    fig.savefig(pngfile, transparent=True, bbox_inches='tight', dpi=300)\ndef plot_metrics(metrics: dict, title: Optional[str] = None, **kwargs):\n    from l2hmc.utils.rich import is_interactive\n    from l2hmc.configs import QUARTO_OUTPUTS_DIR\n    outdir = Path(f\"{QUARTO_OUTPUTS_DIR}/plots-4dSU3/{title}\")\n    outdir.mkdir(exist_ok=True, parents=True)\n    for key, val in metrics.items():\n        fig, ax = plot_metric(val, name=key, **kwargs)\n        if title is not None:\n            ax.set_title(title)\n        log.info(f\"Saving {key} to {outdir}\")\n        savefig(fig, f\"{key}\", outdir=outdir)\n        if not is_interactive():\n            plt.show()\ndef plot_metric(\n        metric: torch.Tensor,\n        name: Optional[str] = None,\n        **kwargs,\n):\n    assert len(metric) &gt; 0\n    if isinstance(metric[0], (int, float, bool, np.floating)):\n        y = np.stack(metric)\n        return plot_scalar(y, ylabel=name, **kwargs)\n    element_shape = metric[0].shape\n    if len(element_shape) == 2:\n        if isinstance(metric, torch.Tensor):\n            y = grab_tensor(torch.stack(metric))\n        else:\n            y = np.stack(metric)\n        return plot_leapfrogs(y, ylabel=name)\n    if len(element_shape) == 1:\n        if isinstance(metric, torch.Tensor):\n            y = grab_tensor(torch.stack(metric))\n        else:\n            y = np.stack(metric)\n        return plot_chains(y, ylabel=name, **kwargs)\n    if len(element_shape) == 0:\n        if isinstance(metric, torch.Tensor):\n            y = grab_tensor(torch.stack(metric))\n        else:\n            y = np.stack(metric)\n        return plot_scalar(y, ylabel=name, **kwargs)\n    raise ValueError\ndef main():\n    from l2hmc.experiment.pytorch.experiment import train_step\n    from l2hmc.configs import CONF_DIR\n    su3conf = Path(CONF_DIR).joinpath('su3-min-cpu.yaml')\n    assert su3conf.is_file()\n    # su3conf = Path('su3-min-cpu.yaml')\n    with su3conf.open('r') as stream:\n        conf = dict(yaml.safe_load(stream))\n\n    log.info(conf)\n    overrides = dict_to_list_of_overrides(conf)\n    ptExpSU3 = get_experiment(overrides=[*overrides], build_networks=True)\n    state = ptExpSU3.trainer.dynamics.random_state(6.0)\n    assert isinstance(state.x, torch.Tensor)\n    assert isinstance(state.beta, torch.Tensor)\n    assert isinstance(ptExpSU3, Experiment)\n    xhmc, history_hmc = evaluate(\n        nsteps=100,\n        exp=ptExpSU3,\n        beta=state.beta,\n        x=state.x,\n        eps=0.1,\n        nleapfrog=1,\n        job_type='hmc',\n        nlog=1,\n        nprint=2,\n        grab=True\n    )\n    xhmc = ptExpSU3.trainer.dynamics.unflatten(xhmc)\n    log.info(f\"checkSU(x_hmc): {g.checkSU(xhmc)}\")\n    plot_metrics(history_hmc.history, title='HMC', marker='.')\n    # ptExpSU3.trainer.dynamics.init_weights(\n    #     method='uniform',\n    #     min=-1e-16,\n    #     max=1e-16,\n    #     bias=True,\n    #     # xeps=0.001,\n    #     # veps=0.001,\n    # )\n    xeval, history_eval = evaluate(\n        nsteps=10,\n        exp=ptExpSU3,\n        beta=6.0,\n        x=state.x,\n        job_type='eval',\n        nlog=1,\n        nprint=2,\n        grab=True,\n    )\n    xeval = ptExpSU3.trainer.dynamics.unflatten(xeval)\n    log.info(f\"checkSU(x_eval): {g.checkSU(xeval)}\")\n    plot_metrics(history_eval.history, title='Evaluate', marker='.')\n\n    history = {}\n    x = state.x\n    for step in range(20):\n        log.info(f'TRAIN STEP: {step}')\n        x, metrics = ptExpSU3.trainer.train_step((x, state.beta))\n        if (step &gt; 0 and step % 2 == 0):\n            print_dict(metrics, grab=True)\n        if (step &gt; 0 and step % 1 == 0):\n            for key, val in metrics.items():\n                try:\n                    history[key].append(val)\n                except KeyError:\n                    history[key] = [val]\n\n    x = ptExpSU3.trainer.dynamics.unflatten(x)\n    log.info(f\"checkSU(x_train): {g.checkSU(x)}\")\n    plot_metrics(history, title='train', marker='.')\n    #\n    # for step in range(20):\n    #     log.info(f\"train step: {step}\")\n    #     x, metrics = ptExpSU3.trainer.train_step((x, state.beta))\n    #     if step % 5 == 0:\n    #         print_dict(metrics, grab=True)\n\n    return x, history\n# main()\nfrom l2hmc.experiment.pytorch.experiment import train_step\n\nfrom l2hmc.configs import CONF_DIR\nsu3conf = Path(CONF_DIR).joinpath('su3-min-cpu.yaml')\nassert su3conf.is_file()\n# su3conf = Path('./conf/su3-min-cpu.yaml')\nwith su3conf.open('r') as stream:\n    conf = dict(yaml.safe_load(stream))\n\nlog.info(conf)\noverrides = dict_to_list_of_overrides(conf)\nptExpSU3 = get_experiment(overrides=[*overrides], build_networks=True)\n\n\n\n\n\n\noutput:\n\n\n\n\n\n\n[2024-07-04 11:44:39][INFO][1705502108:11] - {‘annealing_schedule’: {‘beta_final’: 6.0, ‘beta_init’: 6.0}, ‘backend’: ‘DDP’, ‘conv’: ‘none’, ‘dynamics’: {‘eps’: 0.1, ‘eps_fixed’: True, ‘group’: ‘SU3’, ‘latvolume’: [1, 1, 1, 1], ‘nchains’: 4, ‘nleapfrog’: 1, ‘use_separate_networks’: False, ‘use_split_xnets’: False, ‘verbose’: True}, ‘framework’: ‘pytorch’, ‘init_aim’: False, ‘init_wandb’: False, ‘learning_rate’: {‘clip_norm’: 1.0, ‘lr_init’: 1e-05}, ‘loss’: {‘charge_weight’: 0.0, ‘plaq_weight’: 0.0, ‘rmse_weight’: 1.0, ‘use_mixed_loss’: False}, ‘net_weights’: {‘v’: {‘q’: 1.0, ‘s’: 1.0, ‘t’: 1.0}, ‘x’: {‘q’: 0.0, ‘s’: 0.0, ‘t’: 0.0}}, ‘network’: {‘activation_fn’: ‘tanh’, ‘dropout_prob’: 0.0, ‘units’: [1], ‘use_batch_norm’: False}, ‘restore’: False, ‘save’: False, ‘steps’: {‘log’: 1, ‘nepoch’: 10, ‘nera’: 1, ‘print’: 1, ‘test’: 50}, ‘use_tb’: False, ‘use_wandb’: False}  [2024-07-04 11:44:39][WARNING][trainer:467] - Using torch.float32 on cpu!  [2024-07-04 11:44:39][WARNING][trainer:467] - Using torch.optim.Adam optimizer  [2024-07-04 11:44:39][WARNING][trainer:271] - logging with freq 1 for wandb.watch \n\n\n\n\nstate = ptExpSU3.trainer.dynamics.random_state(6.0)\nassert isinstance(state.x, torch.Tensor)\nassert isinstance(state.beta, torch.Tensor)\nassert isinstance(ptExpSU3, Experiment)\nlog.info(f\"checkSU(x): {g.checkSU(state.x)}\")\n\n\n\n\n\n\noutput:\n\n\n\n\n\n\n[2024-07-04 11:44:39][INFO][2027559912:1] - checkSU(x): (tensor[4] f64 x∈[3.013e-08, 2.845e-07] μ=9.867e-08 σ=1.240e-07 [3.862e-08, 4.138e-08, 2.845e-07, 3.013e-08], tensor[4] f64 x∈[5.401e-08, 5.683e-07] μ=1.875e-07 σ=2.539e-07 [6.819e-08, 5.958e-08, 5.683e-07, 5.401e-08]) \n\n\n\n\nxhmc, history_hmc = evaluate(\n    nsteps=10,\n    exp=ptExpSU3,\n    beta=state.beta,\n    x=state.x,\n    eps=0.1,\n    nleapfrog=1,\n    job_type='hmc',\n    nlog=1,\n    nprint=2,\n    grab=True\n)\nxhmc = ptExpSU3.trainer.dynamics.unflatten(xhmc)\nlog.info(f\"checkSU(x_hmc): {g.checkSU(xhmc)}\")\nplot_metrics(history_hmc.history, title='HMC', marker='.')\n\n\n\n\n\n\noutput:\n\n\n\n\n\n\n[2024-07-04 11:44:39][INFO][2419635544:14] - checkSU(x_hmc): (tensor[4] f64 x∈[2.341e-16, 3.604e-16] μ=2.931e-16 σ=5.613e-17 [2.341e-16, 3.604e-16, 2.624e-16, 3.153e-16], tensor[4] f64 x∈[3.575e-16, 5.398e-16] μ=4.280e-16 σ=8.496e-17 [3.575e-16, 4.483e-16, 3.665e-16, 5.398e-16])  [2024-07-04 11:44:39][INFO][1984252221:10] - Saving energy to /Users/samforeman/projects/saforem2/l2hmc-qcd/qmd/outputs/plots-4dSU3/HMC  [2024-07-04 11:44:40][INFO][1984252221:10] - Saving logprob to /Users/samforeman/projects/saforem2/l2hmc-qcd/qmd/outputs/plots-4dSU3/HMC  [2024-07-04 11:44:40][INFO][1984252221:10] - Saving logdet to /Users/samforeman/projects/saforem2/l2hmc-qcd/qmd/outputs/plots-4dSU3/HMC  [2024-07-04 11:44:40][INFO][1984252221:10] - Saving acc to /Users/samforeman/projects/saforem2/l2hmc-qcd/qmd/outputs/plots-4dSU3/HMC  [2024-07-04 11:44:40][INFO][1984252221:10] - Saving sumlogdet to /Users/samforeman/projects/saforem2/l2hmc-qcd/qmd/outputs/plots-4dSU3/HMC  [2024-07-04 11:44:41][INFO][1984252221:10] - Saving acc_mask to /Users/samforeman/projects/saforem2/l2hmc-qcd/qmd/outputs/plots-4dSU3/HMC  [2024-07-04 11:44:41][INFO][1984252221:10] - Saving plaqs to /Users/samforeman/projects/saforem2/l2hmc-qcd/qmd/outputs/plots-4dSU3/HMC  [2024-07-04 11:44:41][INFO][1984252221:10] - Saving sinQ to /Users/samforeman/projects/saforem2/l2hmc-qcd/qmd/outputs/plots-4dSU3/HMC  [2024-07-04 11:44:41][INFO][1984252221:10] - Saving intQ to /Users/samforeman/projects/saforem2/l2hmc-qcd/qmd/outputs/plots-4dSU3/HMC  [2024-07-04 11:44:42][INFO][1984252221:10] - Saving dQint to /Users/samforeman/projects/saforem2/l2hmc-qcd/qmd/outputs/plots-4dSU3/HMC  [2024-07-04 11:44:42][INFO][1984252221:10] - Saving dQsin to /Users/samforeman/projects/saforem2/l2hmc-qcd/qmd/outputs/plots-4dSU3/HMC  [2024-07-04 11:44:42][INFO][1984252221:10] - Saving loss to /Users/samforeman/projects/saforem2/l2hmc-qcd/qmd/outputs/plots-4dSU3/HMC \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# ptExpSU3.trainer.dynamics.init_weights(\n#     method='uniform',\n#     min=-1e-16,\n#     max=1e-16,\n#     bias=True,\n#     # xeps=0.001,\n#     # veps=0.001,\n# )\nxeval, history_eval = evaluate(\n    nsteps=500,\n    exp=ptExpSU3,\n    beta=6.0,\n    x=state.x,\n    job_type='eval',\n    nlog=2,\n    nprint=5,\n    grab=True,\n)\nxeval = ptExpSU3.trainer.dynamics.unflatten(xeval)\nlog.info(f\"checkSU(x_eval): {g.checkSU(xeval)}\")\nplot_metrics(history_eval.history, title='Evaluate', marker='.')\n\n\n\n\n\n\noutput:\n\n\n\n\n\n\n[2024-07-04 11:44:43][INFO][1629827420:20] - checkSU(x_eval): (tensor[4] f64 x∈[9.602e-14, 0.024] μ=0.009 σ=0.012 [0.024, 2.063e-13, 0.014, 9.602e-14], tensor[4] f64 x∈[1.796e-13, 0.034] μ=0.013 σ=0.016 [0.034, 3.935e-13, 0.016, 1.796e-13])  [2024-07-04 11:44:43][INFO][1984252221:10] - Saving energy to /Users/samforeman/projects/saforem2/l2hmc-qcd/qmd/outputs/plots-4dSU3/Evaluate  [2024-07-04 11:44:43][INFO][1984252221:10] - Saving logprob to /Users/samforeman/projects/saforem2/l2hmc-qcd/qmd/outputs/plots-4dSU3/Evaluate  [2024-07-04 11:44:44][INFO][1984252221:10] - Saving logdet to /Users/samforeman/projects/saforem2/l2hmc-qcd/qmd/outputs/plots-4dSU3/Evaluate  [2024-07-04 11:44:44][INFO][1984252221:10] - Saving sldf to /Users/samforeman/projects/saforem2/l2hmc-qcd/qmd/outputs/plots-4dSU3/Evaluate  [2024-07-04 11:44:44][INFO][1984252221:10] - Saving sldb to /Users/samforeman/projects/saforem2/l2hmc-qcd/qmd/outputs/plots-4dSU3/Evaluate  [2024-07-04 11:44:44][INFO][1984252221:10] - Saving sld to /Users/samforeman/projects/saforem2/l2hmc-qcd/qmd/outputs/plots-4dSU3/Evaluate  [2024-07-04 11:44:45][INFO][1984252221:10] - Saving xeps to /Users/samforeman/projects/saforem2/l2hmc-qcd/qmd/outputs/plots-4dSU3/Evaluate  [2024-07-04 11:44:45][INFO][1984252221:10] - Saving veps to /Users/samforeman/projects/saforem2/l2hmc-qcd/qmd/outputs/plots-4dSU3/Evaluate  [2024-07-04 11:44:45][INFO][1984252221:10] - Saving acc to /Users/samforeman/projects/saforem2/l2hmc-qcd/qmd/outputs/plots-4dSU3/Evaluate  [2024-07-04 11:44:45][INFO][1984252221:10] - Saving sumlogdet to /Users/samforeman/projects/saforem2/l2hmc-qcd/qmd/outputs/plots-4dSU3/Evaluate  [2024-07-04 11:44:45][INFO][1984252221:10] - Saving beta to /Users/samforeman/projects/saforem2/l2hmc-qcd/qmd/outputs/plots-4dSU3/Evaluate  [2024-07-04 11:44:46][INFO][1984252221:10] - Saving acc_mask to /Users/samforeman/projects/saforem2/l2hmc-qcd/qmd/outputs/plots-4dSU3/Evaluate  [2024-07-04 11:44:46][INFO][1984252221:10] - Saving plaqs to /Users/samforeman/projects/saforem2/l2hmc-qcd/qmd/outputs/plots-4dSU3/Evaluate  [2024-07-04 11:44:46][INFO][1984252221:10] - Saving sinQ to /Users/samforeman/projects/saforem2/l2hmc-qcd/qmd/outputs/plots-4dSU3/Evaluate  [2024-07-04 11:44:46][INFO][1984252221:10] - Saving intQ to /Users/samforeman/projects/saforem2/l2hmc-qcd/qmd/outputs/plots-4dSU3/Evaluate  [2024-07-04 11:44:47][INFO][1984252221:10] - Saving dQint to /Users/samforeman/projects/saforem2/l2hmc-qcd/qmd/outputs/plots-4dSU3/Evaluate  [2024-07-04 11:44:47][INFO][1984252221:10] - Saving dQsin to /Users/samforeman/projects/saforem2/l2hmc-qcd/qmd/outputs/plots-4dSU3/Evaluate  [2024-07-04 11:44:47][INFO][1984252221:10] - Saving loss to /Users/samforeman/projects/saforem2/l2hmc-qcd/qmd/outputs/plots-4dSU3/Evaluate \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx = ptExpSU3.trainer.dynamics.unflatten(state.x)\nlog.info(f\"checkSU(x_train): {g.checkSU(x)}\")\n# plot_metrics(history, title='train', marker='.')\n\n\n\n\n\n\noutput:\n\n\n\n\n\n\n[2024-07-04 11:44:49][INFO][3331171632:2] - checkSU(x_train): (tensor[4] f64 x∈[3.013e-08, 2.845e-07] μ=9.867e-08 σ=1.240e-07 [3.862e-08, 4.138e-08, 2.845e-07, 3.013e-08], tensor[4] f64 x∈[5.401e-08, 5.683e-07] μ=1.875e-07 σ=2.539e-07 [6.819e-08, 5.958e-08, 5.683e-07, 5.401e-08])",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "⚛️ AI for Physics",
      "🎢 L2HMC for LQCD",
      "🔳 <code>l2hmc-qcd</code> Example: 4D SU(3)"
    ]
  },
  {
    "objectID": "posts/ai-for-physics/index.html",
    "href": "posts/ai-for-physics/index.html",
    "title": "⚛️ AI for Physics",
    "section": "",
    "text": "CitationBibTeX citation:@online{foreman,\n  author = {Foreman, Sam},\n  title = {⚛️ {AI} for {Physics}},\n  url = {https://samforeman.me},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nForeman, Sam. n.d. “⚛️ AI for Physics.” https://samforeman.me."
  },
  {
    "objectID": "posts/AuroraGPT/mpi4py-reproducer/index.html",
    "href": "posts/AuroraGPT/mpi4py-reproducer/index.html",
    "title": "🐛 mpi4py bug on Sunspot",
    "section": "",
    "text": "Simple reproducer:\n\nLoad my anl_24_q2_release conda environment:\n#[08:42:38 AM][foremans@x1922c2s3b0n0][~]\n$ eval \"$(~/miniconda3/bin/conda shell.zsh hook)\" ; conda activate anl_24_q2_release\nTry python3 -c 'from mpi4py import MPI'\n\nfails ❌\n\n# [08:44:41 AM][foremans@x1922c2s3b0n0][~][anl_24_q2_release]\n$ python3 -c 'from mpi4py import MPI'\nTraceback (most recent call last):\n  File \"&lt;string&gt;\", line 1, in &lt;module&gt;\nImportError: /home/foremans/miniconda3/envs/anl_24_q2_release/lib/python3.9/site-packages/mpi4py/MPI.cpython-39-x86_64-linux-gnu.so: undefined symbol: MPI_Message_c2f\n[1]    14910 exit 1     python3 -c 'from mpi4py import MPI'\nLoad correct modules:\n# [08:44:58 AM][foremans@x1922c2s3b0n0][~][anl_24_q2_release]\n$ module use /home/ftartagl/graphics-compute-runtime/modulefiles ; module load graphics-compute-runtime/agama-ci-devel-803.29 spack-pe-gcc/0.6.1-23.275.2 gcc/12.2.0 ; module use /soft/preview-modulefiles/24.086.0 ; module load oneapi/release/2024.04.15.001\n     UMD: agama-ci-devel-803.29 successfully loaded:\n     UMD: graphics-compute-runtime/agama-ci-devel-803.29\n\nDue to MODULEPATH changes, the following have been reloaded:\n  1) mpich-config/collective-tuning/1024\n\nThe following have been reloaded with a version change:\n  1) intel_compute_runtime/release/agama-devel-736.25 =&gt; intel_compute_runtime/release/775.20     2) mpich/icc-all-pmix-gpu/52.2 =&gt; mpich/icc-all-pmix-gpu/20231026     3) oneapi/eng-compiler/2023.12.15.002 =&gt; oneapi/release/2024.04.15.001\nRetry with new modules:\n\nworks ✅\n\n# [08:45:01 AM][foremans@x1922c2s3b0n0][~][anl_24_q2_release]\n$ python3 -c 'from mpi4py import MPI; print(MPI.__file__)'\n/home/foremans/miniconda3/envs/anl_24_q2_release/lib/python3.9/site-packages/mpi4py/MPI.cpython-39-x86_64-linux-gnu.so\n\n\n\n\nCitationBibTeX citation:@online{foreman2024,\n  author = {Foreman, Sam},\n  title = {🐛 `Mpi4py` Bug on {Sunspot}},\n  date = {2024-05-25},\n  url = {https://samforeman.me},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nForeman, Sam. 2024. “🐛 `Mpi4py` Bug on Sunspot.” May 25,\n2024. https://samforeman.me.",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "🤖 AuroraGPT",
      "🐛 `mpi4py` bug on Sunspot"
    ]
  },
  {
    "objectID": "posts/AuroraGPT/flash-attn-sunspot/index.html#update-2024-06-16",
    "href": "posts/AuroraGPT/flash-attn-sunspot/index.html#update-2024-06-16",
    "title": "📸 flash-attn on Sunspot",
    "section": "Update: 2024-06-16",
    "text": "Update: 2024-06-16\nAfter an interactive debug session with Intel, the root behavior of the apparent discrepancy was identified.\nIn particular, we found that the ALCF/Megatron-DeepSpeed repo was NOT explicitly setting the dropout values to 0.0 (and so, was using the default values of 0.1) for both --attention-dropout and --hidden-dropout.\nAfter making this change, the losses were observed to agree, as can be seen below in\n\n\n\n\n\n\nFigure 1: After correctly setting the dropout values, the loss curves were observed to agree.",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "🤖 AuroraGPT",
      "📸 `flash-attn` on Sunspot"
    ]
  },
  {
    "objectID": "posts/AuroraGPT/flash-attn-sunspot/index.html#impact-on-loss-bug",
    "href": "posts/AuroraGPT/flash-attn-sunspot/index.html#impact-on-loss-bug",
    "title": "📸 flash-attn on Sunspot",
    "section": "🐛 Impact on Loss [Bug?]",
    "text": "🐛 Impact on Loss [Bug?]\nIn the q4-drop, it was observed that toggling flash-attn on / off seemed to produce different loss curves (with otherwise identical configs)\n\n\nshared-config.yaml\n\nTP: 1\nPP: 1\nGAS: 1\nOPT: adamw\ndtype: bf16\nNLAYERS: 10\nMICRO_BATCH: 2\nWORLD_SIZE: 24\n\nThis can be seen clearly in the figure below:\n\nThis was identified, and to be addressed in upcoming release.",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "🤖 AuroraGPT",
      "📸 `flash-attn` on Sunspot"
    ]
  },
  {
    "objectID": "posts/AuroraGPT/flash-attn-sunspot/index.html#llm-framework-release",
    "href": "posts/AuroraGPT/flash-attn-sunspot/index.html#llm-framework-release",
    "title": "📸 flash-attn on Sunspot",
    "section": "📦 LLM Framework Release",
    "text": "📦 LLM Framework Release\nOn 05/14/2024, Intel dropped their new LLM frameworks release:\n\n\n🎁 frameworks_2024_5_v2 Announcement:\n\nHi Venkat,\nWe have shared the official Q2 release in two different forms :\nManual Setup: /gila/Aurora_deployment/anl_24_q2_release.tar.gz\nand\nModule:\nmodule use -a /home/jmitche1/anl_release/2024/q2\nmodule load frameworks_2024_5_v2\n Instructions on how to use modules with Q2 build are anl_24_q2_release/README\n\nThe release includes :\n\nMegatron-DeepSpeed 0.14.2 (with patch)\nIntel® Extension for PyTorch* v2.1.30+xpu\nTorchCCL 2.1.300\nONEAPI 2024.1.0.596.PUBLIC_IDP_2024.1.0_723\nAgama driver: 803.29\n\nThe release provides following key features:\n\nScaleup Performance improvement from the TorchCCl prototype feature enabled by TORCH_LLM_ALLREDUCE=1  details\nAuto TP inference support for more workloads\nFlash Attention V2 improvement for 256 head dimension support; MiCS support.\nLatest Features and Optimizations from DeepSpeed 0.14.2 and Intel® Extension for PyTorch* 2.1.30.\n\n\nThanks, \nJerome\n\n\n📸 flash 🤝 📷 no-flash\nWith this new release, Intel observed that the loss curves agreed exactly for flash / no-flash, using the learning rate settings below:\nlr: 0.00015\nlr_warmup_frac: 0.01\nlr_decay_iters: 320000\nTesting with Jerome’s new release:\nmodule use -a /home/jmitche1/anl_release/2024/q2\nmodule load frameworks_2024_5_v2\nI was able to independently confirm these results, shown in 📸 flash 🤝 📷 no-flash below.\n\n\n🔗 wandb links:\n\n\n[📸 flash] W&B Run: youthful-river-1832\n[📷 no-flash] W&B Run: earthy-wave-1830\n\n\n\n\n📸 flash vs. 📷 no-flash\n\n\n\n\nflash 📸 🤝 📷 no-flash\n\n\n\n\n\n🚧 Broken MPI1\nFor whatever reason, things seemed to have spontaneously broken on the night of 2024-04-14 ??\nWhen trying to run experiments the following day (05/15/2024) I was met with this2:\nAbort(15): Fatal error in internal_Init_thread: Other MPI error\nwhich was discussed further in this thread on slack.\nIt seems Subrata also encountered a similar issue [see: slack thread]\n\n\n✅ mpi4py fix\n\n\nTo resolve this\nAbort(15): Fatal error in internal_Init_thread: Other MPI error\nissue we can simply load the correct modules:\nmodule use -a /home/jmitche1/anl_release/2024/q2\nmodule load frameworks_2024_5_v2\nmodule use /home/ftartagl/graphics-compute-runtime/modulefiles\nmodule load graphics-compute-runtime/agama-ci-devel-803.29 \nmodule load spack-pe-gcc/0.6.1-23.275.2 gcc/12.2.0\nmodule use /soft/preview-modulefiles/24.086.0\nmodule load oneapi/release/2024.04.15.001\nFor full details see mpi4py-reproducer, and this [slack thread].",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "🤖 AuroraGPT",
      "📸 `flash-attn` on Sunspot"
    ]
  },
  {
    "objectID": "posts/AuroraGPT/flash-attn-sunspot/index.html#framework-comparison",
    "href": "posts/AuroraGPT/flash-attn-sunspot/index.html#framework-comparison",
    "title": "📸 flash-attn on Sunspot",
    "section": "🕵🏻‍ Framework Comparison",
    "text": "🕵🏻‍ Framework Comparison\nAs I was re-building MPI, and after talking to Jerome, I realized that most of the dependencies are already present in the provided frameworks/ modules on Sunspot.\nAs a simple test, I tried building a new environment built on the base conda environment3 provided by theframeworks/2023.12.15.001 module, which worked without modification and had ) most of what I needed already installed:\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; torch.__version__\n'2.1.0a0+cxx11.abi'\n&gt;&gt;&gt; import intel_extension_for_pytorch as ipex\n&gt;&gt;&gt; ipex.__version__\n'2.1.10+xpu'\n&gt;&gt;&gt; from mpi4py import MPI\nThe remaining dependencies were installed according to the instructions from the new release frameworks_2024_5_v2.\nDetails included below.\n\n\n📦 pip Install Dependencies\n\nUnfortunately, the frameworks/** don’t appear to provide DeepSpeed.\nWe can create a virtual environment on top of the base conda by\n$ module use frameworks/2023.12.15.001\n$ export PBS_O_WORKDIR=$(pwd) ; source ALCF/helpers.sh && setup_venv_from_conda\nOnce the venv has been created and activated, we can install the remaining dependencies:\nTo build / install DeepSpeed, along with its required dependencies:\n\nintel-extension-for-deepspeed:\npython3 -m pip install intel_extension_for_pytorch_deepspeed\\=\\=2.1.30 -f \"https://pytorch-extension.intel.com/release-whl/stable/xpu/us/intel-extension-for-pytorch-deepspeed/\"\nDeepSpeed:\necho \"build deepspeed\"\ngit clone https://github.com/microsoft/DeepSpeed.git\ncd DeepSpeed\ngit remote add yizhou_ds https://github.com/YizhouZ/DeepSpeed.git\ngit fetch yizhou_ds\ngit checkout yizhou/kernel_path\npip install -r requirements/requirements.txt\npython setup.py develop |& tee build.log\nExtras:\npython3 -m pip install transformers datasets python-etcd tensorboardX packaging sentencepiece bitsandbytes tiktoken neural-speed einops intel-extension-for-transformers\n\n\nLooking around the available modules a bit, I noticed a newer frameworks release (frameworks/2024.04.15.002) that had a newer version of both torch and ipex:\nmodule use /soft/preview-modulefiles/24.086.0\nmodule load frameworks/2024.04.15.002.lua\npython3 -c 'from mpi4py import MPI; print(MPI.__file__)'\n# /soft/datascience/aurora_nre_models_frameworks-2024.1_preview_u1/lib/python3.9/site-packages/mpi4py/MPI.cpython-39-x86_64-linux-gnu.so\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; torch.__version__\n'2.1.0.post2+cxx11.abi'\n&gt;&gt;&gt; import intel_extension_for_pytorch as ipex\n&gt;&gt;&gt; ipex.__version__\n'2.1.30+xpu'\n&gt;&gt;&gt; from mpi4py import MPI; print(MPI.__file__)\n/soft/datascience/aurora_nre_models_frameworks-2024.1_preview_u1/lib/python3.9/site-packages/mpi4py/MPI.cpython-39-x86_64-linux-gnu.so\nThe remaining dependencies were installed identically to what was just done previously for the frameworks/2023.12.15.001 module.\nNOTE: In the figures below, we denote these two environments as:\n\n2024.0:\n\nmodule load frameworks/2023.12.15.001\n\n2024.1:\n\nmodule use /soft/preview-modulefiles/24.086.0\nmodule load frameworks/2024.04.15.002.lua\n\nanl_24_q2_release:\n\neval \"$(~/miniconda3/bin/conda shell.zsh hook)\"\nconda activate anl_24_q2_release\n\n\n\n🥸 Fix in Disguise\nArmed now with functional environment(s) for argonne-lcf/Megatron-DeepSpeed, I was able to resume my previous experiments.\nFrom the discussion with Intel, it was hard to understand / reason about why the flash-attn fix would have any dependence on the learning rate schedule (warmup + decay).\nIf the flash-attn fix works for a particular learning rate schedule, you would reasonably expect that it should work for any learning rate schedule.\nAn additional source of confusion for me was that the discrepancy in the loss curves (seemingly) disappeared when using the learning rate settings provided by Intel4, but not when using the ALCF defaults5.\nAfter thinking about it for a bit and trying to reason about possible causes, I wondered if it might not be a mix of multiple different factors:\n\nSmall learning rate\nVery long decay\n[maybe ?] somehow dependent on the learning rate warmup fraction\n\npreliminary experiments seemed to suggest this was not the case\n\n\nSo, I was curious what would happen if I used the (larger) learning rate value from the ALCF defaults (lr=0.003) with the very long lr-decay-iters: 320000 from Intel.\nThese results are shown below.\nIn particular, for all three experiments the following learning rate settings were used:\nlr: 0.0003\nlr-warmup-frac: 0.05\nlr-decay-iters: 320000\n Looking at this figure ^, it appears that up until the very very end, all three loss curves agree identically.\nHowever, if we look closely at the very end, it looks like there might be a slight difference beginning to appear between the 2024.0 (brown line) and {anl_24_q2_release, 2024.1} ({dark, light} blue lines, respectively).\nThinking that I might be onto something, I then tried again with a smaller lr-decay-iters: 5000.\nThis result is shown below:\n In particular, we can now more clearly see the difference beginning to appear between the 2024.0 and 2024.1 loss curves.\nContinuing on, we see this effect become increasingly dramatic with even smaller values of lr-decay-iters:\n \n In each of these experiments, it appears that:\n\n2024.0:\n\nNot impacted by this lr-decay-iters dependence\nContinue to decrease for the duration of training\n\n2024.1:\n\nImpacted by the lr-decay-iters dependence\nPlateaus towards the end of training\n\n\n\n\nOlder Figs\n\n \n\n\n\n✅ 2024.0 Fix\nEverything seems to work with\nmodule load frameworks/2023.12.15.001\n \n\n\n📊 lr-decay-iters Comparison\n\n2024.0:\n\n\n\n2024.1:",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "🤖 AuroraGPT",
      "📸 `flash-attn` on Sunspot"
    ]
  },
  {
    "objectID": "posts/AuroraGPT/flash-attn-sunspot/index.html#lr-decay-iters-dependence",
    "href": "posts/AuroraGPT/flash-attn-sunspot/index.html#lr-decay-iters-dependence",
    "title": "📸 flash-attn on Sunspot",
    "section": "📈 lr-decay-iters dependence",
    "text": "📈 lr-decay-iters dependence",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "🤖 AuroraGPT",
      "📸 `flash-attn` on Sunspot"
    ]
  },
  {
    "objectID": "posts/AuroraGPT/flash-attn-sunspot/index.html#performance-improvement-in-2024.1",
    "href": "posts/AuroraGPT/flash-attn-sunspot/index.html#performance-improvement-in-2024.1",
    "title": "📸 flash-attn on Sunspot",
    "section": "🏎️ Performance Improvement in 2024.1",
    "text": "🏎️ Performance Improvement in 2024.1\n\n\nlr: 0.0003\nlr-warmup-frac: 0.05\nlr-decay-iters: null",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "🤖 AuroraGPT",
      "📸 `flash-attn` on Sunspot"
    ]
  },
  {
    "objectID": "posts/AuroraGPT/flash-attn-sunspot/index.html#footnotes",
    "href": "posts/AuroraGPT/flash-attn-sunspot/index.html#footnotes",
    "title": "📸 flash-attn on Sunspot",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGremlins, likely↩︎\nhttps://github.com/pmodels/mpich/pull/7001↩︎\nExplicitly, aurora_nre_models_frameworks-2024.0, abbreviated as 2024.0↩︎\nIntel used the following learning rate schedule in their experiments yml   lr: 0.00015   lr-warmup-frac: 0.01   lr-decay-iters: 320000↩︎\nALCF used the following learning rate schedule in their experiments↩︎",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "🤖 AuroraGPT",
      "📸 `flash-attn` on Sunspot"
    ]
  },
  {
    "objectID": "posts/AuroraGPT/aurora-gpt/index.html#install-setup",
    "href": "posts/AuroraGPT/aurora-gpt/index.html#install-setup",
    "title": "🏎️ Megatron-DeepSpeed on Intel XPU",
    "section": "Install / Setup",
    "text": "Install / Setup\n\nSetup script / history:\n\n\nInteractive Session\n\n$ export HTTP_PROXY=http://proxy.alcf.anl.gov:3128\n$ export HTTPS_PROXY=http://proxy.alcf.anl.gov:3128\n$ export http_proxy=http://proxy.alcf.anl.gov:3128\n$ export https_proxy=http://proxy.alcf.anl.gov:3128\n\n$ export DRM_LIB=\"$(pwd)/usr/include/libdrm\"\n# $ export PATH=\"${HOME}/miniconda3/bin:$PATH\"\n\n$ conda create --name anl_release_q4 python=3.9 --y\n$ wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh\n$ bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3\n$ eval \"$(${HOME} shell.zsh hook)\"\n\n$ export DRM_LIB=\"$(pwd)/usr/include/libdrm\"\n$ conda config --add channels conda-forge && conda install -c conda-forge mpi4py -y --no-deps && conda install -c conda-forge libssh -y && conda uninstall mpi -y && python3 -m pip install -r requirements.txt && python3 -m pip install *.whl\n\n$ module unload oneapi/eng-compiler/2022.12.30.003\n$ module unload intel_compute_runtime/release/agama-devel-551\n$ module use -a /soft/modulefiles\n$ module load oneapi/release/2023.12.15.001\n$ module use /home/ftartagl/graphics-compute-runtime/modulefiles\n$ module load graphics-compute-runtime/agama-ci-devel-736.9\n# one-liner for modules:\n# module unload oneapi/eng-compiler/2022.12.30.003 && module unload intel_compute_runtime/release/agama-devel-551&& module use -a /soft/modulefiles && module load oneapi/release/2023.12.15.001 && module use /home/ftartagl/graphics-compute-runtime/modulefiles && module load graphics-compute-runtime/agama-ci-devel-736.9\n\n$ cd torch-ccl\n$ ls\n$ COMPUTE_BACKEND=dpcpp python3 setup.py develop |& tee build.log\n$ cd ../\n\n$ cd intel-extension-for-deepspeed\n$ python3 setup.py develop |& tee build.log\n$ cd ../\n\n$ cd DeepSpeed\n$ ls\n$ python3 -m pip install -r requirements/requirements.txt\n$ python3 setup.py develop |& tee build.log",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "🤖 AuroraGPT",
      "🏎️ Megatron-DeepSpeed on Intel XPU"
    ]
  },
  {
    "objectID": "posts/AuroraGPT/aurora-gpt/index.html#running",
    "href": "posts/AuroraGPT/aurora-gpt/index.html#running",
    "title": "🏎️ Megatron-DeepSpeed on Intel XPU",
    "section": "Running",
    "text": "Running\n\nUsing launch\n\nSetup:\n\n\nSetup\n\n$ qsub -q EarlyAppAccess -A Aurora_Deployment -l walltime=08:00:00 -l select=2 -I\nqsub: waiting for job 604319.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov to start\nqsub: job 604319.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov ready\n\n$ module unload oneapi/eng-compiler/2022.12.30.003 && module unload intel_compute_runtime/release/agama-devel-551&& module use -a /soft/modulefiles && module load oneapi/release/2023.12.15.001 && module use /home/ftartagl/graphics-compute-runtime/modulefiles && module load graphics-compute-runtime/agama-ci-devel-736.9\n UMD: agama-ci-devel-736.9 successfully loaded:\n UMD: graphics-compute-runtime/agama-ci-devel-736.9\n\n$ eval \"$(/home/foremans/miniconda3/bin/conda shell.zsh hook)\"\n\n$ conda activate anl_release_q4\n\n$ git clone https://github.com/saforem2/ezpz\n$ python3 -m pip install -e \"ezpz[dev]\"\n# [BUG] for some reason, need to run twice ¯\\_(ツ)_/¯\n$ source ./ezpz/src/ezpz/bin/savejobenv && source ./ezpz/src/ezpz/bin/savejobenv\n\n\nOutput\n\n┌───────────────────────────────────────────────────────────────────\n│ Writing PBS vars to /home/foremans/.pbsenv\n│ HOSTFILE: /var/spool/pbs/aux/604319.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\n│ NHOSTS: 2\n│ NGPU_PER_HOST: 12 GPUs per host\n│ NGPUS: 24 GPUs total\n└───────────────────────────────────────────────────────────────────\n┌───────────────────────────────────────────────────────────────────\n│ [DIST INFO]:\n│   • Writing Job info to /home/foremans/.pbsenv\n│     • HOSTFILE: /var/spool/pbs/aux/604319.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\n│     • NHOSTS: 2\n│     • NGPU_PER_HOST: 12\n│     • NGPUS = (NHOSTS * NGPU_PER_HOST) = 24\n│ [Hosts]:\n│       • x4502c0s0b0n0.hostmgmt2502.cm.aurora.alcf.anl.gov, x4502c0s2b0n0.hostmgmt2502.cm.aurora.alcf.anl.gov\n│ [Launch]:\n│     • Use: 'launch' (=mpiexec --verbose --envall -n 24 -ppn 12 --hostfile /var/spool/pbs/aux/604319.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov)\n│       to launch job\n└───────────────────────────────────────────────────────────────────\n┌────────────────────────────────────────────────────────────────────────────────\n│ YOU ARE HERE: /home/foremans\n│ Run 'source ./bin/getjobenv' in a NEW SHELL to automatically set env vars\n└────────────────────────────────────────────────────────────────────────────────\n\n\n\n[WIP] Building out python API\n\n$ python3 -m ezpz.savejobenv\n/home/foremans/miniconda3/envs/anl_release_q4/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'If you dont plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?'\n  warn(\n[2024-01-23 10:02:37][INFO][jobs:185] - Saving job env to /home/foremans/PBS-jobs/604319/jobenv.sh\n[2024-01-23 10:02:37][INFO][jobs:193] - Saving job env to dot-env (.env) file in /home/foremans\n[2024-01-23 10:02:37][INFO][jobs:211] - Saving job env to /home/foremans/PBS-jobs/604319/jobenv.json\n[2024-01-23 10:02:37][INFO][jobs:225] - Saving job env to /home/foremans/PBS-jobs/604319/jobenv.yaml\n[2024-01-23 10:02:37][INFO][jobs:253] - Writing PBS env vars to /home/foremans/PBS-jobs/604319 / jobenv{.sh, .yaml, .json}\n[2024-01-23 10:02:37][INFO][jobs:258] - jobenv={\n    \"BACKEND\": \"gloo\",\n    \"DEVICE\": \"xpu\",\n    \"DEVICE_ID\": \"xpu:0\",\n    \"DISTRIBUTED_BACKEND\": \"gloo\",\n    \"FRAMEWORK\": \"pytorch\",\n    \"GPUS_PER_NODE\": 12,\n    \"HOSTFILE\": \"/var/spool/pbs/aux/604319.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\",\n    \"HOSTNAME\": \"x4502c0s0b0n0.hostmgmt2502.cm.aurora.alcf.anl.gov\",\n    \"HOSTS\": \"[x4502c0s0b0n0, x4502c0s2b0n0]\",\n    \"LAUNCH_CMD\": \"mpiexec --verbose --envall -n 24 -ppn 12 --hostfile /var/spool/pbs/aux/604319.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\",\n    \"LOCAL_RANK\": 0,\n    \"MACHINE\": \"Aurora\",\n    \"NGPUS\": 24,\n    \"NGPU_PER_HOST\": \"12\",\n    \"NHOSTS\": \"2\",\n    \"NODE_ID\": 0,\n    \"NUM_NODES\": 2,\n    \"PBS_ACCOUNT\": \"Aurora_Deployment\",\n    \"PBS_ENVIRONMENT\": \"PBS_INTERACTIVE\",\n    \"PBS_HOOK_RESOURCES\": \"eJyVUV1vwjAM/EOblHZbgUV54yfs3TKp22bkozgJqP9+LjAJ9jYpD7HvfL5L0Pt0AbQ21VjATmSPMKDzlck0Gq9opBGLOxOspZVriit2Qe5BShoTL2bvsmVaMeTlDpZlpj/AoXIEXjWM0rYyk6x90H1tPza73a7rNq1SejoECKknM3gsepptABdwJGNTmGshKJQLtKp9V03TfMnlremgUUO3lelo55pNq6MDppwqWzJYOTHqKKK2CJbOxKsncTN7FEIWI4VYz5y+hQIzu8SuLMLltK48oD0Oznt5gkz+ppKjvfk8Vex1SQX9YyilL1IVF8io7adScvSpUiV4Dnjv/TPmberZQiaWYDDKdyYY8tXrtTOlQE+NM3rXgwSivORCIZs75eV3+Ady/coN\",\n    \"PBS_JOBCOOKIE\": \"6B8C4F9D774B0AA5174EAAFB6E2CC14F\",\n    \"PBS_JOBDIR\": \"/home/foremans\",\n    \"PBS_JOBID\": \"604319.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\",\n    \"PBS_JOBNAME\": \"STDIN\",\n    \"PBS_MOMPORT\": \"15003\",\n    \"PBS_NODEFILE\": \"/var/spool/pbs/aux/604319.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\",\n    \"PBS_NODENUM\": \"0\",\n    \"PBS_O_HOME\": \"/home/foremans\",\n    \"PBS_O_HOST\": \"aurora-uan-0010.hostmgmt1000.cm.aurora.alcf.anl.gov\",\n    \"PBS_O_LANG\": \"en_US.UTF-8\",\n    \"PBS_O_LOGNAME\": \"foremans\",\n    \"PBS_O_MAIL\": \"/var/spool/mail/foremans\",\n    \"PBS_O_PATH\": \"/home/foremans/.nvm/versions/node/v21.5.0/bin:/home/foremans/homebrew/bin:/home/foremans/homebrew/sbin:/opt/cray/pals/1.3.2/bin:/opt/cray/libfabric/1.15.2.0/bin:/opt/aurora/23.073.0/support/libraries/intel-compute-samples/2021.27.01:/opt/aurora/23.073.0/support/libraries/khronos/clinfo/default/bin:/opt/aurora/23.073.0/support/tools/gpu_validation:/opt/aurora/23.073.0/intel-gpu-umd/agama-devel-551/compiler/bin:/opt/aurora/23.073.0/intel-gpu-umd/agama-devel-551/driver/bin:/opt/aurora/23.073.0/CNDA/mpich/51.2/mpich-ofi-all-icc-default-pmix-gpu-drop51/bin:/opt/aurora/23.073.0/support/tools/mpi_wrapper_utils:/opt/aurora/23.073.0/oneapi/debugger/2023.0.0/gdb/intel64/bin:/opt/aurora/23.073.0/CNDA/oneapi/compiler/trunk-20230201/dpcpp-ct/bin:/opt/aurora/23.073.0/oneapi/advisor/2023.0.0/bin64:/opt/aurora/23.073.0/CNDA/oneapi/vtune/2023.0.0_624810_nda/bin64:/opt/aurora/23.073.0/CNDA/oneapi/compiler/trunk-20230201/compiler/linux/bin/intel64:/opt/aurora/23.073.0/CNDA/oneapi/compiler/trunk-20230201/compiler/linux/bin:/opt/aurora/23.073.0/oneapi/inspector/2023.0.0/bin64:/opt/cray/pe/gcc/11.2.0/snos/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/home/foremans/.local/bin:/home/foremans/bin:/usr/local/bin:/usr/bin:/bin:/opt/c3/bin:/usr/lib/mit/bin:/usr/lib/mit/sbin:/opt/pbs/bin:/sbin:/home/foremans/.local/share/kitty-ssh-kitten/kitty/bin:/home/foremans/.cargo/bin:/home/foremans/.fzf/bin:/home/foremans/.luarocks/bin:/home/foremans/.luarocks/bin\",\n    \"PBS_O_QUEUE\": \"EarlyAppAccess\",\n    \"PBS_O_SHELL\": \"/bin/zsh\",\n    \"PBS_O_SYSTEM\": \"Linux\",\n    \"PBS_O_TZ\": \"America/Chicago\",\n    \"PBS_O_WORKDIR\": \"/home/foremans\",\n    \"PBS_QUEUE\": \"LustreApps\",\n    \"PBS_TASKNUM\": \"1\",\n    \"RANK\": 0,\n    \"SCHEDULER\": \"PBS\",\n    \"WORLD_SIZE_IN_USE\": 1,\n    \"WORLD_SIZE_TOTAL\": 24,\n    \"jobfile_json\": \"/home/foremans/PBS-jobs/604319/jobenv.json\",\n    \"jobfile_sh\": \"/home/foremans/PBS-jobs/604319/jobenv.sh\",\n    \"jobfile_yaml\": \"/home/foremans/PBS-jobs/604319/jobenv.yaml\"\n}\n\n$ source \"$(tail -1 ~/PBS-jobs.log)/jobenv.sh\"\n$ which launch\nlaunch: aliased to mpiexec --verbose --envall -n 24 -ppn 12 --hostfile /var/spool/pbs/aux/604319.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\nTake 1: Crash with\nRuntimeError: oneCCL: global.cpp:150 getenv_local_coord: EXCEPTION: to get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\n\n\nRun:\n\n$ launch python3 pretrain_llama.py \\\n    --tensor-model-parallel-size 1 \\\n    --pipeline-model-parallel-size 1 \\\n    --num-layers 32 \\\n    --hidden-size 4096 \\\n    --ffn-hidden-size 5504 \\\n    --num-attention-heads 32 \\\n    --micro-batch-size 1 \\\n    --global-batch-size 24 \\\n    --seq-length 2048 \\\n    --max-position-embeddings 2048 \\\n    --train-iters 250000 \\\n    --save /lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/checkpoints/LLAMA_7B_LLAMA_7B_z3_seqlen_mp1_pp1_sp24_nl32_hs4096_gb24_mb1 \\\n    --load /lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/checkpoints/LLAMA_7B_LLAMA_7B_z3_seqlen_mp1_pp1_sp24_nl32_hs4096_gb24_mb1 \\\n    --data-path \\\n    --data-impl mmap \\\n    --tokenizer-type GPTSentencePieceTokenizer \\\n    --tokenizer-model ./tmp/tokenizer.model \\\n    --split 949,50,1 \\\n    --distributed-backend ccl \\\n    --lr 3e-4 \\\n    --lr-decay-style cosine \\\n    --min-lr 3e-5 \\\n    --weight-decay 0.1 \\\n    --clip-grad 1 \\\n    --lr-warmup-iters 2000 \\\n    --optimizer adam \\\n    --adam-beta1 0.9 \\\n    --adam-beta2 0.95 \\\n    --log-interval 1 \\\n    --save-interval 10000 \\\n    --eval-interval 1000 \\\n    --eval-iters 10 \\\n    --bf16 \\\n    --no-query-key-layer-scaling \\\n    --attention-dropout 0 \\\n    --hidden-dropout 0 \\\n    --use-rotary-position-embeddings \\\n    --untie-embeddings-and-output-weightss \\\n    --swiglus \\\n    --normalization rmsnorms \\\n    --disable-bias-linears \\\n    --num-key-value-heads 4s \\\n    --tensorboard-dir /lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/outputs/LLAMA_7B_LLAMA_7B_z3_seqlen_mp1_pp1_sp24_nl32_ \\\n    hs4096_gb24_mb1/tensorboard \\\n    --log-timers-to-tensorboard \\\n    --tensorboard-log-interval 1 \\\n    --data-path /lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/dataset/BookCorpusDataset_text_documents \\\n    --vocab-file /lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/dataset/gpt2-vocab.json \\\n    --merge-file /lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/dataset/gpt2-merges.txt \\\n    --zero-stage=3 \\\n    --deepspeed_config=/lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/deepspeed.json \\\n    --deepspeed\n\nConnected to tcp://x4502c1s0b0n0.hostmgmt2502.cm.aurora.alcf.anl.gov:7919\nFound executable /home/foremans/miniconda3/envs/anl_release_q4/bin/python3\nLaunching application 2e559157-da5e-4185-9902-dc8d932e8bb3\n/home/foremans/miniconda3/envs/anl_release_q4/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'If you dont plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?'\n  warn(\n[2024-01-23 00:02:13,326] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to xpu (auto detect)\n[2024-01-23 00:02:19,177] [INFO] [comm.py:161:init_deepspeed_backend] Initialize ccl backend\n[2024-01-23 00:02:19,177] [INFO] [comm.py:637:init_distributed] cdb=None\n[2024-01-23 00:02:19,177] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n[2024-01-23 00:02:19,888] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=3, local_rank=3, world_size=24, master_addr=10.115.53.137, master_port=29500\n[2024-01-23 00:02:19,888] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=9, local_rank=9, world_size=24, master_addr=10.115.53.137, master_port=29500\n[2024-01-23 00:02:19,888] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=11, local_rank=11, world_size=24, master_addr=10.115.53.137, master_port=29500\n[2024-01-23 00:02:19,888] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=1, local_rank=1, world_size=24, master_addr=10.115.53.137, master_port=29500\n[2024-01-23 00:02:19,888] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=2, local_rank=2, world_size=24, master_addr=10.115.53.137, master_port=29500\n[2024-01-23 00:02:19,888] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=4, local_rank=4, world_size=24, master_addr=10.115.53.137, master_port=29500\n[2024-01-23 00:02:19,888] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=5, local_rank=5, world_size=24, master_addr=10.115.53.137, master_port=29500\n[2024-01-23 00:02:19,888] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=6, local_rank=6, world_size=24, master_addr=10.115.53.137, master_port=29500\n[2024-01-23 00:02:19,888] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=7, local_rank=7, world_size=24, master_addr=10.115.53.137, master_port=29500\n[2024-01-23 00:02:19,888] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=13, local_rank=1, world_size=24, master_addr=10.115.53.137, master_port=29500\n[2024-01-23 00:02:19,888] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=8, local_rank=8, world_size=24, master_addr=10.115.53.137, master_port=29500\n[2024-01-23 00:02:19,888] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=10, local_rank=10, world_size=24, master_addr=10.115.53.137, master_port=29500\n[2024-01-23 00:02:19,888] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=24, master_addr=10.115.53.137, master_port=29500\n[2024-01-23 00:02:19,889] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend ccl\n[2024-01-23 00:02:19,888] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=15, local_rank=3, world_size=24, master_addr=10.115.53.137, master_port=29500\n[2024-01-23 00:02:19,888] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=17, local_rank=5, world_size=24, master_addr=10.115.53.137, master_port=29500\n[2024-01-23 00:02:19,888] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=19, local_rank=7, world_size=24, master_addr=10.115.53.137, master_port=29500\n[2024-01-23 00:02:19,888] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=21, local_rank=9, world_size=24, master_addr=10.115.53.137, master_port=29500\n[2024-01-23 00:02:19,888] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=12, local_rank=0, world_size=24, master_addr=10.115.53.137, master_port=29500\n[2024-01-23 00:02:19,888] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=14, local_rank=2, world_size=24, master_addr=10.115.53.137, master_port=29500\n[2024-01-23 00:02:19,888] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=16, local_rank=4, world_size=24, master_addr=10.115.53.137, master_port=29500\n[2024-01-23 00:02:19,888] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=18, local_rank=6, world_size=24, master_addr=10.115.53.137, master_port=29500\n[2024-01-23 00:02:19,888] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=20, local_rank=8, world_size=24, master_addr=10.115.53.137, master_port=29500\n[2024-01-23 00:02:19,888] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=22, local_rank=10, world_size=24, master_addr=10.115.53.137, master_port=29500\n[2024-01-23 00:02:19,888] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=23, local_rank=11, world_size=24, master_addr=10.115.53.137, master_port=29500\n[2024-01-23 00:02:20][INFO][dist:257] - DistInfo={\n    \"DEVICE\": \"xpu\",\n    \"DEVICE_ID\": \"xpu:0\",\n    \"DISTRIBUTED_BACKEND\": \"gloo\",\n    \"GPUS_PER_NODE\": 12,\n    \"HOSTFILE\": \"/var/spool/pbs/aux/604213.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\",\n    \"HOSTNAME\": \"x4502c1s0b0n0.hostmgmt2502.cm.aurora.alcf.anl.gov\",\n    \"HOSTS\": \"['x4502c1s0b0n0', 'x4502c1s3b0n0']\",\n    \"LOCAL_RANK\": 0,\n    \"MACHINE\": \"Aurora\",\n    \"NGPUS\": 24,\n    \"NODE_ID\": 0,\n    \"NUM_NODES\": 2,\n    \"RANK\": 0,\n    \"SCHEDULER\": \"PBS\",\n    \"WORLD_SIZE_IN_USE\": 24,\n    \"WORLD_SIZE_TOTAL\": 24\n}\n[2024-01-23 00:02:20,987] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to xpu (auto detect)\n--------------------------------------------------\nDeepSpeed C++/CUDA extension op report\n--------------------------------------------------\nNOTE: Ops not installed will be just-in-time (JIT) compiled at\n      runtime if needed. Op compatibility means that your system\n      meet the required dependencies to JIT install the op.\n--------------------------------------------------\nJIT compiled ops requires ninja\nninja .................. [OKAY]\n--------------------------------------------------\nop name ................ installed .. compatible\n--------------------------------------------------\n[2024-01-23 00:02:20][INFO][spawn:38] - icx -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/foremans/miniconda3/envs/anl_release_q4/include -fPIC -O2 -isystem /home/foremans/miniconda3/envs/anl_release_q4/include -fPIC -c /tmp/tmph01efr3s/test.c -o /tmp/tmph01efr3s/test.o\nWARNING: TensorBoard writing requested but is not available (are you using PyTorch 1.1.0 or later?), no TensorBoard logs will be written.\n2024:01:23-00:02:21:(122507) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL\n2024:01:23-00:02:21:(122510) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed to get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\n2024:01:23-00:02:21:(122515) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nto get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\n2024:01:23-00:02:21:(122507) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nto get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\n2024:01:23-00:02:21:(122508) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nto get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\n2024:01:23-00:02:21:(122509) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nto get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\n2024:01:23-00:02:21:(122511) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nto get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\n2024:01:23-00:02:21:(122512) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nto get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\n2024:01:23-00:02:21:(122513) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nto get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\n2024:01:23-00:02:21:(122514) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nto get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\n2024:01:23-00:02:21:(122516) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nto get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\n2024:01:23-00:02:21:(122517) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nto get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\n2024:01:23-00:02:21:(122518) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nto get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\n2024:01:23-00:02:21:(141071) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL\n2024:01:23-00:02:21:(141072) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL\n2024:01:23-00:02:21:(141073) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL\n2024:01:23-00:02:21:(141075) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL\n2024:01:23-00:02:21:(141076) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL\n2024:01:23-00:02:21:(141078) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL\n2024:01:23-00:02:21:(141079) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL\n2024:01:23-00:02:21:(141081) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL\n2024:01:23-00:02:21:(141074) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL\n2024:01:23-00:02:21:(141077) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL\n2024:01:23-00:02:21:(141080) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL\n2024:01:23-00:02:21:(141071) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nto get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\n2024:01:23-00:02:21:(141075) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nto get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\n2024:01:23-00:02:21:(141078) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nto get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\n2024:01:23-00:02:21:(141081) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nto get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\n2024:01:23-00:02:21:(141072) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nto get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\n2024:01:23-00:02:21:(141073) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nto get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\n2024:01:23-00:02:21:(141074) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nto get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\n2024:01:23-00:02:21:(141076) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nto get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\n2024:01:23-00:02:21:(141077) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nto get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\n2024:01:23-00:02:21:(141079) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nto get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\n2024:01:23-00:02:21:(141080) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nto get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\n&gt;fused kernel is only supported in cuda, skip loading fused kernel\n&gt;fused kernel is only supported in cuda, skip loading fused kernel\n&gt;fused kernel is only supported in cuda, skip loading fused kernel\n&gt;fused kernel is only supported in cuda, skip loading fused kernel\n&gt;fused kernel is only supported in cuda, skip loading fused kernel\n&gt;fused kernel is only supported in cuda, skip loading fused kernel\n&gt;fused kernel is only supported in cuda, skip loading fused kernel\n&gt;fused kernel is only supported in cuda, skip loading fused kernel\n&gt;fused kernel is only supported in cuda, skip loading fused kernel\n&gt;fused kernel is only supported in cuda, skip loading fused kernel\n&gt;fused kernel is only supported in cuda, skip loading fused kernel\n&gt;fused kernel is only supported in cuda, skip loading fused kernel\nTraceback (most recent call last):\n  File \"/lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/pretrain_llama.py\", line 583, in &lt;module&gt;\n    model = main()\n  File \"/lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/pretrain_llama.py\", line 561, in main\n    model = pretrain(\n  File \"/lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/megatron/training.py\", line 136, in pretrain\n    torch.distributed.all_reduce(start_time_tensor,\n  File \"/home/foremans/miniconda3/envs/anl_release_q4/lib/python3.9/site-packages/torch/distributed/c10d_logger.py\", line 47, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/foremans/miniconda3/envs/anl_release_q4/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py\", line 2050, in all_reduce\n    work = group.allreduce([tensor], opts)\n\nRuntimeError: oneCCL: global.cpp:150 getenv_local_coord: EXCEPTION: to get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\n\n&lt;/details&gt;\nTake 2: Trying with CCL_ZE_IPC_EXCHANGE=sockets (still no luck)\n\n\nRun:\n\n$ CCL_ZE_IPC_EXCHANGE=sockets !!\n[...]\n2024:01:23-00:03:41:(123335) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL\n2024:01:23-00:03:41:(123330) |CCL_WARN| sockets exchange mode is set. It may cause potential problem of 'Too many open file descriptors'\n2024:01:23-00:03:41:(123337) |CCL_WARN| sockets exchange mode is set. It may cause potential problem of 'Too many open file descriptors'\n2024:01:23-00:03:41:(123327) |CCL_WARN| sockets exchange mode is set. It may cause potential problem of 'Too many open file descriptors'\n[...]\nRuntimeError: oneCCL: global.cpp:150 getenv_local_coord: EXCEPTION: to get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "🤖 AuroraGPT",
      "🏎️ Megatron-DeepSpeed on Intel XPU"
    ]
  },
  {
    "objectID": "posts/AuroraGPT/aurora-gpt/index.html#using-deepspeed",
    "href": "posts/AuroraGPT/aurora-gpt/index.html#using-deepspeed",
    "title": "🏎️ Megatron-DeepSpeed on Intel XPU",
    "section": "Using deepspeed",
    "text": "Using deepspeed\n\nSetup:\n\n\nSetup:\n\n$ cat $PBS_NODEFILE &gt; hostfile ; sed -e 's/$/ slots=12/' -i hostfile\n$ echo \"PATH=${PATH}\" &gt;&gt; .deepspeed_env ; echo \"LD_LIBRARY_PATH=${LD_LIBRARY_PATH}\" &gt;&gt; .deepspeed_env ; echo \"http_proxy=${http_proxy}\" &gt;&gt; .deepspeed_env ; echo \"https_proxy=${https_proxy}\" &gt;&gt; .deepspeed_env\nRun:\n\nCommand:\n  $ RANK=0 LOCAL_RANK=0 MASTER_ADDR=localhost deepspeed --hostfile hostfile pretrain_llama.py --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 32 --hidden-size 4096 --ffn-hidden-size 5504 --num-attention-heads 32 --micro-batch-size 1 --global-batch-size 24 --seq-length 2048 --max-position-embeddings 2048 --train-iters 250000 --save /lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/checkpoints/LLAMA_7B_LLAMA_7B_z3_seqlen_mp1_pp1_sp24_nl32_hs4096_gb24_mb1 --load /lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/checkpoints/LLAMA_7B_LLAMA_7B_z3_seqlen_mp1_pp1_sp24_nl32_hs4096_gb24_mb1 --data-path --data-impl mmap --tokenizer-type GPTSentencePieceTokenizer --tokenizer-model ./tmp/tokenizer.model --split 949,50,1 --distributed-backend ccl --lr 3e-4 --lr-decay-style cosine --min-lr 3e-5 --weight-decay 0.1 --clip-grad 1 --lr-warmup-iters 2000 --optimizer adam --adam-beta1 0.9 --adam-beta2 0.95 --log-interval 1 --save-interval 10000 --eval-interval 1000 --eval-iters 10 --bf16 --no-query-key-layer-scaling --attention-dropout 0 --hidden-dropout 0 --use-rotary-position-embeddings --untie-embeddings-and-output-weights --swiglu --normalization rmsnorm --disable-bias-linear --num-key-value-heads 4 --tensorboard-dir /lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/outputs/LLAMA_7B_LLAMA_7B_z3_seqlen_mp1_pp1_sp24_nl32_hs4096_gb24_mb1/tensorboard --log-timers-to-tensorboard --tensorboard-log-interval 1 --data-path /lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/dataset/BookCorpusDataset_text_document --vocab-file /lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/dataset/gpt2-vocab.json --merge-file /lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/dataset/gpt2-merges.txt --zero-stage=3 --deepspeed_config=/lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/deepspeed.json --deepspeed\nOutput:\n\n\nOutput\n\n$ RANK=0 LOCAL_RANK=0 MASTER_ADDR=localhost deepspeed --hostfile hostfile pretrain_llama.py --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 32 --hidden-size 4096 --ffn-hidden-size 5504 --num-attention-heads 32 --micro-batch-size 1 --global-batch-size 24 --seq-length 2048 --max-position-embeddings 2048 --train-iters 250000 --save /lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/checkpoints/LLAMA_7B_LLAMA_7B_z3_seqlen_mp1_pp1_sp24_nl32_hs4096_gb24_mb1 --load /lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/checkpoints/LLAMA_7B_LLAMA_7B_z3_seqlen_mp1_pp1_sp24_nl32_hs4096_gb24_mb1 --data-path --data-impl mmap --tokenizer-type GPTSentencePieceTokenizer --tokenizer-model ./tmp/tokenizer.model --split 949,50,1 --distributed-backend ccl --lr 3e-4 --lr-decay-style cosine --min-lr 3e-5 --weight-decay 0.1 --clip-grad 1 --lr-warmup-iters 2000 --optimizer adam --adam-beta1 0.9 --adam-beta2 0.95 --log-interval 1 --save-interval 10000 --eval-interval 1000 --eval-iters 10 --bf16 --no-query-key-layer-scaling --attention-dropout 0 --hidden-dropout 0 --use-rotary-position-embeddings --untie-embeddings-and-output-weights --swiglu --normalization rmsnorm --disable-bias-linear --num-key-value-heads 4 --tensorboard-dir /lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/outputs/LLAMA_7B_LLAMA_7B_z3_seqlen_mp1_pp1_sp24_nl32_hs4096_gb24_mb1/tensorboard --log-timers-to-tensorboard --tensorboard-log-interval 1 --data-path /lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/dataset/BookCorpusDataset_text_document --vocab-file /lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/dataset/gpt2-vocab.json --merge-file /lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/dataset/gpt2-merges.txt --zero-stage=3 --deepspeed_config=/lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/deepspeed.json --deepspeed \n\nhome/foremans/miniconda3/envs/anl_release_q4/bin/deepspeed:4: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n  __import__('pkg_resources').require('deepspeed==0.12.3+6ea44d02')\n/home/foremans/miniconda3/envs/anl_release_q4/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'If you dont plan on using image functionality from `torchvision.io`, you can igno re this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?'\n  warn(\nMy guessed rank = 0\n[2024-01-23 00:09:56,016] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to xpu (auto detect)\n[2024-01-23 00:10:00,790] [INFO] [runner.py:463:main] Using IP address of 10.115.53.137 for node x4502c1s0b0n0.hostmgmt2502.cm.aurora.alcf.anl.gov\n[2024-01-23 00:10:00,812] [INFO] [runner.py:559:main] deepspeed_env file = .deepspeed_env\n[2024-01-23 00:10:00,813] [INFO] [runner.py:559:main] deepspeed_env file = .deepspeed_env\n[2024-01-23 00:10:00,813] [INFO] [multinode_runner.py:72:get_cmd] Running on the following workers: x4502c1s0b0n0.hostmgmt2502.cm.aurora.alcf.anl.gov,x4502c1s3b0n0.hostmgmt2502.cm.aurora.alcf.anl.gov\n[2024-01-23 00:10:00,813] [INFO] [runner.py:570:main] cmd = pdsh -S -f 1024 -w x4502c1s0b0n0.hostmgmt2502.cm.aurora.alcf.anl.gov,x4502c1s3b0n0.hostmgmt2502.cm.aurora.alcf.anl.gov export PYTHONSTARTUP=/etc/pythonstart; export PYTHONPATH=/l\nus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed:/soft/compilers/oneapi/2023.12.15.001/oneapi/advisor/2024.0/pythonapi; export PATH=/home/ftartagl/graphics-compute-runtime/agama-ci-devel-736.9/u\nsr/bin:/soft/tools/gpu_validation:/soft/libraries/khronos/clinfo/master-13ae34-2020.12.14/bin:/soft/libraries/intel-compute-samples/2021.27.01:/soft/libraries/intel-gpu-umd/stable_736_25_20231031/compiler/bin:/soft/libraries/intel-gpu-umd\n/stable_736_25_20231031/driver/bin:/soft/restricted/CNDA/updates/mpich/52.2/mpich-ofi-all-icc-default-pmix-gpu-drop52/bin:/soft/tools/mpi_wrapper_utils:/soft/compilers/oneapi/2023.12.15.001/oneapi/dpcpp-ct/2024.0/bin:/soft/compilers/oneap\ni/2023.12.15.001/oneapi/advisor/2024.0/bin64:/soft/compilers/oneapi/2023.12.15.001/oneapi/vtune/2024.0/bin64:/soft/compilers/oneapi/2023.12.15.001/oneapi/inspector/2024.0/bin64:/soft/compilers/oneapi/2023.12.15.001/oneapi/debugger/2024.0/\nopt/debugger/bin:/soft/compilers/oneapi/2023.12.15.001/oneapi/mkl/2024.0/bin:/soft/compilers/oneapi/2023.12.15.001/oneapi/compiler/2024.0/bin:/home/foremans/miniconda3/envs/anl_release_q4/bin:/home/foremans/miniconda3/condabin:/home/forem\nans/.nvm/versions/node/v21.5.0/bin:/home/foremans/homebrew/bin:/home/foremans/homebrew/sbin:/opt/cray/pals/1.3.2/bin:/opt/cray/libfabric/1.15.2.0/bin:/opt/cray/pe/gcc/11.2.0/snos/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/b\nin:/home/foremans/.local/bin:/home/foremans/bin:/usr/local/bin:/usr/bin:/bin:/opt/c3/bin:/usr/lib/mit/bin:/usr/lib/mit/sbin:/opt/pbs/bin:/sbin:/home/foremans/.local/share/kitty-ssh-kitten/kitty/bin:/home/foremans/.cargo/bin:/home/foremans\n/.fzf/bin:/home/foremans/.luarocks/bin; export LD_LIBRARY_PATH=/home/ftartagl/graphics-compute-runtime/agama-ci-devel-736.9/usr/lib64/dri:/home/ftartagl/graphics-compute-runtime/agama-ci-devel-736.9/usr/lib64/mfx:/home/ftartagl/graphics-c\nompute-runtime/agama-ci-devel-736.9/usr/lib64/intel-opencl:/home/ftartagl/graphics-compute-runtime/agama-ci-devel-736.9/usr/lib64:/soft/libraries/khronos/loader/master-2022.05.18/lib64:/soft/libraries/intel-gpu-umd/stable_736_25_20231031/\ncompiler/lib64:/soft/libraries/intel-gpu-umd/stable_736_25_20231031/driver/lib64/intel-opencl:/soft/libraries/intel-gpu-umd/stable_736_25_20231031/driver/lib64:/soft/restricted/CNDA/updates/mpich/52.2/mpich-ofi-all-icc-default-pmix-gpu-dr\nop52/lib:/soft/compilers/oneapi/2023.12.15.001/oneapi/ipp/2021.10/lib:/soft/compilers/oneapi/2023.12.15.001/oneapi/ippcp/2021.9/lib:/soft/compilers/oneapi/2023.12.15.001/oneapi/dpl/2022.3/lib:/soft/compilers/oneapi/2023.12.15.001/oneapi/d\nebugger/2024.0/opt/debugger/lib:/soft/compilers/oneapi/2023.12.15.001/oneapi/ccl/2021.11/lib:/soft/compilers/oneapi/2023.12.15.001/oneapi/dal/2024.0/lib:/soft/compilers/oneapi/2023.12.15.001/oneapi/dnnl/2024.0/lib:/soft/compilers/oneapi/2\n023.12.15.001/oneapi/tbb/2021.11/lib/intel64/gcc4.8:/soft/compilers/oneapi/2023.12.15.001/oneapi/mkl/2024.0/lib:/soft/compilers/oneapi/2023.12.15.001/oneapi/compiler/2024.0/opt/compiler/lib:/soft/compilers/oneapi/2023.12.15.001/oneapi/com\npiler/2024.0/lib:/opt/cray/libfabric/1.15.2.0/lib64:/opt/cray/pe/gcc/11.2.0/snos/lib64; export http_proxy=http://proxy-01.pub.alcf.anl.gov:3128; export https_proxy=http://proxy.alcf.anl.gov:3128;  cd /lus/gecko/projects/Aurora_deployment/\nforemans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed; /home/foremans/miniconda3/envs/anl_release_q4/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJ4NDUwMmMxczBiMG4wLmhvc3RtZ210MjUwMi5jbS5hdXJvcmEuYWxjZi5hbmwuZ292IjogWzAsI\nDEsIDIsIDMsIDQsIDUsIDYsIDcsIDgsIDksIDEwLCAxMV0sICJ4NDUwMmMxczNiMG4wLmhvc3RtZ210MjUwMi5jbS5hdXJvcmEuYWxjZi5hbmwuZ292IjogWzAsIDEsIDIsIDMsIDQsIDUsIDYsIDcsIDgsIDksIDEwLCAxMV19 --node_rank=%n --master_addr=10.115.53.137 --master_port=29500 pre\ntrain_llama.py --tensor-model-parallel-size '1' --pipeline-model-parallel-size '1' --num-layers '32' --hidden-size '4096' --ffn-hidden-size '5504' --num-attention-heads '32' --micro-batch-size '1' --global-batch-size '24' --seq-length '20\n48' --max-position-embeddings '2048' --train-iters '250000' --save '/lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/checkpoints/LLAMA_7B_LLAMA_7B_z3_seqlen_mp1_pp1_sp24_nl32_hs4096_gb24_mb1'\n--load '/lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/checkpoints/LLAMA_7B_LLAMA_7B_z3_seqlen_mp1_pp1_sp24_nl32_hs4096_gb24_mb1' --data-path --data-impl 'mmap' --tokenizer-type 'GPTSentence\nPieceTokenizer' --tokenizer-model './tmp/tokenizer.model' --split '949,50,1' --distributed-backend 'ccl' --lr '3e-4' --lr-decay-style 'cosine' --min-lr '3e-5' --weight-decay '0.1' --clip-grad '1' --lr-warmup-iters '2000' --optimizer 'adam\n' --adam-beta1 '0.9' --adam-beta2 '0.95' --log-interval '1' --save-interval '10000' --eval-interval '1000' --eval-iters '10' --bf16 --no-query-key-layer-scaling --attention-dropout '0' --hidden-dropout '0' --use-rotary-position-embeddings\n --untie-embeddings-and-output-weights --swiglu --normalization 'rmsnorm' --disable-bias-linear --num-key-value-heads '4' --tensorboard-dir '/lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/ou\ntputs/LLAMA_7B_LLAMA_7B_z3_seqlen_mp1_pp1_sp24_nl32_hs4096_gb24_mb1/tensorboard' --log-timers-to-tensorboard --tensorboard-log-interval '1' --data-path '/lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-\nDeepSpeed/dataset/BookCorpusDataset_text_document' --vocab-file '/lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/dataset/gpt2-vocab.json' --merge-file '/lus/gecko/projects/Aurora_deployment/f\noremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/dataset/gpt2-merges.txt' --zero-stage=3 --deepspeed_config=/lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/deepspeed.json --deepspeed\nx4502c1s3b0n0: Warning: Permanently added 'x4502c1s3b0n0.hostmgmt2502.cm.aurora.alcf.anl.gov,10.115.53.138' (ECDSA) to the list of known hosts.\n\nx4502c1s0b0n0: /home/foremans/miniconda3/envs/anl_release_q4/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io\n`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\nx4502c1s0b0n0:   warn(\nx4502c1s3b0n0: /home/foremans/miniconda3/envs/anl_release_q4/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\nx4502c1s3b0n0:   warn(\nx4502c1s0b0n0: [2024-01-23 06:10:07,853] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to xpu (auto detect)\nx4502c1s0b0n0: [2024-01-23 06:10:08,419] [INFO] [launch.py:145:main] WORLD INFO DICT: {'x4502c1s0b0n0.hostmgmt2502.cm.aurora.alcf.anl.gov': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], 'x4502c1s3b0n0.hostmgmt2502.cm.aurora.alcf.anl.gov': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]}\nx4502c1s0b0n0: [2024-01-23 06:10:08,419] [INFO] [launch.py:151:main] nnodes=2, num_local_procs=12, node_rank=0\nx4502c1s0b0n0: [2024-01-23 06:10:08,419] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(&lt;class 'list'&gt;, {'x4502c1s0b0n0.hostmgmt2502.cm.aurora.alcf.anl.gov': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], 'x4502c1s3b0n0.hostmgmt2502.cm.aurora.alcf.anl.gov': [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]})\nx4502c1s0b0n0: [2024-01-23 06:10:08,419] [INFO] [launch.py:163:main] dist_world_size=24\nx4502c1s0b0n0: [2024-01-23 06:10:08,419] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7,8,9,10,11\nx4502c1s3b0n0: [2024-01-23 06:10:08,885] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to xpu (auto detect)\nx4502c1s3b0n0: [2024-01-23 06:10:09,452] [INFO] [launch.py:145:main] WORLD INFO DICT: {'x4502c1s0b0n0.hostmgmt2502.cm.aurora.alcf.anl.gov': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], 'x4502c1s3b0n0.hostmgmt2502.cm.aurora.alcf.anl.gov': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]}\nx4502c1s3b0n0: [2024-01-23 06:10:09,452] [INFO] [launch.py:151:main] nnodes=2, num_local_procs=12, node_rank=1\nx4502c1s3b0n0: [2024-01-23 06:10:09,452] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(&lt;class 'list'&gt;, {'x4502c1s0b0n0.hostmgmt2502.cm.aurora.alcf.anl.gov': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], 'x4502c1s3b0n0.hostmgmt2502.cm.aurora.alcf.anl.gov': [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]})\nx4502c1s3b0n0: [2024-01-23 06:10:09,452] [INFO] [launch.py:163:main] dist_world_size=24\nx4502c1s3b0n0: [2024-01-23 06:10:09,452] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7,8,9,10,11\nx4502c1s0b0n0: My guessed rank = 4\nx4502c1s0b0n0: My guessed rank = 9\nx4502c1s0b0n0: My guessed rank = 8\nx4502c1s0b0n0: My guessed rank = 0\nx4502c1s0b0n0: My guessed rank = 6\nx4502c1s0b0n0: My guessed rank = 7\nx4502c1s0b0n0: My guessed rank = 11\nx4502c1s0b0n0: My guessed rank = 5\nx4502c1s0b0n0: My guessed rank = 10\nx4502c1s0b0n0: My guessed rank = 3\nx4502c1s0b0n0: My guessed rank = 2\nx4502c1s0b0n0: My guessed rank = 1\nx4502c1s3b0n0: My guessed rank = 21\nx4502c1s3b0n0: My guessed rank = 18\nx4502c1s3b0n0: My guessed rank = 22\nx4502c1s3b0n0: My guessed rank = 20\nx4502c1s3b0n0: My guessed rank = 14\nx4502c1s3b0n0: My guessed rank = 12\nx4502c1s3b0n0: My guessed rank = 23\nx4502c1s3b0n0: My guessed rank = 16\nx4502c1s3b0n0: My guessed rank = 17\nx4502c1s3b0n0: My guessed rank = 19\nx4502c1s3b0n0: My guessed rank = 15\nx4502c1s3b0n0: My guessed rank = 13\nx4502c1s0b0n0: [2024-01-23 06:10:14,751] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to xpu (auto detect)\nx4502c1s0b0n0: [2024-01-23 06:10:19,225] [INFO] [comm.py:161:init_deepspeed_backend] Initialize ccl backend\nx4502c1s0b0n0: [2024-01-23 06:10:19,225] [INFO] [comm.py:637:init_distributed] cdb=None\nx4502c1s0b0n0: [2024-01-23 06:10:20,891] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend ccl\nx4502c1s0b0n0: [2024-01-23 06:10:21][INFO][dist:257] - DistInfo={\nx4502c1s0b0n0:     \"DEVICE\": \"xpu\",\nx4502c1s0b0n0:     \"DEVICE_ID\": \"xpu:0\",\nx4502c1s0b0n0:     \"DISTRIBUTED_BACKEND\": \"gloo\",\nx4502c1s0b0n0:     \"GPUS_PER_NODE\": 12,\nx4502c1s0b0n0:     \"HOSTFILE\": \"/var/spool/pbs/aux/604213.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\",\nx4502c1s0b0n0:     \"HOSTNAME\": \"x4502c1s0b0n0.hostmgmt2502.cm.aurora.alcf.anl.gov\",\nx4502c1s0b0n0:     \"HOSTS\": \"['x4502c1s0b0n0', 'x4502c1s3b0n0']\",\nx4502c1s0b0n0:     \"LOCAL_RANK\": 0,\nx4502c1s0b0n0:     \"MACHINE\": \"Aurora\",\nx4502c1s0b0n0:     \"NGPUS\": 24,\nx4502c1s0b0n0:     \"NODE_ID\": 0,\nx4502c1s0b0n0:     \"NUM_NODES\": 2,\nx4502c1s0b0n0:     \"RANK\": 0,\nx4502c1s0b0n0:     \"SCHEDULER\": \"PBS\",\nx4502c1s0b0n0:     \"WORLD_SIZE_IN_USE\": 1,\nx4502c1s0b0n0:     \"WORLD_SIZE_TOTAL\": 24\nx4502c1s0b0n0: }\nx4502c1s0b0n0: [2024-01-23 06:10:21,533] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to xpu (auto detect)\nx4502c1s0b0n0: --------------------------------------------------\nx4502c1s0b0n0: DeepSpeed C++/CUDA extension op report\nx4502c1s0b0n0: --------------------------------------------------\nx4502c1s0b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at\nx4502c1s0b0n0:       runtime if needed. Op compatibility means that your system\nx4502c1s0b0n0:       meet the required dependencies to JIT install the op.\nx4502c1s0b0n0: --------------------------------------------------\nx4502c1s0b0n0: JIT compiled ops requires ninja\nx4502c1s0b0n0: ninja .................. [OKAY]\nx4502c1s0b0n0: --------------------------------------------------\nx4502c1s0b0n0: op name ................ installed .. compatible\nx4502c1s0b0n0: --------------------------------------------------\nx4502c1s0b0n0: [2024-01-23 06:10:21][INFO][spawn:38] - gcc -pthread -B /home/foremans/miniconda3/envs/anl_release_q4/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/foremans/miniconda3/envs/anl_release_q4/include -fPIC -O2 -isystem /home/foremans/miniconda3/envs/anl_release_q4/include -fPIC -c /tmp/tmptqyph55g/test.c -o /tmp/tmptqyph55g/test.o\nx4502c1s3b0n0: [2024-01-23 06:10:21,671] [INFO] [comm.py:161:init_deepspeed_backend] Initialize ccl backend\nx4502c1s3b0n0: [2024-01-23 06:10:21,672] [INFO] [comm.py:637:init_distributed] cdb=None\n\n[...]\nx4502c1s0b0n0: &gt;fused kernel is only supported in cuda, skip loading fused kernel\nx4502c1s0b0n0: 2024:01:23-06:12:16:(153241) |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi\nx4502c1s0b0n0: 2024:01:23-06:12:16:(153241) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL\nx4502c1s0b0n0: 2024:01:23-06:12:16:(153241) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nx4502c1s0b0n0: to get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\nx4502c1s0b0n0: 2024:01:23-06:12:16:(153242) |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi\nx4502c1s0b0n0: 2024:01:23-06:12:16:(153242) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL\nx4502c1s0b0n0: 2024:01:23-06:12:16:(153242) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nx4502c1s0b0n0: to get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\nx4502c1s3b0n0:  &gt; padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\nx4502c1s0b0n0: 2024:01:23-06:12:16:(153237) |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi\nx4502c1s0b0n0: 2024:01:23-06:12:16:(153237) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL\nx4502c1s0b0n0: 2024:01:23-06:12:16:(153237) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nx4502c1s0b0n0: to get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\nx4502c1s0b0n0:  &gt; padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\nx4502c1s0b0n0:  &gt; padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\nx4502c1s3b0n0: 2024:01:23-06:12:16:(129554) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL\nx4502c1s3b0n0: 2024:01:23-06:12:16:(129554) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nx4502c1s3b0n0: to get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\nx4502c1s3b0n0: RuntimeError: oneCCL: global.cpp:150 getenv_local_coord: EXCEPTION: to get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\nx4502c1s3b0n0: Traceback (most recent call last):\nx4502c1s3b0n0:   File \"/lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/pretrain_llama.py\", line 583, in &lt;module&gt;\nx4502c1s3b0n0:     model = main()\nx4502c1s3b0n0:   File \"/lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/pretrain_llama.py\", line 561, in main\nx4502c1s3b0n0:     model = pretrain(\nx4502c1s3b0n0:   File \"/lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/megatron/training.py\", line 136, in pretrain\nx4502c1s3b0n0:     torch.distributed.all_reduce(start_time_tensor,\nx4502c1s3b0n0:   File \"/home/foremans/miniconda3/envs/anl_release_q4/lib/python3.9/site-packages/torch/distributed/c10d_logger.py\", line 47, in wrapper\nx4502c1s3b0n0:     return func(*args, **kwargs)\nx4502c1s3b0n0:   File \"/home/foremans/miniconda3/envs/anl_release_q4/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py\", line 2050, in all_reduce\nx4502c1s3b0n0:     work = group.allreduce([tensor], opts)",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "🤖 AuroraGPT",
      "🏎️ Megatron-DeepSpeed on Intel XPU"
    ]
  },
  {
    "objectID": "posts/AuroraGPT/determinstic-flash-attn/index.html",
    "href": "posts/AuroraGPT/determinstic-flash-attn/index.html",
    "title": "🎰 Deterministic flash-attn",
    "section": "",
    "text": "[NOTE]: For additional details, refer to the W&B Report.\n\n\nSimple tests to confirm the loss is exactly reproducible across independent runs (when launched with the same seed).\n\nIn particular, we set:\noutput = flash_attn_func(q, k, v, None, self.causal, deterministic=True)\nin all the flash_attn_func(...) calls from megatron/model/transformer.py\nAll experiments ran on Polaris @ ALCF, using:\nmachine: Polaris\nargs.zero_stage: 1\nargs.num_layers: 32\nargs.micro_batch_size: 1\nargs.optimizer: \"adamw\"\nargs.use_flash_attn: true\nenv.DFL_STEM: \"books\"\nenv.GRAD_ACC_STEPS: 8\nenv.WORLD_SIZE: 8\n\n\n\n\n\n\n\nFigure 1: Plot of the loss curve for 3 independent runs with deterministic=True\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{foreman2024,\n  author = {Foreman, Sam},\n  title = {🎰 {Deterministic} `Flash-Attn`},\n  date = {2024-06-17},\n  url = {https://samforeman.me},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nForeman, Sam. 2024. “🎰 Deterministic `Flash-Attn`.” June\n17, 2024. https://samforeman.me.",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "🤖 AuroraGPT",
      "🎰 Deterministic `flash-attn`"
    ]
  },
  {
    "objectID": "posts/AuroraGPT/index.html",
    "href": "posts/AuroraGPT/index.html",
    "title": "🤖 AuroraGPT",
    "section": "",
    "text": "CitationBibTeX citation:@online{foreman,\n  author = {Foreman, Sam},\n  title = {🤖 {AuroraGPT}},\n  url = {https://samforeman.me},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nForeman, Sam. n.d. “🤖 AuroraGPT.” https://samforeman.me."
  },
  {
    "objectID": "posts/AuroraGPT/startup-times/index.html#response",
    "href": "posts/AuroraGPT/startup-times/index.html#response",
    "title": "⏰ Starting Up Distributed Training on Aurora",
    "section": "Response",
    "text": "Response\n\nIn Measuring / Calculating Startup Time,I provide a summary of how the startup time is identified and calculated.\nI’m not sure exactly I understand\n\nWill the measurement methodology be the same for distributed training? For examples, we can measure the start-up time for the rank0?\n\nThe startup time is being measured for distributed training (logs only created on RANK = 0)\nI discuss in Minimal Working Example a minimal example that can be used to measure the startup times.\n\nThis is using a library I’ve been working on, ezpz that is designed to help simplify the process of setting up / initializing distributed training across many GPUs.\n\n\n\nMeasuring / Calculating Startup Time\n\nThe startup timing was identified by parsing the logfiles from existing runs and calculating the difference \\delta t = t_{1} - t_{0},\n\nt_{0} is the time stamp at the very beginning of the shell script (defined here) which then launches\nmpiexec ${mpi-args} python3 [...]\n\nt_{0} appears in the logfile as:\nJob started at: 2023-11-02-183323 on x3004c0s13b0n0\n\nt_{1} is identified as the timestamp associated with the completion of the first training step\n\nt_{1} appears in the logfile as:\n[2023-11-02 18:34:13,122] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.999), (0.9, 0.999)]\n\n\nBelow is an example of the bash script use to parse the logfiles and identify these timestamps:\n  $ for f in $(tail -5 logfiles) ; do echo $f; cat $f | grep -E \"Job started|step=0\\,\" | uniq ; echo \"\\n\" ; done\n  /lus/grand/projects/datascience/foremans/locations/polaris/projects/argonne-lcf/Megatron-DeepSpeed/outputs/gpt_actCkpt_GPT1T_4L_z1_seqlen2048_mp8_pp2_sp1_nl4_hs25600_gb16_mb1/logs/foremans-x3004c0s13b0n0-nhosts4-ngpu16-2023-11-02-183323.log\n  Job started at: 2023-11-02-183323 on x3004c0s13b0n0\n  [2023-11-02 18:34:13,122] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.999), (0.9, 0.999)]\n\n  /lus/grand/projects/datascience/foremans/locations/polaris/projects/argonne-lcf/Megatron-DeepSpeed/outputs/gpt_SP_actCkpt_GPT125M_z0_seqlen2048_mp16_pp1_sp1_nl12_hs768_gb1_mb1/logs/foremans-x3015c0s37b0n0-nhosts4-ngpu16-2023-11-02-184240.log\n  Job started at: 2023-11-02-184240 on x3015c0s37b0n0\n\n  /lus/grand/projects/datascience/foremans/locations/polaris/projects/argonne-lcf/Megatron-DeepSpeed/outputs/gpt_SP_actCkpt_GPT125M_z0_seqlen2048_mp16_pp1_sp1_nl12_hs768_gb1_mb1/logs/foremans-x3015c0s37b0n0-nhosts4-ngpu16-2023-11-02-184259.log\n  Job started at: 2023-11-02-184259 on x3015c0s37b0n0\n  [2023-11-02 18:43:23,385] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.999), (0.9, 0.999)]\n\n  /lus/grand/projects/datascience/foremans/locations/polaris/projects/argonne-lcf/Megatron-DeepSpeed/outputs/gpt_SP_actCkpt_GPT125M_z0_seqlen2048_mp16_pp1_sp1_nl12_hs768_gb1_mb1/logs/foremans-x3004c0s13b0n0-nhosts4-ngpu16-2023-11-02-184407.log\n  Job started at: 2023-11-02-184407 on x3004c0s13b0n0\n  [2023-11-02 18:44:32,804] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.999), (0.9, 0.999)]\n\n  /lus/grand/projects/datascience/foremans/locations/polaris/projects/argonne-lcf/Megatron-DeepSpeed/outputs/gpt_actCkpt_GPT1T_4L_z1_seqlen2048_mp8_pp2_sp1_nl4_hs25600_gb16_mb2/logs/foremans-x3108c0s25b1n0-nhosts2-ngpu8-2023-11-02-192739.log\n  Job started at: 2023-11-02-192739 on x3108c0s25b1n0\n\n\n\n\n\n\n\n Startup Times (Perlmutter)\n\n\n\n\n\n\n\n\nTable 1: Startup times on Perlmutter\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n****\nmodel_size\nworld_size\nstart\nstop\nt0\nt1\ndt\n\n\n\n\nforemans-nid008217-nhosts2-ngpu8-2023-10-05-191101.log\nGPT1T_1L\n8\n2023-10-05-191101\n2023-10-05-191215\n191101\n191215\n114\n\n\nforemans-nid008217-nhosts2-ngpu8-2023-10-05-191400.log\nGPT1T_1L\n8\n2023-10-05-191400\n2023-10-05-191511\n191400\n191511\n111\n\n\nforemans-nid008217-nhosts2-ngpu8-2023-10-05-191707.log\nGPT1T_1L\n8\n2023-10-05-191707\n2023-10-05-191817\n191707\n191817\n110\n\n\nforemans-nid008553-nhosts2-ngpu8-2023-10-15-114506.log\nGPT1T_2L\n8\n2023-10-15-114506\n2023-10-15-114616\n114506\n114616\n110\n\n\nforemans-nid008572-nhosts2-ngpu8-2023-10-15-133531.log\nGPT2_7B\n8\n2023-10-15-133531\n2023-10-15-133745\n133531\n133745\n214\n\n\nforemans-nid008572-nhosts2-ngpu8-2023-10-15-135041.log\nGPT2_7B\n8\n2023-10-15-135041\n2023-10-15-135255\n135041\n135255\n214\n\n\nforemans-nid008572-nhosts2-ngpu8-2023-10-15-140806.log\nGPT2_7B\n8\n2023-10-15-140806\n2023-10-15-141236\n140806\n141236\n430\n\n\nforemans-nid008572-nhosts2-ngpu8-2023-10-15-143120.log\nGPT2_7B\n8\n2023-10-15-143120\n2023-10-15-143655\n143120\n143655\n535\n\n\nforemans-nid008268-nhosts2-ngpu8-2023-10-15-154337.log\nGPT2_7B\n8\n2023-10-15-154337\n2023-10-15-154446\n154337\n154446\n109\n\n\nforemans-nid008268-nhosts2-ngpu8-2023-10-15-154943.log\nGPT1T_1L\n8\n2023-10-15-154943\n2023-10-15-155317\n154943\n155317\n374\n\n\nforemans-nid008268-nhosts2-ngpu8-2023-10-15-162315.log\nGPT1T_1L\n8\n2023-10-15-162315\n2023-10-15-162441\n162315\n162441\n126\n\n\nforemans-login12-nhosts2-ngpu8-2023-10-15-180714.log\nGPT2_7B\n8\n2023-10-15-180714\n2023-10-15-180805\n180714\n180805\n91\n\n\nforemans-login12-nhosts2-ngpu8-2023-10-15-181733.log\nGPT2_7B\n8\n2023-10-15-181733\n2023-10-15-181834\n181733\n181834\n101\n\n\nforemans-login12-nhosts2-ngpu8-2023-10-15-182228.log\nGPT1T_1L\n8\n2023-10-15-182228\n2023-10-15-183031\n182228\n183031\n803\n\n\nforemans-login12-nhosts2-ngpu8-2023-10-15-183345.log\nGPT1T_2L\n8\n2023-10-15-183345\n2023-10-15-183750\n183345\n183750\n405\n\n\nforemans-login12-nhosts2-ngpu8-2023-10-15-184442.log\nGPT1T_2L\n8\n2023-10-15-184442\n2023-10-15-184727\n184442\n184727\n285\n\n\nforemans-login12-nhosts2-ngpu8-2023-10-15-185952.log\nGPT1T_1L\n8\n2023-10-15-185952\n2023-10-15-190046\n185952\n190046\n4094\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-15-191508.log\nGPT2_7B\n8\n2023-10-15-191508\n2023-10-15-191608\n191508\n191608\n100\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-15-192404.log\nGPT2_7B\n8\n2023-10-15-192404\n2023-10-15-192504\n192404\n192504\n100\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-15-193041.log\nGPT2_7B\n8\n2023-10-15-193041\n2023-10-15-193137\n193041\n193137\n96\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-15-193448.log\nGPT2_7B\n8\n2023-10-15-193448\n2023-10-15-193540\n193448\n193540\n92\n\n\nforemans-login12-nhosts4-ngpu16-2023-10-15-195802.log\nGPT1T_1L\n16\n2023-10-15-195802\n2023-10-15-195904\n195802\n195904\n102\n\n\nforemans-login12-nhosts4-ngpu16-2023-10-15-200019.log\nGPT2_7B\n16\n2023-10-15-200019\n2023-10-15-200258\n200019\n200258\n239\n\n\nforemans-login12-nhosts4-ngpu16-2023-10-15-200902.log\nGPT2_7B\n16\n2023-10-15-200902\n2023-10-15-201239\n200902\n201239\n337\n\n\nforemans-login12-nhosts4-ngpu16-2023-10-15-201524.log\nGPT2_7B\n16\n2023-10-15-201524\n2023-10-15-201612\n201524\n201612\n88\n\n\nforemans-login12-nhosts4-ngpu16-2023-10-15-201834.log\nGPT2_7B\n16\n2023-10-15-201834\n2023-10-15-201923\n201834\n201923\n89\n\n\nforemans-login12-nhosts4-ngpu16-2023-10-15-202402.log\nGPT2_7B\n16\n2023-10-15-202402\n2023-10-15-202501\n202402\n202501\n99\n\n\nforemans-login12-nhosts4-ngpu16-2023-10-15-202606.log\nGPT2_7B\n16\n2023-10-15-202606\n2023-10-15-202713\n202606\n202713\n107\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-16-084033.log\nGPT1T_1L\n8\n2023-10-16-084033\n2023-10-16-084212\n84033\n84212\n179\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-16-084628.log\nGPT1T_1L\n8\n2023-10-16-084628\n2023-10-16-084728\n84628\n84728\n100\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-16-085401.log\nGPT1T_1L\n8\n2023-10-16-085401\n2023-10-16-085505\n85401\n85505\n104\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-16-090142.log\nGPT1T_1L\n8\n2023-10-16-090142\n2023-10-16-090305\n90142\n90305\n163\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-16-093404.log\nactCkpt_GPT13B\n8\n2023-10-16-093404\n2023-10-16-093504\n93404\n93504\n100\n\n\nforemans-nid008572-nhosts4-ngpu16-2023-10-16-101437.log\nGPT1T_1L\n16\n2023-10-16-101437\n2023-10-16-101549\n101437\n101549\n112\n\n\nforemans-nid008396-nhosts4-ngpu16-2023-10-16-101512.log\nGPT1T_1L\n16\n2023-10-16-101512\n2023-10-16-101615\n101512\n101615\n103\n\n\nforemans-nid008396-nhosts4-ngpu16-2023-10-16-102217.log\nactCkpt_GPT25B\n16\n2023-10-16-102217\n2023-10-16-102452\n102217\n102452\n235\n\n\nforemans-nid008396-nhosts4-ngpu16-2023-10-16-102750.log\nactCkpt_GPT25B\n16\n2023-10-16-102750\n2023-10-16-103243\n102750\n103243\n493\n\n\nforemans-nid008572-nhosts4-ngpu16-2023-10-16-103113.log\nactCkpt_GPT25B\n16\n2023-10-16-103113\n2023-10-16-103237\n103113\n103237\n124\n\n\nforemans-nid008396-nhosts4-ngpu16-2023-10-16-104037.log\nactCkpt_GPT25B\n16\n2023-10-16-104037\n2023-10-16-104148\n104037\n104148\n111\n\n\nforemans-nid008396-nhosts4-ngpu16-2023-10-16-104819.log\nactCkpt_GPT25B\n16\n2023-10-16-104819\n2023-10-16-110002\n104819\n110002\n5183\n\n\nforemans-nid008396-nhosts4-ngpu16-2023-10-16-110119.log\nactCkpt_GPT25B\n16\n2023-10-16-110119\n2023-10-16-110225\n110119\n110225\n106\n\n\nforemans-nid008701-nhosts4-ngpu16-2023-10-16-113715.log\nactCkpt_GPT25B\n16\n2023-10-16-113715\n2023-10-16-113824\n113715\n113824\n109\n\n\nforemans-nid008701-nhosts4-ngpu16-2023-10-16-114236.log\nGPT1T_1L\n16\n2023-10-16-114236\n2023-10-16-114338\n114236\n114338\n102\n\n\nforemans-nid008701-nhosts4-ngpu16-2023-10-16-114610.log\nGPT1T_1L\n16\n2023-10-16-114610\n2023-10-16-114711\n114610\n114711\n101\n\n\nforemans-nid008701-nhosts4-ngpu16-2023-10-16-114819.log\nGPT1T_2L\n16\n2023-10-16-114819\n2023-10-16-114953\n114819\n114953\n134\n\n\nforemans-nid008701-nhosts4-ngpu16-2023-10-16-131058.log\nGPT1T_2L\n16\n2023-10-16-131058\n2023-10-16-131203\n131058\n131203\n145\n\n\nforemans-nid008576-nhosts1-ngpu4-2023-10-16-151427.log\nGPT1T_1L\n4\n2023-10-16-151427\n2023-10-16-151600\n151427\n151600\n173\n\n\nforemans-nid008576-nhosts1-ngpu4-2023-10-16-152528.log\nGPT1T_1L\n4\n2023-10-16-152528\n2023-10-16-152640\n152528\n152640\n112\n\n\nforemans-nid008224-nhosts1-ngpu4-2023-10-16-175717.log\nGPT1T_1L\n4\n2023-10-16-175717\n2023-10-16-175829\n175717\n175829\n112\n\n\nforemans-nid008224-nhosts1-ngpu4-2023-10-16-180457.log\nGPT1T_1L\n4\n2023-10-16-180457\n2023-10-16-180605\n180457\n180605\n148\n\n\nforemans-nid008224-nhosts1-ngpu4-2023-10-16-183116.log\nGPT1T_1L\n4\n2023-10-16-183116\n2023-10-16-183216\n183116\n183216\n100\n\n\nforemans-nid008224-nhosts1-ngpu4-2023-10-16-183921.log\nGPT1T_1L\n4\n2023-10-16-183921\n2023-10-16-184033\n183921\n184033\n112\n\n\nforemans-nid008237-nhosts1-ngpu4-2023-10-16-215614.log\nGPT1T_1L\n4\n2023-10-16-215614\n2023-10-16-215815\n215614\n215815\n201\n\n\nforemans-nid008385-nhosts1-ngpu4-2023-10-17-052944.log\nGPT1T_1L\n4\n2023-10-17-052944\n2023-10-17-053139\n52944\n53139\n195\n\n\nforemans-nid008385-nhosts1-ngpu4-2023-10-17-053529.log\nGPT1T_1L\n4\n2023-10-17-053529\n2023-10-17-053650\n53529\n53650\n121\n\n\nforemans-nid008385-nhosts1-ngpu4-2023-10-17-053910.log\nGPT1T_1L\n4\n2023-10-17-053910\n2023-10-17-054120\n53910\n54120\n210\n\n\nforemans-nid008385-nhosts1-ngpu4-2023-10-17-054238.log\nGPT2_7B\n4\n2023-10-17-054238\n2023-10-17-054346\n54238\n54346\n108\n\n\nforemans-nid008385-nhosts1-ngpu4-2023-10-17-060418.log\nGPT1T_1L\n4\n2023-10-17-060418\n2023-10-17-060600\n60418\n60600\n182\n\n\nforemans-nid008385-nhosts1-ngpu4-2023-10-17-061514.log\nGPT1T_1L\n4\n2023-10-17-061514\n2023-10-17-061653\n61514\n61653\n139\n\n\nforemans-nid008385-nhosts1-ngpu4-2023-10-17-062102.log\nGPT1T_1L\n4\n2023-10-17-062102\n2023-10-17-062252\n62102\n62252\n150\n\n\nforemans-nid008385-nhosts1-ngpu4-2023-10-17-062445.log\nGPT1T_1L\n4\n2023-10-17-062445\n2023-10-17-062720\n62445\n62720\n275\n\n\nforemans-nid008333-nhosts2-ngpu8-2023-10-17-064643.log\nGPT1T_1L\n8\n2023-10-17-064643\n2023-10-17-064848\n64643\n64848\n205\n\n\nforemans-nid008333-nhosts2-ngpu8-2023-10-17-065806.log\nGPT1T_2L\n8\n2023-10-17-065806\n2023-10-17-070003\n65806\n70003\n4197\n\n\nforemans-nid008333-nhosts2-ngpu8-2023-10-17-075152.log\nGPT1T_2L\n8\n2023-10-17-075152\n2023-10-17-075502\n75152\n75502\n350\n\n\nforemans-nid008333-nhosts2-ngpu8-2023-10-17-080059.log\nGPT1T_2L\n8\n2023-10-17-080059\n2023-10-17-080434\n80059\n80434\n375\n\n\nforemans-nid008333-nhosts2-ngpu8-2023-10-17-081404.log\nGPT1T_2L\n8\n2023-10-17-081404\n2023-10-17-081920\n81404\n81920\n516\n\n\nforemans-nid008228-nhosts1-ngpu4-2023-10-17-090344.log\nGPT1T_1L\n4\n2023-10-17-090344\n2023-10-17-090714\n90344\n90714\n370\n\n\nforemans-nid008228-nhosts1-ngpu4-2023-10-17-100759.log\nGPT1T_1L\n4\n2023-10-17-100759\n2023-10-17-100957\n100759\n100957\n198\n\n\nforemans-nid008404-nhosts4-ngpu16-2023-10-17-182501.log\nGPT1T_1L\n16\n2023-10-17-182501\n2023-10-17-184001\n182501\n184001\n1500\n\n\nforemans-nid008404-nhosts4-ngpu16-2023-10-17-193736.log\nGPT1T_1L\n16\n2023-10-17-193736\n2023-10-17-193856\n193736\n193856\n120\n\n\nforemans-nid008404-nhosts4-ngpu16-2023-10-17-195432.log\nGPT1T_1L\n16\n2023-10-17-195432\n2023-10-17-195536\n195432\n195536\n104\n\n\nforemans-nid008404-nhosts4-ngpu16-2023-10-17-201659.log\nGPT1T_2L\n16\n2023-10-17-201659\n2023-10-17-201823\n201659\n201823\n164\n\n\nforemans-nid008404-nhosts4-ngpu16-2023-10-17-202949.log\nGPT1T_2L\n16\n2023-10-17-202949\n2023-10-17-203054\n202949\n203054\n105\n\n\nforemans-nid008404-nhosts4-ngpu16-2023-10-17-205848.log\nGPT1T_1L\n16\n2023-10-17-205848\n2023-10-17-205952\n205848\n205952\n104\n\n\nforemans-nid008577-nhosts8-ngpu32-2023-10-17-213244.log\nGPT1T_1L\n32\n2023-10-17-213244\n2023-10-17-213406\n213244\n213406\n162\n\n\nforemans-nid008577-nhosts8-ngpu32-2023-10-17-213558.log\nGPT1T_1L\n32\n2023-10-17-213558\n2023-10-17-213720\n213558\n213720\n162\n\n\nforemans-nid008577-nhosts8-ngpu32-2023-10-17-214900.log\nGPT1T_2L\n32\n2023-10-17-214900\n2023-10-17-214959\n214900\n214959\n59\n\n\nforemans-nid008577-nhosts8-ngpu32-2023-10-17-215201.log\nGPT1T_2L\n32\n2023-10-17-215201\n2023-10-17-215309\n215201\n215309\n108\n\n\nforemans-nid008577-nhosts8-ngpu32-2023-10-17-215612.log\nGPT1T_2L\n32\n2023-10-17-215612\n2023-10-17-215726\n215612\n215726\n114\n\n\nforemans-nid008577-nhosts8-ngpu32-2023-10-17-215938.log\nGPT1T_2L\n32\n2023-10-17-215938\n2023-10-17-220044\n215938\n220044\n4106\n\n\nforemans-nid008529-nhosts8-ngpu32-2023-10-18-110001.log\nGPT1T_4L\n32\n2023-10-18-110001\n2023-10-18-110143\n110001\n110143\n142\n\n\nforemans-nid008529-nhosts8-ngpu32-2023-10-18-110424.log\nGPT1T_8L\n32\n2023-10-18-110424\n2023-10-18-110550\n110424\n110550\n126\n\n\nforemans-nid008244-nhosts4-ngpu16-2023-10-18-110821.log\nGPT1T_8L\n16\n2023-10-18-110821\n2023-10-18-110952\n110821\n110952\n131\n\n\nforemans-nid008529-nhosts8-ngpu32-2023-10-18-111345.log\nGPT1T_8L\n32\n2023-10-18-111345\n2023-10-18-111458\n111345\n111458\n113\n\n\nforemans-nid008197-nhosts16-ngpu64-2023-10-18-112531.log\nGPT1T_16L\n64\n2023-10-18-112531\n2023-10-18-112728\n112531\n112728\n197\n\n\nforemans-nid008456-nhosts16-ngpu64-2023-10-18-113119.log\nGPT1T_16L\n64\n2023-10-18-113119\n2023-10-18-113343\n113119\n113343\n224\n\n\nforemans-nid008244-nhosts4-ngpu16-2023-10-18-113131.log\nGPT1T_4L\n16\n2023-10-18-113131\n2023-10-18-113257\n113131\n113257\n126\n\n\nforemans-nid008244-nhosts4-ngpu16-2023-10-18-113920.log\nGPT1T_4L\n16\n2023-10-18-113920\n2023-10-18-114157\n113920\n114157\n237\n\n\nforemans-nid008197-nhosts16-ngpu64-2023-10-18-114549.log\nGPT1T_16L\n64\n2023-10-18-114549\n2023-10-18-114721\n114549\n114721\n172\n\n\nforemans-nid008456-nhosts16-ngpu64-2023-10-18-114636.log\nGPT1T_16L\n64\n2023-10-18-114636\n2023-10-18-114805\n114636\n114805\n169\n\n\nforemans-nid008244-nhosts4-ngpu16-2023-10-18-115808.log\nGPT1T_4L\n16\n2023-10-18-115808\n2023-10-18-120146\n115808\n120146\n4338\n\n\nforemans-nid008456-nhosts16-ngpu64-2023-10-18-123039.log\nGPT1T_16L\n64\n2023-10-18-123039\n2023-10-18-123221\n123039\n123221\n182\n\n\nforemans-nid008389-nhosts2-ngpu8-2023-10-18-123135.log\nGPT1T_4L\n8\n2023-10-18-123135\n2023-10-18-123300\n123135\n123300\n165\n\n\nforemans-nid008244-nhosts4-ngpu16-2023-10-18-123206.log\nGPT1T_4L\n16\n2023-10-18-123206\n2023-10-18-123352\n123206\n123352\n146\n\n\nforemans-nid008456-nhosts16-ngpu64-2023-10-18-125022.log\nGPT1T_16L\n64\n2023-10-18-125022\n2023-10-18-125146\n125022\n125146\n124\n\n\nforemans-nid008256-nhosts8-ngpu32-2023-10-22-122736.log\nGPT1T_8L\n32\n2023-10-22-122736\n2023-10-22-122844\n122736\n122844\n108\n\n\nforemans-nid008256-nhosts8-ngpu32-2023-10-22-123824.log\nGPT1T_8L\n32\n2023-10-22-123824\n2023-10-22-123945\n123824\n123945\n121\n\n\nforemans-nid008256-nhosts8-ngpu32-2023-10-22-130148.log\nGPT1T_8L\n32\n2023-10-22-130148\n2023-10-22-130256\n130148\n130256\n108\n\n\nforemans-nid008256-nhosts8-ngpu32-2023-10-22-131746.log\nGPT1T_8L\n32\n2023-10-22-131746\n2023-10-22-131909\n131746\n131909\n163\n\n\nforemans-nid008256-nhosts8-ngpu32-2023-10-22-132700.log\nGPT1T_8L\n32\n2023-10-22-132700\n2023-10-22-132817\n132700\n132817\n117\n\n\nforemans-nid008256-nhosts8-ngpu32-2023-10-22-133459.log\nGPT1T_8L\n32\n2023-10-22-133459\n2023-10-22-133708\n133459\n133708\n249\n\n\nforemans-nid008380-nhosts4-ngpu16-2023-10-22-175049.log\nactCkpt_GPT25B\n16\n2023-10-22-175049\n2023-10-22-175230\n175049\n175230\n181\n\n\nforemans-nid008649-nhosts4-ngpu16-2023-10-22-192352.log\nGPT1T_4L\n16\n2023-10-22-192352\n2023-10-22-192530\n192352\n192530\n178\n\n\nforemans-nid008212-nhosts16-ngpu64-2023-10-23-081527.log\nGPT1T_8L\n64\n2023-10-23-081527\n2023-10-23-081702\n81527\n81702\n175\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-23-091436.log\nGPT1T_2L\n8\n2023-10-23-091436\n2023-10-23-091610\n91436\n91610\n174\n\n\nforemans-nid008197-nhosts32-ngpu128-2023-10-24-102617.log\nGPT1T_32L\n128\n2023-10-24-102617\n2023-10-24-102826\n102617\n102826\n209\n\n\nforemans-nid008192-nhosts64-ngpu256-2023-10-24-191748.log\nGPT1T_64L\n256\n2023-10-24-191748\n2023-10-24-192021\n191748\n192021\n273\n\n\nforemans-nid008192-nhosts128-ngpu512-2023-10-24-201243.log\nGPT1T_128L\n512\n2023-10-24-201243\n2023-10-24-201629\n201243\n201629\n386\n\n\nforemans-nid008192-nhosts128-ngpu512-2023-10-26-005401.log\nGPT1T_128L\n512\n2023-10-26-005401\n2023-10-26-005811\n5401\n5811\n410\n\n\nforemans-nid008192-nhosts32-ngpu128-2023-10-26-082710.log\nGPT1T_32L\n128\n2023-10-26-082710\n2023-10-26-083049\n82710\n83049\n339\n\n\nforemans-nid008585-nhosts2-ngpu8-2023-10-31-044203.log\nGPT1T_2L\n8\n2023-10-31-044203\n2023-10-31-044533\n44203\n44533\n330\n\n\nforemans-nid008272-nhosts4-ngpu16-2023-10-31-072717.log\nGPT1T_4L\n16\n2023-10-31-072717\n2023-10-31-073131\n72717\n73131\n414\n\n\nforemans-nid008221-nhosts8-ngpu32-2023-10-31-083055.log\nGPT1T_8L\n32\n2023-10-31-083055\n2023-10-31-083545\n83055\n83545\n490\n\n\nforemans-nid008196-nhosts16-ngpu64-2023-10-31-100336.log\nGPT1T_16L\n64\n2023-10-31-100336\n2023-10-31-100848\n100336\n100848\n512\n\n\nforemans-nid008285-nhosts2-ngpu8-2023-11-01-200430.log\nGPT1T_2L\n8\n2023-11-01-200430\n2023-11-01-200829\n200430\n200829\n399\n\n\nforemans-nid008193-nhosts8-ngpu32-2023-11-01-201702.log\nGPT1T_8L\n32\n2023-11-01-201702\n2023-11-01-202131\n201702\n202131\n429\n\n\nforemans-nid008240-nhosts16-ngpu64-2023-11-01-210454.log\nGPT1T_16L\n64\n2023-11-01-210454\n2023-11-01-211007\n210454\n211007\n553\n\n\nforemans-nid008321-nhosts2-ngpu8-2023-11-02-154438.log\nGPT1T_2L\n8\n2023-11-02-154438\n2023-11-02-154949\n154438\n154949\n511\n\n\nforemans-nid008192-nhosts128-ngpu512-2023-11-04-001717.log\nGPT1T_128L\n512\n2023-11-04-001717\n2023-11-04-002124\n1717\n2124\n407",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "🤖 AuroraGPT",
      "⏰ Starting Up Distributed Training on Aurora"
    ]
  },
  {
    "objectID": "posts/AuroraGPT/startup-times/index.html#minimal-working-example",
    "href": "posts/AuroraGPT/startup-times/index.html#minimal-working-example",
    "title": "⏰ Starting Up Distributed Training on Aurora",
    "section": "Minimal Working Example",
    "text": "Minimal Working Example\n\nAs for 3:\n\nIf we need to report the startup time for the DL applications, do we need to collect measurements using the actual Aurora NRE workloads or some small benchmarking test cases? For example, we can try to recreate the typical start-up scenarios, like library imports, and measure those separately as shown below.\n\n\nI’ve been working on a library to help simplify this:\n ezpz\nMinimal library that handles the initialization of distributed training\n\n  Working on Aurora, example:\n\nSetup / Install:\n# launch job\n$ qsub -q EarlyAppAccess -A Aurora_Deployment -l walltime=2:00:00 -l select=4 -I\n\n# load frameworks\n$ module use -a /soft/modulefiles ; module --ignore_cache load frameworks\n$ module load frameworks/.2023.12.15.001\n\n# install `ezpz`\n$ git clone https://github.com/saforem2/ezpz\n$ cd ezpz\n$ mkdir -p venvs/aurora/2023.12.15.001\n$ python3 -m venv venvs/aurora/2023.12.15.001 --system-site-packages\n$ source venvs/aurora/2023.12.15.001/bin/activate\n$ python3 -m pip install -e .\n\n# print job info and define `launch` alias\n$ source ezpz/src/ezpz/bin/savejobenv\n┌──────────────────────────────────────────────────────────────────\n│ [Hosts]:\n│     • x4415c6s5b0n0.hostmgmt2415.cm.aurora.alcf.anl.gov\nx4415c6s6b0n0.hostmgmt2415.cm.aurora.alcf.anl.gov\nx4415c6s7b0n0.hostmgmt2415.cm.aurora.alcf.anl.gov\nx4415c7s0b0n0.hostmgmt2415.cm.aurora.alcf.anl.gov\n└──────────────────────────────────────────────────────────────────\n┌──────────────────────────────────────────────────────────────────\n│ [DIST INFO]:\n│     • Loading job env from: /home/foremans/.pbsenv\n│     • HOSTFILE: /var/spool/pbs/aux/297306.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\n│     • NHOSTS: 4\n│     • NGPU_PER_HOST: 12\n│     • NGPUS (NHOSTS x NGPU_PER_HOST): 48\n│     • DIST_LAUNCH: mpiexec --verbose --envall -n 48 -ppn 12 --hostfile /var/spool/pbs/aux/297306.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\n│     • Defining alias: launch: aliased to mpiexec --verbose --envall -n 48 -ppn 12 --hostfile /var/spool/pbs/aux/297306.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\n└──────────────────────────────────────────────────────────────────\nLaunch with framework=pytorch, backend=DDP:\n# ----------------------------------------------------------\n# launch + startup on all workers with\n# • `framework` ∈ {`pytorch`, `tensorflow`}\n# • `backend` ∈ {`horovod`, `deepspeed`, `DDP`}\n# where `deepspeed` and `DDP` only available for `pytorch`\n# ----------------------------------------------------------\n$ launch python3 -m ezpz framework=pytorch backend=DDP\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:26][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:26][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:26][INFO][dist.py:243] - Using DDP for distributed training\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:28][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:28][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:29][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:29][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:29][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:30][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:30][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:30][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:30][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:30][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:30][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:34][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 1 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 2 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 3 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 4 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 0 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 5 / 47\n[2023-12-19 13:33:35][INFO][__main__.py:49] - {\n    \"_target_\": \"ezpz.configs.TrainConfig\",\n    \"framework\": \"pytorch\",\n    \"backend\": \"DDP\",\n    \"ds_config_path\": null,\n    \"port\": null,\n    \"seed\": null,\n    \"use_wandb\": true,\n    \"wandb_project_name\": null,\n    \"precision\": null,\n    \"ngpus\": null\n}\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 9 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 10 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 11 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 7 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 8 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 6 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 12 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 13 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 14 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 15 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 18 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 19 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 20 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 21 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 22 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 23 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 24 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 25 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 26 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 27 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 30 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 16 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 17 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 28 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 32 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 33 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 36 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 37 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 38 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 39 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 43 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 46 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 29 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 47 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 31 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 34 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 35 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 42 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 41 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 44 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 45 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 40 / 47\n[2023-12-19 13:33:47][INFO][dist.py:415] - Setting up wandb from rank: 0\n[2023-12-19 13:33:47][INFO][dist.py:416] - Using: WB PROJECT: ezpz\n[2023-12-19 13:33:58][INFO][dist.py:448] - W&B RUN: [flowing-wood-8](https://wandb.ai/l2hmc-qcd/ezpz/runs/uya29gm5)\n[2023-12-19 13:33:58][INFO][dist.py:490] - Running on x4415c6s5b0n0.hostmgmt2415.cm.aurora.alcf.anl.gov\n[2023-12-19 13:33:58][INFO][dist.py:506] - Reading hosts from /var/spool/pbs/aux/297306.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\n[2023-12-19 13:33:58][INFO][__main__.py:57] - Output dir: /lus/gecko/projects/Aurora_deployment/foremans/projects/saforem2/ezpz/src/ezpz/outputs/runs/pytorch/DDP/2023-12-19/13-33-17\n[2023-12-19 13:33:58][CRITICAL][dist.py:519] - 🚀 flowing-wood-8\n[2023-12-19 13:33:58][CRITICAL][dist.py:520] - 🔗 https://wandb.ai/l2hmc-qcd/ezpz/runs/uya29gm5\n[2023-12-19 13:33:58][CRITICAL][dist.py:521] - 📂/: /lus/gecko/projects/Aurora_deployment/foremans/projects/saforem2/ezpz/src/ezpz/outputs/runs/pytorch/DDP/2023-12-19/13-33-17/wandb/run-20231219_133354-uya29gm5/files\n[2023-12-19 13:33:58][INFO][dist.py:563] - Adding /lus/gecko/projects/Aurora_deployment/foremans/projects/saforem2/ezpz/src/ezpz/ezpz-pt-DDP-xpu.log to W&B artifact...\n[2023-12-19 13:33:58][INFO][dist.py:563] - Adding /lus/gecko/projects/Aurora_deployment/foremans/projects/saforem2/ezpz/src/ezpz/outputs/runs/pytorch/DDP/2023-12-19/13-33-17/__main__.log to W&B artifact...\n[2023-12-19 13:33:58][INFO][dist.py:563] - Adding /lus/gecko/projects/Aurora_deployment/foremans/projects/saforem2/ezpz/src/ezpz/outputs/runs/pytorch/DDP/2023-12-19/13-33-17/main_debug.log to W&B artifact...\n[2023-12-19 13:33:58][INFO][dist.py:563] - Adding /lus/gecko/projects/Aurora_deployment/foremans/projects/saforem2/ezpz/src/ezpz/outputs/runs/pytorch/DDP/2023-12-19/13-33-16/__main__.log to W&B artifact...",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "🤖 AuroraGPT",
      "⏰ Starting Up Distributed Training on Aurora"
    ]
  },
  {
    "objectID": "posts/ai-for-physics/l2hmc-qcd/2dU1/index.html",
    "href": "posts/ai-for-physics/l2hmc-qcd/2dU1/index.html",
    "title": "🎢 l2hmc-qcd Example: 2D U(1)",
    "section": "",
    "text": "l2hmc: Example\n \nThis notebook will (attempt) to walk through the steps needed to successfully instantiate and “run” an experiment.\nFor this example, we wish to train the L2HMC sampler for the 2D U(1) lattice gauge model with Wilson action:\n\\begin{equation*}\nS_{\\beta}(n) = \\beta \\sum_{n}\\sum_{\\mu&lt;\\nu}\\mathrm{Re}\\left[1 - U_{\\mu\\nu}(n) \\right]\n\\end{equation*}\nThis consists of the following steps:\n\nBuild an Experiment by parsing our configuration object\nTrain our model using the Experiment.train() method\nEvaluate our trained model Experiment.evaluate(job_type='eval')\nCompare our trained models’ performance against generic HMC Experiment.evaluate(job_type='hmc')\n\n\nEvaluating Performance\nExplicitly, we measure the performance of our model by comparing the tunneling rate \\delta Q of our trained sampler to that of generic HMC.\nExplicitly, the tunneling rate is given by:\n\n\\delta Q = \\frac{1}{N_{\\mathrm{chains}}}\\sum_{\\mathrm{chains}} \\left|Q_{i+1} - Q_{i}\\right|\n\nwhere the difference is between subsequent states in a chain, and the sum is over all N chains (each being ran in parallel, independently).\nSince our goal is to generate independent configurations, the more our sampler tunnels between different topological sectors (tunneling rate), the more efficient our sampler.\nImports / Setup\n! nvidia-smi | tail --lines -7\n# automatically detect and reload local changes to modules\n%load_ext autoreload\n%autoreload 2\n%matplotlib widget\n\nimport os\nimport warnings\n\nos.environ['COLORTERM'] = 'truecolor'\n\nwarnings.filterwarnings('ignore')\n# --------------------------------------\n# BE SURE TO GRAB A FRESH GPU !\nos.environ['CUDA_VISIBLE_DEVICES'] = '2'\n!echo $CUDA_VISIBLE_DEVICES\n# --------------------------------------\n\n2\ndevices = os.environ.get('CUDA_VISIBLE_DEVICES', None)\nprint(devices)\n!getconf _NPROCESSORS_ONLN  # get number of availble CPUs\n\n2\n256\nos.environ['TORCH_CPP_LOG_LEVEL'] = 'ERROR'\nos.environ['AUTOGRAPH_VERBOSITY'] = '10'\n!echo $CUDA_VISIBLE_DEVICES\n\n2\nfrom __future__ import absolute_import, print_function, annotations, division\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom IPython.display import set_matplotlib_formats\n\nfrom l2hmc.main import build_experiment\nfrom l2hmc.utils.rich import get_console\nfrom l2hmc.utils.plot_helpers import set_plot_style\n\nset_plot_style()\nplt.rcParams['grid.alpha'] = 0.8\nplt.rcParams['grid.color'] = '#404040'\nsns.set(rc={\"figure.dpi\":100, 'savefig.dpi':300})\nsns.set_context('notebook')\nsns.set_style(\"ticks\")\nset_matplotlib_formats('retina')\nplt.rcParams['figure.figsize'] = [12.4, 4.8]\n\nconsole = get_console()\nprint(console.is_jupyter)\nif console.is_jupyter:\n    console.is_jupyter = False\nprint(console.is_jupyter)\n\n--------------------------------------------------------------------------\nWARNING: There was an error initializing an OpenFabrics device.\n\n  Local host:   thetagpu23\n  Local device: mlx5_0\n--------------------------------------------------------------------------\n\n\nTrue\n\n\nFalse\nimport l2hmc\nl2hmc.__file__\n\n'/lus/grand/projects/DLHMC/foremans/locations/thetaGPU/projects/l2hmc-qcd/src/l2hmc/__init__.py'\nInitialize and Build Experiment objects:\n\nThe l2hmc.main module provides a function build_experiment:\n\ndef build_experiment(overrides: list[str]) -&gt; tfExperiment | ptExperiment:\n    ...\nwhich will:\n\nLoad the default options from conf/config.yaml\nOverride the default options with any values provided in overrides\nParse these options and build an ExperimentConfig which uniquely defines an experiment\nInstantiate / return an Experiment from the ExperimentConfig. Depending on framework=pytorch|tensorflow: a. framework=pytorch -&gt; l2hmc.experiment.pytorch.Experiment b. framework=tensorflow -&gt; l2hmc.experiment.tensorflow.Experiment\n\n&gt;&gt;&gt; train_output = experiment.train()\n&gt;&gt;&gt; eval_output = experiment.evaluate(job_type='eval')\n&gt;&gt;&gt; hmc_output = experiment.evaluate(job_type='hmc')\n\nOverriding Defaults\nSpecifics about the training / evaluation / hmc runs can be flexibly overridden by passing arguments to the training / evaluation / hmc runs, respectively\nimport numpy as np\n\n#seed = np.random.randint(100000)\nseed=76043\n\nDEFAULTS = {\n    'seed': f'{seed}',\n    'precision': 'fp16',\n    'init_aim': False,\n    'init_wandb': False,\n    'use_wandb': False,\n    'restore': False,\n    'save': False,\n    'use_tb': False,\n    'dynamics': {\n        'nleapfrog': 10,\n        'nchains': 4096,\n        'eps': 0.05,\n    },\n    'conv': 'none',\n    'steps': {\n        'log': 20,\n        'print': 250,\n        'nepoch': 5000,\n        'nera': 1,\n    },\n    'annealing_schedule': {\n        'beta_init': 4.0,\n        'beta_final': 4.0,\n    },\n    #'learning_rate': {\n    #    #'lr_init': 0.0005,\n    #    #'clip_norm': 10.0,\n    #},\n}\n\noutputs = {\n    'pytorch': {\n        'train': {},\n        'eval': {},\n        'hmc': {},\n    },\n    'tensorflow': {\n        'train': {},\n        'eval': {},\n        'hmc': {},\n    },\n}\nfrom l2hmc.configs import dict_to_list_of_overrides\nOVERRIDES = dict_to_list_of_overrides(DEFAULTS)\nOVERRIDES\n\n['seed=76043',\n 'precision=fp16',\n 'init_aim=False',\n 'init_wandb=False',\n 'use_wandb=False',\n 'restore=False',\n 'save=False',\n 'use_tb=False',\n 'dynamics.nleapfrog=10',\n 'dynamics.nchains=4096',\n 'dynamics.eps=0.05',\n 'conv=none',\n 'steps.log=20',\n 'steps.print=250',\n 'steps.nepoch=5000',\n 'steps.nera=1',\n 'annealing_schedule.beta_init=4.0',\n 'annealing_schedule.beta_final=4.0']\n# Build PyTorch Experiment\nptExpU1 = build_experiment(\n    overrides=[\n        *OVERRIDES,\n        'framework=pytorch',\n        'backend=DDP',\n    ]\n)\n\n[06/23/23 12:57:55][INFO][dist.py:338] - Global Rank: 0 / 0\n\n\n2023-06-23 12:57:58.015160: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n[06/23/23 12:58:15][INFO][dist.py:226] - Caught MASTER_PORT:2345 from environment!\n[06/23/23 12:58:15][INFO][dist.py:226] - Caught MASTER_PORT:2345 from environment!\n[06/23/23 12:58:15][WARNING][trainer.py:435] - Using torch.float16 on cuda!\n[06/23/23 12:58:17][WARNING][trainer.py:435] - Using `torch.optim.Adam` optimizer\n[06/23/23 12:58:17][INFO][trainer.py:283] - num_params in model: 1486740\n[06/23/23 12:58:17][WARNING][trainer.py:250] - logging with freq 20 for wandb.watch\n# Build TensorFlow Experiment\nimport tensorflow as tf\ntf.keras.mixed_precision.set_global_policy('mixed_float16')\n\ntfExpU1 = build_experiment(\n    overrides=[\n        *OVERRIDES,\n        'framework=tensorflow',\n        'backend=horovod',\n    ]\n)\n\nINFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\nYour GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA A100-SXM4-80GB, compute capability 8.0\n[06/23/23 12:58:18][INFO][dist.py:82] - 1, Physical GPUs and 1 Logical GPUs\n[06/23/23 12:58:18][WARNING][dist.py:108] - Using: float32 precision\n[06/23/23 12:58:18][INFO][dist.py:109] - RANK: 0, LOCAL_RANK: 0\nPyTorch\nTraining\noutputs['pytorch']['train'] = ptExpU1.trainer.train()\n    #nera=5,\n    #nepoch=2000,\n    #beta=[4.0, 4.25, 4.5, 4.75, 5.0],\n#)\n\n_ = ptExpU1.save_dataset(job_type='train', nchains=32)\n\n\n\n\n[06/23/23 12:58:19][INFO][trainer.py:439] - [TRAINING] x.dtype: torch.float32\n[06/23/23 12:58:19][INFO][trainer.py:439] - [TRAINING] self._dtype: torch.float16\n[06/23/23 12:58:19][INFO][trainer.py:107] - ┏━━━━━━━━━━━━━━━━━━━━━━━━━┓\n[06/23/23 12:58:19][INFO][trainer.py:108] - ┃ ERA: 0 / 1, BETA: 4.000 ┃\n[06/23/23 12:58:19][INFO][trainer.py:109] - ┗━━━━━━━━━━━━━━━━━━━━━━━━━┛\n[06/23/23 12:58:24][INFO][trainer.py:439] - Thermalizing configs @ 4.00 took 4.7326 s\n[06/23/23 12:58:25][INFO][trainer.py:1722] - era=0 epoch=0 tstep=1 dt=0.781 beta=4.000 loss=59.439 dQsin=0.016 dQint=0.005 energy=398.887 logprob=398.650 logdet=0.237 sldf=0.143 sldb=-0.117 sld=0.237 xeps=0.050 veps=0.050 acc=0.057 sumlogdet=0.003 acc_mask=0.057 plaqs=0.864 intQ=0.009 sinQ=0.006 lr=0.001\n[06/23/23 13:00:59][INFO][trainer.py:1722] - era=0 epoch=240 tstep=241 dt=0.600 beta=4.000 loss=-4.980 dQsin=0.212 dQint=0.069 energy=396.331 logprob=395.966 logdet=0.365 sldf=0.199 sldb=-0.146 sld=0.365 xeps=0.044 veps=0.043 acc=0.781 sumlogdet=-0.003 acc_mask=0.777 plaqs=0.864 intQ=-0.012 sinQ=-0.012 lr=0.001\n[06/23/23 13:03:34][INFO][trainer.py:1722] - era=0 epoch=500 tstep=501 dt=0.599 beta=4.000 loss=-7.162 dQsin=0.239 dQint=0.084 energy=396.375 logprob=395.945 logdet=0.431 sldf=0.234 sldb=-0.186 sld=0.431 xeps=0.051 veps=0.050 acc=0.846 sumlogdet=0.002 acc_mask=0.851 plaqs=0.864 intQ=0.053 sinQ=0.049 lr=0.001\n[06/23/23 13:06:07][INFO][trainer.py:1722] - era=0 epoch=740 tstep=741 dt=0.591 beta=4.000 loss=-8.272 dQsin=0.253 dQint=0.095 energy=396.330 logprob=395.886 logdet=0.444 sldf=0.243 sldb=-0.216 sld=0.444 xeps=0.052 veps=0.051 acc=0.872 sumlogdet=0.001 acc_mask=0.882 plaqs=0.864 intQ=0.013 sinQ=0.015 lr=0.001\n[06/23/23 13:08:39][INFO][trainer.py:1722] - era=0 epoch=1000 tstep=1001 dt=0.594 beta=4.000 loss=-8.689 dQsin=0.246 dQint=0.092 energy=396.763 logprob=396.257 logdet=0.505 sldf=0.277 sldb=-0.258 sld=0.505 xeps=0.058 veps=0.056 acc=0.865 sumlogdet=0.002 acc_mask=0.861 plaqs=0.863 intQ=-0.037 sinQ=-0.038 lr=0.001\n[06/23/23 13:11:12][INFO][trainer.py:1722] - era=0 epoch=1240 tstep=1241 dt=0.607 beta=4.000 loss=-8.190 dQsin=0.242 dQint=0.101 energy=396.304 logprob=395.726 logdet=0.578 sldf=0.316 sldb=-0.282 sld=0.578 xeps=0.065 veps=0.063 acc=0.840 sumlogdet=0.001 acc_mask=0.846 plaqs=0.864 intQ=-0.040 sinQ=-0.035 lr=0.001\n[06/23/23 13:13:44][INFO][trainer.py:1722] - era=0 epoch=1500 tstep=1501 dt=0.592 beta=4.000 loss=-9.732 dQsin=0.238 dQint=0.121 energy=397.387 logprob=396.435 logdet=0.952 sldf=0.519 sldb=-0.430 sld=0.952 xeps=0.083 veps=0.078 acc=0.752 sumlogdet=0.002 acc_mask=0.748 plaqs=0.863 intQ=0.039 sinQ=0.035 lr=0.001\n[06/23/23 13:16:17][INFO][trainer.py:1722] - era=0 epoch=1740 tstep=1741 dt=0.592 beta=4.000 loss=-10.209 dQsin=0.235 dQint=0.134 energy=397.590 logprob=396.320 logdet=1.271 sldf=0.692 sldb=-0.577 sld=1.271 xeps=0.094 veps=0.087 acc=0.725 sumlogdet=0.007 acc_mask=0.723 plaqs=0.864 intQ=0.005 sinQ=0.008 lr=0.001\n[06/23/23 13:18:52][INFO][trainer.py:1722] - era=0 epoch=2000 tstep=2001 dt=0.599 beta=4.000 loss=-12.075 dQsin=0.234 dQint=0.149 energy=399.553 logprob=397.752 logdet=1.800 sldf=0.980 sldb=-0.801 sld=1.800 xeps=0.106 veps=0.094 acc=0.638 sumlogdet=0.005 acc_mask=0.633 plaqs=0.863 intQ=0.013 sinQ=0.007 lr=0.001\n[06/23/23 13:21:25][INFO][trainer.py:1722] - era=0 epoch=2240 tstep=2241 dt=0.592 beta=4.000 loss=-13.515 dQsin=0.239 dQint=0.162 energy=399.697 logprob=397.477 logdet=2.220 sldf=1.209 sldb=-0.991 sld=2.220 xeps=0.114 veps=0.099 acc=0.616 sumlogdet=0.007 acc_mask=0.618 plaqs=0.863 intQ=0.005 sinQ=0.004 lr=0.001\n[06/23/23 13:23:58][INFO][trainer.py:1722] - era=0 epoch=2500 tstep=2501 dt=0.591 beta=4.000 loss=-11.498 dQsin=0.216 dQint=0.155 energy=400.518 logprob=397.818 logdet=2.700 sldf=1.470 sldb=-1.218 sld=2.700 xeps=0.125 veps=0.104 acc=0.538 sumlogdet=0.010 acc_mask=0.541 plaqs=0.863 intQ=-0.033 sinQ=-0.027 lr=0.001\n[06/23/23 13:26:30][INFO][trainer.py:1722] - era=0 epoch=2740 tstep=2741 dt=0.591 beta=4.000 loss=-13.669 dQsin=0.239 dQint=0.178 energy=400.852 logprob=397.768 logdet=3.084 sldf=1.679 sldb=-1.381 sld=3.084 xeps=0.132 veps=0.112 acc=0.586 sumlogdet=0.012 acc_mask=0.589 plaqs=0.864 intQ=0.052 sinQ=0.040 lr=0.001\n[06/23/23 13:29:03][INFO][trainer.py:1722] - era=0 epoch=3000 tstep=3001 dt=0.825 beta=4.000 loss=-13.659 dQsin=0.229 dQint=0.175 energy=402.199 logprob=398.541 logdet=3.658 sldf=1.994 sldb=-1.676 sld=3.658 xeps=0.142 veps=0.118 acc=0.541 sumlogdet=0.008 acc_mask=0.545 plaqs=0.863 intQ=-0.034 sinQ=-0.035 lr=0.001\n[06/23/23 13:31:36][INFO][trainer.py:1722] - era=0 epoch=3240 tstep=3241 dt=0.593 beta=4.000 loss=-14.593 dQsin=0.232 dQint=0.182 energy=403.727 logprob=399.641 logdet=4.087 sldf=2.232 sldb=-1.965 sld=4.087 xeps=0.151 veps=0.121 acc=0.489 sumlogdet=0.012 acc_mask=0.498 plaqs=0.863 intQ=-0.009 sinQ=-0.012 lr=0.001\n[06/23/23 13:34:09][INFO][trainer.py:1722] - era=0 epoch=3500 tstep=3501 dt=0.600 beta=4.000 loss=-10.267 dQsin=0.202 dQint=0.161 energy=404.429 logprob=399.713 logdet=4.716 sldf=2.575 sldb=-2.237 sld=4.716 xeps=0.152 veps=0.130 acc=0.432 sumlogdet=0.010 acc_mask=0.451 plaqs=0.863 intQ=-0.003 sinQ=-0.003 lr=0.001\n[06/23/23 13:36:44][INFO][trainer.py:1722] - era=0 epoch=3740 tstep=3741 dt=0.602 beta=4.000 loss=-16.740 dQsin=0.239 dQint=0.202 energy=404.274 logprob=399.215 logdet=5.059 sldf=2.765 sldb=-2.461 sld=5.059 xeps=0.163 veps=0.133 acc=0.503 sumlogdet=0.013 acc_mask=0.507 plaqs=0.863 intQ=-0.027 sinQ=-0.024 lr=0.001\n[06/23/23 13:39:19][INFO][trainer.py:1722] - era=0 epoch=4000 tstep=4001 dt=0.602 beta=4.000 loss=-17.072 dQsin=0.242 dQint=0.215 energy=405.285 logprob=399.736 logdet=5.549 sldf=3.037 sldb=-2.781 sld=5.549 xeps=0.171 veps=0.135 acc=0.460 sumlogdet=0.012 acc_mask=0.464 plaqs=0.864 intQ=0.013 sinQ=0.013 lr=0.001\n[06/23/23 13:41:53][INFO][trainer.py:1722] - era=0 epoch=4240 tstep=4241 dt=0.600 beta=4.000 loss=-18.798 dQsin=0.236 dQint=0.218 energy=406.449 logprob=400.293 logdet=6.156 sldf=3.370 sldb=-3.104 sld=6.156 xeps=0.179 veps=0.137 acc=0.455 sumlogdet=0.011 acc_mask=0.451 plaqs=0.864 intQ=0.009 sinQ=0.007 lr=0.001\n[06/23/23 13:44:28][INFO][trainer.py:1722] - era=0 epoch=4500 tstep=4501 dt=0.598 beta=4.000 loss=-18.046 dQsin=0.242 dQint=0.215 energy=406.391 logprob=400.047 logdet=6.343 sldf=3.476 sldb=-3.278 sld=6.343 xeps=0.183 veps=0.144 acc=0.463 sumlogdet=0.011 acc_mask=0.465 plaqs=0.864 intQ=-0.019 sinQ=-0.016 lr=0.001\n[06/23/23 13:47:02][INFO][trainer.py:1722] - era=0 epoch=4740 tstep=4741 dt=0.601 beta=4.000 loss=-16.357 dQsin=0.230 dQint=0.206 energy=407.460 logprob=400.501 logdet=6.958 sldf=3.815 sldb=-3.604 sld=6.958 xeps=0.188 veps=0.147 acc=0.423 sumlogdet=0.010 acc_mask=0.426 plaqs=0.864 intQ=0.023 sinQ=0.022 lr=0.001\n[06/23/23 13:49:51][INFO][plot_helpers.py:1005] - Saving figure to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-06-23-125815/pytorch/plots/ridgeplots/svgs/energy_ridgeplot.svg\n[06/23/23 13:49:56][INFO][plot_helpers.py:1005] - Saving figure to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-06-23-125815/pytorch/plots/ridgeplots/svgs/logprob_ridgeplot.svg\n[06/23/23 13:50:00][INFO][plot_helpers.py:1005] - Saving figure to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-06-23-125815/pytorch/plots/ridgeplots/svgs/logdet_ridgeplot.svg\n[06/23/23 13:50:05][INFO][plot_helpers.py:1005] - Saving figure to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-06-23-125815/pytorch/plots/ridgeplots/svgs/sldf_ridgeplot.svg\n[06/23/23 13:50:09][INFO][plot_helpers.py:1005] - Saving figure to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-06-23-125815/pytorch/plots/ridgeplots/svgs/sldb_ridgeplot.svg\n[06/23/23 13:50:13][INFO][plot_helpers.py:1005] - Saving figure to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-06-23-125815/pytorch/plots/ridgeplots/svgs/sld_ridgeplot.svg\n[06/23/23 13:50:56][INFO][common.py:271] - Saving dataset to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-06-23-125815/pytorch/data/train_data.h5\n[06/23/23 13:51:06][INFO][experiment.py:362] - Done saving and analyzing data.\n[06/23/23 13:51:06][INFO][experiment.py:363] - Creating summaries for WandB, Aim\nInference\nEvaluation\noutputs['pytorch']['eval'] = ptExpU1.trainer.eval(\n    job_type='eval',\n    nprint=500,\n    nchains=128,\n    eval_steps=2000,\n)\n_ = ptExpU1.save_dataset(job_type='eval', nchains=32)\n\n[06/23/23 13:52:42][WARNING][trainer.py:435] - x.shape (original): torch.Size([4096, 2, 16, 16])\n[06/23/23 13:52:42][WARNING][trainer.py:435] - x[:nchains].shape: torch.Size([128, 2, 16, 16])\n[06/23/23 13:52:42][INFO][trainer.py:1051] - eps=None\nbeta=4.0\nnlog=10\ntable=&lt;rich.table.Table object at 0x7f2722bfbdf0&gt;\nnprint=500\neval_steps=2000\nnleapfrog=None\n\n\n\n\n\n[06/23/23 13:52:46][INFO][trainer.py:1181] - estep=0 dt=0.278 beta=4.000 loss=-26.568 dQsin=0.310 dQint=0.328 energy=412.448 logprob=405.216 logdet=7.232 sldf=3.974 sldb=-3.865 sld=7.232 xeps=0.193 veps=0.148 acc=0.484 sumlogdet=0.003 acc_mask=0.508 plaqs=0.863 intQ=-0.086 sinQ=-0.055\n[06/23/23 13:54:55][INFO][trainer.py:1181] - estep=500 dt=0.226 beta=4.000 loss=-23.825 dQsin=0.266 dQint=0.227 energy=407.989 logprob=400.742 logdet=7.247 sldf=3.976 sldb=-3.845 sld=7.247 xeps=0.193 veps=0.148 acc=0.470 sumlogdet=0.029 acc_mask=0.492 plaqs=0.862 intQ=-0.164 sinQ=-0.105\n[06/23/23 13:57:02][INFO][trainer.py:1181] - estep=1000 dt=0.228 beta=4.000 loss=-23.745 dQsin=0.270 dQint=0.250 energy=410.211 logprob=402.944 logdet=7.266 sldf=3.987 sldb=-3.842 sld=7.266 xeps=0.193 veps=0.148 acc=0.456 sumlogdet=0.011 acc_mask=0.461 plaqs=0.863 intQ=-0.023 sinQ=-0.042\n[06/23/23 13:59:11][INFO][trainer.py:1181] - estep=1500 dt=0.230 beta=4.000 loss=-18.855 dQsin=0.285 dQint=0.227 energy=408.605 logprob=401.337 logdet=7.267 sldf=3.984 sldb=-3.841 sld=7.267 xeps=0.193 veps=0.148 acc=0.432 sumlogdet=0.019 acc_mask=0.508 plaqs=0.863 intQ=0.125 sinQ=0.103\n[06/23/23 14:01:28][INFO][plot_helpers.py:1005] - Saving figure to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-06-23-125815/pytorch/plots/ridgeplots/svgs/energy_ridgeplot.svg\n[06/23/23 14:01:32][INFO][plot_helpers.py:1005] - Saving figure to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-06-23-125815/pytorch/plots/ridgeplots/svgs/logprob_ridgeplot.svg\n[06/23/23 14:01:37][INFO][plot_helpers.py:1005] - Saving figure to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-06-23-125815/pytorch/plots/ridgeplots/svgs/logdet_ridgeplot.svg\n[06/23/23 14:01:41][INFO][plot_helpers.py:1005] - Saving figure to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-06-23-125815/pytorch/plots/ridgeplots/svgs/sldf_ridgeplot.svg\n[06/23/23 14:01:45][INFO][plot_helpers.py:1005] - Saving figure to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-06-23-125815/pytorch/plots/ridgeplots/svgs/sldb_ridgeplot.svg\n[06/23/23 14:01:50][INFO][plot_helpers.py:1005] - Saving figure to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-06-23-125815/pytorch/plots/ridgeplots/svgs/sld_ridgeplot.svg\n[06/23/23 14:02:02][INFO][common.py:271] - Saving dataset to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-06-23-125815/pytorch/data/eval_data.h5\n[06/23/23 14:02:03][INFO][experiment.py:362] - Done saving and analyzing data.\n[06/23/23 14:02:03][INFO][experiment.py:363] - Creating summaries for WandB, Aim\nHMC\noutputs['pytorch']['hmc'] = ptExpU1.trainer.eval(\n    job_type='hmc',\n    nprint=500,\n    nchains=128,\n    eval_steps=2000,\n)\n_ = ptExpU1.save_dataset(job_type='hmc', nchains=32)\n\n[06/23/23 14:02:13][WARNING][trainer.py:435] - Step size `eps` not specified for HMC! Using default: 0.1000 for generic HMC\n[06/23/23 14:02:13][WARNING][trainer.py:435] - x.shape (original): torch.Size([4096, 2, 16, 16])\n[06/23/23 14:02:13][WARNING][trainer.py:435] - x[:nchains].shape: torch.Size([128, 2, 16, 16])\n[06/23/23 14:02:13][INFO][trainer.py:1051] - eps=0.1\nbeta=4.0\nnlog=10\ntable=&lt;rich.table.Table object at 0x7f266407a500&gt;\nnprint=500\neval_steps=2000\nnleapfrog=20\n\n\n\n\n\n[06/23/23 14:02:17][INFO][trainer.py:1181] - hstep=0 dt=0.034 beta=4.000 loss=-11.965 dQsin=0.256 dQint=0.172 energy=395.464 logprob=395.464 logdet=0.000 acc=0.762 sumlogdet=0.000 acc_mask=0.734 plaqs=0.864 intQ=0.203 sinQ=0.148\n[06/23/23 14:02:47][INFO][trainer.py:1181] - hstep=500 dt=0.035 beta=4.000 loss=-15.159 dQsin=0.263 dQint=0.156 energy=395.520 logprob=395.520 logdet=0.000 acc=0.771 sumlogdet=0.000 acc_mask=0.734 plaqs=0.864 intQ=-0.078 sinQ=-0.086\n[06/23/23 14:03:20][INFO][trainer.py:1181] - hstep=1000 dt=0.035 beta=4.000 loss=-17.856 dQsin=0.307 dQint=0.156 energy=395.126 logprob=395.126 logdet=0.000 acc=0.832 sumlogdet=0.000 acc_mask=0.859 plaqs=0.864 intQ=0.125 sinQ=0.102\n[06/23/23 14:03:52][INFO][trainer.py:1181] - hstep=1500 dt=0.035 beta=4.000 loss=-9.512 dQsin=0.242 dQint=0.055 energy=397.486 logprob=397.486 logdet=0.000 acc=0.791 sumlogdet=0.000 acc_mask=0.812 plaqs=0.863 intQ=-0.148 sinQ=-0.106\n[06/23/23 14:04:26][INFO][plot_helpers.py:1005] - Saving figure to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-06-23-125815/pytorch/plots/ridgeplots/svgs/energy_ridgeplot.svg\n[06/23/23 14:04:32][INFO][plot_helpers.py:1005] - Saving figure to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-06-23-125815/pytorch/plots/ridgeplots/svgs/logprob_ridgeplot.svg\n[06/23/23 14:04:36][INFO][plot_helpers.py:1005] - Saving figure to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-06-23-125815/pytorch/plots/ridgeplots/svgs/logdet_ridgeplot.svg\n[06/23/23 14:04:46][INFO][common.py:271] - Saving dataset to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-06-23-125815/pytorch/data/hmc_data.h5\n[06/23/23 14:04:46][INFO][experiment.py:362] - Done saving and analyzing data.\n[06/23/23 14:04:46][INFO][experiment.py:363] - Creating summaries for WandB, Aim\nTensorFlow\nTrain\noutputs['tensorflow']['train'] = tfExpU1.trainer.train()\n#    nera=5,\n#    nepoch=2000,\n#    beta=[4.0, 4.25, 4.5, 4.75, 5.0],\n#)\n_ = tfExpU1.save_dataset(job_type='train', nchains=32)\n\n[06/23/23 14:05:07][INFO][trainer.py:200] - Looking for checkpoints in: /lus/grand/projects/DLHMC/foremans/locations/thetaGPU/projects/l2hmc-qcd/src/l2hmc/checkpoints/U1/2-16-16/nlf-10/xsplit-True/sepnets-True/merge-True/net-16-16-16_dp-0.2_bn-False/tensorflow\n[06/23/23 14:05:07][INFO][trainer.py:200] - No checkpoints found to load from. Continuing\n\n\n\n\n\n[06/23/23 14:05:07][INFO][trainer.py:1266] - ERA: 0 / 1, BETA: 4.000\n[06/23/23 14:06:32][INFO][trainer.py:200] - Thermalizing configs @ 4.00 took 85.1316 s\n\n\n{\"model_id\":\"3ce8a6d5ef17444abb0644b54156bbcf\",\"version_major\":2,\"version_minor\":0}\n\n\nWARNING:tensorflow:From /lus/grand/projects/datascience/foremans/locations/thetaGPU/miniconda3/envs/2023-04-26/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\nInstructions for updating:\nLambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n[06/23/23 14:08:07][INFO][trainer.py:1089] - era=0 epoch=0 tstep=1.000 dt=93.926 beta=4.000 loss=97.795 dQsin=0.001 dQint=0.001 energy=1281.699 logprob=1281.654 logdet=0.046 sldf=0.060 sldb=0.094 sld=0.046 xeps=0.050 veps=0.050 acc=0.001 sumlogdet=-0.001 acc_mask=0.001 plaqs=0.021 intQ=-0.042 sinQ=0.020 lr=0.001\n[06/23/23 14:09:11][INFO][trainer.py:1089] - era=0 epoch=240 tstep=241.000 dt=0.239 beta=4.000 loss=-0.984 dQsin=0.153 dQint=0.055 energy=395.706 logprob=396.037 logdet=-0.332 sldf=-0.175 sldb=0.045 sld=-0.332 xeps=0.048 veps=0.044 acc=0.550 sumlogdet=0.004 acc_mask=0.555 plaqs=0.864 intQ=0.010 sinQ=0.003 lr=0.001\n[06/23/23 14:10:15][INFO][trainer.py:1089] - era=0 epoch=500 tstep=501.000 dt=0.241 beta=4.000 loss=-4.051 dQsin=0.190 dQint=0.064 energy=394.337 logprob=395.721 logdet=-1.383 sldf=-0.746 sldb=0.488 sld=-1.383 xeps=0.047 veps=0.043 acc=0.709 sumlogdet=-0.025 acc_mask=0.708 plaqs=0.864 intQ=-0.025 sinQ=-0.025 lr=0.001\n[06/23/23 14:11:22][INFO][trainer.py:1089] - era=0 epoch=740 tstep=741.000 dt=0.236 beta=4.000 loss=-6.052 dQsin=0.206 dQint=0.072 energy=394.177 logprob=395.854 logdet=-1.677 sldf=-0.908 sldb=0.629 sld=-1.677 xeps=0.048 veps=0.043 acc=0.759 sumlogdet=-0.001 acc_mask=0.754 plaqs=0.864 intQ=0.006 sinQ=0.003 lr=0.001\n[06/23/23 14:12:27][INFO][trainer.py:1089] - era=0 epoch=1000 tstep=1001.000 dt=0.244 beta=4.000 loss=-6.203 dQsin=0.221 dQint=0.075 energy=394.858 logprob=396.599 logdet=-1.742 sldf=-0.942 sldb=0.653 sld=-1.742 xeps=0.049 veps=0.045 acc=0.811 sumlogdet=-0.011 acc_mask=0.812 plaqs=0.863 intQ=0.029 sinQ=0.026 lr=0.001\n[06/23/23 14:13:32][INFO][trainer.py:1089] - era=0 epoch=1240 tstep=1241.000 dt=0.234 beta=4.000 loss=-7.401 dQsin=0.235 dQint=0.084 energy=394.913 logprob=396.405 logdet=-1.493 sldf=-0.809 sldb=0.544 sld=-1.493 xeps=0.050 veps=0.046 acc=0.833 sumlogdet=0.004 acc_mask=0.831 plaqs=0.863 intQ=0.023 sinQ=0.021 lr=0.001\n[06/23/23 14:14:40][INFO][trainer.py:1089] - era=0 epoch=1500 tstep=1501.000 dt=0.241 beta=4.000 loss=-7.387 dQsin=0.239 dQint=0.089 energy=394.786 logprob=395.871 logdet=-1.084 sldf=-0.586 sldb=0.393 sld=-1.084 xeps=0.051 veps=0.047 acc=0.854 sumlogdet=-0.001 acc_mask=0.854 plaqs=0.864 intQ=-0.008 sinQ=-0.012 lr=0.001\n[06/23/23 14:15:46][INFO][trainer.py:1089] - era=0 epoch=1740 tstep=1741.000 dt=0.276 beta=4.000 loss=-8.684 dQsin=0.250 dQint=0.086 energy=394.998 logprob=395.804 logdet=-0.806 sldf=-0.438 sldb=0.318 sld=-0.806 xeps=0.053 veps=0.049 acc=0.878 sumlogdet=0.001 acc_mask=0.873 plaqs=0.864 intQ=0.036 sinQ=0.023 lr=0.001\n[06/23/23 14:16:52][INFO][trainer.py:1089] - era=0 epoch=2000 tstep=2001.000 dt=0.280 beta=4.000 loss=-8.376 dQsin=0.255 dQint=0.095 energy=394.788 logprob=395.364 logdet=-0.576 sldf=-0.314 sldb=0.244 sld=-0.576 xeps=0.054 veps=0.050 acc=0.896 sumlogdet=0.002 acc_mask=0.897 plaqs=0.863 intQ=-0.023 sinQ=-0.021 lr=0.001\n[06/23/23 14:17:56][INFO][trainer.py:1089] - era=0 epoch=2240 tstep=2241.000 dt=0.238 beta=4.000 loss=-9.100 dQsin=0.258 dQint=0.106 energy=395.875 logprob=396.324 logdet=-0.449 sldf=-0.245 sldb=0.219 sld=-0.449 xeps=0.059 veps=0.054 acc=0.904 sumlogdet=-0.002 acc_mask=0.902 plaqs=0.863 intQ=0.029 sinQ=0.027 lr=0.001\n[06/23/23 14:19:00][INFO][trainer.py:1089] - era=0 epoch=2500 tstep=2501.000 dt=0.244 beta=4.000 loss=-9.489 dQsin=0.247 dQint=0.103 energy=395.602 logprob=395.899 logdet=-0.297 sldf=-0.165 sldb=0.195 sld=-0.297 xeps=0.064 veps=0.058 acc=0.876 sumlogdet=0.001 acc_mask=0.864 plaqs=0.864 intQ=0.028 sinQ=0.024 lr=0.001\n[06/23/23 14:20:04][INFO][trainer.py:1089] - era=0 epoch=2740 tstep=2741.000 dt=0.251 beta=4.000 loss=-9.468 dQsin=0.250 dQint=0.107 energy=395.899 logprob=396.116 logdet=-0.217 sldf=-0.122 sldb=0.183 sld=-0.217 xeps=0.072 veps=0.065 acc=0.857 sumlogdet=0.001 acc_mask=0.854 plaqs=0.863 intQ=-0.045 sinQ=-0.034 lr=0.001\n[06/23/23 14:21:08][INFO][trainer.py:1089] - era=0 epoch=3000 tstep=3001.000 dt=0.236 beta=4.000 loss=-10.554 dQsin=0.248 dQint=0.132 energy=395.727 logprob=395.661 logdet=0.065 sldf=0.030 sldb=0.088 sld=0.065 xeps=0.084 veps=0.071 acc=0.782 sumlogdet=0.002 acc_mask=0.781 plaqs=0.864 intQ=0.024 sinQ=0.015 lr=0.001\n[06/23/23 14:22:12][INFO][trainer.py:1089] - era=0 epoch=3240 tstep=3241.000 dt=0.253 beta=4.000 loss=-10.425 dQsin=0.252 dQint=0.141 energy=396.195 logprob=396.024 logdet=0.171 sldf=0.086 sldb=0.076 sld=0.171 xeps=0.094 veps=0.080 acc=0.790 sumlogdet=0.002 acc_mask=0.795 plaqs=0.864 intQ=-0.002 sinQ=-0.000 lr=0.001\n[06/23/23 14:23:17][INFO][trainer.py:1089] - era=0 epoch=3500 tstep=3501.000 dt=0.271 beta=4.000 loss=-13.095 dQsin=0.254 dQint=0.161 energy=396.836 logprob=396.210 logdet=0.627 sldf=0.335 sldb=-0.134 sld=0.627 xeps=0.109 veps=0.089 acc=0.709 sumlogdet=0.002 acc_mask=0.708 plaqs=0.864 intQ=0.045 sinQ=0.043 lr=0.001\n[06/23/23 14:24:22][INFO][trainer.py:1089] - era=0 epoch=3740 tstep=3741.000 dt=0.242 beta=4.000 loss=-13.164 dQsin=0.226 dQint=0.160 energy=399.160 logprob=397.731 logdet=1.429 sldf=0.772 sldb=-0.496 sld=1.429 xeps=0.123 veps=0.093 acc=0.585 sumlogdet=-0.003 acc_mask=0.574 plaqs=0.864 intQ=0.002 sinQ=-0.000 lr=0.001\n[06/23/23 14:25:27][INFO][trainer.py:1089] - era=0 epoch=4000 tstep=4001.000 dt=0.254 beta=4.000 loss=-15.590 dQsin=0.251 dQint=0.197 energy=399.077 logprob=397.221 logdet=1.856 sldf=1.005 sldb=-0.672 sld=1.856 xeps=0.138 veps=0.104 acc=0.600 sumlogdet=-0.006 acc_mask=0.601 plaqs=0.863 intQ=-0.021 sinQ=-0.013 lr=0.001\n[06/23/23 14:26:31][INFO][trainer.py:1089] - era=0 epoch=4240 tstep=4241.000 dt=0.244 beta=4.000 loss=-14.301 dQsin=0.232 dQint=0.177 energy=401.006 logprob=398.483 logdet=2.523 sldf=1.369 sldb=-1.005 sld=2.523 xeps=0.150 veps=0.109 acc=0.538 sumlogdet=0.006 acc_mask=0.539 plaqs=0.864 intQ=0.018 sinQ=0.008 lr=0.001\n[06/23/23 14:27:38][INFO][trainer.py:1089] - era=0 epoch=4500 tstep=4501.000 dt=0.245 beta=4.000 loss=-14.125 dQsin=0.209 dQint=0.183 energy=403.764 logprob=400.618 logdet=3.145 sldf=1.714 sldb=-1.357 sld=3.145 xeps=0.166 veps=0.109 acc=0.411 sumlogdet=-0.002 acc_mask=0.407 plaqs=0.863 intQ=0.014 sinQ=0.017 lr=0.001\n[06/23/23 14:28:43][INFO][trainer.py:1089] - era=0 epoch=4740 tstep=4741.000 dt=0.241 beta=4.000 loss=-21.004 dQsin=0.266 dQint=0.235 energy=402.266 logprob=399.061 logdet=3.205 sldf=1.750 sldb=-1.493 sld=3.205 xeps=0.172 veps=0.121 acc=0.536 sumlogdet=0.002 acc_mask=0.539 plaqs=0.863 intQ=-0.024 sinQ=-0.016 lr=0.001\n[06/23/23 14:29:47][INFO][trainer.py:1303] - Saving took: 3.12328e-05s\n[06/23/23 14:29:47][INFO][trainer.py:1304] - Checkpoint saved to: /lus/grand/projects/DLHMC/foremans/locations/thetaGPU/projects/l2hmc-qcd/src/l2hmc/checkpoints/U1/2-16-16/nlf-10/xsplit-True/sepnets-True/merge-True/net-16-16-16_dp-0.2_bn-False/tensorflow\n[06/23/23 14:29:47][INFO][trainer.py:1305] - Era 0 took: 1480.06s\n[06/23/23 14:29:52][INFO][plot_helpers.py:1005] - Saving figure to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-06-23-125818/tensorflow/plots/ridgeplots/svgs/energy_ridgeplot.svg\n[06/23/23 14:29:58][INFO][plot_helpers.py:1005] - Saving figure to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-06-23-125818/tensorflow/plots/ridgeplots/svgs/logprob_ridgeplot.svg\n[06/23/23 14:30:03][INFO][plot_helpers.py:1005] - Saving figure to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-06-23-125818/tensorflow/plots/ridgeplots/svgs/logdet_ridgeplot.svg\n[06/23/23 14:30:08][INFO][plot_helpers.py:1005] - Saving figure to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-06-23-125818/tensorflow/plots/ridgeplots/svgs/sldf_ridgeplot.svg\n[06/23/23 14:30:13][INFO][plot_helpers.py:1005] - Saving figure to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-06-23-125818/tensorflow/plots/ridgeplots/svgs/sldb_ridgeplot.svg\n[06/23/23 14:30:18][INFO][plot_helpers.py:1005] - Saving figure to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-06-23-125818/tensorflow/plots/ridgeplots/svgs/sld_ridgeplot.svg\n[06/23/23 14:31:02][INFO][common.py:271] - Saving dataset to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-06-23-125818/tensorflow/data/train_data.h5\n[06/23/23 14:31:12][INFO][experiment.py:362] - Done saving and analyzing data.\n[06/23/23 14:31:12][INFO][experiment.py:363] - Creating summaries for WandB, Aim\nInference\nEvaluate\noutputs['tensorflow']['eval'] = tfExpU1.trainer.eval(\n    job_type='eval',\n    nprint=500,\n    nchains=128,\n    eval_steps=2000,\n)\n_ = tfExpU1.save_dataset(job_type='eval', nchains=32)\n\n[06/23/23 14:31:23][WARNING][trainer.py:196] - x.shape (original): (4096, 2, 16, 16)\n[06/23/23 14:31:23][WARNING][trainer.py:196] - x[:nchains].shape: (128, 2, 16, 16)\n[06/23/23 14:31:23][INFO][trainer.py:200] - eps = None\nbeta = 4.0\nnlog = 10\ntable = &lt;rich.table.Table object at 0x7f26042c9e70&gt;\nnprint = 500\neval_steps = 2000\nnleapfrog = None\n\n\n\n\n\n{\"model_id\":\"42d6b6d371eb4dbca473bb047e79f408\",\"version_major\":2,\"version_minor\":0}\n\n\n[06/23/23 14:33:00][INFO][trainer.py:200] - estep=0 dt=13.921 beta=4.000 loss=-34.934 dQsin=0.296 dQint=0.242 energy=402.696 logprob=398.796 logdet=3.900 sldf=2.138 sldb=-1.896 sld=3.900 xeps=0.183 veps=0.124 acc=0.472 sumlogdet=0.008 acc_mask=0.469 plaqs=0.865 intQ=0.094 sinQ=0.060\n[06/23/23 14:33:49][INFO][trainer.py:200] - estep=500 dt=0.049 beta=4.000 loss=-14.736 dQsin=0.258 dQint=0.203 energy=404.299 logprob=400.366 logdet=3.932 sldf=2.151 sldb=-1.896 sld=3.932 xeps=0.183 veps=0.124 acc=0.456 sumlogdet=-0.009 acc_mask=0.500 plaqs=0.862 intQ=-0.211 sinQ=-0.169\n[06/23/23 14:34:27][INFO][trainer.py:200] - estep=1000 dt=0.048 beta=4.000 loss=-14.039 dQsin=0.233 dQint=0.211 energy=403.103 logprob=399.185 logdet=3.917 sldf=2.142 sldb=-1.890 sld=3.917 xeps=0.183 veps=0.124 acc=0.477 sumlogdet=0.034 acc_mask=0.477 plaqs=0.864 intQ=0.070 sinQ=0.055\n[06/23/23 14:35:05][INFO][trainer.py:200] - estep=1500 dt=0.048 beta=4.000 loss=-19.743 dQsin=0.225 dQint=0.203 energy=402.832 logprob=398.931 logdet=3.901 sldf=2.136 sldb=-1.895 sld=3.901 xeps=0.183 veps=0.124 acc=0.437 sumlogdet=-0.012 acc_mask=0.453 plaqs=0.864 intQ=-0.016 sinQ=-0.026\n[06/23/23 14:35:49][INFO][plot_helpers.py:1005] - Saving figure to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-06-23-125818/tensorflow/plots/ridgeplots/svgs/energy_ridgeplot.svg\n[06/23/23 14:35:54][INFO][plot_helpers.py:1005] - Saving figure to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-06-23-125818/tensorflow/plots/ridgeplots/svgs/logprob_ridgeplot.svg\n[06/23/23 14:35:59][INFO][plot_helpers.py:1005] - Saving figure to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-06-23-125818/tensorflow/plots/ridgeplots/svgs/logdet_ridgeplot.svg\n[06/23/23 14:36:04][INFO][plot_helpers.py:1005] - Saving figure to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-06-23-125818/tensorflow/plots/ridgeplots/svgs/sldf_ridgeplot.svg\n[06/23/23 14:36:09][INFO][plot_helpers.py:1005] - Saving figure to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-06-23-125818/tensorflow/plots/ridgeplots/svgs/sldb_ridgeplot.svg\n[06/23/23 14:36:14][INFO][plot_helpers.py:1005] - Saving figure to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-06-23-125818/tensorflow/plots/ridgeplots/svgs/sld_ridgeplot.svg\n[06/23/23 14:36:29][INFO][common.py:271] - Saving dataset to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-06-23-125818/tensorflow/data/eval_data.h5\n[06/23/23 14:36:29][INFO][experiment.py:362] - Done saving and analyzing data.\n[06/23/23 14:36:29][INFO][experiment.py:363] - Creating summaries for WandB, Aim\nHMC\noutputs['tensorflow']['hmc'] = tfExpU1.trainer.eval(\n    job_type='hmc',\n    nprint=500,\n    nchains=128,\n    eval_steps=2000,\n)\n_ = tfExpU1.save_dataset(job_type='hmc', nchains=32)\n\n[06/23/23 14:36:40][WARNING][trainer.py:196] - Step size `eps` not specified for HMC! Using default: 0.1000 for generic HMC\n[06/23/23 14:36:40][WARNING][trainer.py:196] - x.shape (original): (4096, 2, 16, 16)\n[06/23/23 14:36:40][WARNING][trainer.py:196] - x[:nchains].shape: (128, 2, 16, 16)\n[06/23/23 14:36:40][INFO][trainer.py:200] - eps = 0.1\nbeta = 4.0\nnlog = 10\ntable = &lt;rich.table.Table object at 0x7f17f07da080&gt;\nnprint = 500\neval_steps = 2000\nnleapfrog = 20\n\n\n\n\n\n{\"model_id\":\"c9e14ead5fda4789a4a40038a941d064\",\"version_major\":2,\"version_minor\":0}\n\n\n[06/23/23 14:38:03][INFO][trainer.py:200] - hstep=0 dt=0.197 beta=4.000 loss=-14.990 dQsin=0.288 dQint=0.195 energy=397.008 logprob=397.008 logdet=0.000 acc=0.822 sumlogdet=0.000 acc_mask=0.828 plaqs=0.862 intQ=-0.148 sinQ=-0.153\n[06/23/23 14:39:55][INFO][trainer.py:200] - hstep=500 dt=0.193 beta=4.000 loss=-11.040 dQsin=0.261 dQint=0.141 energy=396.582 logprob=396.582 logdet=0.000 acc=0.815 sumlogdet=0.000 acc_mask=0.781 plaqs=0.862 intQ=0.055 sinQ=0.060\n[06/23/23 14:41:47][INFO][trainer.py:200] - hstep=1000 dt=0.193 beta=4.000 loss=-14.025 dQsin=0.287 dQint=0.180 energy=395.838 logprob=395.838 logdet=0.000 acc=0.818 sumlogdet=0.000 acc_mask=0.836 plaqs=0.863 intQ=-0.117 sinQ=-0.090\n[06/23/23 14:43:39][INFO][trainer.py:200] - hstep=1500 dt=0.193 beta=4.000 loss=-18.793 dQsin=0.300 dQint=0.195 energy=393.051 logprob=393.051 logdet=0.000 acc=0.813 sumlogdet=0.000 acc_mask=0.844 plaqs=0.862 intQ=0.047 sinQ=0.039\n[06/23/23 14:45:36][INFO][plot_helpers.py:1005] - Saving figure to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-06-23-125818/tensorflow/plots/ridgeplots/svgs/energy_ridgeplot.svg\n[06/23/23 14:45:45][INFO][plot_helpers.py:1005] - Saving figure to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-06-23-125818/tensorflow/plots/ridgeplots/svgs/logprob_ridgeplot.svg\n[06/23/23 14:45:49][INFO][plot_helpers.py:1005] - Saving figure to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-06-23-125818/tensorflow/plots/ridgeplots/svgs/logdet_ridgeplot.svg\n[06/23/23 14:46:01][INFO][common.py:271] - Saving dataset to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-06-23-125818/tensorflow/data/hmc_data.h5\n[06/23/23 14:46:01][INFO][experiment.py:362] - Done saving and analyzing data.\n[06/23/23 14:46:01][INFO][experiment.py:363] - Creating summaries for WandB, Aim\nModel Performance\nOur goal is improving the efficiency of our MCMC sampler.\nIn particular, we are interested in generating independent save_datasetrations which we can then use to calculate expectation values of physical observables.\nFor our purposes, we are interested in obtaining lattice configurations from distinct topological charge sectors, as characterized by a configurations topological charge, Q.\nHMC is known to suffer from critical slowing down, a phenomenon in which our configurations remains stuck in some local topological charge sector and fails to produce distinct configurations.\nIn particular, it is known that the integrated autocorrelation time of the topological charge \\tau grows exponentially with decreasing lattice spacing (i.e. continuum limit), making this theory especially problematic.\nBecause of this, we can assess our models’ performance by looking at the tunneling rate, i.e. the rate at which our sampler jumps between these different charge sectors.\nWe can write this quantity as:\n\n\\delta Q = |Q^{(i)} - Q^{(i-1)}|\n\nwhere we look at the difference in the topological charge between sequential configurations.\n\nNote: The efficiency of our sampler is directly proportional to the tunneling rate, which is inversely proportional to the integrated autocorrelation time \\tau, i.e.\n \n\n\\text{Efficiency} \\propto \\delta Q \\propto \\frac{1}{\\tau}\n\nExplicitly, this means that the more efficient the model \\longrightarrow\n- the larger tunneling rate - the smaller integrated autocorrelation time for Q\nimport xarray as xr\n\ndef get_thermalized_configs(\n        x: np.ndarray | xr.DataArray,\n        drop: int = 5\n) -&gt; np.ndarray | xr.DataArray:\n    \"\"\"Drop the first `drop` states across all chains.\n\n    x.shape = [draws, chains]\n    \"\"\"\n    if isinstance(x, np.ndarray):\n        return np.sort(x)[..., :-drop]\n    if isinstance(x, xr.DataArray):\n        return x.sortby(\n            ['chain', 'draw'],\n            ascending=False\n        )[..., :-drop]\n    raise TypeError\nComparisons\nWe can measure our models’ performance explicitly by looking at the average tunneling rate, \\delta Q_{\\mathbb{Z}}, for our trained model and comparing it against generic HMC.\nRecall,\n\\delta Q_{\\mathbb{Z}} := \\big|Q^{(i+1)}_{\\mathbb{Z}} - Q^{(i)}_{\\mathbb{Z}}\\big|\nwhere a higher value of \\delta Q_{\\mathbb{Z}} corresponds to better tunneling of the topological charge, Q_{\\mathbb{Z}}.\nNote that we can get a concise representation of the data from different parts of our run via:\nNote that the data from each of the different parts of our experiment (i.e. train, eval, and hmc) are stored as a dict, e.g.\n&gt;&gt;&gt; list(ptExpU1.trainer.histories.keys())\n['train', 'eval', 'hmc']\n&gt;&gt;&gt; train_history = ptExpU1.trainer.histories['train']\n&gt;&gt;&gt; train_dset = train_history.get_dataset()\n&gt;&gt;&gt; assert isinstance(train_history, l2hmc.utils.history.BaseHistory)\n&gt;&gt;&gt; assert isinstance(train_dset, xarray.Dataset)\n(see below, for example)\nWe aggregate the data into the dsets dict below, grouped by:\n\nFramework (pytorch / tensorflow)\nJob type (train, eval, hmc)\nimport logging\nlog = logging.getLogger(__name__)\ndsets = {}\nfws = ['pt', 'tf']\nmodes = ['train', 'eval', 'hmc']\nfor fw in fws:\n    dsets[fw] = {}\n    for mode in modes:\n        hist = None\n        if fw == 'pt':\n            hist = ptExpU1.trainer.histories.get(mode, None)\n        elif fw == 'tf':\n            hist = tfExpU1.trainer.histories.get(mode, None)\n        if hist is not None:\n            console.print(f'Getting dataset for {fw}: {mode}')\n            dsets[fw][mode] = hist.get_dataset()\n\nGetting dataset for pt: train\nGetting dataset for pt: eval\nGetting dataset for pt: hmc\nGetting dataset for tf: train\nGetting dataset for tf: eval\nGetting dataset for tf: hmc\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.rcParams['text.usetex'] = False\nimport matplotlib.pyplot as plt\nfrom l2hmc.utils.plot_helpers import COLORS, set_plot_style\n\nset_plot_style()\n\nfig, ax = plt.subplots(figsize=(16, 3), ncols=2)\n\n# ---------------------------------------------\n# ---- DROP FIRST 20% FOR THERMALIZATION ------\n# ---------------------------------------------\nKEEP = int(0.8 * len(dsets['tf']['eval'].draw))\ndqpte = get_thermalized_configs(dsets['pt']['eval']['dQint'].astype('int'))\ndqpth = get_thermalized_configs(dsets['pt']['hmc']['dQint'].astype('int'))\n\ndqtfe = get_thermalized_configs(dsets['tf']['eval']['dQint'].astype('int'))\ndqtfh = get_thermalized_configs(dsets['tf']['hmc']['dQint'].astype('int'))\n\n_ = sns.distplot(\n    dqpte.sum('chain'),\n    kde=False,\n    color=COLORS['blue'],\n    label='Eval',\n    ax=ax[0]\n)\n_ = sns.distplot(\n    dqpth.sum('chain'),\n    kde=False,\n    color=COLORS['red'],\n    label='HMC',\n    ax=ax[0]\n)\n\n_ = ax[0].set_title('PyTorch')\n_ = ax[0].set_xlabel(\n    f'# tunneling events / {dqpte.shape[-1]} configurations'\n)\n_ = ax[0].legend(loc='best', frameon=False)\nplt.legend()\n\n_ = sns.distplot(\n    dqtfe.sum('chain'),\n    kde=False,\n    color=COLORS['blue'],\n    label='Eval',\n    ax=ax[1]\n)\n_ = sns.distplot(\n    dqtfh.sum('chain'),\n    kde=False,\n    color=COLORS['red'],\n    label='HMC',\n    ax=ax[1]\n)\n\n_ = ax[1].set_title('TensorFlow')\n_ = ax[1].set_xlabel(\n    #r\"\"\"$\\sum_{i=0} \\left|\\delta Q_{i}\\right|$\"\"\",\n    #fontsize='large',\n    f'# tunneling events / {dqpte.shape[-1]} configurations'\n)\n_ = ax[1].legend(loc='best', frameon=False)\nTensorFlow Results\nimport rich\nsns.set_context('notebook')\nndraws = len(dsets['tf']['eval']['dQint'].draw)\ndrop = int(0.1 * ndraws)\nkeep = int(0.9 * ndraws)\n\ndqe = dsets['tf']['eval']['dQint'][:, -90:]\ndqh = dsets['tf']['hmc']['dQint'][:, -90:]\n\netot = dqe.astype(int).sum()\nhtot = dqh.astype(int).sum()\n\nfsize = plt.rcParams['figure.figsize']\nfigsize = (2.5 * fsize[0], fsize[1])\nfig, ax = plt.subplots(figsize=figsize, ncols=2)\n_ = dqe.astype(int).plot(ax=ax[0])\n_ = dqh.astype(int).plot(ax=ax[1])\n_ = ax[0].set_title(f'Eval, total: {etot.values}', fontsize='x-large');\n_ = ax[1].set_title(f'HMC, total: {htot.values}', fontsize='x-large');\n_ = fig.suptitle(fr'TensorFlow Improvement: {100*(etot / htot):3.0f}%', fontsize='x-large')\n\nconsole.print(f\"TensorFlow, EVAL\\n dQint.sum('chain'):\\n {dqe.astype(int).sum('chain').T}\")\nconsole.print(f\"dQint.sum(): {dqe.astype(int).sum().T}\")\nconsole.print(f\"TensorFlow, HMC\\n dQint.sum('chain'):\\n {dqh.astype(int).sum('chain').T}\")\nconsole.print(f\"dQint.sum(): {dqh.astype(int).sum().T}\")\n\nTensorFlow, EVAL\n dQint.sum('chain'):\n &lt;xarray.DataArray 'dQint' (draw: 90)&gt;\narray([13, 22, 11, 25, 14, 19, 20, 25, 13, 19, 22, 18, 10, 10, 15, 12, 17,\n       10, 19, 23, 17, 16, 14, 24, 16, 29, 15, 18, 16, 16, 20, 14,  5,  8,\n        9, 13, 14, 20, 24, 12, 12, 15, 23, 20,  8, 14, 16, 12, 17, 28, 18,\n       19, 18, 12, 27, 16, 24, 14, 21, 20, 19, 14, 14, 21, 22, 11, 22, 17,\n       23, 20, 17, 15, 22, 11, 12, 13, 17, 12, 17, 24, 27, 16, 12, 13, 12,\n       17, 18, 18, 16, 24])\nCoordinates:\n  * draw     (draw) int64 110 111 112 113 114 115 ... 194 195 196 197 198 199\ndQint.sum(): &lt;xarray.DataArray 'dQint' ()&gt;\narray(1527)\nTensorFlow, HMC\n dQint.sum('chain'):\n &lt;xarray.DataArray 'dQint' (draw: 90)&gt;\narray([ 8,  5,  9,  7, 14, 16, 12, 12, 15, 12, 10, 13, 13, 12,  8, 13, 12,\n        3, 11, 12,  7, 12, 10,  6,  8, 16,  8, 17,  8,  9,  7,  1, 10, 12,\n       13, 11, 21, 15, 11, 11,  7, 10,  6,  6, 13,  7,  8,  9, 11,  5, 12,\n       15, 13, 10,  6, 10,  6,  8,  7,  6, 11, 12, 12, 13,  7, 16,  8, 10,\n       14, 17, 11, 11, 13,  9,  9, 11,  9, 11, 13,  9, 11,  9,  7,  4,  6,\n        7, 10, 12, 17, 14])\nCoordinates:\n  * draw     (draw) int64 110 111 112 113 114 115 ... 194 195 196 197 198 199\ndQint.sum(): &lt;xarray.DataArray 'dQint' ()&gt;\narray(928)\nPyTorch Results\nsns.set_context('notebook')\nndraws = len(dsets['pt']['eval']['dQint'].draw)\ndrop = int(0.1 * ndraws)\nkeep = int(0.9 * ndraws)\n\ndqe = dsets['pt']['eval']['dQint'][:, -90:]\ndqh = dsets['pt']['hmc']['dQint'][:, -90:]\n\netot = dqe.astype(int).sum()\nhtot = dqh.astype(int).sum()\n\nfsize = plt.rcParams['figure.figsize']\nfigsize = (2.5 * fsize[0], 0.8 * fsize[1])\nfig, ax = plt.subplots(figsize=figsize, ncols=2)\n_ = dqe.astype(int).plot(ax=ax[0])\n_ = dqh.astype(int).plot(ax=ax[1])\n_ = ax[0].set_title(f'Eval, total: {etot.values}', fontsize='x-large');\n_ = ax[1].set_title(f'HMC, total: {htot.values}', fontsize='x-large');\n_ = fig.suptitle(fr'PyTorch Improvement: {100*(etot / htot):3.0f}%', fontsize='x-large')\n\nconsole.print(60 * '-')\nconsole.print(f\"PyTorch, EVAL\\n dQint.sum('chain'):\\n {dqe.astype(int).sum('chain').T.values}\")\nconsole.print(f\"dQint.sum(): {dqe.astype(int).sum().T.values}\")\nconsole.print(60 * '-')\nconsole.print(f\"PyTorch, HMC\\n dQint.sum('chain'):\\n {dqh.astype(int).sum('chain').T.values}\")\nconsole.print(f\"dQint.sum(): {dqh.astype(int).sum().T.values}\")\n\n------------------------------------------------------------\nPyTorch, EVAL\n dQint.sum('chain'):\n [26 16 12 23 13 16 39 18 18 18 15 16 27 17 25 16 11 21 20 18 22 21 13 20\n 16 19 12 26 17 16 13 17 14 18 15 15 18 23 29 20 17 23 11 16 15 15 19 22\n 25 22 19 28 20 20 20 11 24 24 13 15 26 22 14 22 23 23 19 17 21 10 20 14\n 16 17 19 11 21 19 15 20 13 16  9 20 21 20 21 22 23 15]\ndQint.sum(): 1677\n------------------------------------------------------------\nPyTorch, HMC\n dQint.sum('chain'):\n [14  6 10  5  7  9 14  8 12 10 19  8  4  6  9  7  9 17  9  7 11 13  9 11\n  4  9  7 14 10  6 15  6 10  9 13  7 15 10  7  9  3 14  8  6 11  9  9  6\n  9  6 16  6  8 10 14 16  9 12 15 10  9  9  5  6 12 17  6  8  9 12  5 12\n 16  9  7  8 11 15 16 12 12  7  5 14  9  9 13  6 12 10]\ndQint.sum(): 883\nComparisons\nimport matplotlib.pyplot as plt\nfrom l2hmc.utils.plot_helpers import set_plot_style, COLORS\n\nimport seaborn as sns\nset_plot_style()\nplt.rcParams['axes.linewidth'] = 2.0\nsns.set_context('notebook')\nfigsize = plt.rcParamsDefault['figure.figsize']\nplt.rcParams['figure.dpi'] = plt.rcParamsDefault['figure.dpi']\n\nfor idx in range(4):\n    fig, (ax, ax1) = plt.subplots(\n        ncols=2,\n        #nrows=4,\n        figsize=(3. * figsize[0], figsize[1]),\n    )\n    _ = ax.plot(\n        dsets['pt']['eval'].intQ[idx] + 5,  # .dQint.mean('chain')[100:],\n        color=COLORS['red'],\n        ls=':',\n        label='Trained',\n        lw=1.5,\n    );\n\n    _ = ax.plot(\n        dsets['pt']['hmc'].intQ[idx] - 5,  # .dQint.mean('chain')[100:],\n        ls='-',\n        label='HMC',\n        color='#666666',\n        zorder=5,\n        lw=2.0,\n    );\n\n    _ = ax1.plot(\n        dsets['tf']['eval'].intQ[idx] + 5,  # .dQint.mean('chain')[-100:],\n        color=COLORS['blue'],\n        ls=':',\n        label='Trained',\n        lw=1.5,\n\n    );\n    _ = ax1.plot(\n        dsets['tf']['hmc'].intQ[idx] - 5,  # .dQint.mean('chain')[-100:],\n        color='#666666',\n        ls='-',\n        label='HMC',\n        zorder=5,\n        lw=2.0,\n    );\n    _ = ax.set_title('PyTorch', fontsize='x-large')\n    _ = ax1.set_title('TensorFlow', fontsize='x-large')\n    #_ = ax1.set_ylim(ax.get_ylim())\n    _ = ax.grid(True, alpha=0.2)\n    _ = ax1.grid(True, alpha=0.2)\n    _ = ax.set_xlabel('MD Step', fontsize='large')\n    _ = ax1.set_xlabel('MD Step', fontsize='large')\n    _ = ax.set_ylabel('dQint', fontsize='large')\n    _ = ax.legend(loc='best', ncol=2, labelcolor='#939393')\n    _ = ax1.legend(loc='best', ncol=2, labelcolor='#939393')",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "⚛️ AI for Physics",
      "🎢 L2HMC for LQCD",
      "🎢 <code>l2hmc-qcd</code> Example: 2D U(1)"
    ]
  },
  {
    "objectID": "posts/ai-for-physics/l2hmc-qcd/2dU1/index.html#contents",
    "href": "posts/ai-for-physics/l2hmc-qcd/2dU1/index.html#contents",
    "title": "🎢 l2hmc-qcd Example: 2D U(1)",
    "section": "Contents",
    "text": "Contents\n\nl2hmc: Example\nImports / Setup\nInitialize and Build Experiment objects:\nPyTorch\nTraining\nInference\nTensorFlow\nTrain\nInference\nModel Performance\nComparisons\nTensorFlow Results\nPyTorch Results\nComparisons",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "⚛️ AI for Physics",
      "🎢 L2HMC for LQCD",
      "🎢 <code>l2hmc-qcd</code> Example: 2D U(1)"
    ]
  },
  {
    "objectID": "posts/ai-for-physics/l2hmc-qcd/2dU1/index.html#imports-setup",
    "href": "posts/ai-for-physics/l2hmc-qcd/2dU1/index.html#imports-setup",
    "title": "🎢 l2hmc-qcd Example: 2D U(1)",
    "section": "Imports / Setup",
    "text": "Imports / Setup",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "⚛️ AI for Physics",
      "🎢 L2HMC for LQCD",
      "🎢 <code>l2hmc-qcd</code> Example: 2D U(1)"
    ]
  },
  {
    "objectID": "posts/ai-for-physics/l2hmc-qcd/2dU1/index.html#pytorch",
    "href": "posts/ai-for-physics/l2hmc-qcd/2dU1/index.html#pytorch",
    "title": "🎢 l2hmc-qcd Example: 2D U(1)",
    "section": "PyTorch",
    "text": "PyTorch",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "⚛️ AI for Physics",
      "🎢 L2HMC for LQCD",
      "🎢 <code>l2hmc-qcd</code> Example: 2D U(1)"
    ]
  },
  {
    "objectID": "posts/ai-for-physics/l2hmc-qcd/2dU1/index.html#tensorflow",
    "href": "posts/ai-for-physics/l2hmc-qcd/2dU1/index.html#tensorflow",
    "title": "🎢 l2hmc-qcd Example: 2D U(1)",
    "section": "TensorFlow",
    "text": "TensorFlow",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "⚛️ AI for Physics",
      "🎢 L2HMC for LQCD",
      "🎢 <code>l2hmc-qcd</code> Example: 2D U(1)"
    ]
  },
  {
    "objectID": "posts/ai-for-physics/l2hmc-qcd/2dU1/index.html#tensorflow-results",
    "href": "posts/ai-for-physics/l2hmc-qcd/2dU1/index.html#tensorflow-results",
    "title": "🎢 l2hmc-qcd Example: 2D U(1)",
    "section": "TensorFlow Results",
    "text": "TensorFlow Results",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "⚛️ AI for Physics",
      "🎢 L2HMC for LQCD",
      "🎢 <code>l2hmc-qcd</code> Example: 2D U(1)"
    ]
  },
  {
    "objectID": "posts/ai-for-physics/l2hmc-qcd/2dU1/index.html#comparisons-1",
    "href": "posts/ai-for-physics/l2hmc-qcd/2dU1/index.html#comparisons-1",
    "title": "🎢 l2hmc-qcd Example: 2D U(1)",
    "section": "Comparisons",
    "text": "Comparisons",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "⚛️ AI for Physics",
      "🎢 L2HMC for LQCD",
      "🎢 <code>l2hmc-qcd</code> Example: 2D U(1)"
    ]
  },
  {
    "objectID": "posts/ai-for-physics/l2hmc-qcd/index.html",
    "href": "posts/ai-for-physics/l2hmc-qcd/index.html",
    "title": "🎢 L2HMC for LQCD",
    "section": "",
    "text": "CitationBibTeX citation:@online{foreman,\n  author = {Foreman, Sam},\n  title = {🎢 {L2HMC} for {LQCD}},\n  url = {https://samforeman.me},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nForeman, Sam. n.d. “🎢 L2HMC for LQCD.” https://samforeman.me."
  },
  {
    "objectID": "posts/index.html#hot-off-the-press",
    "href": "posts/index.html#hot-off-the-press",
    "title": "📬 Posts",
    "section": "🔥 Hot off the Press 📰",
    "text": "🔥 Hot off the Press 📰\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nSep 11, 2024\n\n\n🍋 ezpz @ ALCF\n\n\nSam Foreman \n\n\n\n\nAug 13, 2024\n\n\n💅 How to Make Dope Slides\n\n\nSam Foreman \n\n\n\n\nJun 17, 2024\n\n\n🎰 Deterministic flash-attn\n\n\nSam Foreman \n\n\n\n\nJun 17, 2024\n\n\n📸 flash-attn on Sunspot\n\n\nSam Foreman \n\n\n\n\nJun 15, 2024\n\n\n🏎️ Megatron-DeepSpeed on Intel XPU\n\n\nSam Foreman \n\n\n\n\nMay 25, 2024\n\n\n🐛 mpi4py bug on Sunspot\n\n\nSam Foreman \n\n\n\n\nApr 15, 2024\n\n\n🎲 MCMC + Diffusion Sampling\n\n\nSam Foreman \n\n\n\n\nMar 21, 2024\n\n\n⏰ Starting Up Distributed Training on Aurora\n\n\nSam Foreman \n\n\n\n\nFeb 12, 2024\n\n\n🚂 Loooooooong Sequence Lengths\n\n\nSam Foreman \n\n\n\n\nFeb 12, 2024\n\n\nl2hmc Example: 2D U(1)\n\n\nSam Foreman \n\n\n\n\nDec 14, 2023\n\n\n🎢 l2hmc-qcd Example: 2D U(1)\n\n\nSam Foreman \n\n\n\n\nDec 6, 2023\n\n\n🔳 l2hmc-qcd Example: 4D SU(3)\n\n\nSam Foreman \n\n\n\n\n\nNo matching items",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>"
    ]
  },
  {
    "objectID": "posts/jupyter/test/index.html",
    "href": "posts/jupyter/test/index.html",
    "title": "l2hmc Example: 2D U(1)",
    "section": "",
    "text": "Sam Foreman  2023-12-14\nThis notebook will (attempt) to walk through the steps needed to successfully instantiate and run an experiment.\nFor this example, we wish to train the L2HMC sampler for the 2D U(1) lattice gauge model with Wilson action:\nS_{\\beta}(n) = \\beta \\sum_{n}\\sum_{\\mu&lt;\\nu}\\mathrm{Re}\\left[1 - U_{\\mu\\nu}(n) \\right]\nThis consists of the following steps:",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "📗 Jupyter",
      "`l2hmc` Example: 2D $U(1)$"
    ]
  },
  {
    "objectID": "posts/jupyter/test/index.html#imports-setup",
    "href": "posts/jupyter/test/index.html#imports-setup",
    "title": "l2hmc Example: 2D U(1)",
    "section": "Imports / Setup",
    "text": "Imports / Setup\n! nvidia-smi | tail --lines -7\n\n\n\n\noutput\n\n\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    1   N/A  N/A   4027423      C   ...PU/2023-04-26/bin/python3     2915MiB |\n|    2   N/A  N/A   4054944      C   ...PU/2023-04-26/bin/python3     5793MiB |\n|    3   N/A  N/A   3989894      C   ...PU/2023-04-26/bin/python3     6021MiB |\n|    4   N/A  N/A   3981679      C   ...PU/2023-04-26/bin/python3     3951MiB |\n+-----------------------------------------------------------------------------+\n\n\n\n\nimport os\ndevices = os.environ.get('CUDA_VISIBLE_DEVICES', None)\nprint(devices)\n!getconf _NPROCESSORS_ONLN  # get number of availble CPUs\n\n\n\n\noutput\n\n\nNone\n256\n\n\n\n\nos.environ['TORCH_CPP_LOG_LEVEL'] = 'ERROR'\nos.environ['AUTOGRAPH_VERBOSITY'] = '10'\n!echo $CUDA_VISIBLE_DEVICES\nimport lovely_tensors as lt\nlt.monkey_patch()\nlt.set_config(color=False)\n# automatically detect and reload local changes to modules\n%load_ext autoreload\n%autoreload 2\n\n# automatically detect and reload local changes to modules\n%matplotlib inline\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats('svg', 'retina')\n\n\n\n\noutput\n\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n\nimport os\nimport warnings\n\nos.environ['COLORTERM'] = 'truecolor'\nwarnings.filterwarnings('ignore')\n# --------------------------------------\n# BE SURE TO GRAB A FRESH GPU !\nos.environ['CUDA_VISIBLE_DEVICES'] = '5'\n!echo $CUDA_VISIBLE_DEVICES\n# --------------------------------------\n\n\n\n\noutput\n\n\n5\n\n\n\n\nimport yaml\nimport logging\nfrom l2hmc.configs import CONF_DIR\n\nrlog_yaml = CONF_DIR.joinpath('hydra', 'job_logging', 'rich_jupyter.yaml')\nwith rlog_yaml.open('r') as stream:\n    logconf = dict(yaml.safe_load(stream))\n\nlogging.config.dictConfig(logconf)\nlog = logging.getLogger()\nlog.setLevel('INFO')\n\n\n\n\noutput\n\n\n--------------------------------------------------------------------------\nWARNING: There was an error initializing an OpenFabrics device.\n\n  Local host:   thetagpu23\n  Local device: mlx5_0\n--------------------------------------------------------------------------\nUsing device: cuda \n\n\n\n\nimport torch\nimport opinionated\n\nimport seaborn as sns\nimport numpy as np\nimport lovely_tensors as lt\nimport matplotlib.pyplot as plt\nimport l2hmc.group.su3.pytorch.group as g\n\nfrom pathlib import Path\nfrom typing import Optional\n\nfrom ezpz import setup_torch\n#from l2hmc.utils.dist import setup_torch_distributed\n\nfrom l2hmc.common import grab_tensor, print_dict\nfrom l2hmc.configs import dict_to_list_of_overrides, get_experiment\nfrom l2hmc.experiment.pytorch.experiment import Experiment, evaluate  # noqa  # noqa\nfrom l2hmc.utils.plot_helpers import set_plot_style\nfrom l2hmc.utils.history import BaseHistory\nfrom l2hmc.utils.plot_helpers import (  # noqa\n    set_plot_style,\n    plot_scalar,\n    plot_chains,\n    plot_leapfrogs\n)\n\nos.environ['COLORTERM'] = 'truecolor'\nPORT = np.random.randint(5000, 6000)\n#SEED = np.random.randint(0, 2 ** 16)\nSEED = 4351\nlog.critical(f'{SEED=}')\nlog.info(f'{PORT=}')\nos.environ['MASTER_PORT'] = str(PORT)\n\n#_ = setup_torch_distributed(backend='DDP', )\n_ = setup_torch(backend='DDP', seed=SEED, port=PORT)\n\n_ = (\n    torch.set_default_device('cuda')\n    if torch.cuda.is_available() else None\n)\n#torch.set_default_dtype(torch.bfloat16)\n#_ = (\n#    torch.set_autocast_gpu_dtype(torch.bfloat16)\n#    if torch.cuda.is_available() else None\n#)\n\nset_plot_style()\nplt.style.use(opinionated.STYLES['opinionated_min'])\nsns.set_context('notebook', font_scale=1.25)\n\n\n\n\noutput\n\n\nFailed to download font: Source Sans Pro, skipping!\nFailed to download font: Titillium WebRoboto Condensed, skipping!\n\n\n2023-12-05 11:12:19.257964: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n[2023-12-05 11:12:22,359] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect) \n[2023-12-05 11:12:27][CRITICAL][747729381.py:32] - SEED=4351 \n[2023-12-05 11:12:27][INFO][747729381.py:33] - PORT=5249 \n[2023-12-05 11:12:27][INFO][dist.py:185] - Using DDP for distributed training \n[2023-12-05 11:12:27][INFO][dist.py:162] - Caught MASTER_PORT:5249 from environment! \n[2023-12-05 11:12:27][INFO][distributed_c10d.py:442] - Added key: store_based_barrier_key:1 to store for rank: 0 \n[2023-12-05 11:12:27][INFO][distributed_c10d.py:476] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes. \n[2023-12-05 11:12:27][INFO][dist.py:240] - RANK: 0 / 0 \n\n\n\n\nimport l2hmc\nlog.info(f'{l2hmc.__file__=}')\n\n\n\n\noutput\n\n\n[2023-12-05 11:12:28][INFO][1221488284.py:2] - l2hmc.__file__='/lus/grand/projects/DLHMC/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/__init__.py'",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "📗 Jupyter",
      "`l2hmc` Example: 2D $U(1)$"
    ]
  },
  {
    "objectID": "posts/jupyter/test/index.html#pytorch",
    "href": "posts/jupyter/test/index.html#pytorch",
    "title": "l2hmc Example: 2D U(1)",
    "section": "PyTorch",
    "text": "PyTorch\nimport time\nfrom l2hmc.utils.history import BaseHistory, summarize_dict\nimport l2hmc.utils.live_plots as plotter\n\nplt.rcParams['xaxis.labellocation'] = 'center'\nplt.rcParams['yaxis.labellocation'] = 'center'\n\nbeta = 4.0\nstate = ptExpU1.trainer.dynamics.random_state(beta)\nstate.x.device\n\n\n\n\noutput\n\n\ndevice(type='cuda', index=0)\n\n\n\n\n\nTraining\noutputs['pytorch']['train'] = ptExpU1.trainer.train(\n    nera=1,\n    nepoch=5000,\n    beta=4.0,\n    # beta=[4.0, 4.25, 4.5, 4.75, 5.0],\n)\n\n\n\n\n\noutput\n\n\n\n[2023-12-05 11:14:13][INFO][trainer.py:108] - ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓  \n[2023-12-05 11:14:13][INFO][trainer.py:109] - ┃ ERA: 0 / 1, BETA: 4.000 ┃ \n[2023-12-05 11:14:13][INFO][trainer.py:110] - ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ \n[2023-12-05 11:14:14][INFO][trainer.py:471] - Thermalizing configs @ 4.00 took 1.7203 s \n[2023-12-05 11:14:15][INFO][trainer.py:1623] - era=0 epoch=0 tstep=1 dt=0.311 beta=4.000 loss=81.851 dQsin=0.006 dQint=0.002 energy=411.822 logprob=411.697 logdet=0.125 sldf=0.120 sldb=-0.107 sld=0.125 xeps=0.050 veps=0.050 acc=0.026 sumlogdet=0.003 acc_mask=0.026 plaqs=0.854 intQ=0.053 sinQ=0.037 lr=0.001 \n[2023-12-05 11:14:15][INFO][distributed.py:1140] - Reducer buckets have been rebuilt in this iteration. \n[2023-12-05 11:15:18][INFO][trainer.py:1623] - era=0 epoch=200 tstep=201 dt=0.291 beta=4.000 loss=10.499 dQsin=0.076 dQint=0.011 energy=396.810 logprob=396.496 logdet=0.314 sldf=0.195 sldb=-0.154 sld=0.314 xeps=0.021 veps=0.022 acc=0.598 sumlogdet=-0.006 acc_mask=0.601 plaqs=0.863 intQ=0.056 sinQ=0.060 lr=0.001 \n[2023-12-05 11:16:19][INFO][trainer.py:1623] - era=0 epoch=400 tstep=401 dt=0.289 beta=4.000 loss=7.030 dQsin=0.099 dQint=0.014 energy=396.425 logprob=395.868 logdet=0.557 sldf=0.347 sldb=-0.310 sld=0.557 xeps=0.029 veps=0.030 acc=0.630 sumlogdet=-0.002 acc_mask=0.631 plaqs=0.864 intQ=0.047 sinQ=0.036 lr=0.001 \n[2023-12-05 11:17:22][INFO][trainer.py:1623] - era=0 epoch=600 tstep=601 dt=0.291 beta=4.000 loss=2.930 dQsin=0.130 dQint=0.023 energy=397.770 logprob=396.874 logdet=0.896 sldf=0.553 sldb=-0.487 sld=0.896 xeps=0.039 veps=0.040 acc=0.664 sumlogdet=-0.003 acc_mask=0.676 plaqs=0.864 intQ=0.035 sinQ=0.023 lr=0.001 \n[2023-12-05 11:18:25][INFO][trainer.py:1623] - era=0 epoch=800 tstep=801 dt=0.291 beta=4.000 loss=0.603 dQsin=0.154 dQint=0.029 energy=399.160 logprob=397.946 logdet=1.214 sldf=0.750 sldb=-0.663 sld=1.214 xeps=0.047 veps=0.050 acc=0.757 sumlogdet=-0.000 acc_mask=0.746 plaqs=0.863 intQ=0.030 sinQ=0.015 lr=0.001 \n[2023-12-05 11:19:29][INFO][trainer.py:1623] - era=0 epoch=1000 tstep=1001 dt=0.300 beta=4.000 loss=-0.085 dQsin=0.168 dQint=0.025 energy=398.964 logprob=397.578 logdet=1.386 sldf=0.858 sldb=-0.783 sld=1.386 xeps=0.052 veps=0.055 acc=0.814 sumlogdet=0.000 acc_mask=0.808 plaqs=0.864 intQ=0.010 sinQ=0.003 lr=0.001 \n[2023-12-05 11:20:34][INFO][trainer.py:1623] - era=0 epoch=1200 tstep=1201 dt=0.313 beta=4.000 loss=-1.749 dQsin=0.181 dQint=0.045 energy=399.484 logprob=397.867 logdet=1.617 sldf=1.003 sldb=-0.942 sld=1.617 xeps=0.062 veps=0.065 acc=0.814 sumlogdet=0.003 acc_mask=0.794 plaqs=0.863 intQ=-0.012 sinQ=-0.007 lr=0.001 \n[2023-12-05 11:21:38][INFO][trainer.py:1623] - era=0 epoch=1400 tstep=1401 dt=0.297 beta=4.000 loss=-2.117 dQsin=0.192 dQint=0.042 energy=399.738 logprob=397.874 logdet=1.864 sldf=1.158 sldb=-1.104 sld=1.864 xeps=0.072 veps=0.076 acc=0.832 sumlogdet=0.007 acc_mask=0.831 plaqs=0.864 intQ=-0.052 sinQ=-0.047 lr=0.001 \n[2023-12-05 11:22:41][INFO][trainer.py:1623] - era=0 epoch=1600 tstep=1601 dt=0.297 beta=4.000 loss=-3.168 dQsin=0.205 dQint=0.063 energy=401.640 logprob=399.423 logdet=2.218 sldf=1.373 sldb=-1.267 sld=2.218 xeps=0.085 veps=0.089 acc=0.813 sumlogdet=-0.000 acc_mask=0.815 plaqs=0.863 intQ=0.031 sinQ=0.022 lr=0.001 \n[2023-12-05 11:23:44][INFO][trainer.py:1623] - era=0 epoch=1800 tstep=1801 dt=0.297 beta=4.000 loss=-4.396 dQsin=0.212 dQint=0.054 energy=400.607 logprob=398.216 logdet=2.391 sldf=1.477 sldb=-1.335 sld=2.391 xeps=0.091 veps=0.095 acc=0.820 sumlogdet=-0.003 acc_mask=0.822 plaqs=0.863 intQ=0.005 sinQ=0.000 lr=0.001 \n[2023-12-05 11:24:49][INFO][trainer.py:1623] - era=0 epoch=2000 tstep=2001 dt=0.319 beta=4.000 loss=-4.874 dQsin=0.220 dQint=0.060 energy=400.020 logprob=397.462 logdet=2.557 sldf=1.575 sldb=-1.388 sld=2.557 xeps=0.097 veps=0.100 acc=0.825 sumlogdet=0.002 acc_mask=0.832 plaqs=0.863 intQ=-0.021 sinQ=-0.021 lr=0.001 \n[2023-12-05 11:25:53][INFO][trainer.py:1623] - era=0 epoch=2200 tstep=2201 dt=0.297 beta=4.000 loss=-5.154 dQsin=0.222 dQint=0.066 energy=400.023 logprob=397.279 logdet=2.743 sldf=1.685 sldb=-1.462 sld=2.743 xeps=0.105 veps=0.108 acc=0.840 sumlogdet=0.011 acc_mask=0.847 plaqs=0.863 intQ=0.014 sinQ=0.017 lr=0.001 \n[2023-12-05 11:26:56][INFO][trainer.py:1623] - era=0 epoch=2400 tstep=2401 dt=0.298 beta=4.000 loss=-7.020 dQsin=0.231 dQint=0.070 energy=400.368 logprob=397.445 logdet=2.922 sldf=1.793 sldb=-1.533 sld=2.922 xeps=0.114 veps=0.116 acc=0.858 sumlogdet=0.005 acc_mask=0.848 plaqs=0.863 intQ=0.062 sinQ=0.059 lr=0.001 \n[2023-12-05 11:28:00][INFO][trainer.py:1623] - era=0 epoch=2600 tstep=2601 dt=0.297 beta=4.000 loss=-7.241 dQsin=0.240 dQint=0.091 energy=401.233 logprob=398.224 logdet=3.009 sldf=1.847 sldb=-1.578 sld=3.009 xeps=0.120 veps=0.120 acc=0.865 sumlogdet=0.002 acc_mask=0.856 plaqs=0.863 intQ=0.047 sinQ=0.042 lr=0.001 \n[2023-12-05 11:29:03][INFO][trainer.py:1623] - era=0 epoch=2800 tstep=2801 dt=0.304 beta=4.000 loss=-6.760 dQsin=0.237 dQint=0.086 energy=399.980 logprob=396.791 logdet=3.189 sldf=1.955 sldb=-1.653 sld=3.189 xeps=0.127 veps=0.128 acc=0.870 sumlogdet=0.000 acc_mask=0.870 plaqs=0.863 intQ=0.014 sinQ=0.007 lr=0.001 \n[2023-12-05 11:30:08][INFO][trainer.py:1623] - era=0 epoch=3000 tstep=3001 dt=0.302 beta=4.000 loss=-7.325 dQsin=0.243 dQint=0.083 energy=401.154 logprob=397.836 logdet=3.319 sldf=2.032 sldb=-1.711 sld=3.319 xeps=0.131 veps=0.133 acc=0.878 sumlogdet=0.010 acc_mask=0.876 plaqs=0.863 intQ=-0.017 sinQ=-0.011 lr=0.001 \n[2023-12-05 11:31:12][INFO][trainer.py:1623] - era=0 epoch=3200 tstep=3201 dt=0.302 beta=4.000 loss=-7.431 dQsin=0.242 dQint=0.082 energy=400.859 logprob=397.497 logdet=3.362 sldf=2.059 sldb=-1.728 sld=3.362 xeps=0.134 veps=0.135 acc=0.885 sumlogdet=0.006 acc_mask=0.883 plaqs=0.863 intQ=0.012 sinQ=0.006 lr=0.001 \n[2023-12-05 11:32:16][INFO][trainer.py:1623] - era=0 epoch=3400 tstep=3401 dt=0.302 beta=4.000 loss=-6.296 dQsin=0.229 dQint=0.084 energy=400.674 logprob=397.367 logdet=3.307 sldf=2.026 sldb=-1.714 sld=3.307 xeps=0.132 veps=0.132 acc=0.885 sumlogdet=0.006 acc_mask=0.881 plaqs=0.863 intQ=0.045 sinQ=0.041 lr=0.001 \n[2023-12-05 11:33:20][INFO][trainer.py:1623] - era=0 epoch=3600 tstep=3601 dt=0.302 beta=4.000 loss=-7.885 dQsin=0.252 dQint=0.092 energy=399.823 logprob=396.495 logdet=3.328 sldf=2.039 sldb=-1.725 sld=3.328 xeps=0.132 veps=0.133 acc=0.900 sumlogdet=0.008 acc_mask=0.903 plaqs=0.864 intQ=-0.002 sinQ=0.000 lr=0.001 \n[2023-12-05 11:34:25][INFO][trainer.py:1623] - era=0 epoch=3800 tstep=3801 dt=0.303 beta=4.000 loss=-8.489 dQsin=0.257 dQint=0.091 energy=400.076 logprob=396.664 logdet=3.412 sldf=2.091 sldb=-1.762 sld=3.412 xeps=0.135 veps=0.137 acc=0.897 sumlogdet=-0.005 acc_mask=0.913 plaqs=0.863 intQ=-0.035 sinQ=-0.029 lr=0.001 \n[2023-12-05 11:35:30][INFO][trainer.py:1623] - era=0 epoch=4000 tstep=4001 dt=0.306 beta=4.000 loss=-7.836 dQsin=0.245 dQint=0.085 energy=400.851 logprob=397.384 logdet=3.468 sldf=2.125 sldb=-1.793 sld=3.468 xeps=0.137 veps=0.139 acc=0.891 sumlogdet=0.002 acc_mask=0.893 plaqs=0.863 intQ=0.022 sinQ=0.013 lr=0.001 \n[2023-12-05 11:36:34][INFO][trainer.py:1623] - era=0 epoch=4200 tstep=4201 dt=0.305 beta=4.000 loss=-7.812 dQsin=0.252 dQint=0.084 energy=400.178 logprob=396.688 logdet=3.490 sldf=2.137 sldb=-1.801 sld=3.490 xeps=0.137 veps=0.139 acc=0.904 sumlogdet=0.015 acc_mask=0.906 plaqs=0.864 intQ=-0.042 sinQ=-0.032 lr=0.001 \n[2023-12-05 11:37:38][INFO][trainer.py:1623] - era=0 epoch=4400 tstep=4401 dt=0.302 beta=4.000 loss=-7.997 dQsin=0.251 dQint=0.088 energy=400.410 logprob=396.859 logdet=3.550 sldf=2.175 sldb=-1.834 sld=3.550 xeps=0.140 veps=0.142 acc=0.898 sumlogdet=0.012 acc_mask=0.911 plaqs=0.863 intQ=-0.001 sinQ=-0.003 lr=0.001 \n[2023-12-05 11:38:42][INFO][trainer.py:1623] - era=0 epoch=4600 tstep=4601 dt=0.306 beta=4.000 loss=-8.629 dQsin=0.252 dQint=0.088 energy=400.759 logprob=397.157 logdet=3.601 sldf=2.208 sldb=-1.865 sld=3.601 xeps=0.142 veps=0.144 acc=0.896 sumlogdet=-0.003 acc_mask=0.902 plaqs=0.863 intQ=0.017 sinQ=0.013 lr=0.001 \n[2023-12-05 11:39:46][INFO][trainer.py:1623] - era=0 epoch=4800 tstep=4801 dt=0.304 beta=4.000 loss=-8.538 dQsin=0.256 dQint=0.095 energy=400.788 logprob=397.108 logdet=3.680 sldf=2.257 sldb=-1.907 sld=3.680 xeps=0.146 veps=0.148 acc=0.891 sumlogdet=-0.012 acc_mask=0.892 plaqs=0.863 intQ=0.018 sinQ=0.016 lr=0.001 \n\n\n\n\n\n# dset_train = ptExpU1.trainer.histories['train'].plot_all(num_chains=128)\ndset_train_pt = ptExpU1.save_dataset(job_type='train', nchains=32)\n\n\n\n\noutput\n\n\n[2023-12-05 11:41:01][INFO][plot_helpers.py:1046] - Saving figure to: /lus/grand/projects/DLHMC/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-12-05-111355/pytorch/plots/ridgeplots/svgs/energy_ridgeplot.svg \n[2023-12-05 11:41:03][INFO][plot_helpers.py:1046] - Saving figure to: /lus/grand/projects/DLHMC/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-12-05-111355/pytorch/plots/ridgeplots/svgs/logprob_ridgeplot.svg \n[2023-12-05 11:41:05][INFO][plot_helpers.py:1046] - Saving figure to: /lus/grand/projects/DLHMC/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-12-05-111355/pytorch/plots/ridgeplots/svgs/logdet_ridgeplot.svg \n[2023-12-05 11:41:07][INFO][plot_helpers.py:1046] - Saving figure to: /lus/grand/projects/DLHMC/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-12-05-111355/pytorch/plots/ridgeplots/svgs/sldf_ridgeplot.svg \n[2023-12-05 11:41:09][INFO][plot_helpers.py:1046] - Saving figure to: /lus/grand/projects/DLHMC/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-12-05-111355/pytorch/plots/ridgeplots/svgs/sldb_ridgeplot.svg \n[2023-12-05 11:41:11][INFO][plot_helpers.py:1046] - Saving figure to: /lus/grand/projects/DLHMC/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-12-05-111355/pytorch/plots/ridgeplots/svgs/sld_ridgeplot.svg \n[2023-12-05 11:41:41][INFO][common.py:275] - Saving dataset to: /lus/grand/projects/DLHMC/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-12-05-111355/pytorch/data/train_data.h5 \n[2023-12-05 11:41:42][INFO][experiment.py:378] - Done saving and analyzing data. \n[2023-12-05 11:41:42][INFO][experiment.py:379] - Creating summaries for WandB, Aim \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInference\n\nEvaluation\noutputs['pytorch']['eval'] = ptExpU1.trainer.eval(\n    job_type='eval',\n    nprint=500,\n    nchains=128,\n    eval_steps=2000,\n)\ndset_eval_pt = ptExpU1.save_dataset(job_type='eval', nchains=32)\n# dset_eval_pt = ptExpU1.trainer.histories['eval'].plot_all()\n\n\n\n\noutput\n\n\n[2023-12-05 11:42:03][WARNING][trainer.py:467] - x.shape (original): torch.Size([2048, 2, 16, 16]) \n[2023-12-05 11:42:03][WARNING][trainer.py:467] - x[:nchains].shape: torch.Size([128, 2, 16, 16]) \n[2023-12-05 11:42:03][INFO][trainer.py:1077] - eps=None\nbeta=4.0\nnlog=10\ntable=&lt;rich.table.Table object at 0x7efbf40b16f0&gt;\nnprint=500\neval_steps=2000\nnleapfrog=None \n\n[2023-12-05 11:42:06][INFO][trainer.py:1207] - estep=0 dt=0.140 beta=4.000 loss=-9.137 dQsin=0.269 dQint=0.148 energy=407.745 logprob=404.067 logdet=3.678 sldf=2.253 sldb=-1.896 sld=3.678 xeps=0.145 veps=0.147 acc=0.913 sumlogdet=0.004 acc_mask=0.930 plaqs=0.854 intQ=0.156 sinQ=0.151 \n[2023-12-05 11:43:38][INFO][trainer.py:1207] - estep=500 dt=0.119 beta=4.000 loss=-7.346 dQsin=0.264 dQint=0.125 energy=403.427 logprob=399.747 logdet=3.680 sldf=2.255 sldb=-1.906 sld=3.680 xeps=0.145 veps=0.147 acc=0.909 sumlogdet=0.005 acc_mask=0.883 plaqs=0.864 intQ=-0.305 sinQ=-0.216 \n[2023-12-05 11:45:11][INFO][trainer.py:1207] - estep=1000 dt=0.119 beta=4.000 loss=-8.075 dQsin=0.287 dQint=0.133 energy=402.009 logprob=398.331 logdet=3.678 sldf=2.253 sldb=-1.898 sld=3.678 xeps=0.145 veps=0.147 acc=0.897 sumlogdet=0.005 acc_mask=0.945 plaqs=0.863 intQ=-0.023 sinQ=-0.042 \n[2023-12-05 11:46:44][INFO][trainer.py:1207] - estep=1500 dt=0.119 beta=4.000 loss=-11.254 dQsin=0.261 dQint=0.109 energy=401.410 logprob=397.734 logdet=3.676 sldf=2.254 sldb=-1.918 sld=3.676 xeps=0.145 veps=0.147 acc=0.896 sumlogdet=0.004 acc_mask=0.875 plaqs=0.862 intQ=0.078 sinQ=0.071 \n[2023-12-05 11:48:21][INFO][plot_helpers.py:1046] - Saving figure to: /lus/grand/projects/DLHMC/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-12-05-111355/pytorch/plots/ridgeplots/svgs/energy_ridgeplot.svg \n[2023-12-05 11:48:23][INFO][plot_helpers.py:1046] - Saving figure to: /lus/grand/projects/DLHMC/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-12-05-111355/pytorch/plots/ridgeplots/svgs/logprob_ridgeplot.svg \n[2023-12-05 11:48:25][INFO][plot_helpers.py:1046] - Saving figure to: /lus/grand/projects/DLHMC/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-12-05-111355/pytorch/plots/ridgeplots/svgs/logdet_ridgeplot.svg \n[2023-12-05 11:48:27][INFO][plot_helpers.py:1046] - Saving figure to: /lus/grand/projects/DLHMC/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-12-05-111355/pytorch/plots/ridgeplots/svgs/sldf_ridgeplot.svg \n[2023-12-05 11:48:28][INFO][plot_helpers.py:1046] - Saving figure to: /lus/grand/projects/DLHMC/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-12-05-111355/pytorch/plots/ridgeplots/svgs/sldb_ridgeplot.svg \n[2023-12-05 11:48:30][INFO][plot_helpers.py:1046] - Saving figure to: /lus/grand/projects/DLHMC/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-12-05-111355/pytorch/plots/ridgeplots/svgs/sld_ridgeplot.svg \n[2023-12-05 11:48:45][INFO][common.py:275] - Saving dataset to: /lus/grand/projects/DLHMC/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-12-05-111355/pytorch/data/eval_data.h5 \n[2023-12-05 11:48:45][INFO][experiment.py:378] - Done saving and analyzing data. \n[2023-12-05 11:48:45][INFO][experiment.py:379] - Creating summaries for WandB, Aim \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHMC\noutputs['pytorch']['hmc'] = ptExpU1.trainer.eval(\n    job_type='hmc',\n    nprint=500,\n    nchains=128,\n    eval_steps=2000,\n)\ndset_hmc_pt = ptExpU1.save_dataset(job_type='hmc', nchains=32)\n# dset_hmc_pt = ptExpU1.trainer.histories['hmc'].plot_all()\n\n\n\n\noutput\n\n\n[2023-12-05 11:49:07][WARNING][trainer.py:467] - Step size `eps` not specified for HMC! Using default: 0.2500 for generic HMC \n[2023-12-05 11:49:07][WARNING][trainer.py:467] - x.shape (original): torch.Size([2048, 2, 16, 16]) \n[2023-12-05 11:49:07][WARNING][trainer.py:467] - x[:nchains].shape: torch.Size([128, 2, 16, 16]) \n[2023-12-05 11:49:07][INFO][trainer.py:1077] - eps=0.25\nbeta=4.0\nnlog=10\ntable=&lt;rich.table.Table object at 0x7efbf4167580&gt;\nnprint=500\neval_steps=2000\nnleapfrog=8 \n\n[2023-12-05 11:49:09][INFO][trainer.py:1207] - hstep=0 dt=0.018 beta=4.000 loss=46.645 dQsin=0.039 dQint=0.031 energy=412.712 logprob=412.712 logdet=0.000 acc=0.114 sumlogdet=0.000 acc_mask=0.125 plaqs=0.853 intQ=-0.117 sinQ=-0.126 \n[2023-12-05 11:49:50][INFO][trainer.py:1207] - hstep=500 dt=0.018 beta=4.000 loss=51.958 dQsin=0.014 dQint=0.000 energy=401.030 logprob=401.030 logdet=0.000 acc=0.054 sumlogdet=0.000 acc_mask=0.055 plaqs=0.863 intQ=-0.016 sinQ=-0.038 \n[2023-12-05 11:50:31][INFO][trainer.py:1207] - hstep=1000 dt=0.017 beta=4.000 loss=58.470 dQsin=0.017 dQint=0.016 energy=403.846 logprob=403.846 logdet=0.000 acc=0.055 sumlogdet=0.000 acc_mask=0.055 plaqs=0.862 intQ=-0.078 sinQ=-0.089 \n[2023-12-05 11:51:13][INFO][trainer.py:1207] - hstep=1500 dt=0.017 beta=4.000 loss=54.941 dQsin=0.014 dQint=0.000 energy=400.502 logprob=400.502 logdet=0.000 acc=0.056 sumlogdet=0.000 acc_mask=0.047 plaqs=0.865 intQ=-0.117 sinQ=-0.096 \n[2023-12-05 11:51:58][INFO][plot_helpers.py:1046] - Saving figure to: /lus/grand/projects/DLHMC/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-12-05-111355/pytorch/plots/ridgeplots/svgs/energy_ridgeplot.svg \n[2023-12-05 11:52:00][INFO][plot_helpers.py:1046] - Saving figure to: /lus/grand/projects/DLHMC/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-12-05-111355/pytorch/plots/ridgeplots/svgs/logprob_ridgeplot.svg \n[2023-12-05 11:52:02][INFO][plot_helpers.py:1046] - Saving figure to: /lus/grand/projects/DLHMC/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-12-05-111355/pytorch/plots/ridgeplots/svgs/logdet_ridgeplot.svg \n[2023-12-05 11:52:14][INFO][common.py:275] - Saving dataset to: /lus/grand/projects/DLHMC/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-12-05-111355/pytorch/data/hmc_data.h5 \n[2023-12-05 11:52:14][INFO][experiment.py:378] - Done saving and analyzing data. \n[2023-12-05 11:52:14][INFO][experiment.py:379] - Creating summaries for WandB, Aim",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "📗 Jupyter",
      "`l2hmc` Example: 2D $U(1)$"
    ]
  },
  {
    "objectID": "posts/jupyter/test/index.html#tensorflow",
    "href": "posts/jupyter/test/index.html#tensorflow",
    "title": "l2hmc Example: 2D U(1)",
    "section": "TensorFlow",
    "text": "TensorFlow\n\nTrain\noutputs['tensorflow']['train'] = tfExpU1.trainer.train(\n    nera=1,\n    nepoch=5000,\n    beta=4.0,\n    # beta=[4.0, 4.25, 4.5, 4.75, 5.0],\n)\n# dset_train_tf = tfExpU1.trainer.histories['train'].plot_all()\n_ = tfExpU1.save_dataset(job_type='train', nchains=32)\n\n\n\n\n\noutput\n\n\n[2023-12-05 11:52:30][INFO][trainer.py:198] - Looking for checkpoints in: /lus/grand/projects/DLHMC/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/checkpoints/U1/2-16-16/nlf-4/xsplit-True/sepnets-True/merge-True/net-16-16-16-16_dp-0.2_bn-True/tensorflow \n[2023-12-05 11:52:30][INFO][trainer.py:198] - No checkpoints found to load from. Continuing \n\n[2023-12-05 11:52:31][INFO][trainer.py:1259] - ERA: 0 / 1, BETA: 4.000 \n[2023-12-05 11:53:11][INFO][trainer.py:198] - Thermalizing configs @ 4.00 took 40.3690 s \nTraining:   0%|          | 0/5000 [00:00&lt;?, ?it/s]\n[2023-12-05 11:53:21][WARNING][deprecation.py:350] - From /lus/grand/projects/datascience/foremans/locations/thetaGPU/miniconda3/envs/2023-04-26/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\nInstructions for updating:\nLambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089 \n[2023-12-05 11:53:55][WARNING][trainer.py:194] - Resetting optimizer state! \n[2023-12-05 11:53:55][WARNING][trainer.py:194] - Chains are stuck! Re-drawing x ! \n[2023-12-05 11:53:56][INFO][trainer.py:1085] - era=0 epoch=0 tstep=1.000 dt=44.185 beta=4.000 loss=99.113 dQsin=0.000 dQint=0.000 energy=1288.705 logprob=1288.768 logdet=-0.063 sldf=-0.013 sldb=-0.029 sld=-0.063 xeps=0.051 veps=0.049 acc=0.000 sumlogdet=0.000 acc_mask=0.000 plaqs=0.000 intQ=0.046 sinQ=0.038 lr=0.001 \n[2023-12-05 11:54:23][INFO][trainer.py:1085] - era=0 epoch=200 tstep=201.000 dt=0.103 beta=4.000 loss=2.764 dQsin=0.126 dQint=0.027 energy=397.227 logprob=397.120 logdet=0.107 sldf=0.070 sldb=-0.071 sld=0.107 xeps=0.051 veps=0.047 acc=0.610 sumlogdet=-0.015 acc_mask=0.610 plaqs=0.863 intQ=-0.045 sinQ=-0.039 lr=0.001 \n[2023-12-05 11:54:51][INFO][trainer.py:1085] - era=0 epoch=400 tstep=401.000 dt=0.105 beta=4.000 loss=0.957 dQsin=0.151 dQint=0.028 energy=395.704 logprob=395.575 logdet=0.130 sldf=0.082 sldb=-0.076 sld=0.130 xeps=0.050 veps=0.047 acc=0.741 sumlogdet=-0.002 acc_mask=0.737 plaqs=0.864 intQ=-0.063 sinQ=-0.049 lr=0.001 \n[2023-12-05 11:55:18][INFO][trainer.py:1085] - era=0 epoch=600 tstep=601.000 dt=0.094 beta=4.000 loss=0.614 dQsin=0.168 dQint=0.034 energy=396.128 logprob=395.997 logdet=0.132 sldf=0.081 sldb=-0.070 sld=0.132 xeps=0.051 veps=0.048 acc=0.816 sumlogdet=-0.004 acc_mask=0.826 plaqs=0.864 intQ=0.005 sinQ=0.009 lr=0.001 \n[2023-12-05 11:55:45][INFO][trainer.py:1085] - era=0 epoch=800 tstep=801.000 dt=0.103 beta=4.000 loss=-0.637 dQsin=0.175 dQint=0.034 energy=395.974 logprob=395.842 logdet=0.132 sldf=0.080 sldb=-0.064 sld=0.132 xeps=0.053 veps=0.050 acc=0.866 sumlogdet=0.001 acc_mask=0.853 plaqs=0.863 intQ=0.012 sinQ=0.013 lr=0.001 \n[2023-12-05 11:56:11][INFO][trainer.py:1085] - era=0 epoch=1000 tstep=1001.000 dt=0.095 beta=4.000 loss=-0.714 dQsin=0.185 dQint=0.038 energy=395.177 logprob=395.041 logdet=0.135 sldf=0.083 sldb=-0.067 sld=0.135 xeps=0.055 veps=0.051 acc=0.883 sumlogdet=-0.002 acc_mask=0.886 plaqs=0.864 intQ=0.028 sinQ=0.020 lr=0.001 \n[2023-12-05 11:56:37][INFO][trainer.py:1085] - era=0 epoch=1200 tstep=1201.000 dt=0.097 beta=4.000 loss=-2.043 dQsin=0.197 dQint=0.047 energy=396.446 logprob=396.308 logdet=0.138 sldf=0.084 sldb=-0.067 sld=0.138 xeps=0.057 veps=0.054 acc=0.893 sumlogdet=0.001 acc_mask=0.904 plaqs=0.863 intQ=0.029 sinQ=0.022 lr=0.001 \n[2023-12-05 11:57:04][INFO][trainer.py:1085] - era=0 epoch=1400 tstep=1401.000 dt=0.095 beta=4.000 loss=-1.262 dQsin=0.193 dQint=0.044 energy=397.347 logprob=397.203 logdet=0.144 sldf=0.088 sldb=-0.071 sld=0.144 xeps=0.061 veps=0.057 acc=0.909 sumlogdet=-0.000 acc_mask=0.898 plaqs=0.863 intQ=-0.008 sinQ=0.003 lr=0.001 \n[2023-12-05 11:57:30][INFO][trainer.py:1085] - era=0 epoch=1600 tstep=1601.000 dt=0.096 beta=4.000 loss=-2.389 dQsin=0.203 dQint=0.050 energy=396.358 logprob=396.205 logdet=0.153 sldf=0.094 sldb=-0.079 sld=0.153 xeps=0.065 veps=0.060 acc=0.915 sumlogdet=0.001 acc_mask=0.922 plaqs=0.863 intQ=0.010 sinQ=0.001 lr=0.001 \n[2023-12-05 11:57:55][INFO][trainer.py:1085] - era=0 epoch=1800 tstep=1801.000 dt=0.093 beta=4.000 loss=-3.667 dQsin=0.215 dQint=0.056 energy=396.103 logprob=395.927 logdet=0.175 sldf=0.108 sldb=-0.093 sld=0.175 xeps=0.071 veps=0.066 acc=0.923 sumlogdet=0.001 acc_mask=0.926 plaqs=0.864 intQ=0.023 sinQ=0.025 lr=0.001 \n[2023-12-05 11:58:19][INFO][trainer.py:1085] - era=0 epoch=2000 tstep=2001.000 dt=0.086 beta=4.000 loss=-3.192 dQsin=0.211 dQint=0.050 energy=395.770 logprob=395.575 logdet=0.195 sldf=0.120 sldb=-0.108 sld=0.195 xeps=0.077 veps=0.071 acc=0.932 sumlogdet=-0.001 acc_mask=0.925 plaqs=0.864 intQ=0.042 sinQ=0.034 lr=0.001 \n[2023-12-05 11:58:44][INFO][trainer.py:1085] - era=0 epoch=2200 tstep=2201.000 dt=0.088 beta=4.000 loss=-3.860 dQsin=0.222 dQint=0.052 energy=395.970 logprob=395.744 logdet=0.226 sldf=0.139 sldb=-0.120 sld=0.226 xeps=0.083 veps=0.076 acc=0.932 sumlogdet=0.000 acc_mask=0.942 plaqs=0.864 intQ=-0.017 sinQ=-0.018 lr=0.001 \n[2023-12-05 11:59:08][INFO][trainer.py:1085] - era=0 epoch=2400 tstep=2401.000 dt=0.089 beta=4.000 loss=-5.338 dQsin=0.234 dQint=0.063 energy=396.330 logprob=396.058 logdet=0.271 sldf=0.165 sldb=-0.130 sld=0.271 xeps=0.092 veps=0.084 acc=0.927 sumlogdet=0.000 acc_mask=0.935 plaqs=0.863 intQ=-0.083 sinQ=-0.069 lr=0.001 \n[2023-12-05 11:59:33][INFO][trainer.py:1085] - era=0 epoch=2600 tstep=2601.000 dt=0.089 beta=4.000 loss=-6.596 dQsin=0.238 dQint=0.067 energy=396.078 logprob=395.751 logdet=0.327 sldf=0.197 sldb=-0.137 sld=0.327 xeps=0.100 veps=0.091 acc=0.919 sumlogdet=-0.000 acc_mask=0.911 plaqs=0.863 intQ=-0.020 sinQ=-0.018 lr=0.001 \n[2023-12-05 11:59:58][INFO][trainer.py:1085] - era=0 epoch=2800 tstep=2801.000 dt=0.087 beta=4.000 loss=-6.121 dQsin=0.239 dQint=0.071 energy=396.373 logprob=396.000 logdet=0.373 sldf=0.222 sldb=-0.138 sld=0.373 xeps=0.108 veps=0.097 acc=0.912 sumlogdet=-0.000 acc_mask=0.908 plaqs=0.863 intQ=-0.003 sinQ=-0.007 lr=0.001 \n[2023-12-05 12:00:24][INFO][trainer.py:1085] - era=0 epoch=3000 tstep=3001.000 dt=0.092 beta=4.000 loss=-7.409 dQsin=0.247 dQint=0.078 energy=396.537 logprob=396.127 logdet=0.411 sldf=0.244 sldb=-0.141 sld=0.411 xeps=0.113 veps=0.101 acc=0.914 sumlogdet=-0.000 acc_mask=0.915 plaqs=0.863 intQ=-0.025 sinQ=-0.023 lr=0.001 \n[2023-12-05 12:00:50][INFO][trainer.py:1085] - era=0 epoch=3200 tstep=3201.000 dt=0.094 beta=4.000 loss=-7.105 dQsin=0.242 dQint=0.063 energy=396.792 logprob=396.322 logdet=0.469 sldf=0.277 sldb=-0.145 sld=0.469 xeps=0.121 veps=0.107 acc=0.918 sumlogdet=0.001 acc_mask=0.917 plaqs=0.863 intQ=0.019 sinQ=0.016 lr=0.001 \n[2023-12-05 12:01:15][INFO][trainer.py:1085] - era=0 epoch=3400 tstep=3401.000 dt=0.090 beta=4.000 loss=-7.398 dQsin=0.244 dQint=0.082 energy=396.890 logprob=396.384 logdet=0.506 sldf=0.298 sldb=-0.151 sld=0.506 xeps=0.126 veps=0.111 acc=0.912 sumlogdet=-0.000 acc_mask=0.901 plaqs=0.863 intQ=0.006 sinQ=0.006 lr=0.001 \n[2023-12-05 12:01:41][INFO][trainer.py:1085] - era=0 epoch=3600 tstep=3601.000 dt=0.092 beta=4.000 loss=-7.570 dQsin=0.248 dQint=0.073 energy=396.491 logprob=395.964 logdet=0.528 sldf=0.312 sldb=-0.167 sld=0.528 xeps=0.129 veps=0.114 acc=0.910 sumlogdet=-0.000 acc_mask=0.914 plaqs=0.864 intQ=0.028 sinQ=0.021 lr=0.001 \n[2023-12-05 12:02:07][INFO][trainer.py:1085] - era=0 epoch=3800 tstep=3801.000 dt=0.102 beta=4.000 loss=-7.497 dQsin=0.245 dQint=0.095 energy=396.474 logprob=395.923 logdet=0.551 sldf=0.326 sldb=-0.180 sld=0.551 xeps=0.132 veps=0.116 acc=0.913 sumlogdet=-0.000 acc_mask=0.901 plaqs=0.863 intQ=0.016 sinQ=0.017 lr=0.001 \n[2023-12-05 12:02:33][INFO][trainer.py:1085] - era=0 epoch=4000 tstep=4001.000 dt=0.092 beta=4.000 loss=-8.825 dQsin=0.254 dQint=0.087 energy=397.397 logprob=396.827 logdet=0.570 sldf=0.338 sldb=-0.194 sld=0.570 xeps=0.136 veps=0.119 acc=0.908 sumlogdet=-0.000 acc_mask=0.902 plaqs=0.863 intQ=0.003 sinQ=-0.000 lr=0.001 \n[2023-12-05 12:02:57][INFO][trainer.py:1085] - era=0 epoch=4200 tstep=4201.000 dt=0.094 beta=4.000 loss=-7.265 dQsin=0.244 dQint=0.074 energy=396.583 logprob=395.992 logdet=0.591 sldf=0.351 sldb=-0.207 sld=0.591 xeps=0.139 veps=0.121 acc=0.910 sumlogdet=-0.001 acc_mask=0.902 plaqs=0.864 intQ=0.042 sinQ=0.031 lr=0.001 \n[2023-12-05 12:03:22][INFO][trainer.py:1085] - era=0 epoch=4400 tstep=4401.000 dt=0.092 beta=4.000 loss=-7.974 dQsin=0.256 dQint=0.096 energy=397.000 logprob=396.407 logdet=0.593 sldf=0.353 sldb=-0.214 sld=0.593 xeps=0.140 veps=0.122 acc=0.919 sumlogdet=0.000 acc_mask=0.928 plaqs=0.863 intQ=-0.015 sinQ=-0.010 lr=0.001 \n[2023-12-05 12:03:47][INFO][trainer.py:1085] - era=0 epoch=4600 tstep=4601.000 dt=0.092 beta=4.000 loss=-8.677 dQsin=0.258 dQint=0.094 energy=396.710 logprob=396.109 logdet=0.601 sldf=0.359 sldb=-0.223 sld=0.601 xeps=0.142 veps=0.122 acc=0.903 sumlogdet=-0.000 acc_mask=0.897 plaqs=0.864 intQ=0.012 sinQ=0.007 lr=0.001 \n[2023-12-05 12:04:12][INFO][trainer.py:1085] - era=0 epoch=4800 tstep=4801.000 dt=0.094 beta=4.000 loss=-8.739 dQsin=0.258 dQint=0.087 energy=396.618 logprob=396.036 logdet=0.583 sldf=0.348 sldb=-0.218 sld=0.583 xeps=0.140 veps=0.121 acc=0.921 sumlogdet=-0.000 acc_mask=0.914 plaqs=0.864 intQ=-0.034 sinQ=-0.027 lr=0.001 \n[2023-12-05 12:04:37][INFO][trainer.py:1296] - Saving took: 4.76837e-06s \n[2023-12-05 12:04:37][INFO][trainer.py:1297] - Checkpoint saved to: /lus/grand/projects/DLHMC/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/checkpoints/U1/2-16-16/nlf-4/xsplit-True/sepnets-True/merge-True/net-16-16-16-16_dp-0.2_bn-True/tensorflow \n[2023-12-05 12:04:37][INFO][trainer.py:1298] - Era 0 took: 725.949s \n[2023-12-05 12:04:38][INFO][plot_helpers.py:1046] - Saving figure to: /lus/grand/projects/DLHMC/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-12-05-111405/tensorflow/plots/ridgeplots/svgs/energy_ridgeplot.svg \n[2023-12-05 12:04:40][INFO][plot_helpers.py:1046] - Saving figure to: /lus/grand/projects/DLHMC/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-12-05-111405/tensorflow/plots/ridgeplots/svgs/logprob_ridgeplot.svg \n[2023-12-05 12:04:42][INFO][plot_helpers.py:1046] - Saving figure to: /lus/grand/projects/DLHMC/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-12-05-111405/tensorflow/plots/ridgeplots/svgs/logdet_ridgeplot.svg \n[2023-12-05 12:04:44][INFO][plot_helpers.py:1046] - Saving figure to: /lus/grand/projects/DLHMC/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-12-05-111405/tensorflow/plots/ridgeplots/svgs/sldf_ridgeplot.svg \n[2023-12-05 12:04:46][INFO][plot_helpers.py:1046] - Saving figure to: /lus/grand/projects/DLHMC/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-12-05-111405/tensorflow/plots/ridgeplots/svgs/sldb_ridgeplot.svg \n[2023-12-05 12:04:48][INFO][plot_helpers.py:1046] - Saving figure to: /lus/grand/projects/DLHMC/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-12-05-111405/tensorflow/plots/ridgeplots/svgs/sld_ridgeplot.svg \n[2023-12-05 12:05:19][INFO][common.py:275] - Saving dataset to: /lus/grand/projects/DLHMC/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-12-05-111405/tensorflow/data/train_data.h5 \n[2023-12-05 12:05:20][INFO][experiment.py:378] - Done saving and analyzing data. \n[2023-12-05 12:05:20][INFO][experiment.py:379] - Creating summaries for WandB, Aim \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInference\n\nEvaluate\noutputs['tensorflow']['eval'] = tfExpU1.trainer.eval(\n    job_type='eval',\n    nprint=500,\n    nchains=128,\n    eval_steps=2000,\n)\n# dset_eval_tf = tfExpU1.trainer.histories['eval'].plot_all()\n_ = tfExpU1.save_dataset(job_type='eval', nchains=32)\n\n\n\n\noutput\n\n\n[2023-12-05 12:05:41][WARNING][trainer.py:194] - x.shape (original): (2048, 2, 16, 16) \n[2023-12-05 12:05:41][WARNING][trainer.py:194] - x[:nchains].shape: (128, 2, 16, 16) \n[2023-12-05 12:05:41][INFO][trainer.py:198] - eps = None\nbeta = 4.0\nnlog = 10\ntable = &lt;rich.table.Table object at 0x7efa683ea0e0&gt;\nnprint = 500\neval_steps = 2000\nnleapfrog = None \n\n  0%|          | 0/2000 [00:00&lt;?, ?it/s]\n[2023-12-05 12:06:28][INFO][trainer.py:198] - estep=0 dt=4.940 beta=4.000 loss=-7.502 dQsin=0.238 dQint=0.117 energy=396.232 logprob=395.648 logdet=0.584 sldf=0.349 sldb=-0.222 sld=0.584 xeps=0.141 veps=0.121 acc=0.928 sumlogdet=0.001 acc_mask=0.914 plaqs=0.863 intQ=0.023 sinQ=0.036 \n[2023-12-05 12:07:17][INFO][trainer.py:198] - estep=500 dt=0.024 beta=4.000 loss=-3.405 dQsin=0.239 dQint=0.047 energy=395.434 logprob=394.850 logdet=0.584 sldf=0.349 sldb=-0.223 sld=0.584 xeps=0.141 veps=0.121 acc=0.934 sumlogdet=0.000 acc_mask=0.969 plaqs=0.865 intQ=0.008 sinQ=0.001 \n[2023-12-05 12:08:01][INFO][trainer.py:198] - estep=1000 dt=0.024 beta=4.000 loss=-5.784 dQsin=0.227 dQint=0.102 energy=393.733 logprob=393.149 logdet=0.584 sldf=0.349 sldb=-0.222 sld=0.584 xeps=0.141 veps=0.121 acc=0.913 sumlogdet=0.000 acc_mask=0.953 plaqs=0.863 intQ=0.188 sinQ=0.178 \n[2023-12-05 12:08:44][INFO][trainer.py:198] - estep=1500 dt=0.024 beta=4.000 loss=-7.127 dQsin=0.226 dQint=0.063 energy=396.790 logprob=396.205 logdet=0.584 sldf=0.349 sldb=-0.223 sld=0.584 xeps=0.141 veps=0.121 acc=0.902 sumlogdet=-0.000 acc_mask=0.898 plaqs=0.864 intQ=-0.172 sinQ=-0.126 \n[2023-12-05 12:09:32][INFO][plot_helpers.py:1046] - Saving figure to: /lus/grand/projects/DLHMC/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-12-05-111405/tensorflow/plots/ridgeplots/svgs/energy_ridgeplot.svg \n[2023-12-05 12:09:34][INFO][plot_helpers.py:1046] - Saving figure to: /lus/grand/projects/DLHMC/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-12-05-111405/tensorflow/plots/ridgeplots/svgs/logprob_ridgeplot.svg \n[2023-12-05 12:09:36][INFO][plot_helpers.py:1046] - Saving figure to: /lus/grand/projects/DLHMC/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-12-05-111405/tensorflow/plots/ridgeplots/svgs/logdet_ridgeplot.svg \n[2023-12-05 12:09:38][INFO][plot_helpers.py:1046] - Saving figure to: /lus/grand/projects/DLHMC/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-12-05-111405/tensorflow/plots/ridgeplots/svgs/sldf_ridgeplot.svg \n[2023-12-05 12:09:40][INFO][plot_helpers.py:1046] - Saving figure to: /lus/grand/projects/DLHMC/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-12-05-111405/tensorflow/plots/ridgeplots/svgs/sldb_ridgeplot.svg \n[2023-12-05 12:09:42][INFO][plot_helpers.py:1046] - Saving figure to: /lus/grand/projects/DLHMC/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-12-05-111405/tensorflow/plots/ridgeplots/svgs/sld_ridgeplot.svg \n[2023-12-05 12:09:57][INFO][common.py:275] - Saving dataset to: /lus/grand/projects/DLHMC/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-12-05-111405/tensorflow/data/eval_data.h5 \n[2023-12-05 12:09:57][INFO][experiment.py:378] - Done saving and analyzing data. \n[2023-12-05 12:09:57][INFO][experiment.py:379] - Creating summaries for WandB, Aim \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHMC\noutputs['tensorflow']['hmc'] = tfExpU1.trainer.eval(\n    job_type='hmc',\n    nprint=500,\n    nchains=128,\n    eval_steps=2000,\n)\n_ = tfExpU1.save_dataset(job_type='hmc', nchains=32)\n\n\n\n\noutput\n\noutput:\n\n[2023-12-05 12:10:19][WARNING][trainer.py:194] - Step size `eps` not specified for HMC! Using default: 0.2500 for generic HMC \n[2023-12-05 12:10:19][WARNING][trainer.py:194] - x.shape (original): (2048, 2, 16, 16) \n[2023-12-05 12:10:19][WARNING][trainer.py:194] - x[:nchains].shape: (128, 2, 16, 16) \n[2023-12-05 12:10:19][INFO][trainer.py:198] - eps = 0.25\nbeta = 4.0\nnlog = 10\ntable = &lt;rich.table.Table object at 0x7ef93c654940&gt;\nnprint = 500\neval_steps = 2000\nnleapfrog = 8 \n\n  0%|          | 0/2000 [00:00&lt;?, ?it/s]\n[2023-12-05 12:11:01][INFO][trainer.py:198] - hstep=0 dt=0.089 beta=4.000 loss=59.310 dQsin=0.049 dQint=0.039 energy=403.394 logprob=403.394 logdet=0.000 acc=0.059 sumlogdet=0.000 acc_mask=0.078 plaqs=0.863 intQ=-0.109 sinQ=-0.093 \n[2023-12-05 12:12:14][INFO][trainer.py:198] - hstep=500 dt=0.083 beta=4.000 loss=55.566 dQsin=0.021 dQint=0.016 energy=400.521 logprob=400.521 logdet=0.000 acc=0.061 sumlogdet=0.000 acc_mask=0.047 plaqs=0.864 intQ=0.148 sinQ=0.112 \n[2023-12-05 12:13:28][INFO][trainer.py:198] - hstep=1000 dt=0.084 beta=4.000 loss=63.178 dQsin=0.019 dQint=0.016 energy=401.798 logprob=401.798 logdet=0.000 acc=0.039 sumlogdet=0.000 acc_mask=0.039 plaqs=0.865 intQ=-0.016 sinQ=-0.016 \n[2023-12-05 12:14:43][INFO][trainer.py:198] - hstep=1500 dt=0.084 beta=4.000 loss=61.681 dQsin=0.018 dQint=0.008 energy=398.577 logprob=398.577 logdet=0.000 acc=0.058 sumlogdet=0.000 acc_mask=0.062 plaqs=0.865 intQ=-0.148 sinQ=-0.140 \n[2023-12-05 12:16:02][INFO][plot_helpers.py:1046] - Saving figure to: /lus/grand/projects/DLHMC/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-12-05-111405/tensorflow/plots/ridgeplots/svgs/energy_ridgeplot.svg \n[2023-12-05 12:16:03][INFO][plot_helpers.py:1046] - Saving figure to: /lus/grand/projects/DLHMC/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-12-05-111405/tensorflow/plots/ridgeplots/svgs/logprob_ridgeplot.svg \n[2023-12-05 12:16:05][INFO][plot_helpers.py:1046] - Saving figure to: /lus/grand/projects/DLHMC/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-12-05-111405/tensorflow/plots/ridgeplots/svgs/logdet_ridgeplot.svg \n[2023-12-05 12:16:17][INFO][common.py:275] - Saving dataset to: /lus/grand/projects/DLHMC/foremans/locations/thetaGPU/projects/saforem2/l2hmc-qcd/src/l2hmc/notebooks/outputs/2023-12-05-111405/tensorflow/data/hmc_data.h5 \n[2023-12-05 12:16:17][INFO][experiment.py:378] - Done saving and analyzing data. \n[2023-12-05 12:16:17][INFO][experiment.py:379] - Creating summaries for WandB, Aim",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "📗 Jupyter",
      "`l2hmc` Example: 2D $U(1)$"
    ]
  },
  {
    "objectID": "posts/jupyter/test/index.html#tensorflow-results",
    "href": "posts/jupyter/test/index.html#tensorflow-results",
    "title": "l2hmc Example: 2D U(1)",
    "section": "TensorFlow Results",
    "text": "TensorFlow Results\nsns.set_context('notebook')\nndraws = len(dsets['tf']['eval']['dQint'].draw)\ndrop = int(0.1 * ndraws)\nkeep = int(0.9 * ndraws)\n\ndqe = dsets['tf']['eval']['dQint'][:, -90:]\ndqh = dsets['tf']['hmc']['dQint'][:, -90:]\n\netot = dqe.astype(int).sum()\nhtot = dqh.astype(int).sum()\n\nfsize = plt.rcParams['figure.figsize']\n#figsize = (2.5 * fsize[0], fsize[1])\nfig, ax = plt.subplots(figsize=figsize, ncols=2)\n_ = dqe.astype(int).plot(ax=ax[0])\n_ = dqh.astype(int).plot(ax=ax[1])\n_ = ax[0].set_title(f'Eval, total: {etot.values}');\n_ = ax[1].set_title(f'HMC, total: {htot.values}');\n_ = fig.suptitle(fr'TensorFlow Improvement: {100*(etot / htot):3.0f}%')\n\ndqe_tot = dqe.astype(int).sum().T.values.sum()\ndqh_tot = dqh.astype(int).sum().T.values.sum()\ndqeh_ratio = dqe_tot / dqh_tot\n\nlog.info(f\"TensorFlow, EVAL\\n {dqe.astype(int).sum('chain').T=}\")\nlog.info(f\"Eval: {dqe.astype(int).sum().T.values.sum()=}\")\nlog.info(f\"TensorFlow, HMC\\n {dqh.astype(int).sum('chain').T=}\")\nlog.info(f\"HMC: {dqh.astype(int).sum().T.values.sum()=}\")\nlog.critical(f\"dQ_eval / dQ_hmc: {dqeh_ratio:.4f}\")\n\n\n\n\noutput\n\n\n[2023-12-05 12:33:43][INFO][3549449091.py:25] - TensorFlow, EVAL\n dqe.astype(int).sum('chain').T=&lt;xarray.DataArray 'dQint' (draw: 90)&gt;\narray([ 4,  2,  7,  5, 10,  6,  9,  6,  4,  5,  7,  6,  3,  5,  2,  7,  9,\n        7,  2,  5,  8,  8, 10,  6,  5,  9,  5, 10,  7,  6,  7,  8,  3,  7,\n        9,  4,  8,  8,  4,  5,  3,  4,  5, 10,  9,  4,  9,  8,  4,  9,  5,\n        5,  6,  9,  4,  7,  5,  5,  7,  7,  6,  3,  8,  8, 11,  4, 10,  7,\n        7,  7,  5,  9,  7,  7,  7,  9,  5,  8,  6,  5,  7,  6,  6,  6, 10,\n        6,  8,  7,  7,  4])\nCoordinates:\n  * draw     (draw) int64 110 111 112 113 114 115 ... 194 195 196 197 198 199 \n[2023-12-05 12:33:43][INFO][3549449091.py:26] - Eval: dqe.astype(int).sum().T.values.sum()=579 \n[2023-12-05 12:33:43][INFO][3549449091.py:27] - TensorFlow, HMC\n dqh.astype(int).sum('chain').T=&lt;xarray.DataArray 'dQint' (draw: 90)&gt;\narray([0, 1, 0, 1, 0, 0, 2, 0, 3, 0, 1, 2, 0, 2, 1, 3, 1, 0, 2, 2, 0, 0,\n       0, 0, 2, 1, 2, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 2, 1, 1, 1, 0, 1, 0,\n       2, 1, 1, 1, 2, 3, 3, 1, 0, 2, 1, 0, 0, 0, 1, 0, 0, 3, 1, 0, 5, 0,\n       0, 1, 0, 0, 0, 1, 1, 2, 1, 3, 1, 1, 0, 2, 0, 1, 0, 0, 1, 3, 0, 0,\n       0, 1])\nCoordinates:\n  * draw     (draw) int64 110 111 112 113 114 115 ... 194 195 196 197 198 199 \n[2023-12-05 12:33:43][INFO][3549449091.py:28] - HMC: dqh.astype(int).sum().T.values.sum()=80 \n[2023-12-05 12:33:43][CRITICAL][3549449091.py:29] - dQ_eval / dQ_hmc: 7.2375 \n\n\n\n\n\n\nPyTorch Results\nsns.set_context('notebook', font_scale=1.25)\n\nndraws = len(dsets['pt']['eval']['dQint'].draw)\ndrop = int(0.1 * ndraws)\nkeep = int(0.9 * ndraws)\n\ndqe = dsets['pt']['eval']['dQint'][:, -90:]\ndqh = dsets['pt']['hmc']['dQint'][:, -90:]\n\netot = dqe.astype(int).sum()\nhtot = dqh.astype(int).sum()\n\nfsize = plt.rcParams['figure.figsize']\nfigsize = (2.5 * fsize[0], 0.8 * fsize[1])\nfig, ax = plt.subplots(figsize=figsize, ncols=2)\n_ = dqe.astype(int).plot(ax=ax[0])\n_ = dqh.astype(int).plot(ax=ax[1])\n_ = ax[0].set_title(f'Eval, total: {etot.values}');\n_ = ax[1].set_title(f'HMC, total: {htot.values}');\n#_ = fig.suptitle(fr'PyTorch Improvement: {100*(etot / htot):3.0f}%')\n\n\n\n\noutput\n\n\n\n\n\n\n\ndqe_tot = dqe.astype(int).sum().T.values.sum()\ndqh_tot = dqh.astype(int).sum().T.values.sum()\ndqeh_ratio = dqe_tot / dqh_tot\n\nlog.info(f\"PyTorch, EVAL\\n {dqe.astype(int).sum('chain').T=}\")\nlog.info(f\"Eval: {dqe.astype(int).sum().T.values.sum()=}\")\nlog.info(f\"TensorFlow, HMC\\n {dqh.astype(int).sum('chain').T=}\")\nlog.info(f\"HMC: {dqh.astype(int).sum().T.values.sum()=}\")\nlog.critical(f\"dQ_eval / dQ_hmc: {dqeh_ratio:.4f}\")\n\n\n\n\noutput\n\n\n[2023-12-05 12:35:35][INFO][2202273834.py:5] - PyTorch, EVAL\n dqe.astype(int).sum('chain').T=&lt;xarray.DataArray 'dQint' (draw: 90)&gt;\narray([ 8,  6,  8,  8,  5,  6,  5, 10, 13,  8,  2,  4,  7,  9,  6,  6,  8,\n        8,  8, 10,  5,  9,  6,  6, 12,  3,  6,  7,  5,  8,  8, 12,  7,  4,\n        8,  7,  3,  6,  4,  5,  7,  6,  6, 10,  7,  4,  4, 11,  7,  7,  7,\n        4,  6,  7,  6,  6, 10,  9,  5,  6,  6,  5, 13,  2,  9,  9, 14,  7,\n        3,  5,  7,  6,  9,  9,  3,  9,  4,  2,  6,  9,  5,  3, 10,  7,  8,\n        8,  7,  6,  6,  7])\nCoordinates:\n  * draw     (draw) int64 110 111 112 113 114 115 ... 194 195 196 197 198 199 \n[2023-12-05 12:35:35][INFO][2202273834.py:6] - Eval: dqe.astype(int).sum().T.values.sum()=615 \n[2023-12-05 12:35:35][INFO][2202273834.py:7] - TensorFlow, HMC\n dqh.astype(int).sum('chain').T=&lt;xarray.DataArray 'dQint' (draw: 90)&gt;\narray([2, 3, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 2, 1, 3, 0, 0, 5, 3, 0, 0, 0,\n       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 3, 4, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n       0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 2, 1, 2, 2, 0, 1, 2, 4, 0, 1, 2, 1,\n       1, 0, 0, 1, 0, 1, 0, 2, 0, 2, 2, 0, 0, 0, 1, 0, 1, 1, 2, 0, 0, 1,\n       0, 0])\nCoordinates:\n  * draw     (draw) int64 110 111 112 113 114 115 ... 194 195 196 197 198 199 \n[2023-12-05 12:35:35][INFO][2202273834.py:8] - HMC: dqh.astype(int).sum().T.values.sum()=73 \n[2023-12-05 12:35:35][CRITICAL][2202273834.py:9] - dQ_eval / dQ_hmc: 8.4247",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "📗 Jupyter",
      "`l2hmc` Example: 2D $U(1)$"
    ]
  },
  {
    "objectID": "posts/jupyter/test/index.html#comparisons-1",
    "href": "posts/jupyter/test/index.html#comparisons-1",
    "title": "l2hmc Example: 2D U(1)",
    "section": "Comparisons",
    "text": "Comparisons\nimport matplotlib.pyplot as plt\nfrom l2hmc.utils.plot_helpers import set_plot_style, COLORS\n\nimport seaborn as sns\nset_plot_style()\nplt.rcParams['axes.linewidth'] = 2.0\nsns.set_context('notebook', font_scale=1.25)\nfigsize = plt.rcParamsDefault['figure.figsize']\nplt.rcParams['figure.dpi'] = plt.rcParamsDefault['figure.dpi']\n\nfor idx in range(4):\n    fig, (ax, ax1) = plt.subplots(\n        ncols=2,\n        #nrows=4,\n        figsize=(3. * figsize[0], figsize[1]),\n    )\n    _ = ax.plot(\n        dsets['pt']['eval'].intQ[idx] + 5,  # .dQint.mean('chain')[100:],\n        color=COLORS['red'],\n        ls=':',\n        label='Trained',\n        lw=1.5,\n    );\n\n    _ = ax.plot(\n        dsets['pt']['hmc'].intQ[idx] - 5,  # .dQint.mean('chain')[100:],\n        ls='-',\n        label='HMC',\n        color='#666666',\n        zorder=5,\n        lw=2.0,\n    );\n\n    _ = ax1.plot(\n        dsets['tf']['eval'].intQ[idx] + 5,  # .dQint.mean('chain')[-100:],\n        color=COLORS['blue'],\n        ls=':',\n        label='Trained',\n        lw=1.5,\n\n    );\n    _ = ax1.plot(\n        dsets['tf']['hmc'].intQ[idx] - 5,  # .dQint.mean('chain')[-100:],\n        color='#666666',\n        ls='-',\n        label='HMC',\n        zorder=5,\n        lw=2.0,\n    );\n    _ = ax.set_title('PyTorch')\n    _ = ax1.set_title('TensorFlow')\n    #_ = ax1.set_ylim(ax.get_ylim())\n    _ = ax.grid(True, alpha=0.2)\n    _ = ax1.grid(True, alpha=0.2)\n    _ = ax.set_xlabel('MD Step')\n    _ = ax1.set_xlabel('MD Step')\n    _ = ax.set_ylabel('dQint'\n                     )\n    _ = ax.legend(loc='best', ncol=2, labelcolor='#939393')\n    _ = ax1.legend(loc='best', ncol=2, labelcolor='#939393')\n\n\n\n\noutput",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "📗 Jupyter",
      "`l2hmc` Example: 2D $U(1)$"
    ]
  },
  {
    "objectID": "qmd/projects/index.html",
    "href": "qmd/projects/index.html",
    "title": "📦 Projects",
    "section": "",
    "text": "📊 GitHub Stats\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nEven More !!\n\n\n\nWakatime\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n📂 saforem2/\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{foreman,\n  author = {Foreman, Sam},\n  title = {📦 {Projects}},\n  url = {https://samforeman.me},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nForeman, Sam. n.d. “📦 Projects.” https://samforeman.me.",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>projects</code>",
      "📚 All Projects"
    ]
  },
  {
    "objectID": "posts/AuroraGPT/long-sequences/index.html",
    "href": "posts/AuroraGPT/long-sequences/index.html",
    "title": "🚂 Loooooooong Sequence Lengths",
    "section": "",
    "text": "The new Megatron-DeepSpeed release contains a variety of improvements / optimizations to enable pre-training Transformer based architectures with significantly longer sequences than was previously possible.",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "🤖 AuroraGPT",
      "🚂 Loooooooong Sequence Lengths"
    ]
  },
  {
    "objectID": "posts/AuroraGPT/long-sequences/index.html#deepspeed4science-092023",
    "href": "posts/AuroraGPT/long-sequences/index.html#deepspeed4science-092023",
    "title": "🚂 Loooooooong Sequence Lengths",
    "section": "DeepSpeed4Science (09/2023)",
    "text": "DeepSpeed4Science (09/2023)\n\nNew Features\n\nEnabled Megatron-LM’s sequence parallel.\nEnabled rotary positional embedding.\nEnabled FlashAttention v1 and v2.\nEnabled new fused kernels from NVIDIA.\n\n\n\nNew optimizations\n\nEnabled attention map memory optimization, where we first generated attention mask on CPU memory and then moved it into GPU memory to avoid out-of-memory errors when training with very large sequence lengths.\nPosition embedding partitioning, where we split weights of position encoding across all GPUs when enabling sequence parallel to further reduce the memory footprint.\n\n\n\nInitial Results\n\n\n\nTable 1: Long sequence length support1 from microsoft/Megatron-DeepSpeed\n\n\n\n\n\n\n\n\n\n\nSequence Length\nOld Megatron-DeepSpeed (TFLOPS)\nNew Megatron-DeepSpeed (TFLOPS)\n\n\n\n\n2k\n25\n68\n\n\n4k\n28\n80\n\n\n8k\nOOM\n86\n\n\n16k\nOOM\n92\n\n\n32k\nOOM\n100\n\n\n64k\nOOM\n106\n\n\n128k\nOOM\n119\n\n\n256k\nOOM\n94\n\n\n\n\n\n\n\n\nData\ngpus = ('32', '64', '128')\n\ncolors = {\n    'Old Megatron-DS': '#FF5252',\n    'Megatron-LM': '#76b900',\n    'New Megatron-DS':  '#1A8FFF',\n}\n\ndata = {\n    '25B': {\n        'Old Megatron-DS': np.array([36, 42, 42]),\n        'Megatron-LM': np.array([26, 48, 52]),\n        'New Megatron-DS': np.array([192, 448, 512]),\n    },\n    '33B': {\n        'Old Megatron-DS': np.array([28, 32, 32]),\n        'Megatron-LM': np.array([14, 46, 52]),\n        'New Megatron-DS': np.array([128, 384, 448]),\n    },\n}\n\n\n\nMake the plots\nx = np.arange(len(gpus))\nwidth = 0.25\nmultiplier = 0\n\noutdir = Path(os.getcwd()).joinpath('assets')\noutdir.mkdir(exist_ok=True, parents=True)\n\nimprovement = {}\nfor idx, (model_size, d) in enumerate(data.items()):\n    multiplier = 0\n    figure, axes = plt.subplots(figsize=(7.5, 4))\n    fig = plt.gcf()\n    ax = plt.gca()\n    for label, value in d.items():\n        offset = width * multiplier\n        rects = ax.barh(\n          x + offset,\n          value,\n          width,\n          label=label,\n          color=colors[label],\n          alpha=0.8\n        )\n        ax.bar_label(\n          rects,\n          padding=3,\n          color=colors[label],\n          family='monospace',\n          weight='bold'\n        )\n        multiplier += 1\n    ax.set_ylabel(\n        'GPUs',\n        fontsize=18,\n        family='sans-serif',\n        loc='center',\n    )\n    ax.set_yticks(x + width, gpus)\n    plt.figtext(\n        0.005, 0.93, f\"{model_size}\", fontsize=24, fontweight='bold', ha='left'\n    )\n    ax.set_xlabel(\n        'Sequence Length (k)', fontsize=18, loc='center'\n    )\n    ax.legend(\n        bbox_to_anchor=(0.005, 1.04, 0.99, .098),\n        alignment='center',\n        edgecolor=\"#83838320\",\n        frameon=True,\n        ncols=3,\n        fontsize=13,\n        mode=\"expand\",\n        borderaxespad=0.01\n    )\n    save_figure(fname=f'{model_size}', outdir=outdir)\n    _ = plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nGPT-25B Model\n\n\n\n\n\n\n\nGPT-33B Model\n\n\n\n\n\n\n\nFigure 2: Pre-training with long sequence support across different model sizes and numbers of GPUs. In each case, the new (current) implementation significantly outperforms both NVIDIA/Megatron-LM as well as our previous implementation.",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "🤖 AuroraGPT",
      "🚂 Loooooooong Sequence Lengths"
    ]
  },
  {
    "objectID": "posts/AuroraGPT/long-sequences/index.html#installation",
    "href": "posts/AuroraGPT/long-sequences/index.html#installation",
    "title": "🚂 Loooooooong Sequence Lengths",
    "section": "Installation",
    "text": "Installation\n\nUsing install.sh\n\n\n\n\n\n\nInstallation\n\n\n\n\n\nImportant To install, simply:\ngit clone https://github.com/ramanthanlab/GenSLM/\ncd GenSLM/examples/long-sequences/\n./install.sh\nExplicitly, ./install.sh will:\n\nAutomatically create a virtual environment on top of the latest conda module\nInstall (+ update2) / build all the required dependencies into this virtual environment\n\n\n\n\n\n\nStep-by-Step\nFor completeness, we describe below the steps for installing and building each of the dependencies.\n\nClone GitHub repo:\ngit clone https://github.com/ramanthanlab/GenSLM\nLoad conda module:\n\nThetaGPU:\n# ThetaGPU:\nif [[ \"$(hostname)==theta*\" ]]; then\n    export MACHINE=\"ThetaGPU\"\n    export CONDA_DATE=\"2023-01-10\"\n    module load conda/2023-01-11\n    conda activate base\nfi\nPolaris:\n# Polaris:\nif [[ \"$(hostname)==x3*\" ]]; then\n    export MACHINE=\"Polaris\"\n    export CONDA_DATE=\"2023-01-10\"\n    module load conda/2023-01-10-unstable\n    conda activate base\nfi\n\nSetup Virtual Environment3:\ncd ./genslm/examples/long-sequences\n# create a new virtual environment\nmkdir -p \"venvs/${MACHINE}/${CONDA_DATE}\"\npython3 -m venv \"venvs/${MACHINE}/${CONDA_DATE}\" --system-site-packages\nsource \"venvs/${MACHINE}/${CONDA_DATE}/bin/activate\"\nCreate a new folder (genslm/examples/long-sequences/deps/${MACHINE}) where we’ll installing dependencies locally:\nmkdir -p \"deps/${MACHINE}\"\ncd \"deps/${MACHINE}\"\n\n\nDependencies\nWe provide below the details needed to install each of the required dependencies.\n\n\n saforem2/ezpz\n\n\n saforem2/ezpz\npip install -e \"git+https://github.com/saforem2/ezpz.git#egg=ezpz\"\n\n\n\n\n Microsoft/DeepSpeed\n\n\n Microsoft/DeepSpeed\ngit clone https://github.com/microsoft/DeepSpeed.git\ncd DeepSpeed\npython3 -m pip install -e .\n\n\n\n\n Microsoft/Megatron-DeepSpeed\n\n\n Microsoft/Megatron-DeepSpeed:\ngit clone https://github.com/microsoft/Megatron-DeepSpeed.git\n\n\n\n\n NVIDIA/apex\n\n\n NVIDIA/apex\ngit clone https://github.com/NVIDIA/apex\ncd ../apex/\npip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" -e ./\n\n\n\n\n pybind/PyBind11\n\n\n pybind/PyBind11\npip install pybind11\n\n\n\n\n Dao-AILab/flash-attention\n\n\n Dao-AILab/flash-attention:\n\n\n\n\n\n\nFlash Attention\n\n\n\n\n\n\nThe new release supports three different implementations of FlashAttention: (v1.0.4, v2.x, triton)\nFlashAttention v2.x may have numerical instability issues. For the best performance, we recommend using FlashAttention + Triton\n\n\n\n\n\nv1.0.4:\npython3 -m pip install flash-attn==1.0.4\nv2.x:\ngit clone https://github.com/Dao-AILab/flash-attention\ncd flash-attention\npython3 setup.py install\nopenai/triton:\ngit clone -b legacy-backend https://github.com/openai/triton\ncd triton/python\npython3 -m pip install cmake\npython3 -m pip install .",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "🤖 AuroraGPT",
      "🚂 Loooooooong Sequence Lengths"
    ]
  },
  {
    "objectID": "posts/AuroraGPT/long-sequences/index.html#running",
    "href": "posts/AuroraGPT/long-sequences/index.html#running",
    "title": "🚂 Loooooooong Sequence Lengths",
    "section": "Running",
    "text": "Running\nThe ALCF/ directory contains shell scripts for setting up the environment and specifying the options to be used when launching.\nVarious options can be specified dynamically at runtime by setting them in your environment, e.g.:\nMODEL_SIZE_KEY=\"GPT25B\" SEQ_LEN=128000 USE_FLASH_ATTN=1 MICRO_BATCH=1 GAS=1 SP_TYPE=\"megatron\" ZERO_STAGE=1 ./ALCF/train-gpt3.sh\nExplicitly:\n\nALCF/train-gpt3.sh: Main entry point for training\n\nThis script will automatically source the rest of the required ALCF/*.sh scripts below\n\nALCF/models.sh: Contains some example model architectures for GPT3-style models\nALCF/args.sh: Logic for parsing / setting up runtime options for Megatron and DeepSpeed\nALCF/setup.sh: Locate and activate virtual environment to be used, ensure MPI variables are set properly\nALCF/launch.sh: Identify available resources and build the command to be executed\n\ni.e. figure out how many: {nodes, GPUs per node, GPUs total}, to pass to mpi{run,exec}\nthen, use this to build mpiexec &lt;mpiexec-args&gt; python3 pretrain_gpt.py",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "🤖 AuroraGPT",
      "🚂 Loooooooong Sequence Lengths"
    ]
  },
  {
    "objectID": "posts/AuroraGPT/long-sequences/index.html#zero-offloading",
    "href": "posts/AuroraGPT/long-sequences/index.html#zero-offloading",
    "title": "🚂 Loooooooong Sequence Lengths",
    "section": "ZeRO Offloading",
    "text": "ZeRO Offloading\n🚀 W&B Report: Looooooooong Sequences\nThese newly introduced optimizations, in combination with ZeRO-Offload allows us to go even further.\nBy employing ZeRO-Offloading, we are able to free up additional memory which can be used for even longer sequences.\nThough work is still ongoing, this is a promising direction that will allow us to consider significantly larger genomes than previously possible.\nWe use Weights & Biases to track these experiments, and have aggregated our initial results in the W&B Report below.\nWe can evaluate the performance of our model by looking at two different metrics for throughput: samples_per_sec and TFLOPS.\nExplicitly, we see that we are able to scale up to significantly longer sequences (420k / 128k ~ 3.3x) with only a minimal impact on throughput performance (81 / 105 ~ 77\\%)4.\n\n\n\nTable 2: Impact on TFLOPS as a function of increasing sequence length. Table from: throughput/TFLOPS\n\n\n\n\n\n\n\n\n\n\n\n\nName\nSequence Length (k)\n(seq_len / min_seq_len)\nTFLOPS\nTFLOPS (% of peak)\n\n\n\n\nGPT25B\n420\n3.28125\n81.77225\n77.867\n\n\nGPT25B\n400\n3.125\n90.62\n86.297\n\n\nGPT25B\n360\n2.8125\n81.6325\n77.7348\n\n\nGPT25B\n360\n2.8125\n82.6824\n78.7346\n\n\nGPT25B\n192\n1.5\n115.8228\n110.2927\n\n\nGPT25B\n128\n1\n106.672\n101.5788\n\n\nGPT25B\n128\n1\n105.014\n100.00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Weights & Biases Report",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "🤖 AuroraGPT",
      "🚂 Loooooooong Sequence Lengths"
    ]
  },
  {
    "objectID": "posts/AuroraGPT/long-sequences/index.html#footnotes",
    "href": "posts/AuroraGPT/long-sequences/index.html#footnotes",
    "title": "🚂 Loooooooong Sequence Lengths",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe described experiments were performed on 4 NVIDIA DGX A100-40GB nodes, all using TPSIZE=32[^tpsize], connected through 8 HDR InfiniBand (200Gb/s per HDR).↩︎\n\n\ndeepspeed-0.10.3\npytorch==2.0.0+cu118\n\n↩︎\nWhere \"${MACHINE}\" \\in {\"ThetaGPU\", \"Polaris\"} and \"${CONDA_DATE}\" \\in {\"2023-01-10\", \"2023-01-11\"}↩︎\nthroughput/TFLOPS↩︎",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "<code>posts</code>",
      "🤖 AuroraGPT",
      "🚂 Loooooooong Sequence Lengths"
    ]
  },
  {
    "objectID": "index.html#sec-about",
    "href": "index.html#sec-about",
    "title": "",
    "section": "🧑🏻‍💻 About Me",
    "text": "🧑🏻‍💻 About Me\n\n\n\n\n\n💻 Computational scientist\n@ Argonne National Laboratory (ALCF)\n🧪 Interested in:\n\n{AI, HPC} for science1\n🚀 scaling large models across2 thousands of GPUs\n\n\n\n\n\n\n\n\n\n\n🎤 Recent Talks\n\n\n\n\n\n📊 here ( + how I make them! )\n\n\n\n\n\n\n\n\n\n Now Playing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n👀 If you’re curious\n\n\n\n\n🔥 What I work on\n\nAs a member of the AI / ML Group at ALCF, I work on3:\n\n\n\n🤖 🧪 AI + Science\n🎲 Building better sampling methods for Lattice QCD\n🧬 Genome-Scale Language Models\n\n GenSLM\n🥇 ACM Gordon Bell Special Prize\n\n\n\n\n\n🌍 Foundation models for long term climate forecasting\n🏃‍♂️ Scaling Large Language Models\n🏎️ Distributed training across thousands of GPUs\n\n\n\n\n\n\n📍 How I got here\n\nMy current research focuses on using deep generative modeling to help build better sampling algorithms in lattice gauge theory. In particular, I’m interested in building gauge equivariant neural network architectures and using inductive priors to incorporate physical symmetries into machine learning models.\nI received my PhD in Physics from the University of Iowa in 2019 and my thesis was on Learning Better Physics: A Machine Learning Approach to Lattice Gauge Theory.\nPrior to this, I completed two bachelors degrees (Engineering Physics and Applied Mathematics, 2015) at The University of Illinois at Urbana-Champaign. My undergraduate dissertation was titled Energy Storage in Quantum Resonators and was supervised by Professor Alfred Hübler within the Center for Complex Systems Research at UIUC4.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n📝 Work🦜 Talks📬 Posts📦 Projects🪖 Experience🎶 Music\n\n\n[NOTE]: You can find a full list of my publications on my Google Scholar.\n\nIntro to HPC Bootcamp: Engaging New Communities Through Energy Justice Projects\nJournal of Computational Science, 2024\nThorough Characterization and Analysis of Large Transformer Model Training At-Scale\nProc. ACM Meas. Anal. Comput. Syst. 03/2024\nMLMC: Machine Learning Monte Carlo for Lattice Gauge Theory\nS. Foreman et al. Lattice, 2023 (Proceedings), 12/2023\nProtein Generation via Genome-scale Language Models with Bio-physical Scoring\n@ SC’23, 11/2023\n DeepSpeed4Science Initiative: Enabling Large-Scale Scientific Discovery […]\n@ NeurIPS 2023 AI For Science Workshop, 10/2023\n\n DeepSpeed4Science.ai Blog Post\n Loooooooong Sequence Lengths\n\nComprehensive Performance Study of LLMs on Novel AI Accelerators\nM. Emani, S. Foreman, et al., IPDPS 2024, 10/2023\nExploratory Analysis of Climate Data with ClimRR\nS. Foreman, Intro to HPC Bootcamp @ NERSC, 08/2023\n🏆 GenSLMs: Genome-scale language models reveal SARS-Cov-2 evolutionary dynamics\n@ SC’22 10/2022\n\n🥇 ACM Gordon Bell Special Prize\n\nLattice QCD and Particle Physics\nA.S. Kronfeld et al., 07/2022\nApplications of ML to Lattice QFT\nD. Boyda, S. Calí, S. Foreman, et al., [arXiv:2202.05838], 02/2022\nLeapFrogLayers: Trainable Framework for Effective Sampling\nS. Foreman, X.Y. Jin, J.C. Osborn, Lattice, 2021\nHMC with Normalizing Flows [slides]\nS. Foreman et al., Lattice, 2021\nDeep Learning Hamiltonian Monte Carlo [+ poster]\nS. Foreman, X.Y. Jin, & J.C. Osborn, @ SimDL Workshop @ ICLR, 2021\nMachine Learning and Neural Networks for Field Theory\nS. Foreman, X.Y. Jin, & J.C. Osborn, SnowMass, 2020\nExamples of renormalization group transformations for image sets\nS. Foreman et al., Physical Review E., 2018\nRG inspired Machine Learning for lattice field theory\nS. Foreman et al., arXiv:1710.02079, 2017\nLarge Energy Density in Three-Plate Nanocapacitors due to Coulomb Blockade\nS. Foreman et al., J. Appl. Phys, 2018\n\n\n\n\n\n\n\n\nTitle\n\n\nlocation\n\n\nDate\n\n\n\n\n\n\nAuroraGPT\n\n\nHPC User Forum Fall ’24\n\n\n2024-09-04\n\n\n\n\nTraining LLMs at Scale\n\n\nATPESC 2024\n\n\n2024-08-09\n\n\n\n\nLLMs on Polaris\n\n\nSciFM Summer School ’24\n\n\n2024-07-17\n\n\n\n\nMLMC: Machine Learning Monte Carlo\n\n\nLattice 2023\n\n\n2023-07-31\n\n\n\n\n\nNo matching items\n\n\n\n📆 2024\n\n\n\n\n\n\nAuroraGPT @ HPC User Forum, 2024 [09/2024]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTraining LLMs at Scale @ ATPESC, 2024 [08/2024]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLLMs on Polaris @ Center for Scientific Foundation Models, Summer School 24’ [07/2024]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParallel Training Techniques @ AI-4-Science Training Series [03/2024]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLLMs from Scratch @ LLM Tutorial Workshop [02/2024]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n📆 2023\n\n\n\n\n\n\nCreating Small(-ish) LLMs @ LLM Tutorial Workshop (1) [11/2023]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExascale Science on Aurora @ Intel oneAPI Workshop @ UIC [10/2023]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLLM Lunch Talk @ ALCF Hands On HPC Workshop [10/2023]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScaling LLMs for Science @ Data-Intensive Computing + AI/ML at Scale [08/2023]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMLMC: Machine Learning Monte Carlo @ Lattice 2023 [07/2023]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative Modeling and Efficient Sampling @ PASC23 [07/2023]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEfficient Sampling for LGT @ Deep Fridays @ U. Bologna [04/2023]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n📆 2022\n\n\n\n\n\n\nLarge Scale Training @ AI4Science on Supercomputers (ALCF) [11/2022]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHyperparameter Management @ ALCF SDL Workshop [10/2022]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical Learning @ ATPESC 2022 [08/2022]\n\n\n\n\n\n\n📕 accompanying notebook\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScientific Data Science: An Emerging Symbiosis @ ANL (05/2022)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMachine Learning in HEP @ UNC Greensboro [03/2022]\n\n\n\n\n\n\nMachine Learning in HEP, at UNC Greensboro, March 2022\n\n\n\n\n\n\n\n\n\n\n📆 2021\n\n\n\n\n\n\nAccelerated Sampling Methods for LGT, @ DWQ @ 25 [BNL] [12/2021]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTraining Topological Samplers for LGT @ ML4HEP, ECT* Trento [09/2021]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nl2hmc-qcd @ MIT Lattice Group Seminar [2021]\n\n\n\n\n\nl2hmc-qcd at the MIT Lattice Group Seminar, 2021\n\n\n\n\n\n\n\n\n\nDeep Learning HMC for Improved Gauge Generation @ ML in LQCD Workshop [2021]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n📆 2020\n\n\n\n\n\n\nMachine Learning for Lattice QCD @ U. Iowa [2020]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\n🍋 ezpz @ ALCF\n\n\n2024-09-11\n\n\n\n\n💅 How to Make Dope Slides\n\n\n2024-08-13\n\n\n\n\n🎰 Deterministic flash-attn\n\n\n2024-06-17\n\n\n\n\n📸 flash-attn on Sunspot\n\n\n2024-06-17\n\n\n\n\n🏎️ Megatron-DeepSpeed on Intel XPU\n\n\n2024-06-15\n\n\n\n\n🐛 mpi4py bug on Sunspot\n\n\n2024-05-25\n\n\n\n\n🎲 MCMC + Diffusion Sampling\n\n\n2024-04-15\n\n\n\n\n⏰ Starting Up Distributed Training on Aurora\n\n\n2024-03-21\n\n\n\n\n🚂 Loooooooong Sequence Lengths\n\n\n2024-02-12\n\n\n\n\nl2hmc Example: 2D U(1)\n\n\n2024-02-12\n\n\n\n\n🎢 l2hmc-qcd Example: 2D U(1)\n\n\n2023-12-14\n\n\n\n\n🔳 l2hmc-qcd Example: 4D SU(3)\n\n\n2023-12-06\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\n\n\n\n\n\n📊 GitHub Stats\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nEven More !!\n\n\n\nWakatime\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n📂 saforem2/\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n🎪 Events\n\nOrganizer for:\n\nSC24 Workshop: High Performance Python for Science at Scale (HPPSS), November 2024\nSC23 Workshop: High Performance Python for Science at Scale (HPPSS), November 2023\nMachine Learning and Quantum Computing for Earth Sciences at 17th U. S. National Congress on Computational Mechanics, July 2023\n\n\n\n\n👔 Employment\n\n\n\nTable 1: 📟 Experience\n\n\n\n\n\nPosition\n@\nStart\nEnd\n\n\n\n\nAssistant Computational Scientist\nALCF\n2022\n–\n\n\nPostdoc\nALCF\n2019\n2022\n\n\nGraduate Researcher\nANL\n2018\n2019\n\n\n\n\n\n\n\n\n🍎 School\n\n\n\nTable 2: 🎓 Education\n\n\n\n\n\nDegree\nIn\n@\nEnd\n\n\n\n\nPhD\nPhysics\nUniversity of Iowa\n2019\n\n\nB.Sc\nPhysics\nUIUC\n2015\n\n\nB.Sc\nMath\nUIUC\n2015\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n🧾 Last Update\n\n\n\n\n\nhighlight yellow\nhighlight pink\nhighlight green\nhighlight-blue\ncircle sketch highlight\nimport datetime\nfrom rich import print\nnow = datetime.datetime.now()\nday = now.strftime(\"%Y-%m-%d\")\ntime = now.strftime(\"%H:%M:%S\")\nprint(' '.join([\n    \"[#838383]Last Updated[/]:\",\n    f\"[#E599F7]{day}[/]\",\n    \"[#838383]@[/]\",\n    f\"[#00CCFF]{time}[/]\"\n]))\nLast Updated: 2024-09-13 @ 18:48:41"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSo far, for: {Lattice QCD, Quantum Mechanics, Biology (Protein Generation, Drug Discovery), and Climate Modeling / Weather Forecasting}↩︎\nMostly trying to get supercomputers to stop yelling at each other 🫠↩︎\nIf this sounds like something you’d be interested in doing, please feel free to reach out to me!↩︎\nAnd resulted in a patent !!↩︎"
  }
]