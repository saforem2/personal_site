[
  {
    "objectID": "posts/ai-for-physics/diffusion/index.html",
    "href": "posts/ai-for-physics/diffusion/index.html",
    "title": "🎲 MCMC + Diffusion Sampling",
    "section": "",
    "text": "2D U(1)\nfrom l2hmc.configs import dict_to_list_of_overrides\n\nseed = np.random.randint(0, 2**32)\nconsole.print(f\"seed = {seed}\")\n\noverrides = {\n    \"seed\": f\"{seed}\",\n    \"precision\": \"float32\",\n    \"init_wandb\": False,\n    \"init_aim\": False,\n    \"use_wandb\": False,\n    \"dynamics\": {\n        \"latvolume\": [32, 32],\n        \"nleapfrog\": 10,\n        \"nchains\": 16,\n        \"eps\": 0.05,\n    },\n    \"network\": {\n        \"use_batch_norm\": False,\n    },\n    'annealing_schedule': {\n        'beta_init': 6.0,\n        'beta_final': 6.0,\n    },\n\n}\nOVERRIDES = dict_to_list_of_overrides(overrides)\n\nseed = 1675333995\nfrom pathlib import Path\nfrom l2hmc.common import get_timestamp\nfrom enrich.console import get_theme, Console\nconsole = Console(theme=get_theme())\n\nOUTDIR = Path(\n    'l2hmc-diffusion-2dU1'\n).joinpath(get_timestamp(\"%Y-%m-%d\"))\nOUTDIR.mkdir(exist_ok=True, parents=True)\nconsole.print(f\"OUTDIR: {OUTDIR}\")\n\ndate = get_timestamp('%Y-%m-%d')\nPLOTS_DIR = OUTDIR.joinpath('plots')\nPLOTS_DIR.mkdir(exist_ok=True, parents=True)\nconsole.print(f\"Saving figures to: {PLOTS_DIR}\")\n\nOUTDIR: l2hmc-diffusion-2dU1/2023-09-21\n\n\n\nSaving figures to: l2hmc-diffusion-2dU1/2023-09-21/plots\n#os.environ['MASTER_PORT'] = '5436'\n\nexp = build_experiment(\n    overrides=[\n        *OVERRIDES,\n        'framework=pytorch',\n        'backend=DDP'\n    ]\n)\n\n[09/21/23 12:23:55][INFO][dist.py:226] - Caught MASTER_PORT:5561 from environment!\n[09/21/23 12:23:55][INFO][dist.py:338] - Global Rank: 0 / 0\n[09/21/23 12:23:58][INFO][experiment.py:251] - Creating outputs/2023-09-21-122358/pytorch/train\n[09/21/23 12:23:58][INFO][experiment.py:251] - Creating outputs/2023-09-21-122358/pytorch/eval\n[09/21/23 12:23:58][INFO][experiment.py:251] - Creating outputs/2023-09-21-122358/pytorch/hmc\n[09/21/23 12:23:58][INFO][dist.py:226] - Caught MASTER_PORT:5561 from environment!\n[09/21/23 12:23:58][INFO][dist.py:226] - Caught MASTER_PORT:5561 from environment!\n[09/21/23 12:24:06][INFO][trainer.py:441] - Looking for checkpoints in:\n /Users/samforeman/projects/saforem2/l2hmc-qcd/src/l2hmc/checkpoints/U1/2-32-32/nlf-10/xsplit-True/sepnets-True/merge-True/conv-8-16-32-64-128_5-3-3-3-2_2-2-2-2-2/net-16-16-16-16_dp-0.2_bn-False/pytorch\n[09/21/23 12:24:06][WARNING][trainer.py:437] - No checkpoints found to load from\n[09/21/23 12:24:06][WARNING][trainer.py:437] - Restoring global step from ckpt! self._gstep: 0\n[09/21/23 12:24:06][WARNING][trainer.py:437] - Using `torch.optim.Adam` optimizer\n[09/21/23 12:24:06][INFO][trainer.py:284] - num_params in model: 958628260\n[09/21/23 12:24:09][WARNING][trainer.py:250] - logging with freq 50 for wandb.watch\nstate = exp.trainer.dynamics.random_state(6.0)\nxdim = state.x.flatten().shape[0]\n\ndim = xdim\nlow_bound = (-np.pi) * np.ones(dim)\nhigh_bound = (np.pi) * np.ones(dim)\nsigma = 0.15\nretrains = 10\nsamples_per_retrain = 100\ndiffusion_prob = 0.1\nsns.set_context('notebook')\n\noutputs = {}\noutputs['hmc'] = exp.trainer.eval(\n    job_type='hmc',\n    beta=6.0,\n    nprint=100,\n    nchains=16,\n    eval_steps=1000\n)\n#hdset = exp.save_dataset(job_type='hmc', nchains=1)\n\n[09/21/23 12:24:21][WARNING][trainer.py:437] - Step size `eps` not specified for HMC! Using default: 0.1000 for generic HMC\n[09/21/23 12:24:21][WARNING][trainer.py:437] - x.shape (original): torch.Size([16, 2, 32, 32])\n[09/21/23 12:24:21][WARNING][trainer.py:437] - x[:nchains].shape: torch.Size([16, 2, 32, 32])\n[09/21/23 12:24:21][INFO][trainer.py:1058] - eps=0.1\nbeta=6.0\nnlog=10\ntable=&lt;rich.table.Table object at 0x2e1b98520&gt;\nnprint=100\neval_steps=1000\nnleapfrog=20\n\n\n\n\n\n\n\n\n\n[09/21/23 12:24:24][INFO][trainer.py:1188] - hstep=0 dt=0.024 beta=6.000 loss=3.410 dQsin=0.125 dQint=0.000 energy=1586.502 logprob=1586.502 logdet=0.000 acc=0.472 sumlogdet=0.000 acc_mask=0.500 plaqs=0.909 intQ=0.000 sinQ=0.051\n[09/21/23 12:24:27][INFO][trainer.py:1188] - hstep=100 dt=0.026 beta=6.000 loss=2.876 dQsin=0.163 dQint=0.000 energy=1555.800 logprob=1555.800 logdet=0.000 acc=0.593 sumlogdet=0.000 acc_mask=0.688 plaqs=0.912 intQ=-0.125 sinQ=-0.159\n[09/21/23 12:24:31][INFO][trainer.py:1188] - hstep=200 dt=0.025 beta=6.000 loss=4.678 dQsin=0.088 dQint=0.063 energy=1569.994 logprob=1569.994 logdet=0.000 acc=0.451 sumlogdet=0.000 acc_mask=0.250 plaqs=0.912 intQ=-0.187 sinQ=-0.149\n[09/21/23 12:24:34][INFO][trainer.py:1188] - hstep=300 dt=0.024 beta=6.000 loss=14.041 dQsin=0.094 dQint=0.000 energy=1554.118 logprob=1554.118 logdet=0.000 acc=0.438 sumlogdet=0.000 acc_mask=0.438 plaqs=0.914 intQ=-0.125 sinQ=-0.114\n[09/21/23 12:24:38][INFO][trainer.py:1188] - hstep=400 dt=0.024 beta=6.000 loss=-0.739 dQsin=0.199 dQint=0.000 energy=1566.516 logprob=1566.516 logdet=0.000 acc=0.509 sumlogdet=0.000 acc_mask=0.562 plaqs=0.912 intQ=-0.437 sinQ=-0.452\n[09/21/23 12:24:41][INFO][trainer.py:1188] - hstep=500 dt=0.045 beta=6.000 loss=1.545 dQsin=0.100 dQint=0.000 energy=1570.837 logprob=1570.837 logdet=0.000 acc=0.448 sumlogdet=0.000 acc_mask=0.562 plaqs=0.911 intQ=0.125 sinQ=0.189\n[09/21/23 12:24:45][INFO][trainer.py:1188] - hstep=600 dt=0.025 beta=6.000 loss=3.780 dQsin=0.094 dQint=0.000 energy=1568.012 logprob=1568.012 logdet=0.000 acc=0.463 sumlogdet=0.000 acc_mask=0.500 plaqs=0.913 intQ=0.438 sinQ=0.466\n[09/21/23 12:24:50][INFO][trainer.py:1188] - hstep=700 dt=0.023 beta=6.000 loss=-0.902 dQsin=0.113 dQint=0.000 energy=1563.778 logprob=1563.778 logdet=0.000 acc=0.475 sumlogdet=0.000 acc_mask=0.375 plaqs=0.913 intQ=0.688 sinQ=0.628\n[09/21/23 12:24:53][INFO][trainer.py:1188] - hstep=800 dt=0.024 beta=6.000 loss=11.416 dQsin=0.061 dQint=0.000 energy=1561.427 logprob=1561.427 logdet=0.000 acc=0.339 sumlogdet=0.000 acc_mask=0.438 plaqs=0.913 intQ=0.813 sinQ=0.755\n[09/21/23 12:24:57][INFO][trainer.py:1188] - hstep=900 dt=0.028 beta=6.000 loss=1.114 dQsin=0.127 dQint=0.000 energy=1564.465 logprob=1564.465 logdet=0.000 acc=0.699 sumlogdet=0.000 acc_mask=0.625 plaqs=0.913 intQ=0.938 sinQ=0.893\n# %matplotlib inline\nfrom l2hmc.common import plot_dataset\nsns.set_context('notebook')\nhdataset = outputs['hmc']['history'].get_dataset()\nplot_dataset(hdataset, outdir=PLOTS_DIR, job_type='HMC')\n\n[09/21/23 12:25:06][INFO][plot_helpers.py:1049] - Saving figure to: l2hmc-diffusion-2dU1/2023-09-21/plots/ridgeplots/svgs/energy_ridgeplot.svg\n[09/21/23 12:25:09][INFO][plot_helpers.py:1049] - Saving figure to: l2hmc-diffusion-2dU1/2023-09-21/plots/ridgeplots/svgs/logprob_ridgeplot.svg\n[09/21/23 12:25:11][INFO][plot_helpers.py:1049] - Saving figure to: l2hmc-diffusion-2dU1/2023-09-21/plots/ridgeplots/svgs/logdet_ridgeplot.svg\nimport torch\n\ninitial_states = []\nstate_init = exp.trainer.dynamics.random_state(6.0)\nx = state_init.x\nbeta = state_init.beta\n\nNSAMPLES = 1000\nfor idx in range(NSAMPLES + int(0.1 * NSAMPLES)):\n    if idx % 100 == 0:\n        console.print(f\"step: {idx}\")\n        \n    x, metrics = exp.trainer.hmc_step((x, beta))\n    if idx &gt; int((0.1 * NSAMPLES)):\n        initial_states.append(x)\n\ninitial_states = torch.stack(initial_states).squeeze()\ninitial_states_np = initial_states.detach().cpu().numpy()\n\nstep: 0\n\n\n\nstep: 100\n\n\n\nstep: 200\n\n\n\nstep: 300\n\n\n\nstep: 400\n\n\n\nstep: 500\n\n\n\nstep: 600\n\n\n\nstep: 700\n\n\n\nstep: 800\n\n\n\nstep: 900\n\n\n\nstep: 1000\ninitial_states_np.shape\n\n(999, 16, 2048)\nx_ = initial_states_np.reshape(-1, 16, 2, 32, 32)\ntmp_ = x_[:, 0, ...]\nconsole.print(f'{x_.shape}')\nconsole.print(f'{tmp_.shape}')\n\n(999, 16, 2, 32, 32)\n\n\n\n(999, 2, 32, 32)\nfrom l2hmc.common import savefig\n\n#x_ = initial_states_np[:100].reshape(-1, 2, 32, 32)\ntmp_ = x_[:, 0, ...]\nfig, ax = plt.subplots()\nsns.kdeplot(\n    x=tmp_[-100:, 0].flatten(),\n    y=tmp_[-100:, 1].flatten(),\n    # ax=ax,\n    cmap='viridis',\n    # ax=axes[0],\n    # cmap=\"Blues\",\n    shade=False,\n    # bw_adjust=0.5,\n    thresh=0\n)\nax.set_xlim((-4, 4))\nax.set_ylim((-4, 4))\nsavefig(\n    f'hmc_samples-{NSAMPLES}',\n    Path(PLOTS_DIR),\n    tstamp=True,\n)\n\nSaving hmc_samples-1000-2023-09-21-122840 to l2hmc-diffusion-2dU1/2023-09-21/plots\nclass Diffusion:\n    def __init__(\n            self,\n            noise_steps: int = 1000,\n            beta_start: float = 1e-4,\n            beta_end: float = 0.02,\n            nchannels: int = 2,\n            img_size: int = 256,\n            device: str = \"cuda\"\n    ):\n        self.noise_steps = noise_steps\n        self.beta_start = beta_start\n        self.beta_end = beta_end\n        self.img_size = img_size\n        self.device = device\n        self.nchannels = nchannels\n\n        self.beta = self.prepare_noise_schedule().to(device)\n        self.alpha = 1. - self.beta\n        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n\n    def prepare_noise_schedule(self):\n        return torch.linspace(self.beta_start, self.beta_end, self.noise_steps)\n\n    def noise_images(self, x, t):\n        sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:, None, None, None]\n        sqrt_one_minus_alpha_hat = torch.sqrt(\n            1 - self.alpha_hat[t]\n        )[:, None, None, None]\n        eps = torch.randn_like(x)\n        return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * eps, eps\n\n    def sample_timesteps(self, n):\n        return torch.randint(low=1, high=self.noise_steps, size=(n,))\n\n    def sample(self, model, n):\n        # console.print(f\"Sampling {n} new images....\")\n        model.eval()\n        with torch.no_grad():\n            x = torch.randn(\n                (n, self.nchannels, self.img_size, self.img_size)\n            ).to(self.device)\n            sample_bar = tqdm(\n                reversed(range(1, self.noise_steps)),\n                position=0,\n                total=self.noise_steps - 1,\n                dynamic_ncols=True,\n            )\n            for i in sample_bar:\n                t = (torch.ones(n) * i).long().to(self.device)\n                predicted_noise = model(x, t)\n                alpha = self.alpha[t][:, None, None, None]\n                alpha_hat = self.alpha_hat[t][:, None, None, None]\n                beta = self.beta[t][:, None, None, None]\n                if i &gt; 1:\n                    noise = torch.randn_like(x)\n                else:\n                    noise = torch.zeros_like(x)\n                x = (\n                    (1 / torch.sqrt(alpha))\n                    * (\n                        x \n                        - ((1 - alpha) / (torch.sqrt(1 - alpha_hat)))\n                        * predicted_noise\n                    ) \n                    + (torch.sqrt(beta) * noise)\n                )\n        model.train()\n        x = (x + np.pi) % (2 * np.pi) - np.pi\n        return x\ninitial_states.shape\n\ntorch.Size([999, 16, 2048])\nTrain Diffusion Model\nimport torchvision\nimport os\nimport random\nfrom pathlib import Path\nimport torch\nimport torchvision\nimport torchvision.transforms as T\nimport numpy as np\nfrom PIL import Image\n#from fastdownload import FastDownload\nfrom torch.utils.data import DataLoader\n\ndef save_images(images, path, **kwargs):\n    grid = torchvision.utils.make_grid(images, **kwargs)\n    ndarr = grid.permute(1, 2, 0).to('cpu').numpy()\n    im = Image.fromarray(ndarr)\n    im.save(path)\nBuild Diffusion Model with UNet Architecure\nfrom torch import nn\nfrom torch import optim\nimport torch.nn.functional as F\nfrom tqdm.auto import tqdm\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import TensorDataset\n\nfrom l2hmc.common import savefig\nfrom l2hmc.diffusion.modules import NoiseScheduler, UNet\nfrom l2hmc.diffusion import ddpm\n\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nconfig = {\n    'channels_in': 2,\n    'channels_out': 2,\n    'train_batch_size': 5,\n    'learning_rate': 0.001,\n    'num_epochs': 1,\n    'noise_steps': 100,\n    'beta': 6.0,\n    'img_size': 32,\n    'retrains': 10,\n    'samples_per_retrain': 500,\n    'diffusion_prob': 0.1,\n}\n\nmodel = UNet(c_in=2, c_out=2)\n\ndataset = TensorDataset(initial_states.reshape(-1, 2, 32, 32))\ndataloader = DataLoader(\n    dataset,\n    batch_size=config[\"train_batch_size\"],\n    shuffle=False,\n    drop_last=True\n)\n\n\noptimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'])\nmse = nn.MSELoss()\ndiffusion = Diffusion(\n    noise_steps=100,\n    img_size=32,\n    device=DEVICE,\n    nchannels=2,\n)\n#logger = SummaryWriter(os.path.join(\"runs\", args.run_name))\nl = len(dataloader)\n\nrun_name = 'diffusion2dU1'\nPerform initial training on HMC samples\nfrom torch import optim\ndevice = 'cpu'\n#dataloader = get_data(args)\n#model = UNet().to(device)\n\nsampled_images_history = []\n\nfor epoch in range(config['num_epochs']):\n    console.print(f\"Starting epoch {epoch}:\")\n    pbar = tqdm(dataloader)\n    for i, images in enumerate(pbar):\n        if isinstance(images, (tuple, list)) and len(images) == 1:\n            images = images[0]\n        t = diffusion.sample_timesteps(images.shape[0]).to(device)\n        x_t, noise = diffusion.noise_images(images, t)\n        predicted_noise = model(x_t, t)\n        loss = mse(noise, predicted_noise)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        pbar.set_postfix({'epoch': epoch, 'batch': i, 'MSE': loss.item()})\n    console.print(f'epoch: {epoch}, loss: {loss.item()}')\n    sampled_images = diffusion.sample(model, n=images.shape[0])\n    sampled_images_history.append(sampled_images)\n    sns.set_context('notebook')\n    #tmp = initial_states.reshape(-1, 2, 32, 32)\n    fig, ax = plt.subplots(ncols=2)\n    _ = ax[0].imshow(sampled_images[0, 0, :, :])\n    _ = ax[1].imshow(sampled_images[0, 1, :, :])\n    _ = ax[0].set_xticklabels([])\n    _ = ax[1].set_xticklabels([])\n    _ = ax[0].set_yticklabels([])\n    _ = ax[1].set_yticklabels([])\n    _ = ax[0].set_title(r\"$U_{0}$\", loc='center')\n    _ = ax[1].set_title(r\"$U_{1}$\", loc='center')\n    _ = fig.suptitle('Diffusion Samples', y=0.8)\n    plt.show()\n    savefig(fname=f'sampled_image_epoch{epoch}', outdir=PLOTS_DIR, tstamp=True)\n    MODEL_FILE = OUTDIR.joinpath(\"models\", f\"unet-diffusion-epoch{epoch}.pt\")\n    MODEL_FILE.parent.mkdir(exist_ok=True, parents=True)\n    console.print(f\"Saving model checkpoint to: {MODEL_FILE}\")\n    torch.save(model.state_dict(), MODEL_FILE)\n\nStarting epoch 0:\n\n\n\n{\"model_id\":\"19b415c346b24bef8b60336d7f7bc355\",\"version_major\":2,\"version_minor\":0}\n\n\nepoch: 0, loss: 0.6023472547531128\n\n\n\n{\"model_id\":\"eea24504754f4cb9ab4d9925a6225c10\",\"version_major\":2,\"version_minor\":0}\n\n\n\n\n\n\n\n\n\nSaving sampled_image_epoch0-2023-09-21-124506 to l2hmc-diffusion-2dU1/2023-09-21/plots\n\n\nSaving model checkpoint to: l2hmc-diffusion-2dU1/2023-09-21/models/unet-diffusion-epoch0.pt\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\nsns.set_context('notebook')\ntmp = initial_states.reshape(-1, 2, 32, 32)\nfig, ax = plt.subplots(ncols=2)\n_ = ax[0].imshow(tmp[0, 0, :, :])\n_ = ax[1].imshow(tmp[0, 1, :, :])\n_ = ax[0].set_title(r\"$U_{0}$\", loc='center')\n_ = ax[0].set_xticklabels([])\n_ = ax[1].set_xticklabels([])\n_ = ax[0].set_yticklabels([])\n_ = ax[1].set_yticklabels([])\n_ = ax[1].set_title(r\"$U_{1}$\", loc='center')\n_ = fig.suptitle('HMC Samples', y=0.8)\nsampled_images_history_ = torch.stack(sampled_images_history)\nsampled_images_history_.shape\n\ntorch.Size([1, 5, 2, 32, 32])\nsns.set_context('notebook')\nfig, ax = plt.subplots(ncols=2)\n_ = ax[0].imshow(sampled_images_history_[0][0][0])\n_ = ax[1].imshow(sampled_images_history_[0][0][1])\n_ = ax[0].set_xticklabels([])\n_ = ax[1].set_xticklabels([])\n_ = ax[0].set_yticklabels([])\n_ = ax[1].set_yticklabels([])\n_ = ax[0].set_title(r\"$U_{0}$\", loc='center')\n_ = ax[1].set_title(r\"$U_{1}$\", loc='center')\n_ = fig.suptitle('Diffusion Samples', y=0.85)\nfor idx in range(sampled_images_history_.shape[0]):\n    q = exp.trainer.lattice.charges(x=sampled_images_history_[idx])\n    console.print(f'{idx}: {q}')\n\n0: Charges(intQ=tensor([ 5.0000e+00, -4.0000e+00, -6.0000e+00, -4.5535e-07,  1.0000e+00]), sinQ=tensor([ 1.6426, -1.7244, -4.4651,  0.5680,  0.7046]))\nHMC Sampling with Diffusion\n#for retrain_iter in range(config['retrains']):\nstate = exp.trainer.dynamics.random_state(config['beta'])\nx = state.x\n\nhistories = {}\nsamples = []\nhmc_samples = []\ndiffusion_samples = []\n\nglobal_step = 0\nwatcher = {}\nupdate_types = []\ncombined_samples = {}\nglobal_step\n\n0\nfor retrain_iter in range(2):\n    console.print(f'retrain_iter: {retrain_iter}')\n    ndiff_acc = 0\n    ndiff_proposed = 0\n    histories[retrain_iter] = {\n        'diffusion': [],\n        'hmc': [],\n    }\n    #for idx in range(config['samples_per_retrain']):\n    sbar = tqdm(range(10))\n    for idx in sbar:\n        t0_ = time.perf_counter()\n        if idx % 100 == 0:\n            console.print(f'sample idx: {idx}')\n        rand = np.random.uniform()\n        if (retrain_iter &gt;= 1) and rand &lt; diffusion_prob:\n            console.print(f'rand: {rand} &lt; {diffusion_prob}')\n            # Sample from diffusion model\n            x_ = diffusion.sample(model, n=x.shape[0])\n            ll_ = exp.trainer.dynamics.potential_energy(x_, config['beta'])\n            ll = exp.trainer.dynamics.potential_energy(x, config['beta'])\n            ratio = ll_ / ll\n            a = torch.min(torch.ones_like(ratio), ratio)\n            u = torch.rand(a.shape)\n            #u = np.random.uniform()\n            #for jdx in range(u.shape[0]):\n            #    if u[jdx] &lt; a[jdx]:\n            #        samples.append(x_[jdx])\n            #        diffusion_samples.append(x_[jdx])\n            #x = torch.where((u &lt; a), x_, x.reshape_as(x_)).reshape_as(x)\n            x = torch.where((u &lt; a)[:, None, None, None], x_, x.reshape_as(x_))\n            samples.append(x)\n            diffusion_samples.append(x)\n            combined_samples[global_step] = x\n            watcher[global_step] = 'diffusion'\n            #diffusion_samples.extend(x)\n            #samples.extend(x)\n            #ndiff_acc += \n            #if u &lt; a:\n            #    console.print('Accepted diffusion sample!')\n            #    console.print(f'{ndiff_acc} / {ndiff_proposed}')\n            #    ndiff_acc += 1\n            #    x = x_\n            #    diffusion_samples.append(x)\n            #    samples.append(x)\n        else:\n            # Oherwise, HMC\n            x, metrics = exp.trainer.hmc_step((x, config['beta']))\n            hmc_samples.append(x)\n            samples.append(x)\n            combined_samples[global_step] = x\n            watcher[global_step] = 'HMC'\n        smetrics = {\n            'idx': idx,\n            'global_step': global_step,\n            'dt': time.perf_counter() - t0_,\n        }\n        global_step += 1\n        #smetrics |= {\n        #    f'{k}': {torch.tensor(v).mean().item()} for k, v in metrics.items()\n        #}\n        sbar.set_postfix(smetrics)\n    # Train loop\n    dataset = TensorDataset(\n        torch.stack(hmc_samples).reshape(-1, 2, 32, 32)\n    )\n    dataloader = DataLoader(\n        dataset,\n        shuffle=False,\n        drop_last=True,\n        batch_size=config[\"train_batch_size\"],\n    )\n    pbar = tqdm(dataloader)\n    for i, batch in enumerate(pbar):\n        if i == 0:\n            console.print('Retraining...')\n        if isinstance(batch, (tuple, list)) and len(batch) == 1:\n            batch, = batch\n        batch = batch.reshape(-1, 2, 32, 32)\n        t0 = time.time()\n        t = diffusion.sample_timesteps(batch.shape[0]).to(device)\n        x_t, noise = diffusion.noise_images(batch, t)\n        predicted_noise = model(x_t, t)\n        loss = mse(noise, predicted_noise)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        t1 = time.time()\n        pbar.set_postfix(\n            {\n                'global_step': global_step,\n                'retrain_iter': retrain_iter,\n                'batch': i,\n                'dt': t1 - t0,\n                'MSE': loss.item()\n            }\n        )\n\nretrain_iter: 0\n\n\n\n{\"model_id\":\"17132d7ca8624fa387ee9467e4f1fa4d\",\"version_major\":2,\"version_minor\":0}\n\n\nsample idx: 0\n\n\n\n{\"model_id\":\"0ed1080fdebd4f7b9aae80db0d36b96b\",\"version_major\":2,\"version_minor\":0}\n\n\nRetraining...\n\n\n\nretrain_iter: 1\n\n\n\n{\"model_id\":\"d0346019e21b4d2a9b624dc59e84015b\",\"version_major\":2,\"version_minor\":0}\n\n\nsample idx: 0\n\n\n\nrand: 0.05506106760134255 &lt; 0.1\n\n\n\n{\"model_id\":\"c02b09d53ada46a194a47921f0ab3cba\",\"version_major\":2,\"version_minor\":0}\n\n\nrand: 0.07860283644524213 &lt; 0.1\n\n\n\n{\"model_id\":\"184df3f1c9714ece9756866b2617ed02\",\"version_major\":2,\"version_minor\":0}\n\n\n{\"model_id\":\"eaa0d84229c04618b7a2bffe2a4b1739\",\"version_major\":2,\"version_minor\":0}\n\n\nRetraining...\nconsole.print('\\n'.join([f\"{i.shape}\" for i in samples[:100]]))\n\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2, 32, 32])\ntorch.Size([16, 2048])\ntorch.Size([16, 2, 32, 32])\nsamples_ = torch.stack([i.reshape(-1, 2, 32, 32) for i in samples])\nsamples_.shape\n\ntorch.Size([30, 16, 2, 32, 32])\nlen(hmc_samples)\n\n28\nlen(diffusion_samples)\n\n2\nhmc_samples_ = torch.stack([i.reshape(-1, 2, 32, 32) for i in hmc_samples])\ndiffusion_samples_ = torch.stack(\n    [i.reshape(-1, 2, 32, 32) for i in diffusion_samples]\n)\nhmc_samples_.shape\n\ntorch.Size([28, 16, 2, 32, 32])\ndiffusion_samples_.shape\n\ntorch.Size([2, 16, 2, 32, 32])\nsamples_.shape\n\ntorch.Size([30, 16, 2, 32, 32])\ndef calc_plaqs(x):\n    return torch.stack([\n        exp.trainer.lattice.plaqs(\n            x[:, idx]\n        ) for idx in range(x.shape[1])\n    ], -1)\n\ndef calc_intQ(x):\n    return torch.stack([\n        exp.trainer.lattice.int_charges(\n            x[:, idx]\n        ) for idx in range(x.shape[1])\n    ], -1)\n    \ndef calc_sinQ(x):\n    return torch.stack([\n        exp.trainer.lattice.sin_charges(\n            x[:, idx]\n        ) for idx in range(x.shape[1])\n    ], -1)\nsamples_init_ = initial_states.reshape(-1, initial_states.shape[1], 2, 32, 32)\nsamples_init_.shape\n\ntorch.Size([999, 16, 2, 32, 32])\nmetrics_init_ = {\n    'plaqs': calc_plaqs(samples_init_),\n    'intQ': calc_intQ(samples_init_),\n    'sinQ': calc_sinQ(samples_init_)\n}\n    \nmetrics_ = {\n    'plaqs': calc_plaqs(samples_),\n    'intQ': calc_intQ(samples_),\n    'sinQ': calc_sinQ(samples_)\n}\n\nmetrics_hmc_ = {\n    'plaqs': calc_plaqs(hmc_samples_),\n    'intQ': calc_intQ(hmc_samples_),\n    'sinQ': calc_sinQ(hmc_samples_)\n}\n\nmetrics_diffusion_ = {\n    'plaqs': calc_plaqs(diffusion_samples_),\n    'intQ': calc_intQ(diffusion_samples_),\n    'sinQ': calc_sinQ(diffusion_samples_)\n}\nmetrics_['plaqs'].shape\n\ntorch.Size([30, 16])\nconsole.print('\\n'.join([f\"{k}: {v}\" for k, v in watcher.items()]))\n\n0: HMC\n1: HMC\n2: HMC\n3: HMC\n4: HMC\n5: HMC\n6: HMC\n7: HMC\n8: HMC\n9: HMC\n10: HMC\n11: HMC\n12: HMC\n13: HMC\n14: HMC\n15: HMC\n16: HMC\n17: HMC\n18: HMC\n19: HMC\n20: HMC\n21: HMC\n22: HMC\n23: HMC\n24: HMC\n25: HMC\n26: HMC\n27: diffusion\n28: HMC\n29: diffusion\nfig, ax = plt.subplots()\n\n_ = ax.plot(metrics_['plaqs'][:, 0], label='Combined')\n_ = ax.plot(metrics_hmc_['plaqs'][:, 0], label='HMC')\n_ = ax.plot(metrics_diffusion_['plaqs'][:, 0], label='Diffusion')\n#_ = ax.plot(metrics_hmc1['plaqs'], label='HMC 1')\n#_ = ax.plot(metrics_diff_['plaqs'], label='Diffusion')\n_ = ax.legend(loc='upper left', bbox_to_anchor=(1.05, 1.00))\nfig, ax = plt.subplots(ncols=3, figsize=(14, 4))\nfor idx, (key, val) in enumerate(metrics_init_.items()):\n    _ = ax[idx].plot(val[:, 0], label='HMC (Initial Samples)')\n    _ = ax[idx].set_ylabel(key, loc='center')\n    _ = ax[idx].set_xlabel('Draw', loc='center')\n    #_ = ax[idx].legend(loc='best', frameon=True, edgecolor=\"#838383\")\n\n_ = fig.suptitle(f\"Initial HMC States\", y=0.92)\nfig, ax = plt.subplots(ncols=3, figsize=(14, 4))\nfor idx, (key, val) in enumerate(metrics_.items()):\n    _ = ax[idx].plot(val[:, 0], label='Combined')\n    _ = ax[idx].set_ylabel(key, loc='center')\n    _ = ax[idx].set_xlabel('Draw', loc='center')\n    #_ = ax[idx].legend(loc='best', frameon=True, edgecolor=\"#838383\")\n\n_ = fig.suptitle(f\"Combined Samples\", y=0.92)\nfig, ax = plt.subplots(ncols=3, figsize=(14, 4))\nfor idx, (key, val) in enumerate(metrics_hmc_.items()):\n    _ = ax[idx].plot(val[:, 0], label='HMC')\n    _ = ax[idx].set_ylabel(key, loc='center')\n    _ = ax[idx].set_xlabel('Draw', loc='center')\n_ = fig.suptitle(f\"Generated HMC States\", y=0.92)\nfig, ax = plt.subplots(ncols=3, figsize=(14, 4))\nfor idx, (key, val) in enumerate(metrics_diffusion_.items()):\n    _ = ax[idx].plot(val[:, 0], label='Diffusion')\n    _ = ax[idx].set_ylabel(key, loc='center')\n    _ = ax[idx].set_xlabel('Draw', loc='center')\n_ = fig.suptitle(f\"Generated Diffusion States\", y=0.92)\nfrom l2hmc.lattice.u1.pytorch.lattice import plaq_exact\nplaq_exact(torch.tensor(6.0))\n\ntensor(0.9124)\nfig, ax = plt.subplots()\n#_ = plt.hist(metrics_['intQ'].flatten(), color='C0', alpha=0.6, label='Combined', edgecolor='none')\n_ = ax.hist(\n    metrics_diffusion_['intQ'].flatten(),\n    color='C0',\n    alpha=0.6,\n    edgecolor='none',\n    label='Diffusion',\n    density=True,\n)\n_ = ax.hist(\n    metrics_hmc_['intQ'].flatten(),\n    color='C1',\n    alpha=0.6,\n    edgecolor='none',\n    label='HMC',\n    density=True,\n)\n_ = ax.legend(loc='best', frameon=True, edgecolor='#666666')\n_ = ax.set_xlabel(r\"$Q$\", loc='center')\n_ = ax.set_title('Topological Charge ($Q$) Distribution', loc='center')\nfig, ax = plt.subplots()\n_ = plt.plot(metrics_['plaqs'][:, 0], color='C0', label='Diffusion')\n_ = plt.plot(metrics_hmc_['plaqs'][:, 0], color='C1', label='HMC')\n_ = ax.legend(loc='best', frameon=True, edgecolor='#666666', ncols=2)\n_ = ax.set_ylabel(r\"$\\left\\langle U_{\\mu\\nu}\\right\\rangle $\", loc='center')\n_ = ax.set_xlabel(f\"Draw\", loc='center')\nwloops = {\n    'hmc': [\n        exp.trainer.lattice.wilson_loops(i) for i in hmc_samples_\n    ],\n    'diffusion': [\n        exp.trainer.lattice.wilson_loops(i) for i in diffusion_samples_\n    ],\n}\n\nplaqs = {\n    'hmc': [\n        exp.trainer.lattice.plaqs(i) for i in hmc_samples_\n    ],\n    'diffusion': [\n        exp.trainer.lattice.plaqs(i) for i in diffusion_samples_\n    ],\n}\nwlhmc = torch.stack(wloops['hmc']).squeeze()\nwldiff = torch.stack(wloops['diffusion']).squeeze()\nwlhmc.shape\n\ntorch.Size([28, 16, 32, 32])\n_ = plt.tight_layout()\nfor idx in range(2):\n    fig, ax = plt.subplots(ncols=2)\n    _ = ax[0].imshow(wlhmc[idx, 0])\n    _ = ax[0].set_title(\"HMC\", loc='center')\n    _ = ax[1].imshow(wldiff[idx, 0])\n    _ = ax[1].set_title(\"Diffusion\", loc='center')\n    _ = fig.suptitle(r\"$U_{\\mu\\nu}$\", y=0.8)\n    for ax_ in ax:\n        _ = ax_.set_xticklabels([])\n        _ = ax_.set_yticklabels([])\n\n&lt;Figure size 640x480 with 0 Axes&gt;\nqhmc = metrics_hmc_['intQ']\nqdiff = metrics_diffusion_['intQ']\nqhmc.shape\n\ntorch.Size([28, 16])\nphmc = torch.stack(plaqs['hmc']).squeeze()\npdiff = torch.stack(plaqs['diffusion']).squeeze()\nphmc.shape\n\ntorch.Size([28, 16])\npdiff.shape\n\ntorch.Size([2, 16])\nfig, ax = plt.subplots()\n\n_ = ax.hist(\n    metrics_['plaqs'].flatten(),\n    color='C1',\n    histtype='step',\n    stacked=True,\n    density=True,\n    label='HMC',\n    linewidth=1.5\n)\n_ = ax.hist(\n    metrics_diffusion_['plaqs'].flatten(),\n    color='C0',\n    histtype='step',\n    stacked=True,\n    density=True,\n    label='Diffusion',\n    linewidth=1.5\n)\n_ = ax.hist(\n    metrics_hmc_['plaqs'].flatten(),\n    color='C2',\n    histtype='step',\n    stacked=True,\n    density=True,\n    label='Combined',\n    linewidth=1.5\n)\n_ = ax.set_xlabel(r\"$U_{\\mu\\nu}$\", loc='center')\n_ = ax.legend(\n    loc='upper left',\n    frameon=True,\n    #ncols=2,\n    bbox_to_anchor=(0.55, 1.00),\n    edgecolor=\"#838383\",\n)\n_ = ax.set_title('Plaquette Distribution', loc='center')\nfig, ax = plt.subplots()\n\n_ = ax.hist(\n    metrics_['intQ'].flatten(),\n    color='C1',\n    histtype='step',\n    stacked=True,\n    density=True,\n    label='HMC',\n    linewidth=1.5\n)\n_ = ax.hist(\n    metrics_diffusion_['intQ'].flatten(),\n    color='C0',\n    histtype='step',\n    stacked=True,\n    density=True,\n    label='Diffusion',\n    linewidth=1.5\n)\n_ = ax.hist(\n    metrics_hmc_['intQ'].flatten(),\n    color='C2',\n    histtype='step',\n    stacked=True,\n    density=True,\n    label='Combined',\n    linewidth=1.5\n)\n_ = ax.set_xlabel('$Q_{\\mathbb{Z}}$', loc='center')\n_ = ax.legend(\n    loc='upper left',\n    frameon=True,\n    #ncols=2,\n    bbox_to_anchor=(0.55, 1.00),\n    edgecolor=\"#838383\",\n)\n_ = ax.set_title('Charge Distribution', loc='center')\nglobal_step = 0\nframes = []\nlosses = []\nprint(\"Training model...\")\nfor epoch in range(config[\"num_epochs\"]):\n    model.train()\n    progress_bar = tqdm(total=len(dataloader))\n    progress_bar.set_description(f\"Epoch {epoch}\")\n    for step, batch in enumerate(dataloader):\n        t = diffusion.sample_timesteps(images.shape[0]).to(device)\n\n        noise = torch.randn(batch.shape)\n        timesteps = torch.randint(\n            0, noise_scheduler.num_timesteps, (batch.shape[0],)\n        ).long()\n\n        #noisy = noise_scheduler.add_noise(batch, noise, timesteps)\n        noisy = noise_scheduler.noise_images(batch, timesteps)\n        noise_pred = model(noisy, timesteps)\n        loss = F.mse_loss(noise_pred, noise)\n        loss.backward(loss)\n\n        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        optimizer.zero_grad()\n\n        progress_bar.update(1)\n        logs = {\"loss\": loss.detach().item(), \"step\": global_step}\n        losses.append(loss.detach().item())\n        progress_bar.set_postfix(**logs)\n        global_step += 1\n    progress_bar.close()\n\n    if epoch % config[\"save_images_step\"] == 0 or epoch == config[\"num_epochs\"] - 1:\n        # generate data with the model to later visualize the learning process\n        model.eval()\n        sample = torch.randn(config[\"eval_batch_size\"], 2)\n        timesteps = list(range(len(noise_scheduler)))[::-1]\n        for i, t in enumerate(tqdm(timesteps)):\n            t = torch.from_numpy(np.repeat(t, config[\"eval_batch_size\"])).long()\n            with torch.no_grad():\n                residual = model(sample, t)\n            sample = noise_scheduler.step(residual, t[0], sample)\n        frames.append(sample.numpy())\ndataset[6]\nlen(dataloader)\neval_batch_size = 10\nnum_timesteps = 50\nplot_step = 5\nnoise_scheduler = ddpm.NoiseScheduler(num_timesteps=num_timesteps)\nsample = torch.randn(eval_batch_size, 2)\ntimesteps = list(range(num_timesteps))[::-1]\nsamples = []\nsteps = []\n\nretrains = 10\ndiffusion_prob = 0.3\nsamples_per_retrain = 100\neval_batch_size = 10\nt = torch.from_numpy(np.repeat(timesteps[0], eval_batch_size)).long()\nwith torch.no_grad():\n    residual = model(sample, t)\nsample_ = noise_scheduler.step(residual, t[0], sample)\nsample.shape\nresidual.shape\nsample_.shape\ndiffusion_samples = []\nhmc_samples = []\nbeta = 1.\nfor retrain_iter in range(retrains):\n    console.print(f'retrain_iter: {retrain_iter}')\n    ndiff_acc = 0\n    ndiff_proposed = 0\n    for idx in range(samples_per_retrain):\n        console.print(f'sample idx: {idx}')\n        rand = np.random.uniform()\n        if rand &lt; diffusion_prob:\n            ndiff_proposed += 1\n            rand_pick = randrange(len(dataloader))\n            #theta_prime = dataset[rand_pick]\n            t = torch.from_numpy(np.repeat(t, eval_batch_size)).long()\n            with torch.no_grad():\n                residual = model(sample, t)\n            sample_ = noise_scheduler.step(residual, t[0], sample)\n            ratio = (\n                log_likelihood_2dU1(sample_, 2)\n                / log_likelihood_2dU1(sample, 2)\n            )\n            a = min(1, ratio)\n            u = np.random.uniform()\n            if u &lt; a:\n                ndiff_acc += 1\n                sample = sample_\n                diffusion_samples.append(sample)\n        else:\n            sample_, metrics = exp.trainer.hmc_step((sample_, beta))\n            hmc_samples.append(sample)\nfor i, t in enumerate(tqdm(timesteps)):\n    t = torch.from_numpy(np.repeat(t, eval_batch_size)).long()\n    with torch.no_grad():\n        residual = model(sample, t)\n    sample = noise_scheduler.step(residual, t[0], sample)\n    if (i + 1) % plot_step == 0:\n        samples.append(sample.numpy())\n        steps.append(i + 1)\nAlternate\ndiffusion_ = DiffusionAlt(img_size=64, device='cpu')\nunet\nimage = torch.rand(1, 2, 64, 64)\nt = diffusion_.sample_timesteps(image.shape[0]).to('cpu')\nunet(image, t)\ndiffusion_.sample(",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "🪧 Posts",
      "⚛️ AI for Physics",
      "🎲 MCMC + Diffusion Sampling"
    ]
  },
  {
    "objectID": "posts/ai-for-physics/diffusion/index.html#denoising-diffusion-probabilistic-models",
    "href": "posts/ai-for-physics/diffusion/index.html#denoising-diffusion-probabilistic-models",
    "title": "🎲 MCMC + Diffusion Sampling",
    "section": "Denoising Diffusion Probabilistic Models",
    "text": "Denoising Diffusion Probabilistic Models",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "🪧 Posts",
      "⚛️ AI for Physics",
      "🎲 MCMC + Diffusion Sampling"
    ]
  },
  {
    "objectID": "posts/ai-for-physics/diffusion/index.html#imports--setup",
    "href": "posts/ai-for-physics/diffusion/index.html#imports--setup",
    "title": "🎲 MCMC + Diffusion Sampling",
    "section": "Imports / Setup",
    "text": "Imports / Setup\n\nfrom __future__ import absolute_import, print_function, annotations, division\nfrom dataclasses import dataclass\n\nimport sys\nimport os\nimport math\nimport numpy as np\nimport scipy\nimport time\nfrom random import randrange\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n\nfrom ezpz.dist import setup_torch\n\nport = np.random.randint(5000, 6000)\nprint(f\"Using port: {port}\")\n\nRANK = setup_torch(\n    backend=\"DDP\",\n    port=f\"{port}\"\n)\n\n\n    Using port: 5561\n\n\nUsing DDP for distributed training\n\n\n\nGlobal Rank: 0 / 0\n\n\n:::\n\n%matplotlib inline\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n\nfrom l2hmc.main import build_experiment\nfrom l2hmc.utils.rich import get_console\nfrom l2hmc.utils.plot_helpers import set_plot_style\n\nimport opinionated\nfrom l2hmc.diffusion.diffusion import PureMH, MH_Diffusion\nfrom l2hmc.utils.plot_helpers import set_plot_style\n\nfrom pandas.io.formats import style\nimport scipy\nimport time\nfrom random import randrange\nfrom l2hmc.diffusion.diffusion import PureMH, MH_Diffusion\n\nset_plot_style()\nconsole = get_console()\nprint(console.is_jupyter)\nif console.is_jupyter:\n    console.is_jupyter = False\nprint(console.is_jupyter)\n\nUsing device: cpu\n\n\n\nFailed to download font: Source Sans Pro, skipping! Failed to download font: Titillium WebRoboto Condensed, skipping!\n\n\nTrue\n\n\n\nFalse\n\n\n\nplt.style.use(opinionated.STYLES['opinionated_min'])\nsns.set_context('notebook')",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "🪧 Posts",
      "⚛️ AI for Physics",
      "🎲 MCMC + Diffusion Sampling"
    ]
  },
  {
    "objectID": "posts/ai-for-physics/diffusion/index.html#2d-u1",
    "href": "posts/ai-for-physics/diffusion/index.html#2d-u1",
    "title": "🎲 MCMC + Diffusion Sampling",
    "section": "2D U(1)",
    "text": "2D U(1)",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "🪧 Posts",
      "⚛️ AI for Physics",
      "🎲 MCMC + Diffusion Sampling"
    ]
  },
  {
    "objectID": "posts/ai-for-physics/diffusion/index.html#train-diffusion-model",
    "href": "posts/ai-for-physics/diffusion/index.html#train-diffusion-model",
    "title": "🎲 MCMC + Diffusion Sampling",
    "section": "Train Diffusion Model",
    "text": "Train Diffusion Model",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "🪧 Posts",
      "⚛️ AI for Physics",
      "🎲 MCMC + Diffusion Sampling"
    ]
  },
  {
    "objectID": "posts/ai-for-physics/diffusion/index.html#build-diffusion-model-with-unet-architecure",
    "href": "posts/ai-for-physics/diffusion/index.html#build-diffusion-model-with-unet-architecure",
    "title": "🎲 MCMC + Diffusion Sampling",
    "section": "Build Diffusion Model with UNet Architecure",
    "text": "Build Diffusion Model with UNet Architecure",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "🪧 Posts",
      "⚛️ AI for Physics",
      "🎲 MCMC + Diffusion Sampling"
    ]
  },
  {
    "objectID": "posts/ai-for-physics/diffusion/index.html#hmc-sampling-with-diffusion",
    "href": "posts/ai-for-physics/diffusion/index.html#hmc-sampling-with-diffusion",
    "title": "🎲 MCMC + Diffusion Sampling",
    "section": "HMC Sampling with Diffusion",
    "text": "HMC Sampling with Diffusion",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "🪧 Posts",
      "⚛️ AI for Physics",
      "🎲 MCMC + Diffusion Sampling"
    ]
  },
  {
    "objectID": "posts/ai-for-physics/diffusion/index.html#alternate",
    "href": "posts/ai-for-physics/diffusion/index.html#alternate",
    "title": "🎲 MCMC + Diffusion Sampling",
    "section": "Alternate",
    "text": "Alternate",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "🪧 Posts",
      "⚛️ AI for Physics",
      "🎲 MCMC + Diffusion Sampling"
    ]
  },
  {
    "objectID": "talks/llms-on-polaris/index.html#transformer-architecture",
    "href": "talks/llms-on-polaris/index.html#transformer-architecture",
    "title": "LLMs on Polaris",
    "section": "Transformer Architecture",
    "text": "Transformer Architecture\n\n\nFigure 10: Vaswani et al. (2017)",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "🦜 Talks",
      "Polaris Overview + LLMs"
    ]
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "",
    "section": "🧑🏻‍💻 About Me",
    "text": "🧑🏻‍💻 About Me\n\n\n\n💻 Computational scientist at Argonne National Laboratory (ALCF)\n🧪 Interested in {AI, HPC} for science1\n\n🚀 working on scaling large (language, vision, multi-modal) models2 across thousands of GPUs\n\n\n\n\n\n\n\n\n\n\n🎤 Recent Talks\n\n\n\n\n\nlive: here ( + how I make them! )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n👀 If you’re curious\n\n\n\n\n🔥 What I work on\n\nAs a member of the AI / ML Group at ALCF, I work on3:\n\n\n\n🤖 🧪 AI + Science\n🎲 Building better sampling methods for Lattice QCD\n🧬 Genome-Scale Language Models\n\n GenSLM\n🥇 ACM Gordon Bell Special Prize\n\n\n\n\n\n🌍 Foundation models for long term climate forecasting\n🏃‍♂️ Scaling Large Language Models\n🏎️ Distributed training across thousands of GPUs\n\n\n\n\n\n\n📍 How I got here\n\nMy current research focuses on using deep generative modeling to help build better sampling algorithms in lattice gauge theory. In particular, I’m interested in building gauge equivariant neural network architectures and using inductive priors to incorporate physical symmetries into machine learning models.\nI received my PhD in Physics from the University of Iowa in 2019 and my thesis was on Learning Better Physics: A Machine Learning Approach to Lattice Gauge Theory.\nPrior to this, I completed two bachelors degrees (Engineering Physics and Applied Mathematics, 2015) at The University of Illinois at Urbana-Champaign. My undergraduate dissertation was titled Energy Storage in Quantum Resonators and was supervised by Professor Alfred Hübler within the Center for Complex Systems Research at UIUC4.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n📝 Work🦜 Talks📬 Posts📦 Projects🪖 Experience🎶 Music\n\n\n[NOTE]: You can find a full list of my publications on my Google Scholar.\n\nIntro to HPC Bootcamp: Engaging New Communities Through Energy Justice Projects\nJournal of Computational Science, 2024\nThorough Characterization and Analysis of Large Transformer Model Training At-Scale\nProc. ACM Meas. Anal. Comput. Syst. 03/2024\nMLMC: Machine Learning Monte Carlo for Lattice Gauge Theory\nS. Foreman et al. Lattice, 2023 (Proceedings), 12/2023\nProtein Generation via Genome-scale Language Models with Bio-physical Scoring\n@ SC’23, 11/2023\n DeepSpeed4Science Initiative: Enabling Large-Scale Scientific Discovery […]\n@ NeurIPS 2023 AI For Science Workshop, 10/2023\n\n DeepSpeed4Science.ai Blog Post\n Loooooooong Sequence Lengths\n\nComprehensive Performance Study of LLMs on Novel AI Accelerators\nM. Emani, S. Foreman, et al., IPDPS 2024, 10/2023\nExploratory Analysis of Climate Data with ClimRR\nS. Foreman, Intro to HPC Bootcamp @ NERSC, 08/2023\n🏆 GenSLMs: Genome-scale language models reveal SARS-Cov-2 evolutionary dynamics\n@ SC’22 10/2022\n\n🥇 ACM Gordon Bell Special Prize\n\nLattice QCD and Particle Physics\nA.S. Kronfeld et al., 07/2022\nApplications of ML to Lattice QFT\nD. Boyda, S. Calí, S. Foreman, et al., [arXiv:2202.05838], 02/2022\nLeapFrogLayers: Trainable Framework for Effective Sampling\nS. Foreman, X.Y. Jin, J.C. Osborn, Lattice, 2021\nHMC with Normalizing Flows [slides]\nS. Foreman et al., Lattice, 2021\nDeep Learning Hamiltonian Monte Carlo [+ poster]\nS. Foreman, X.Y. Jin, & J.C. Osborn, @ SimDL Workshop @ ICLR, 2021\nMachine Learning and Neural Networks for Field Theory\nS. Foreman, X.Y. Jin, & J.C. Osborn, SnowMass, 2020\nExamples of renormalization group transformations for image sets\nS. Foreman et al., Physical Review E., 2018\nRG inspired Machine Learning for lattice field theory\nS. Foreman et al., arXiv:1710.02079, 2017\nLarge Energy Density in Three-Plate Nanocapacitors due to Coulomb Blockade\nS. Foreman et al., J. Appl. Phys, 2018\n\n\n\n\n\n📆 2024\n\n\n\n\n\n\nLLMs on Polaris @ Center for Scientific Foundation Models, Summer School 24’ [07/2024]\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\nParallel Training Techniques @ AI-4-Science Training Series [03/2024]\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\nLLMs from Scratch @ LLM Tutorial Workshop [02/2024]\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n📆 2023\n\n\n\n\n\n\nCreating Small(-ish) LLMs @ LLM Tutorial Workshop (1) [11/2023]\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\nExascale Science on Aurora @ Intel oneAPI Workshop @ UIC [10/2023]\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLLM Lunch Talk @ ALCF Hands On HPC Workshop [10/2023]\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\nScaling LLMs for Science @ Data-Intensive Computing + AI/ML at Scale [08/2023]\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\nMLMC: Machine Learning Monte Carlo @ Lattice 2023 [07/2023]\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\nGenerative Modeling and Efficient Sampling @ PASC23 [07/2023]\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\nEfficient Sampling for LGT @ Deep Fridays @ U. Bologna [04/2023]\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n📆 2022\n\n\n\n\n\n\nLarge Scale Training @ AI4Science on Supercomputers (ALCF) [11/2022]\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\nHyperparameter Management @ ALCF SDL Workshop [10/2022]\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\nStatistical Learning @ ATPESC 2022 [08/2022]\n\n\n\n\n\n\n📕 accompanying notebook\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\nScientific Data Science: An Emerging Symbiosis @ ANL (05/2022)\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\nMachine Learning in HEP @ UNC Greensboro [03/2022]\n\n\n\n\n\n\nMachine Learning in HEP, at UNC Greensboro, March 2022\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n📆 2021\n\n\n\n\n\n\nAccelerated Sampling Methods for LGT, @ DWQ @ 25 [BNL] [12/2021]\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\nTraining Topological Samplers for LGT @ ML4HEP, ECT* Trento [09/2021]\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\nl2hmc-qcd @ MIT Lattice Group Seminar [2021]\n\n\n\n\n\nl2hmc-qcd at the MIT Lattice Group Seminar, 2021\n\n\n\n\n\n\n\n\n\nDeep Learning HMC for Improved Gauge Generation @ ML in LQCD Workshop [2021]\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n📆 2020\n\n\n\n\n\n\nMachine Learning for Lattice QCD @ U. Iowa [2020]\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nAug 7, 2024\n\n\n💅 How to Make Dope Slides\n\n\nSam Foreman \n\n\n\n\nAug 7, 2024\n\n\n🍋 ezpz @ ALCF\n\n\nSam Foreman \n\n\n\n\nJun 17, 2024\n\n\n🎰 Deterministic flash-attn\n\n\nSam Foreman \n\n\n\n\nJun 17, 2024\n\n\n📸 flash-attn on Sunspot\n\n\nSam Foreman \n\n\n\n\nJun 15, 2024\n\n\n🏎️ Megatron-DeepSpeed on Intel XPU\n\n\nSam Foreman \n\n\n\n\nMay 25, 2024\n\n\n🐛 mpi4py bug on Sunspot\n\n\nSam Foreman \n\n\n\n\nApr 15, 2024\n\n\n🎲 MCMC + Diffusion Sampling\n\n\nSam Foreman \n\n\n\n\nMar 21, 2024\n\n\n⏰ Starting Up Distributed Training\n\n\nSam Foreman \n\n\n\n\nFeb 12, 2024\n\n\n🚂 Loooooooong Sequence Lengths\n\n\nSam Foreman \n\n\n\n\nFeb 12, 2024\n\n\nl2hmc Example: 2D U(1)\n\n\nSam Foreman \n\n\n\n\nDec 14, 2023\n\n\n🎢 l2hmc-qcd Example: 2D U(1)\n\n\nSam Foreman \n\n\n\n\nDec 6, 2023\n\n\n🔳 l2hmc-qcd Example: 4D SU(3)\n\n\nSam Foreman \n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\n\n\n\n\n\n📊 GitHub Stats\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nEven More !!\n\n\n\nWakatime\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n📂 saforem2/\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n🎪 Events\n\nOrganizer for:\n\nSC24 Workshop: High Performance Python for Science at Scale (HPPSS), November 2024\nSC23 Workshop: High Performance Python for Science at Scale (HPPSS), November 2023\nMachine Learning and Quantum Computing for Earth Sciences at 17th U. S. National Congress on Computational Mechanics, July 2023\n\n\n\n\n👔 Employment\n\n\n\nTable 1: 📟 Experience\n\n\n\n\n\nPosition\n@\nStart\nEnd\n\n\n\n\nAssistant Computational Scientist\nALCF\n2022\n–\n\n\nPostdoc\nALCF\n2019\n2022\n\n\nGraduate Researcher\nANL\n2018\n2019\n\n\n\n\n\n\n\n\n🍎 School\n\n\n\nTable 2: 🎓 Education\n\n\n\n\n\nDegree\nIn\n@\nEnd\n\n\n\n\nPhD\nPhysics\nUniversity of Iowa\n2019\n\n\nB.Sc\nPhysics\nUIUC\n2015\n\n\nB.Sc\nMath\nUIUC\n2015\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n📅 Updated @\n\n# import ezpz as ez\n# print(f\"[{ez.get_timestamp('%Y-%m-%d @ %H:%M:%S')}]\" + \"{.dim-text}\")\nimport datetime\nfrom rich import print\nnow = datetime.datetime.now()\nday = now.strftime('%m/%d/%Y')\ntime = now.strftime('%H:%M:%S')\nprint(' '.join([\n  \"[dim italic]Last Updated[/]:\",\n  f\"[#F06292]{day}[/]\",\n  f\"[dim]@[/]\",\n  f\"[#1A8FFF]{time}[/]\"\n]))\nLast Updated: 08/08/2024 @ 13:08:07"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSo far, for: {Lattice QCD, Quantum Mechanics, Biology (Protein Generation, Drug Discovery), and Climate Modeling / Weather Forecasting}↩︎\nMostly trying to get supercomputers to stop yelling at each other 🫠↩︎\nIf this sounds like something you’d be interested in doing, please feel free to reach out to me!↩︎\nAnd resulted in a patent !!↩︎"
  },
  {
    "objectID": "talks/index.html",
    "href": "talks/index.html",
    "title": "🦜 Recent Talks",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nAug 8, 2024\n\n\nTraining LLMs at Scale\n\n\nSam Foreman\n\n\n\n\nJul 17, 2024\n\n\nLLMs on Polaris\n\n\nSam Foreman\n\n\n\n\n\nNo matching items\n\n\n\n📆 2024\n\n\n\n\n\n\nLLMs on Polaris @ Center for Scientific Foundation Models, Summer School 24’ [07/2024]\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\nParallel Training Techniques @ AI-4-Science Training Series [03/2024]\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\nLLMs from Scratch @ LLM Tutorial Workshop [02/2024]\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n📆 2023\n\n\n\n\n\n\nCreating Small(-ish) LLMs @ LLM Tutorial Workshop (1) [11/2023]\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\nExascale Science on Aurora @ Intel oneAPI Workshop @ UIC [10/2023]\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLLM Lunch Talk @ ALCF Hands On HPC Workshop [10/2023]\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\nScaling LLMs for Science @ Data-Intensive Computing + AI/ML at Scale [08/2023]\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\nMLMC: Machine Learning Monte Carlo @ Lattice 2023 [07/2023]\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\nGenerative Modeling and Efficient Sampling @ PASC23 [07/2023]\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\nEfficient Sampling for LGT @ Deep Fridays @ U. Bologna [04/2023]\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n📆 2022\n\n\n\n\n\n\nLarge Scale Training @ AI4Science on Supercomputers (ALCF) [11/2022]\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\nHyperparameter Management @ ALCF SDL Workshop [10/2022]\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\nStatistical Learning @ ATPESC 2022 [08/2022]\n\n\n\n\n\n\n📕 accompanying notebook\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\nScientific Data Science: An Emerging Symbiosis @ ANL (05/2022)\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\nMachine Learning in HEP @ UNC Greensboro [03/2022]\n\n\n\n\n\n\nMachine Learning in HEP, at UNC Greensboro, March 2022\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n📆 2021\n\n\n\n\n\n\nAccelerated Sampling Methods for LGT, @ DWQ @ 25 [BNL] [12/2021]\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\nTraining Topological Samplers for LGT @ ML4HEP, ECT* Trento [09/2021]\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\nl2hmc-qcd @ MIT Lattice Group Seminar [2021]\n\n\n\n\n\nl2hmc-qcd at the MIT Lattice Group Seminar, 2021\n\n\n\n\n\n\n\n\n\nDeep Learning HMC for Improved Gauge Generation @ ML in LQCD Workshop [2021]\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n📆 2020\n\n\n\n\n\n\nMachine Learning for Lattice QCD @ U. Iowa [2020]\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{foreman2024,\n  author = {Foreman, Sam},\n  title = {🦜 {Recent} {Talks}},\n  date = {2024-08-07},\n  url = {https://samforeman.me/talks/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nForeman, Sam. 2024. “🦜 Recent Talks.” August 7, 2024. https://samforeman.me/talks/.",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "🦜 Talks",
      "📢 All Talks"
    ]
  },
  {
    "objectID": "talks/llms-at-scale/index.html#data-parallel-training",
    "href": "talks/llms-at-scale/index.html#data-parallel-training",
    "title": "Training LLMs at Scale",
    "section": "Data Parallel Training",
    "text": "Data Parallel Training\n\n\n\n\n\n\n\n🔗 Links:\n\nPyTorch Distributed Overview\nDistributed Data Parallel — PyTorch master documentation\n🤗 Efficient Training on Multiple GPUs\nGetting Started - DeepSpeed\n\n\n\n\n \n\n\n\n\n\n\n\n\nFigure 6"
  },
  {
    "objectID": "talks/llms-at-scale/index.html#data-parallel-training-1",
    "href": "talks/llms-at-scale/index.html#data-parallel-training-1",
    "title": "Training LLMs at Scale",
    "section": "Data Parallel Training",
    "text": "Data Parallel Training\n\n\n\n\n\n\n\nTypically easier to implement\n\nRelatively simple to get up and running (minor modifications to code)\n\n saforem2/ezpz\n\n\nExisting frameworks:\n\nHorovod\nDeepSpeed,\nDDP, etc)\n\nRecent presentation on “Parallel Training Techniques” on 🎬 YouTube"
  },
  {
    "objectID": "talks/llms-at-scale/index.html#data-parallel-training-2",
    "href": "talks/llms-at-scale/index.html#data-parallel-training-2",
    "title": "Training LLMs at Scale",
    "section": "Data Parallel Training",
    "text": "Data Parallel Training\n\n\n\n\n\n\n\nEach worker has copy of complete model\nGlobal batch of data split into multiple mini-batches\n\nEach worker computes the corresponding loss and gradients from local data\n\nBefore updating parameters, loss and gradients averaged across workers"
  },
  {
    "objectID": "talks/llms-at-scale/index.html#data-parallel-training-3",
    "href": "talks/llms-at-scale/index.html#data-parallel-training-3",
    "title": "Training LLMs at Scale",
    "section": "Data Parallel Training",
    "text": "Data Parallel Training\n\n\n\n\n\n\nFigure 7"
  },
  {
    "objectID": "talks/llms-at-scale/index.html#deal-with-data",
    "href": "talks/llms-at-scale/index.html#deal-with-data",
    "title": "Training LLMs at Scale",
    "section": "Deal with Data",
    "text": "Deal with Data\n\n\nAt each training step, we want to ensure that each worker receives unique data\nThis can be done in one of two ways:\n\nManually partition data (ahead of time) and assign different sections to different workers\n\nEach worker can only see their local portion of the data\n\nFrom each worker, randomly select a mini-batch\n\nEach worker can see the full dataset\n\n\n\n\n\n\n⚠️ Warning\n\n\nDon’t forget your seed!\nWhen randomly selecting, it is important that each worker uses different seeds to ensure they receive unique data"
  },
  {
    "objectID": "talks/llms-at-scale/index.html#broadcast-initial-state",
    "href": "talks/llms-at-scale/index.html#broadcast-initial-state",
    "title": "Training LLMs at Scale",
    "section": "Broadcast Initial State",
    "text": "Broadcast Initial State\n\nAt the start of training (or when loading from a checkpoint), we want all of our workers to be initialized consistently\n\nBroadcast the model and optimizer states from rank() == 0 worker\n\n\n\n\n\n\n\n  flowchart TD\n    0[\"GPU0\"] --&gt; 1[\"GPU 1\"]\n    0 --&gt; 2[\"GPU 2\"]\n    0 --&gt;|Model + Optimizer State| 3[\"GPU 3\"]\n    0 --&gt; ...\n    0 --&gt; N[\"GPU N\"]"
  },
  {
    "objectID": "talks/llms-at-scale/index.html#best-practices",
    "href": "talks/llms-at-scale/index.html#best-practices",
    "title": "Training LLMs at Scale",
    "section": "Best Practices",
    "text": "Best Practices\n\nUse parallel IO whenever possible\n\nFeed each rank from different files\nUse MPI IO to have each rank read its own batch from a file\nUse several ranks to read data, MPI to scatter to remaining ranks\n\nMost practical in big at-scale training\n\n\n\n\n\n\n🤝 Keeping things in Sync\n\n\nComputation stalls during communication !!\nKeeping the communication to computation ratio small is important for effective scaling.\n\n\n\n\nTake advantage of data storage\n\nUse striping on lustre\nUse the right optimizations for Aurora, Polaris, etc.\n\nPreload data when possible\n\nOffloading to a GPU frees CPU cycles for loading the next batch of data\n\nminimize IO latency this way"
  },
  {
    "objectID": "talks/llms-at-scale/index.html#why-distributed-training",
    "href": "talks/llms-at-scale/index.html#why-distributed-training",
    "title": "Training LLMs at Scale",
    "section": "Why Distributed Training?",
    "text": "Why Distributed Training?\n\nSplitting data across workers \\longrightarrow larger batch size1\n\n[micro_batch_size = 1] \\times [N GPUs] \\rightarrow [global_batch_size = N]\n\nSmooth loss landscape\nImproved gradient estimators\nLess iterations needed for same number of epochs\n\nMay need to train for more epochs if another change is not made\ne.g. scaling learning rate\n\nSee Large Batch Training of Convolutional Networks\n\nmicro_batch_size = batch_size per GPU"
  },
  {
    "objectID": "talks/llms-at-scale/index.html#recent-progress",
    "href": "talks/llms-at-scale/index.html#recent-progress",
    "title": "Training LLMs at Scale",
    "section": "Recent Progress",
    "text": "Recent Progress\n\n\n\nTable 1: Batch-Size-Scaling\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear\nAuthor\nBatch Size\nProcessor\n# Processors\nTime\nAccuracy\n\n\n\n\n2016\nHe\n256\nP100\n8\n29 Hour\n75.30%\n\n\n2019\nYamazaki\n81,920\nV100\n2048\n1.2 Min\n75.08%"
  },
  {
    "objectID": "talks/llms-at-scale/index.html#deciding-on-a-parallelism-strategy",
    "href": "talks/llms-at-scale/index.html#deciding-on-a-parallelism-strategy",
    "title": "Training LLMs at Scale",
    "section": "Deciding on a Parallelism Strategy",
    "text": "Deciding on a Parallelism Strategy\n\nSingle GPUSingle Node / Multi-GPUMulti-Node / Multi-GPU\n\n\n\nModel fits onto a single GPU:\n\nNormal use\n\nModel DOES NOT fit on a single GPU:\n\nZeRO + Offload CPU (or, optionally, NVMe)\n\nLargest layer DOES NOT fit on a single GPU:\n\nZeRO + Enable Memory Centric Tiling (MCT)1\n\n\n\n\n\nModel fits onto a single GPU\n\nDDP\nZeRO\n\nModel DOES NOT fit onto a single GPU2\n\nPP\nZeRO\nTP\n\n\n\n\n\nWhen you have fast inter-node connectivity:\n\nZeRO (virtually NO modifications)\nPP + ZeRO + TP + DP (less communication, at the cost of MAJOR modifications)\n\nwhen you have slow inter-node connectivity and still low on GPU memory:\nDP + PP + TP + ZeRO-1\n\nNOTE: TP is almost always used within a single node, e.g. TP &lt;= GPUS_PER_NODE\n\n\n\n\n\n\nAllows running of arbitrarily large layers by automatically splitting them and executing them sequentially.\nWith sufficiently fast connectivity between nodes, these three strategies should be comparable. Otherwise, PP &gt; ZeRO \\simeq TP."
  },
  {
    "objectID": "talks/llms-at-scale/index.html#model-parallel-training",
    "href": "talks/llms-at-scale/index.html#model-parallel-training",
    "title": "Training LLMs at Scale",
    "section": "Model Parallel Training",
    "text": "Model Parallel Training\n\n\n\n\n\n\n\nSplit up network over multiple workers\n\nEach receives disjoint subset\nAll communication associated with subsets are distributed\n\nCommunication whenever dataflow between two subsets\nTypically more complicated to implement than data parallel training\nSuitable when the model is too large to fit onto a single device (CPU / GPU)\n argonne-lcf/Megatron-DeepSpeed\n🤗 huggingface/nanotron\n\n\n\n\n\n\n\n\n\nFigure 8"
  },
  {
    "objectID": "talks/llms-at-scale/index.html#tensor-model-parallelismefficient-large-scale",
    "href": "talks/llms-at-scale/index.html#tensor-model-parallelismefficient-large-scale",
    "title": "Training LLMs at Scale",
    "section": "Tensor (Model) Parallelism1",
    "text": "Tensor (Model) Parallelism1\n\nIn Tensor Paralleism each GPU processes only a slice of a tensor and only aggregates the full tensor for operations that require the whole thing.\n\nThe main building block of any transformer is a fully connected nn.Linear followed by a nonlinear activation GeLU.\n\nY = GeLU(XA), where X and Y are the input and output vectors, and A is the weight matrix.\n\nIf we look at the computation in matrix form, it’s easy to see how the matrix multiplication can be split between multiple GPUs:\n\n\nEfficient Large-Scale Language Model Training on GPU Clusters"
  },
  {
    "objectID": "talks/llms-at-scale/index.html#tensor-parallelism",
    "href": "talks/llms-at-scale/index.html#tensor-parallelism",
    "title": "Training LLMs at Scale",
    "section": "Tensor Parallelism",
    "text": "Tensor Parallelism\n\n\n\n\nThis information is based on (the much more in-depth) TP Overview by @anton-l"
  },
  {
    "objectID": "talks/llms-at-scale/index.html#d-parallelism",
    "href": "talks/llms-at-scale/index.html#d-parallelism",
    "title": "Training LLMs at Scale",
    "section": "3D Parallelism",
    "text": "3D Parallelism\n\nDP + TP + PP (3D) Parallelism\n\n\n\n\n\n\n\nFigure 9: 3D Parallelism illustration. Figure from: https://www.deepspeed.ai/"
  },
  {
    "objectID": "talks/llms-at-scale/index.html#d-parallelism-1",
    "href": "talks/llms-at-scale/index.html#d-parallelism-1",
    "title": "Training LLMs at Scale",
    "section": "3D Parallelism",
    "text": "3D Parallelism\n\nDP + TP + PP (3D) Parallelism\n\n\n\n\n\n\n\nFigure 10: Figure taken from 3D parallelism: Scaling to trillion-parameter models"
  },
  {
    "objectID": "talks/llms-at-scale/index.html#collective-operations",
    "href": "talks/llms-at-scale/index.html#collective-operations",
    "title": "Training LLMs at Scale",
    "section": "Collective Operations",
    "text": "Collective Operations\n\n\n\n\n⌛ Timeouts\n\n\n\nCollective operations have to be called for each rank to form a complete collective operation.\n\nFailure to do so will result in other ranks waiting indefinitely"
  },
  {
    "objectID": "talks/llms-at-scale/index.html#status-of-large-language-models",
    "href": "talks/llms-at-scale/index.html#status-of-large-language-models",
    "title": "Training LLMs at Scale",
    "section": "Status of Large Language Models",
    "text": "Status of Large Language Models\n\n\n\n\n\n\nFigure 16: Large Language Models have (LLM)s have taken the NLP community world by storm1\n\n\n\n Hannibal046/Awesome-LLM"
  },
  {
    "objectID": "talks/llms-at-scale/index.html#emergent-abilities",
    "href": "talks/llms-at-scale/index.html#emergent-abilities",
    "title": "Training LLMs at Scale",
    "section": "Emergent Abilities",
    "text": "Emergent Abilities\n\n\nEmergent abilities of Large Language Models Yao et al. (2023)"
  },
  {
    "objectID": "talks/llms-at-scale/index.html#training-llms",
    "href": "talks/llms-at-scale/index.html#training-llms",
    "title": "Training LLMs at Scale",
    "section": "Training LLMs",
    "text": "Training LLMs\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 17: Visualization from Yang et al. (2023)"
  },
  {
    "objectID": "talks/llms-at-scale/index.html#recent-work-2017-now",
    "href": "talks/llms-at-scale/index.html#recent-work-2017-now",
    "title": "Training LLMs at Scale",
    "section": "Recent Work (2017 – Now)",
    "text": "Recent Work (2017 – Now)\n\n\nPapers, 2017–*\n\n\n\n\n\n\n\n\n\nDate\nPaper\nkeywords\nInstitute\nPublication\n\n\n\n\n06/2017\nAttention Is All You Need\nTransformers\nGoogle\nNeurIPS \n\n\n06/2018\nImproving Language Understanding by Generative Pre-Training\nGPT 1.0\nOpenAI\n\n\n\n10/2018\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\nBERT\nGoogle\nNAACL \n\n\n02/2019\nLanguage Models are Unsupervised Multitask Learners\nGPT 2.0\nOpenAI\n\n\n\n09/2019\nMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nMegatron-LM\nNVIDIA\n\n\n\n10/2019\nExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\nT5\nGoogle\nJMLR \n\n\n10/2019\nZeRO: Memory Optimizations Toward Training Trillion Parameter Models\nZeRO\nMicrosoft\nSC \n\n\n01/2020\nScaling Laws for Neural Language Models\nScaling Law\nOpenAI\n\n\n\n05/2020\nLanguage models are few-shot learners\nGPT 3.0\nOpenAI\nNeurIPS  \n\n\n01/2021\nSwitch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity\nSwitch Transformers\nGoogle\nJMLR \n\n\n08/2021\nEvaluating Large Language Models Trained on Code\nCodex\nOpenAI\n\n\n\n08/2021\nOn the Opportunities and Risks of Foundation Models\nFoundation Models\nStanford\n\n\n\n09/2021\nFinetuned Language Models are Zero-Shot Learners\nFLAN\nGoogle\nICLR \n\n\n10/2021\nMultitask Prompted Training Enables Zero-Shot Task Generalization\nT0\nHuggingFace et al.\nICLR \n\n\n12/2021\nGLaM: Efficient Scaling of Language Models with Mixture-of-Experts\nGLaM\nGoogle\nICML \n\n\n12/2021\nWebGPT: Browser-assisted question-answering with human feedback\nWebGPT\nOpenAI\n\n\n\n12/2021\nImproving language models by retrieving from trillions of tokens\nRetro\nDeepMind\nICML \n\n\n12/2021\nScaling Language Models: Methods, Analysis & Insights from Training Gopher\nGopher\nDeepMind\n\n\n\n01/2022\nChain-of-Thought Prompting Elicits Reasoning in Large Language Models\nCOT\nGoogle\nNeurIPS\n\n\n01/2022\nLaMDA: Language Models for Dialog Applications\nLaMDA\nGoogle\n\n\n\n01/2022\nSolving Quantitative Reasoning Problems with Language Models\nMinerva\nGoogle\nNeurIPS \n\n\n01/2022\nUsing DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model\nMegatron-Turing NLG\nMicrosoft&NVIDIA\n\n\n\n03/2022\nTraining language models to follow instructions with human feedback\nInstructGPT\nOpenAI\n\n\n\n04/2022\nPaLM: Scaling Language Modeling with Pathways\nPaLM\nGoogle\n\n\n\n04/2022\nAn empirical analysis of compute-optimal large language model training\nChinchilla\nDeepMind\nNeurIPS \n\n\n05/2022\nOPT: Open Pre-trained Transformer Language Models\nOPT\nMeta\n\n\n\n05/2022\nUnifying Language Learning Paradigms\nUL2\nGoogle\n\n\n\n06/2022\nEmergent Abilities of Large Language Models\nEmergent Abilities\nGoogle\nTMLR\n\n\n06/2022\nBeyond the Imitation Game: Quantifying and extrapolating the capabilities of language models\nBIG-bench\nGoogle\n\n\n\n06/2022\nLanguage Models are General-Purpose Interfaces\nMETALM\nMicrosoft\n\n\n\n09/2022\nImproving alignment of dialogue agents via targeted human judgements\nSparrow\nDeepMind\n\n\n\n10/2022\nScaling Instruction-Finetuned Language Models\nFlan-T5/PaLM\nGoogle\n\n\n\n10/2022\nGLM-130B: An Open Bilingual Pre-trained Model\nGLM-130B\nTsinghua\nICLR \n\n\n11/2022\nHolistic Evaluation of Language Models\nHELM\nStanford\n\n\n\n11/2022\nBLOOM: A 176B-Parameter Open-Access Multilingual Language Model\nBLOOM\nBigScience\n\n\n\n11/2022\nGalactica: A Large Language Model for Science\nGalactica\nMeta\n\n\n\n12/2022\nOPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization\nOPT-IML\nMeta\n\n\n\n01/2023\nThe Flan Collection: Designing Data and Methods for Effective Instruction Tuning\nFlan 2022 Collection\nGoogle\n\n\n\n02/2023\nLLaMA: Open and Efficient Foundation Language Models\nLLaMA\nMeta\n\n\n\n02/2023\nLanguage Is Not All You Need: Aligning Perception with Language Models\nKosmos-1\nMicrosoft\n\n\n\n03/2023\nPaLM-E: An Embodied Multimodal Language Model\nPaLM-E\nGoogle\n\n\n\n03/2023\nGPT-4 Technical Report\nGPT 4\nOpenAI\n\n\n\n04/2023\nPythia: A Suite for Analyzing Large Language Models Across Training and Scaling\nPythia\nEleutherAI et al.\nICML\n\n\n05/2023\nPrinciple-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision\nDromedary\nCMU et al.\n\n\n\n05/2023\nPaLM 2 Technical Report\nPaLM 2\nGoogle\n\n\n\n05/2023\nRWKV: Reinventing RNNs for the Transformer Era\nRWKV\nBo Peng\n\n\n\n05/2023\nDirect Preference Optimization: Your Language Model is Secretly a Reward Model\nDPO\nStanford\n\n\n\n07/2023\nLlama 2: Open Foundation and Fine-Tuned Chat Models\nLLaMA 2\nMeta\n\n\n\n\n\n\n\n Hannibal046/Awesome-LLM"
  },
  {
    "objectID": "talks/llms-at-scale/index.html#life-cycle-of-the-llm",
    "href": "talks/llms-at-scale/index.html#life-cycle-of-the-llm",
    "title": "Training LLMs at Scale",
    "section": "Life-Cycle of the LLM",
    "text": "Life-Cycle of the LLM\n\n\n\n\n\n\n\nData collection + preprocessing\nPre-training\n\nArchitecture decisions:\n{model_size, hyperparameters,\nparallelism, lr_schedule, ...}\n\nSupervised Fine-Tuning\n\nInstruction Tuning\nAlignment\n\nDeploy (+ monitor, re-evaluate, etc.)\n\n\n\n\n\n\n\n\n\nFigure 18: Pre-training: Virtually all of the compute used during pretraining phase1.\n\n\n\n\n\n\nFigure from The Illustrated Transformer"
  },
  {
    "objectID": "talks/llms-at-scale/index.html#life-cycle-of-the-llm-pre-training",
    "href": "talks/llms-at-scale/index.html#life-cycle-of-the-llm-pre-training",
    "title": "Training LLMs at Scale",
    "section": "Life-Cycle of the LLM: Pre-training",
    "text": "Life-Cycle of the LLM: Pre-training\n\n\n\n\n\n\nFigure 19: Pre-training: Virtually all of the compute used during pretraining phase"
  },
  {
    "objectID": "talks/llms-at-scale/index.html#life-cycle-of-the-llm-fine-tuning",
    "href": "talks/llms-at-scale/index.html#life-cycle-of-the-llm-fine-tuning",
    "title": "Training LLMs at Scale",
    "section": "Life-Cycle of the LLM: Fine-Tuning",
    "text": "Life-Cycle of the LLM: Fine-Tuning\n\n\n\n\n\n\nFigure 20: Fine-tuning1: Fine-tuning actually updates the model’s weights to make the model better at a certain task.\n\n\n\nFigure from The Illustrated Transformer"
  },
  {
    "objectID": "talks/llms-at-scale/index.html#forward-pass",
    "href": "talks/llms-at-scale/index.html#forward-pass",
    "title": "Training LLMs at Scale",
    "section": "Forward Pass",
    "text": "Forward Pass\n\n\n\n\n\n\n\nFigure 21: Language Model trained for causal language modeling. Video from: 🤗 Generation with LLMs"
  },
  {
    "objectID": "talks/llms-at-scale/index.html#generating-text",
    "href": "talks/llms-at-scale/index.html#generating-text",
    "title": "Training LLMs at Scale",
    "section": "Generating Text",
    "text": "Generating Text\n\n\n\n\n\n\n\nFigure 22: Language Model trained for causal language modeling. Video from: 🤗 Generation with LLMs"
  }
]