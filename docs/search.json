[
  {
    "objectID": "slides.html#section",
    "href": "slides.html#section",
    "title": "Recent Talks",
    "section": "📆 2024",
    "text": "📆 2024"
  },
  {
    "objectID": "slides.html#section-1",
    "href": "slides.html#section-1",
    "title": "Recent Talks",
    "section": "📆 2023",
    "text": "📆 2023\n\n\n\n\n\n\nCreating Small(-ish) LLMs\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\n**LLM Lunch Talk\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\n🪧 Exascale Science on Aurora @ Intel oneAPI Workshop @ UIC (10/2023)\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\n🪧 LLMs on Polaris @ ALCF Hands On HPC Workshop (10/2023)\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\n🪧 Scaling LLMs for Science and Ongoing Collaborations @ Data-Intensive Computing and AI/ML at Scale (08/2023)\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\n🪧 MLMC: Machine Learning Monte Carlo @ Lattice 2023 (07/2023)\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\n🪧 Generative Modeling and Efficient Sampling @ PASC23 (07/2023)\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\n🪧 Efficient Sampling for Lattice Gauge Theory @ Deep Fridays @ U. Bologna (04/2023)\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;"
  },
  {
    "objectID": "slides.html#section-2",
    "href": "slides.html#section-2",
    "title": "Recent Talks",
    "section": "📆 2022",
    "text": "📆 2022\n\n\n\n\n\n\n🪧 Large Scale Training @ Introduction to AI4Science on Supercomputers (ALCF) (11/2022)\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\n🪧 Hyperparameter Management @ ALCF Simulation, Data, and Learning Workshop (10/2022)\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\n🪧 Statistical Learning @ ATPESC 2022 (08/2022)\n\n\n\n\n\n\n📕 accompanying notebook\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\n🪧 Scientific Data Science: An Emerging Symbiosis @ ANL (05/2022)\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\n🪧 Machine Learning in HEP @ UNC Greensboro (03/2022)\n\n\n\n\n\n\nMachine Learning in HEP, at UNC Greensboro, March 2022\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;"
  },
  {
    "objectID": "slides.html#section-3",
    "href": "slides.html#section-3",
    "title": "Recent Talks",
    "section": "📆 2021",
    "text": "📆 2021\n\n\n\n\n\n\n🪧 Accelerated Sampling Methods for Lattice Gauge Theory, @ BNL / DWQ @ 25 (12/2021)\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\n🪧 Training Topological Samplers for Lattice Gauge Theory @ ML4HEP, ECT* Trento (09/2021)\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\n🪧 l2hmc-qcd @ MIT Lattice Group Seminar (2021)\n\n\n\n\n\nl2hmc-qcd at the MIT Lattice Group Seminar, 2021\n\n\n\n\n\n\n\n\n\n🪧 Deep Learning HMC for Improved Gauge Generation @ ML in LQCD Workshop (2021)\n\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;"
  },
  {
    "objectID": "slides.html#section-4",
    "href": "slides.html#section-4",
    "title": "Recent Talks",
    "section": "📆 2020",
    "text": "📆 2020\n\n\n\n\n\n\n🪧 Machine Learning for Lattice QCD @ U. Iowa (2020)\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;"
  },
  {
    "objectID": "qmd/projects.html",
    "href": "qmd/projects.html",
    "title": "Projects",
    "section": "",
    "text": "Projects\n\n l2hmc-qcd\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{foreman2023,\n  author = {Foreman, Sam},\n  title = {Personal {Website}},\n  date = {2023-10-05},\n  url = {https://samforeman.me},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nForeman, Sam. 2023. “Personal Website.” October 5, 2023. https://samforeman.me."
  },
  {
    "objectID": "qmd/mpi4py-reproducer/index.html",
    "href": "qmd/mpi4py-reproducer/index.html",
    "title": "",
    "section": "",
    "text": "🐛 mpi4py bug on Sunspot\nSimple reproducer:\n\nLoad my anl_24_q2_release conda environment:\n#[08:42:38 AM][foremans@x1922c2s3b0n0][~]\n$ eval \"$(~/miniconda3/bin/conda shell.zsh hook)\" ; conda activate anl_24_q2_release\nTry python3 -c 'from mpi4py import MPI'\n\nfails ❌\n\n# [08:44:41 AM][foremans@x1922c2s3b0n0][~][anl_24_q2_release]\n$ python3 -c 'from mpi4py import MPI'\nTraceback (most recent call last):\n  File \"&lt;string&gt;\", line 1, in &lt;module&gt;\nImportError: /home/foremans/miniconda3/envs/anl_24_q2_release/lib/python3.9/site-packages/mpi4py/MPI.cpython-39-x86_64-linux-gnu.so: undefined symbol: MPI_Message_c2f\n[1]    14910 exit 1     python3 -c 'from mpi4py import MPI'\nLoad correct modules:\n# [08:44:58 AM][foremans@x1922c2s3b0n0][~][anl_24_q2_release]\n$ module use /home/ftartagl/graphics-compute-runtime/modulefiles ; module load graphics-compute-runtime/agama-ci-devel-803.29 spack-pe-gcc/0.6.1-23.275.2 gcc/12.2.0 ; module use /soft/preview-modulefiles/24.086.0 ; module load oneapi/release/2024.04.15.001\n     UMD: agama-ci-devel-803.29 successfully loaded:\n     UMD: graphics-compute-runtime/agama-ci-devel-803.29\n\nDue to MODULEPATH changes, the following have been reloaded:\n  1) mpich-config/collective-tuning/1024\n\nThe following have been reloaded with a version change:\n  1) intel_compute_runtime/release/agama-devel-736.25 =&gt; intel_compute_runtime/release/775.20     2) mpich/icc-all-pmix-gpu/52.2 =&gt; mpich/icc-all-pmix-gpu/20231026     3) oneapi/eng-compiler/2023.12.15.002 =&gt; oneapi/release/2024.04.15.001\nRetry with new modules:\n\nworks ✅\n\n# [08:45:01 AM][foremans@x1922c2s3b0n0][~][anl_24_q2_release]\n$ python3 -c 'from mpi4py import MPI; print(MPI.__file__)'\n/home/foremans/miniconda3/envs/anl_24_q2_release/lib/python3.9/site-packages/mpi4py/MPI.cpython-39-x86_64-linux-gnu.so\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{foreman2024,\n  author = {Foreman, Sam},\n  title = {Personal {Website}},\n  date = {2024-05-25},\n  url = {https://samforeman.me},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nForeman, Sam. 2024. “Personal Website.” May 25, 2024. https://samforeman.me."
  },
  {
    "objectID": "qmd/home-alt/index.html#footnotes",
    "href": "qmd/home-alt/index.html#footnotes",
    "title": "👋 Sam, I am",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSo far, for: {Lattice QCD, Quantum Mechanics, Biology (Protein Generation, Drug Discovery), and Climate Modeling / Weather Forecasting}↩︎\nAnd resulted in a patent !!↩︎\n⚡ powered by ezpz↩︎\nForked from   saforem2/opinionated↩︎"
  },
  {
    "objectID": "qmd/features.html",
    "href": "qmd/features.html",
    "title": "Posts",
    "section": "",
    "text": "Features\n\n\n\n\nBlog\nLatest from the blog\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 29, 2024\n\n\nSam Foreman \n\n\n\n\n\n\n\n\n\n\n\n\n🔥 Hot off the Press 📰\n\n\n\n\n\n\n\n\n\n\n\nMay 29, 2024\n\n\nSam Foreman \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 29, 2024\n\n\nSam Foreman \n\n\n\n\n\n\n\n\n\n\n\n\n📸 flash-attn on Sunspot\n\n\n\n\n\n\n\n\n\n\n\nMay 29, 2024\n\n\nSam Foreman \n\n\n\n\n\n\n\n\n\n\n\n\n🐛 mpi4py bug on Sunspot\n\n\n\n\n\n\n\n\n\n\n\nMay 25, 2024\n\n\nSam Foreman \n\n\n\n\n\n\n\n\n\n\n\n\nHow to Make Dope Slides\n\n\n\n\n\n\n\n\n\n\n\nMay 14, 2024\n\n\nSam Foreman \n\n\n\n\n\n\n\n\n\n\n\n\n👋 Sam, I am\n\n\n\n\n\n\n\n\n\n\n\nApr 18, 2024\n\n\nSam Foreman \n\n\n\n\n\n\n\n\n\n\n\n\nMCMC + Diffusion Sampling\n\n\n\n\n\n\n\n\n\n\n\nApr 15, 2024\n\n\nSam Foreman \n\n\n\n\n\n\n\n\n\n\n\n\nRecent Talks\n\n\n\n\n\n\n\n\n\n\n\nApr 15, 2024\n\n\nSam Foreman \n\n\n\n\n\n\n\n\n\n\n\n\nMegatron DeepSpeed on xpu\n\n\n\n\n\n\n\n\n\n\n\nApr 3, 2024\n\n\nSam Foreman \n\n\n\n\n\n\n\n\n\n\n\n\nStarting Up Distributed Training\n\n\n\n\n\n\n\n\n\n\n\nMar 21, 2024\n\n\nSam Foreman \n\n\n\n\n\n\n\n\n\n\n\n\nLoooooooong Sequence Lengths\n\n\n\n\n\n\n\n\n\n\n\nFeb 12, 2024\n\n\nSam Foreman \n\n\n\n\n\n\n\n\n\n\n\n\nProjects\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2023\n\n\nSam Foreman \n\n\n\n\n\n\nNo matching items\n\n\n\n\nBlog\n\n\n\n\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{foreman2024,\n  author = {Foreman, Sam},\n  title = {Personal {Website}},\n  date = {2024-03-17},\n  url = {https://samforeman.me},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nForeman, Sam. 2024. “Personal Website.” March 17, 2024. https://samforeman.me."
  },
  {
    "objectID": "qmd/dsblog.html",
    "href": "qmd/dsblog.html",
    "title": "Loooooooong Sequence Lengths",
    "section": "",
    "text": "The new Megatron-DeepSpeed release contains a variety of improvements / optimizations to enable pre-training Transformer based architectures with significantly longer sequences than was previously possible."
  },
  {
    "objectID": "qmd/dsblog.html#deepspeed4science-092023",
    "href": "qmd/dsblog.html#deepspeed4science-092023",
    "title": "Loooooooong Sequence Lengths",
    "section": "DeepSpeed4Science (09/2023)",
    "text": "DeepSpeed4Science (09/2023)\n\nNew Features\n\nEnabled Megatron-LM’s sequence parallel.\nEnabled rotary positional embedding.\nEnabled FlashAttention v1 and v2.\nEnabled new fused kernels from NVIDIA.\n\n\n\nNew optimizations\n\nEnabled attention map memory optimization, where we first generated attention mask on CPU memory and then moved it into GPU memory to avoid out-of-memory errors when training with very large sequence lengths.\nPosition embedding partitioning, where we split weights of position encoding across all GPUs when enabling sequence parallel to further reduce the memory footprint.\n\n\n\nInitial Results\n\n\n\nTable 1: Long sequence length support1 from microsoft/Megatron-DeepSpeed\n\n\n\n\n\n\n\n\n\n\nSequence Length\nOld Megatron-DeepSpeed (TFLOPS)\nNew Megatron-DeepSpeed (TFLOPS)\n\n\n\n\n2k\n25\n68\n\n\n4k\n28\n80\n\n\n8k\nOOM\n86\n\n\n16k\nOOM\n92\n\n\n32k\nOOM\n100\n\n\n64k\nOOM\n106\n\n\n128k\nOOM\n119\n\n\n256k\nOOM\n94\n\n\n\n\n\n\n\n\nData\ngpus = ('32', '64', '128')\n\ncolors = {\n    'Old Megatron-DS': '#FF5252',\n    'Megatron-LM': '#76b900',\n    'New Megatron-DS':  '#1A8FFF',\n}\n\ndata = {\n    '25B': {\n        'Old Megatron-DS': np.array([36, 42, 42]),\n        'Megatron-LM': np.array([26, 48, 52]),\n        'New Megatron-DS': np.array([192, 448, 512]),\n    },\n    '33B': {\n        'Old Megatron-DS': np.array([28, 32, 32]),\n        'Megatron-LM': np.array([14, 46, 52]),\n        'New Megatron-DS': np.array([128, 384, 448]),\n    },\n}\n\n\n\nMake the plots\nx = np.arange(len(gpus))\nwidth = 0.25\nmultiplier = 0\n\noutdir = Path(os.getcwd()).joinpath('assets')\noutdir.mkdir(exist_ok=True, parents=True)\n\nimprovement = {}\nfor idx, (model_size, d) in enumerate(data.items()):\n    multiplier = 0\n    figure, axes = plt.subplots(figsize=(7.5, 4))\n    fig = plt.gcf()\n    ax = plt.gca()\n    for label, value in d.items():\n        offset = width * multiplier\n        rects = ax.barh(\n          x + offset,\n          value,\n          width,\n          label=label,\n          color=colors[label],\n          alpha=0.8\n        )\n        ax.bar_label(\n          rects,\n          padding=3,\n          color=colors[label],\n          family='monospace',\n          weight='bold'\n        )\n        multiplier += 1\n    ax.set_ylabel(\n        'GPUs',\n        fontsize=18,\n        family='sans-serif',\n        loc='center',\n    )\n    ax.set_yticks(x + width, gpus)\n    plt.figtext(\n        0.005, 0.93, f\"{model_size}\", fontsize=24, fontweight='bold', ha='left'\n    )\n    ax.set_xlabel(\n        'Sequence Length (k)', fontsize=18, loc='center'\n    )\n    ax.legend(\n        bbox_to_anchor=(0.005, 1.04, 0.99, .098),\n        alignment='center',\n        edgecolor=\"#83838320\",\n        frameon=True,\n        ncols=3,\n        fontsize=13,\n        mode=\"expand\",\n        borderaxespad=0.01\n    )\n    save_figure(fname=f'{model_size}', outdir=outdir)\n    _ = plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nGPT-25B Model\n\n\n\n\n\n\n\nGPT-33B Model\n\n\n\n\n\n\n\nFigure 2: Pre-training with long sequence support across different model sizes and numbers of GPUs. In each case, the new (current) implementation significantly outperforms both NVIDIA/Megatron-LM as well as our previous implementation."
  },
  {
    "objectID": "qmd/dsblog.html#installation",
    "href": "qmd/dsblog.html#installation",
    "title": "Loooooooong Sequence Lengths",
    "section": "Installation",
    "text": "Installation\n\nUsing install.sh\n\n\n\n\n\n\nInstallation\n\n\n\n\n\nImportant To install, simply:\ngit clone https://github.com/ramanthanlab/GenSLM/\ncd GenSLM/examples/long-sequences/\n./install.sh\nExplicitly, ./install.sh will:\n\nAutomatically create a virtual environment on top of the latest conda module\nInstall (+ update2) / build all the required dependencies into this virtual environment\n\n\n\n\n\n\nStep-by-Step\nFor completeness, we describe below the steps for installing and building each of the dependencies.\n\nClone GitHub repo:\ngit clone https://github.com/ramanthanlab/GenSLM\nLoad conda module:\n\nThetaGPU:\n# ThetaGPU:\nif [[ \"$(hostname)==theta*\" ]]; then\n    export MACHINE=\"ThetaGPU\"\n    export CONDA_DATE=\"2023-01-10\"\n    module load conda/2023-01-11\n    conda activate base\nfi\nPolaris:\n# Polaris:\nif [[ \"$(hostname)==x3*\" ]]; then\n    export MACHINE=\"Polaris\"\n    export CONDA_DATE=\"2023-01-10\"\n    module load conda/2023-01-10-unstable\n    conda activate base\nfi\n\nSetup Virtual Environment3:\ncd ./genslm/examples/long-sequences\n# create a new virtual environment\nmkdir -p \"venvs/${MACHINE}/${CONDA_DATE}\"\npython3 -m venv \"venvs/${MACHINE}/${CONDA_DATE}\" --system-site-packages\nsource \"venvs/${MACHINE}/${CONDA_DATE}/bin/activate\"\nCreate a new folder (genslm/examples/long-sequences/deps/${MACHINE}) where we’ll installing dependencies locally:\nmkdir -p \"deps/${MACHINE}\"\ncd \"deps/${MACHINE}\"\n\n\nDependencies\nWe provide below the details needed to install each of the required dependencies.\n\n\n saforem2/ezpz\n\n\n saforem2/ezpz\npip install -e \"git+https://github.com/saforem2/ezpz.git#egg=ezpz\"\n\n\n\n\n Microsoft/DeepSpeed\n\n\n Microsoft/DeepSpeed\ngit clone https://github.com/microsoft/DeepSpeed.git\ncd DeepSpeed\npython3 -m pip install -e .\n\n\n\n\n Microsoft/Megatron-DeepSpeed\n\n\n Microsoft/Megatron-DeepSpeed:\ngit clone https://github.com/microsoft/Megatron-DeepSpeed.git\n\n\n\n\n NVIDIA/apex\n\n\n NVIDIA/apex\ngit clone https://github.com/NVIDIA/apex\ncd ../apex/\npip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" -e ./\n\n\n\n\n pybind/PyBind11\n\n\n pybind/PyBind11\npip install pybind11\n\n\n\n\n Dao-AILab/flash-attention\n\n\n Dao-AILab/flash-attention:\n\n\n\n\n\n\nFlash Attention\n\n\n\n\n\n\nThe new release supports three different implementations of FlashAttention: (v1.0.4, v2.x, triton)\nFlashAttention v2.x may have numerical instability issues. For the best performance, we recommend using FlashAttention + Triton\n\n\n\n\n\nv1.0.4:\npython3 -m pip install flash-attn==1.0.4\nv2.x:\ngit clone https://github.com/Dao-AILab/flash-attention\ncd flash-attention\npython3 setup.py install\nopenai/triton:\ngit clone -b legacy-backend https://github.com/openai/triton\ncd triton/python\npython3 -m pip install cmake\npython3 -m pip install ."
  },
  {
    "objectID": "qmd/dsblog.html#running",
    "href": "qmd/dsblog.html#running",
    "title": "Loooooooong Sequence Lengths",
    "section": "Running",
    "text": "Running\nThe ALCF/ directory contains shell scripts for setting up the environment and specifying the options to be used when launching.\nVarious options can be specified dynamically at runtime by setting them in your environment, e.g.:\nMODEL_SIZE_KEY=\"GPT25B\" SEQ_LEN=128000 USE_FLASH_ATTN=1 MICRO_BATCH=1 GAS=1 SP_TYPE=\"megatron\" ZERO_STAGE=1 ./ALCF/train-gpt3.sh\nExplicitly:\n\nALCF/train-gpt3.sh: Main entry point for training\n\nThis script will automatically source the rest of the required ALCF/*.sh scripts below\n\nALCF/models.sh: Contains some example model architectures for GPT3-style models\nALCF/args.sh: Logic for parsing / setting up runtime options for Megatron and DeepSpeed\nALCF/setup.sh: Locate and activate virtual environment to be used, ensure MPI variables are set properly\nALCF/launch.sh: Identify available resources and build the command to be executed\n\ni.e. figure out how many: {nodes, GPUs per node, GPUs total}, to pass to mpi{run,exec}\nthen, use this to build mpiexec &lt;mpiexec-args&gt; python3 pretrain_gpt.py"
  },
  {
    "objectID": "qmd/dsblog.html#zero-offloading",
    "href": "qmd/dsblog.html#zero-offloading",
    "title": "Loooooooong Sequence Lengths",
    "section": "ZeRO Offloading",
    "text": "ZeRO Offloading\n🚀 W&B Report: Looooooooong Sequences\nThese newly introduced optimizations, in combination with ZeRO-Offload allows us to go even further.\nBy employing ZeRO-Offloading, we are able to free up additional memory which can be used for even longer sequences.\nThough work is still ongoing, this is a promising direction that will allow us to consider significantly larger genomes than previously possible.\nWe use Weights & Biases to track these experiments, and have aggregated our initial results in the W&B Report below.\nWe can evaluate the performance of our model by looking at two different metrics for throughput: samples_per_sec and TFLOPS.\nExplicitly, we see that we are able to scale up to significantly longer sequences (420k / 128k ~ 3.3x) with only a minimal impact on throughput performance (81 / 105 ~ 77\\%)4.\n\n\n\nTable 2: Impact on TFLOPS as a function of increasing sequence length. Table from: throughput/TFLOPS\n\n\n\n\n\n\n\n\n\n\n\n\nName\nSequence Length (k)\n(seq_len / min_seq_len)\nTFLOPS\nTFLOPS (% of peak)\n\n\n\n\nGPT25B\n420\n3.28125\n81.77225\n77.867\n\n\nGPT25B\n400\n3.125\n90.62\n86.297\n\n\nGPT25B\n360\n2.8125\n81.6325\n77.7348\n\n\nGPT25B\n360\n2.8125\n82.6824\n78.7346\n\n\nGPT25B\n192\n1.5\n115.8228\n110.2927\n\n\nGPT25B\n128\n1\n106.672\n101.5788\n\n\nGPT25B\n128\n1\n105.014\n100.00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Weights & Biases Report\n\n\n\n\n\n\nFigure 1: This work was done as part of the DeepSpeed4Science project, in collaboration with Microsoft.\nGPT-25B Model\nGPT-33B Model"
  },
  {
    "objectID": "qmd/dsblog.html#footnotes",
    "href": "qmd/dsblog.html#footnotes",
    "title": "Loooooooong Sequence Lengths",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe described experiments were performed on 4 NVIDIA DGX A100-40GB nodes, all using TPSIZE=32[^tpsize], connected through 8 HDR InfiniBand (200Gb/s per HDR).↩︎\n\n\ndeepspeed-0.10.3\npytorch==2.0.0+cu118\n\n↩︎\nWhere \"${MACHINE}\" \\in {\"ThetaGPU\", \"Polaris\"} and \"${CONDA_DATE}\" \\in {\"2023-01-10\", \"2023-01-11\"}↩︎\nthroughput/TFLOPS↩︎"
  },
  {
    "objectID": "qmd/aurora-gpt/megatron-ds-intel.html#install-setup",
    "href": "qmd/aurora-gpt/megatron-ds-intel.html#install-setup",
    "title": "Megatron DeepSpeed on xpu",
    "section": "Install / Setup",
    "text": "Install / Setup\n\nSetup script / history:\n\n\nInteractive Session\n\n$ export HTTP_PROXY=http://proxy.alcf.anl.gov:3128\n$ export HTTPS_PROXY=http://proxy.alcf.anl.gov:3128\n$ export http_proxy=http://proxy.alcf.anl.gov:3128\n$ export https_proxy=http://proxy.alcf.anl.gov:3128\n\n$ export DRM_LIB=\"$(pwd)/usr/include/libdrm\"\n# $ export PATH=\"${HOME}/miniconda3/bin:$PATH\"\n\n$ conda create --name anl_release_q4 python=3.9 --y\n$ wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh\n$ bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3\n$ eval \"$(${HOME} shell.zsh hook)\"\n\n$ export DRM_LIB=\"$(pwd)/usr/include/libdrm\"\n$ conda config --add channels conda-forge && conda install -c conda-forge mpi4py -y --no-deps && conda install -c conda-forge libssh -y && conda uninstall mpi -y && python3 -m pip install -r requirements.txt && python3 -m pip install *.whl\n\n$ module unload oneapi/eng-compiler/2022.12.30.003\n$ module unload intel_compute_runtime/release/agama-devel-551\n$ module use -a /soft/modulefiles\n$ module load oneapi/release/2023.12.15.001\n$ module use /home/ftartagl/graphics-compute-runtime/modulefiles\n$ module load graphics-compute-runtime/agama-ci-devel-736.9\n# one-liner for modules:\n# module unload oneapi/eng-compiler/2022.12.30.003 && module unload intel_compute_runtime/release/agama-devel-551&& module use -a /soft/modulefiles && module load oneapi/release/2023.12.15.001 && module use /home/ftartagl/graphics-compute-runtime/modulefiles && module load graphics-compute-runtime/agama-ci-devel-736.9\n\n$ cd torch-ccl\n$ ls\n$ COMPUTE_BACKEND=dpcpp python3 setup.py develop |& tee build.log\n$ cd ../\n\n$ cd intel-extension-for-deepspeed\n$ python3 setup.py develop |& tee build.log\n$ cd ../\n\n$ cd DeepSpeed\n$ ls\n$ python3 -m pip install -r requirements/requirements.txt\n$ python3 setup.py develop |& tee build.log"
  },
  {
    "objectID": "qmd/aurora-gpt/megatron-ds-intel.html#running",
    "href": "qmd/aurora-gpt/megatron-ds-intel.html#running",
    "title": "Megatron DeepSpeed on xpu",
    "section": "Running",
    "text": "Running\n\nUsing launch\n\nSetup:\n\n\nSetup\n\n$ qsub -q EarlyAppAccess -A Aurora_Deployment -l walltime=08:00:00 -l select=2 -I\nqsub: waiting for job 604319.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov to start\nqsub: job 604319.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov ready\n\n$ module unload oneapi/eng-compiler/2022.12.30.003 && module unload intel_compute_runtime/release/agama-devel-551&& module use -a /soft/modulefiles && module load oneapi/release/2023.12.15.001 && module use /home/ftartagl/graphics-compute-runtime/modulefiles && module load graphics-compute-runtime/agama-ci-devel-736.9\n UMD: agama-ci-devel-736.9 successfully loaded:\n UMD: graphics-compute-runtime/agama-ci-devel-736.9\n\n$ eval \"$(/home/foremans/miniconda3/bin/conda shell.zsh hook)\"\n\n$ conda activate anl_release_q4\n\n$ git clone https://github.com/saforem2/ezpz\n$ python3 -m pip install -e \"ezpz[dev]\"\n# [BUG] for some reason, need to run twice ¯\\_(ツ)_/¯\n$ source ./ezpz/src/ezpz/bin/savejobenv && source ./ezpz/src/ezpz/bin/savejobenv\n\n\nOutput\n\n┌───────────────────────────────────────────────────────────────────\n│ Writing PBS vars to /home/foremans/.pbsenv\n│ HOSTFILE: /var/spool/pbs/aux/604319.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\n│ NHOSTS: 2\n│ NGPU_PER_HOST: 12 GPUs per host\n│ NGPUS: 24 GPUs total\n└───────────────────────────────────────────────────────────────────\n┌───────────────────────────────────────────────────────────────────\n│ [DIST INFO]:\n│   • Writing Job info to /home/foremans/.pbsenv\n│     • HOSTFILE: /var/spool/pbs/aux/604319.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\n│     • NHOSTS: 2\n│     • NGPU_PER_HOST: 12\n│     • NGPUS = (NHOSTS * NGPU_PER_HOST) = 24\n│ [Hosts]:\n│       • x4502c0s0b0n0.hostmgmt2502.cm.aurora.alcf.anl.gov, x4502c0s2b0n0.hostmgmt2502.cm.aurora.alcf.anl.gov\n│ [Launch]:\n│     • Use: 'launch' (=mpiexec --verbose --envall -n 24 -ppn 12 --hostfile /var/spool/pbs/aux/604319.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov)\n│       to launch job\n└───────────────────────────────────────────────────────────────────\n┌────────────────────────────────────────────────────────────────────────────────\n│ YOU ARE HERE: /home/foremans\n│ Run 'source ./bin/getjobenv' in a NEW SHELL to automatically set env vars\n└────────────────────────────────────────────────────────────────────────────────\n\n\n\n[WIP] Building out python API\n\n$ python3 -m ezpz.savejobenv\n/home/foremans/miniconda3/envs/anl_release_q4/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'If you dont plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?'\n  warn(\n[2024-01-23 10:02:37][INFO][jobs:185] - Saving job env to /home/foremans/PBS-jobs/604319/jobenv.sh\n[2024-01-23 10:02:37][INFO][jobs:193] - Saving job env to dot-env (.env) file in /home/foremans\n[2024-01-23 10:02:37][INFO][jobs:211] - Saving job env to /home/foremans/PBS-jobs/604319/jobenv.json\n[2024-01-23 10:02:37][INFO][jobs:225] - Saving job env to /home/foremans/PBS-jobs/604319/jobenv.yaml\n[2024-01-23 10:02:37][INFO][jobs:253] - Writing PBS env vars to /home/foremans/PBS-jobs/604319 / jobenv{.sh, .yaml, .json}\n[2024-01-23 10:02:37][INFO][jobs:258] - jobenv={\n    \"BACKEND\": \"gloo\",\n    \"DEVICE\": \"xpu\",\n    \"DEVICE_ID\": \"xpu:0\",\n    \"DISTRIBUTED_BACKEND\": \"gloo\",\n    \"FRAMEWORK\": \"pytorch\",\n    \"GPUS_PER_NODE\": 12,\n    \"HOSTFILE\": \"/var/spool/pbs/aux/604319.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\",\n    \"HOSTNAME\": \"x4502c0s0b0n0.hostmgmt2502.cm.aurora.alcf.anl.gov\",\n    \"HOSTS\": \"[x4502c0s0b0n0, x4502c0s2b0n0]\",\n    \"LAUNCH_CMD\": \"mpiexec --verbose --envall -n 24 -ppn 12 --hostfile /var/spool/pbs/aux/604319.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\",\n    \"LOCAL_RANK\": 0,\n    \"MACHINE\": \"Aurora\",\n    \"NGPUS\": 24,\n    \"NGPU_PER_HOST\": \"12\",\n    \"NHOSTS\": \"2\",\n    \"NODE_ID\": 0,\n    \"NUM_NODES\": 2,\n    \"PBS_ACCOUNT\": \"Aurora_Deployment\",\n    \"PBS_ENVIRONMENT\": \"PBS_INTERACTIVE\",\n    \"PBS_HOOK_RESOURCES\": \"eJyVUV1vwjAM/EOblHZbgUV54yfs3TKp22bkozgJqP9+LjAJ9jYpD7HvfL5L0Pt0AbQ21VjATmSPMKDzlck0Gq9opBGLOxOspZVriit2Qe5BShoTL2bvsmVaMeTlDpZlpj/AoXIEXjWM0rYyk6x90H1tPza73a7rNq1SejoECKknM3gsepptABdwJGNTmGshKJQLtKp9V03TfMnlremgUUO3lelo55pNq6MDppwqWzJYOTHqKKK2CJbOxKsncTN7FEIWI4VYz5y+hQIzu8SuLMLltK48oD0Oznt5gkz+ppKjvfk8Vex1SQX9YyilL1IVF8io7adScvSpUiV4Dnjv/TPmberZQiaWYDDKdyYY8tXrtTOlQE+NM3rXgwSivORCIZs75eV3+Ady/coN\",\n    \"PBS_JOBCOOKIE\": \"6B8C4F9D774B0AA5174EAAFB6E2CC14F\",\n    \"PBS_JOBDIR\": \"/home/foremans\",\n    \"PBS_JOBID\": \"604319.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\",\n    \"PBS_JOBNAME\": \"STDIN\",\n    \"PBS_MOMPORT\": \"15003\",\n    \"PBS_NODEFILE\": \"/var/spool/pbs/aux/604319.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\",\n    \"PBS_NODENUM\": \"0\",\n    \"PBS_O_HOME\": \"/home/foremans\",\n    \"PBS_O_HOST\": \"aurora-uan-0010.hostmgmt1000.cm.aurora.alcf.anl.gov\",\n    \"PBS_O_LANG\": \"en_US.UTF-8\",\n    \"PBS_O_LOGNAME\": \"foremans\",\n    \"PBS_O_MAIL\": \"/var/spool/mail/foremans\",\n    \"PBS_O_PATH\": \"/home/foremans/.nvm/versions/node/v21.5.0/bin:/home/foremans/homebrew/bin:/home/foremans/homebrew/sbin:/opt/cray/pals/1.3.2/bin:/opt/cray/libfabric/1.15.2.0/bin:/opt/aurora/23.073.0/support/libraries/intel-compute-samples/2021.27.01:/opt/aurora/23.073.0/support/libraries/khronos/clinfo/default/bin:/opt/aurora/23.073.0/support/tools/gpu_validation:/opt/aurora/23.073.0/intel-gpu-umd/agama-devel-551/compiler/bin:/opt/aurora/23.073.0/intel-gpu-umd/agama-devel-551/driver/bin:/opt/aurora/23.073.0/CNDA/mpich/51.2/mpich-ofi-all-icc-default-pmix-gpu-drop51/bin:/opt/aurora/23.073.0/support/tools/mpi_wrapper_utils:/opt/aurora/23.073.0/oneapi/debugger/2023.0.0/gdb/intel64/bin:/opt/aurora/23.073.0/CNDA/oneapi/compiler/trunk-20230201/dpcpp-ct/bin:/opt/aurora/23.073.0/oneapi/advisor/2023.0.0/bin64:/opt/aurora/23.073.0/CNDA/oneapi/vtune/2023.0.0_624810_nda/bin64:/opt/aurora/23.073.0/CNDA/oneapi/compiler/trunk-20230201/compiler/linux/bin/intel64:/opt/aurora/23.073.0/CNDA/oneapi/compiler/trunk-20230201/compiler/linux/bin:/opt/aurora/23.073.0/oneapi/inspector/2023.0.0/bin64:/opt/cray/pe/gcc/11.2.0/snos/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/home/foremans/.local/bin:/home/foremans/bin:/usr/local/bin:/usr/bin:/bin:/opt/c3/bin:/usr/lib/mit/bin:/usr/lib/mit/sbin:/opt/pbs/bin:/sbin:/home/foremans/.local/share/kitty-ssh-kitten/kitty/bin:/home/foremans/.cargo/bin:/home/foremans/.fzf/bin:/home/foremans/.luarocks/bin:/home/foremans/.luarocks/bin\",\n    \"PBS_O_QUEUE\": \"EarlyAppAccess\",\n    \"PBS_O_SHELL\": \"/bin/zsh\",\n    \"PBS_O_SYSTEM\": \"Linux\",\n    \"PBS_O_TZ\": \"America/Chicago\",\n    \"PBS_O_WORKDIR\": \"/home/foremans\",\n    \"PBS_QUEUE\": \"LustreApps\",\n    \"PBS_TASKNUM\": \"1\",\n    \"RANK\": 0,\n    \"SCHEDULER\": \"PBS\",\n    \"WORLD_SIZE_IN_USE\": 1,\n    \"WORLD_SIZE_TOTAL\": 24,\n    \"jobfile_json\": \"/home/foremans/PBS-jobs/604319/jobenv.json\",\n    \"jobfile_sh\": \"/home/foremans/PBS-jobs/604319/jobenv.sh\",\n    \"jobfile_yaml\": \"/home/foremans/PBS-jobs/604319/jobenv.yaml\"\n}\n\n$ source \"$(tail -1 ~/PBS-jobs.log)/jobenv.sh\"\n$ which launch\nlaunch: aliased to mpiexec --verbose --envall -n 24 -ppn 12 --hostfile /var/spool/pbs/aux/604319.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\nTake 1: Crash with\nRuntimeError: oneCCL: global.cpp:150 getenv_local_coord: EXCEPTION: to get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\n\n\nRun:\n\n$ launch python3 pretrain_llama.py \\\n    --tensor-model-parallel-size 1 \\\n    --pipeline-model-parallel-size 1 \\\n    --num-layers 32 \\\n    --hidden-size 4096 \\\n    --ffn-hidden-size 5504 \\\n    --num-attention-heads 32 \\\n    --micro-batch-size 1 \\\n    --global-batch-size 24 \\\n    --seq-length 2048 \\\n    --max-position-embeddings 2048 \\\n    --train-iters 250000 \\\n    --save /lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/checkpoints/LLAMA_7B_LLAMA_7B_z3_seqlen_mp1_pp1_sp24_nl32_hs4096_gb24_mb1 \\\n    --load /lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/checkpoints/LLAMA_7B_LLAMA_7B_z3_seqlen_mp1_pp1_sp24_nl32_hs4096_gb24_mb1 \\\n    --data-path \\\n    --data-impl mmap \\\n    --tokenizer-type GPTSentencePieceTokenizer \\\n    --tokenizer-model ./tmp/tokenizer.model \\\n    --split 949,50,1 \\\n    --distributed-backend ccl \\\n    --lr 3e-4 \\\n    --lr-decay-style cosine \\\n    --min-lr 3e-5 \\\n    --weight-decay 0.1 \\\n    --clip-grad 1 \\\n    --lr-warmup-iters 2000 \\\n    --optimizer adam \\\n    --adam-beta1 0.9 \\\n    --adam-beta2 0.95 \\\n    --log-interval 1 \\\n    --save-interval 10000 \\\n    --eval-interval 1000 \\\n    --eval-iters 10 \\\n    --bf16 \\\n    --no-query-key-layer-scaling \\\n    --attention-dropout 0 \\\n    --hidden-dropout 0 \\\n    --use-rotary-position-embeddings \\\n    --untie-embeddings-and-output-weightss \\\n    --swiglus \\\n    --normalization rmsnorms \\\n    --disable-bias-linears \\\n    --num-key-value-heads 4s \\\n    --tensorboard-dir /lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/outputs/LLAMA_7B_LLAMA_7B_z3_seqlen_mp1_pp1_sp24_nl32_ \\\n    hs4096_gb24_mb1/tensorboard \\\n    --log-timers-to-tensorboard \\\n    --tensorboard-log-interval 1 \\\n    --data-path /lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/dataset/BookCorpusDataset_text_documents \\\n    --vocab-file /lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/dataset/gpt2-vocab.json \\\n    --merge-file /lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/dataset/gpt2-merges.txt \\\n    --zero-stage=3 \\\n    --deepspeed_config=/lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/deepspeed.json \\\n    --deepspeed\n\nConnected to tcp://x4502c1s0b0n0.hostmgmt2502.cm.aurora.alcf.anl.gov:7919\nFound executable /home/foremans/miniconda3/envs/anl_release_q4/bin/python3\nLaunching application 2e559157-da5e-4185-9902-dc8d932e8bb3\n/home/foremans/miniconda3/envs/anl_release_q4/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'If you dont plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?'\n  warn(\n[2024-01-23 00:02:13,326] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to xpu (auto detect)\n[2024-01-23 00:02:19,177] [INFO] [comm.py:161:init_deepspeed_backend] Initialize ccl backend\n[2024-01-23 00:02:19,177] [INFO] [comm.py:637:init_distributed] cdb=None\n[2024-01-23 00:02:19,177] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n[2024-01-23 00:02:19,888] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=3, local_rank=3, world_size=24, master_addr=10.115.53.137, master_port=29500\n[2024-01-23 00:02:19,888] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=9, local_rank=9, world_size=24, master_addr=10.115.53.137, master_port=29500\n[2024-01-23 00:02:19,888] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=11, local_rank=11, world_size=24, master_addr=10.115.53.137, master_port=29500\n[2024-01-23 00:02:19,888] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=1, local_rank=1, world_size=24, master_addr=10.115.53.137, master_port=29500\n[2024-01-23 00:02:19,888] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=2, local_rank=2, world_size=24, master_addr=10.115.53.137, master_port=29500\n[2024-01-23 00:02:19,888] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=4, local_rank=4, world_size=24, master_addr=10.115.53.137, master_port=29500\n[2024-01-23 00:02:19,888] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=5, local_rank=5, world_size=24, master_addr=10.115.53.137, master_port=29500\n[2024-01-23 00:02:19,888] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=6, local_rank=6, world_size=24, master_addr=10.115.53.137, master_port=29500\n[2024-01-23 00:02:19,888] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=7, local_rank=7, world_size=24, master_addr=10.115.53.137, master_port=29500\n[2024-01-23 00:02:19,888] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=13, local_rank=1, world_size=24, master_addr=10.115.53.137, master_port=29500\n[2024-01-23 00:02:19,888] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=8, local_rank=8, world_size=24, master_addr=10.115.53.137, master_port=29500\n[2024-01-23 00:02:19,888] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=10, local_rank=10, world_size=24, master_addr=10.115.53.137, master_port=29500\n[2024-01-23 00:02:19,888] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=24, master_addr=10.115.53.137, master_port=29500\n[2024-01-23 00:02:19,889] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend ccl\n[2024-01-23 00:02:19,888] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=15, local_rank=3, world_size=24, master_addr=10.115.53.137, master_port=29500\n[2024-01-23 00:02:19,888] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=17, local_rank=5, world_size=24, master_addr=10.115.53.137, master_port=29500\n[2024-01-23 00:02:19,888] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=19, local_rank=7, world_size=24, master_addr=10.115.53.137, master_port=29500\n[2024-01-23 00:02:19,888] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=21, local_rank=9, world_size=24, master_addr=10.115.53.137, master_port=29500\n[2024-01-23 00:02:19,888] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=12, local_rank=0, world_size=24, master_addr=10.115.53.137, master_port=29500\n[2024-01-23 00:02:19,888] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=14, local_rank=2, world_size=24, master_addr=10.115.53.137, master_port=29500\n[2024-01-23 00:02:19,888] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=16, local_rank=4, world_size=24, master_addr=10.115.53.137, master_port=29500\n[2024-01-23 00:02:19,888] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=18, local_rank=6, world_size=24, master_addr=10.115.53.137, master_port=29500\n[2024-01-23 00:02:19,888] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=20, local_rank=8, world_size=24, master_addr=10.115.53.137, master_port=29500\n[2024-01-23 00:02:19,888] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=22, local_rank=10, world_size=24, master_addr=10.115.53.137, master_port=29500\n[2024-01-23 00:02:19,888] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=23, local_rank=11, world_size=24, master_addr=10.115.53.137, master_port=29500\n[2024-01-23 00:02:20][INFO][dist:257] - DistInfo={\n    \"DEVICE\": \"xpu\",\n    \"DEVICE_ID\": \"xpu:0\",\n    \"DISTRIBUTED_BACKEND\": \"gloo\",\n    \"GPUS_PER_NODE\": 12,\n    \"HOSTFILE\": \"/var/spool/pbs/aux/604213.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\",\n    \"HOSTNAME\": \"x4502c1s0b0n0.hostmgmt2502.cm.aurora.alcf.anl.gov\",\n    \"HOSTS\": \"['x4502c1s0b0n0', 'x4502c1s3b0n0']\",\n    \"LOCAL_RANK\": 0,\n    \"MACHINE\": \"Aurora\",\n    \"NGPUS\": 24,\n    \"NODE_ID\": 0,\n    \"NUM_NODES\": 2,\n    \"RANK\": 0,\n    \"SCHEDULER\": \"PBS\",\n    \"WORLD_SIZE_IN_USE\": 24,\n    \"WORLD_SIZE_TOTAL\": 24\n}\n[2024-01-23 00:02:20,987] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to xpu (auto detect)\n--------------------------------------------------\nDeepSpeed C++/CUDA extension op report\n--------------------------------------------------\nNOTE: Ops not installed will be just-in-time (JIT) compiled at\n      runtime if needed. Op compatibility means that your system\n      meet the required dependencies to JIT install the op.\n--------------------------------------------------\nJIT compiled ops requires ninja\nninja .................. [OKAY]\n--------------------------------------------------\nop name ................ installed .. compatible\n--------------------------------------------------\n[2024-01-23 00:02:20][INFO][spawn:38] - icx -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/foremans/miniconda3/envs/anl_release_q4/include -fPIC -O2 -isystem /home/foremans/miniconda3/envs/anl_release_q4/include -fPIC -c /tmp/tmph01efr3s/test.c -o /tmp/tmph01efr3s/test.o\nWARNING: TensorBoard writing requested but is not available (are you using PyTorch 1.1.0 or later?), no TensorBoard logs will be written.\n2024:01:23-00:02:21:(122507) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL\n2024:01:23-00:02:21:(122510) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed to get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\n2024:01:23-00:02:21:(122515) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nto get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\n2024:01:23-00:02:21:(122507) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nto get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\n2024:01:23-00:02:21:(122508) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nto get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\n2024:01:23-00:02:21:(122509) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nto get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\n2024:01:23-00:02:21:(122511) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nto get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\n2024:01:23-00:02:21:(122512) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nto get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\n2024:01:23-00:02:21:(122513) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nto get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\n2024:01:23-00:02:21:(122514) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nto get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\n2024:01:23-00:02:21:(122516) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nto get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\n2024:01:23-00:02:21:(122517) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nto get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\n2024:01:23-00:02:21:(122518) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nto get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\n2024:01:23-00:02:21:(141071) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL\n2024:01:23-00:02:21:(141072) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL\n2024:01:23-00:02:21:(141073) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL\n2024:01:23-00:02:21:(141075) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL\n2024:01:23-00:02:21:(141076) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL\n2024:01:23-00:02:21:(141078) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL\n2024:01:23-00:02:21:(141079) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL\n2024:01:23-00:02:21:(141081) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL\n2024:01:23-00:02:21:(141074) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL\n2024:01:23-00:02:21:(141077) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL\n2024:01:23-00:02:21:(141080) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL\n2024:01:23-00:02:21:(141071) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nto get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\n2024:01:23-00:02:21:(141075) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nto get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\n2024:01:23-00:02:21:(141078) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nto get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\n2024:01:23-00:02:21:(141081) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nto get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\n2024:01:23-00:02:21:(141072) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nto get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\n2024:01:23-00:02:21:(141073) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nto get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\n2024:01:23-00:02:21:(141074) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nto get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\n2024:01:23-00:02:21:(141076) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nto get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\n2024:01:23-00:02:21:(141077) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nto get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\n2024:01:23-00:02:21:(141079) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nto get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\n2024:01:23-00:02:21:(141080) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nto get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\n&gt;fused kernel is only supported in cuda, skip loading fused kernel\n&gt;fused kernel is only supported in cuda, skip loading fused kernel\n&gt;fused kernel is only supported in cuda, skip loading fused kernel\n&gt;fused kernel is only supported in cuda, skip loading fused kernel\n&gt;fused kernel is only supported in cuda, skip loading fused kernel\n&gt;fused kernel is only supported in cuda, skip loading fused kernel\n&gt;fused kernel is only supported in cuda, skip loading fused kernel\n&gt;fused kernel is only supported in cuda, skip loading fused kernel\n&gt;fused kernel is only supported in cuda, skip loading fused kernel\n&gt;fused kernel is only supported in cuda, skip loading fused kernel\n&gt;fused kernel is only supported in cuda, skip loading fused kernel\n&gt;fused kernel is only supported in cuda, skip loading fused kernel\nTraceback (most recent call last):\n  File \"/lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/pretrain_llama.py\", line 583, in &lt;module&gt;\n    model = main()\n  File \"/lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/pretrain_llama.py\", line 561, in main\n    model = pretrain(\n  File \"/lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/megatron/training.py\", line 136, in pretrain\n    torch.distributed.all_reduce(start_time_tensor,\n  File \"/home/foremans/miniconda3/envs/anl_release_q4/lib/python3.9/site-packages/torch/distributed/c10d_logger.py\", line 47, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/foremans/miniconda3/envs/anl_release_q4/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py\", line 2050, in all_reduce\n    work = group.allreduce([tensor], opts)\n\nRuntimeError: oneCCL: global.cpp:150 getenv_local_coord: EXCEPTION: to get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\n\n&lt;/details&gt;\nTake 2: Trying with CCL_ZE_IPC_EXCHANGE=sockets (still no luck)\n\n\nRun:\n\n$ CCL_ZE_IPC_EXCHANGE=sockets !!\n[...]\n2024:01:23-00:03:41:(123335) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL\n2024:01:23-00:03:41:(123330) |CCL_WARN| sockets exchange mode is set. It may cause potential problem of 'Too many open file descriptors'\n2024:01:23-00:03:41:(123337) |CCL_WARN| sockets exchange mode is set. It may cause potential problem of 'Too many open file descriptors'\n2024:01:23-00:03:41:(123327) |CCL_WARN| sockets exchange mode is set. It may cause potential problem of 'Too many open file descriptors'\n[...]\nRuntimeError: oneCCL: global.cpp:150 getenv_local_coord: EXCEPTION: to get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly"
  },
  {
    "objectID": "qmd/aurora-gpt/megatron-ds-intel.html#using-deepspeed",
    "href": "qmd/aurora-gpt/megatron-ds-intel.html#using-deepspeed",
    "title": "Megatron DeepSpeed on xpu",
    "section": "Using deepspeed",
    "text": "Using deepspeed\n\nSetup:\n\n\nSetup:\n\n$ cat $PBS_NODEFILE &gt; hostfile ; sed -e 's/$/ slots=12/' -i hostfile\n$ echo \"PATH=${PATH}\" &gt;&gt; .deepspeed_env ; echo \"LD_LIBRARY_PATH=${LD_LIBRARY_PATH}\" &gt;&gt; .deepspeed_env ; echo \"http_proxy=${http_proxy}\" &gt;&gt; .deepspeed_env ; echo \"https_proxy=${https_proxy}\" &gt;&gt; .deepspeed_env\nRun:\n\nCommand:\n  $ RANK=0 LOCAL_RANK=0 MASTER_ADDR=localhost deepspeed --hostfile hostfile pretrain_llama.py --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 32 --hidden-size 4096 --ffn-hidden-size 5504 --num-attention-heads 32 --micro-batch-size 1 --global-batch-size 24 --seq-length 2048 --max-position-embeddings 2048 --train-iters 250000 --save /lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/checkpoints/LLAMA_7B_LLAMA_7B_z3_seqlen_mp1_pp1_sp24_nl32_hs4096_gb24_mb1 --load /lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/checkpoints/LLAMA_7B_LLAMA_7B_z3_seqlen_mp1_pp1_sp24_nl32_hs4096_gb24_mb1 --data-path --data-impl mmap --tokenizer-type GPTSentencePieceTokenizer --tokenizer-model ./tmp/tokenizer.model --split 949,50,1 --distributed-backend ccl --lr 3e-4 --lr-decay-style cosine --min-lr 3e-5 --weight-decay 0.1 --clip-grad 1 --lr-warmup-iters 2000 --optimizer adam --adam-beta1 0.9 --adam-beta2 0.95 --log-interval 1 --save-interval 10000 --eval-interval 1000 --eval-iters 10 --bf16 --no-query-key-layer-scaling --attention-dropout 0 --hidden-dropout 0 --use-rotary-position-embeddings --untie-embeddings-and-output-weights --swiglu --normalization rmsnorm --disable-bias-linear --num-key-value-heads 4 --tensorboard-dir /lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/outputs/LLAMA_7B_LLAMA_7B_z3_seqlen_mp1_pp1_sp24_nl32_hs4096_gb24_mb1/tensorboard --log-timers-to-tensorboard --tensorboard-log-interval 1 --data-path /lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/dataset/BookCorpusDataset_text_document --vocab-file /lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/dataset/gpt2-vocab.json --merge-file /lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/dataset/gpt2-merges.txt --zero-stage=3 --deepspeed_config=/lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/deepspeed.json --deepspeed\nOutput:\n\n\nOutput\n\n$ RANK=0 LOCAL_RANK=0 MASTER_ADDR=localhost deepspeed --hostfile hostfile pretrain_llama.py --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 32 --hidden-size 4096 --ffn-hidden-size 5504 --num-attention-heads 32 --micro-batch-size 1 --global-batch-size 24 --seq-length 2048 --max-position-embeddings 2048 --train-iters 250000 --save /lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/checkpoints/LLAMA_7B_LLAMA_7B_z3_seqlen_mp1_pp1_sp24_nl32_hs4096_gb24_mb1 --load /lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/checkpoints/LLAMA_7B_LLAMA_7B_z3_seqlen_mp1_pp1_sp24_nl32_hs4096_gb24_mb1 --data-path --data-impl mmap --tokenizer-type GPTSentencePieceTokenizer --tokenizer-model ./tmp/tokenizer.model --split 949,50,1 --distributed-backend ccl --lr 3e-4 --lr-decay-style cosine --min-lr 3e-5 --weight-decay 0.1 --clip-grad 1 --lr-warmup-iters 2000 --optimizer adam --adam-beta1 0.9 --adam-beta2 0.95 --log-interval 1 --save-interval 10000 --eval-interval 1000 --eval-iters 10 --bf16 --no-query-key-layer-scaling --attention-dropout 0 --hidden-dropout 0 --use-rotary-position-embeddings --untie-embeddings-and-output-weights --swiglu --normalization rmsnorm --disable-bias-linear --num-key-value-heads 4 --tensorboard-dir /lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/outputs/LLAMA_7B_LLAMA_7B_z3_seqlen_mp1_pp1_sp24_nl32_hs4096_gb24_mb1/tensorboard --log-timers-to-tensorboard --tensorboard-log-interval 1 --data-path /lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/dataset/BookCorpusDataset_text_document --vocab-file /lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/dataset/gpt2-vocab.json --merge-file /lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/dataset/gpt2-merges.txt --zero-stage=3 --deepspeed_config=/lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/deepspeed.json --deepspeed \n\nhome/foremans/miniconda3/envs/anl_release_q4/bin/deepspeed:4: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n  __import__('pkg_resources').require('deepspeed==0.12.3+6ea44d02')\n/home/foremans/miniconda3/envs/anl_release_q4/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'If you dont plan on using image functionality from `torchvision.io`, you can igno re this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?'\n  warn(\nMy guessed rank = 0\n[2024-01-23 00:09:56,016] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to xpu (auto detect)\n[2024-01-23 00:10:00,790] [INFO] [runner.py:463:main] Using IP address of 10.115.53.137 for node x4502c1s0b0n0.hostmgmt2502.cm.aurora.alcf.anl.gov\n[2024-01-23 00:10:00,812] [INFO] [runner.py:559:main] deepspeed_env file = .deepspeed_env\n[2024-01-23 00:10:00,813] [INFO] [runner.py:559:main] deepspeed_env file = .deepspeed_env\n[2024-01-23 00:10:00,813] [INFO] [multinode_runner.py:72:get_cmd] Running on the following workers: x4502c1s0b0n0.hostmgmt2502.cm.aurora.alcf.anl.gov,x4502c1s3b0n0.hostmgmt2502.cm.aurora.alcf.anl.gov\n[2024-01-23 00:10:00,813] [INFO] [runner.py:570:main] cmd = pdsh -S -f 1024 -w x4502c1s0b0n0.hostmgmt2502.cm.aurora.alcf.anl.gov,x4502c1s3b0n0.hostmgmt2502.cm.aurora.alcf.anl.gov export PYTHONSTARTUP=/etc/pythonstart; export PYTHONPATH=/l\nus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed:/soft/compilers/oneapi/2023.12.15.001/oneapi/advisor/2024.0/pythonapi; export PATH=/home/ftartagl/graphics-compute-runtime/agama-ci-devel-736.9/u\nsr/bin:/soft/tools/gpu_validation:/soft/libraries/khronos/clinfo/master-13ae34-2020.12.14/bin:/soft/libraries/intel-compute-samples/2021.27.01:/soft/libraries/intel-gpu-umd/stable_736_25_20231031/compiler/bin:/soft/libraries/intel-gpu-umd\n/stable_736_25_20231031/driver/bin:/soft/restricted/CNDA/updates/mpich/52.2/mpich-ofi-all-icc-default-pmix-gpu-drop52/bin:/soft/tools/mpi_wrapper_utils:/soft/compilers/oneapi/2023.12.15.001/oneapi/dpcpp-ct/2024.0/bin:/soft/compilers/oneap\ni/2023.12.15.001/oneapi/advisor/2024.0/bin64:/soft/compilers/oneapi/2023.12.15.001/oneapi/vtune/2024.0/bin64:/soft/compilers/oneapi/2023.12.15.001/oneapi/inspector/2024.0/bin64:/soft/compilers/oneapi/2023.12.15.001/oneapi/debugger/2024.0/\nopt/debugger/bin:/soft/compilers/oneapi/2023.12.15.001/oneapi/mkl/2024.0/bin:/soft/compilers/oneapi/2023.12.15.001/oneapi/compiler/2024.0/bin:/home/foremans/miniconda3/envs/anl_release_q4/bin:/home/foremans/miniconda3/condabin:/home/forem\nans/.nvm/versions/node/v21.5.0/bin:/home/foremans/homebrew/bin:/home/foremans/homebrew/sbin:/opt/cray/pals/1.3.2/bin:/opt/cray/libfabric/1.15.2.0/bin:/opt/cray/pe/gcc/11.2.0/snos/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/b\nin:/home/foremans/.local/bin:/home/foremans/bin:/usr/local/bin:/usr/bin:/bin:/opt/c3/bin:/usr/lib/mit/bin:/usr/lib/mit/sbin:/opt/pbs/bin:/sbin:/home/foremans/.local/share/kitty-ssh-kitten/kitty/bin:/home/foremans/.cargo/bin:/home/foremans\n/.fzf/bin:/home/foremans/.luarocks/bin; export LD_LIBRARY_PATH=/home/ftartagl/graphics-compute-runtime/agama-ci-devel-736.9/usr/lib64/dri:/home/ftartagl/graphics-compute-runtime/agama-ci-devel-736.9/usr/lib64/mfx:/home/ftartagl/graphics-c\nompute-runtime/agama-ci-devel-736.9/usr/lib64/intel-opencl:/home/ftartagl/graphics-compute-runtime/agama-ci-devel-736.9/usr/lib64:/soft/libraries/khronos/loader/master-2022.05.18/lib64:/soft/libraries/intel-gpu-umd/stable_736_25_20231031/\ncompiler/lib64:/soft/libraries/intel-gpu-umd/stable_736_25_20231031/driver/lib64/intel-opencl:/soft/libraries/intel-gpu-umd/stable_736_25_20231031/driver/lib64:/soft/restricted/CNDA/updates/mpich/52.2/mpich-ofi-all-icc-default-pmix-gpu-dr\nop52/lib:/soft/compilers/oneapi/2023.12.15.001/oneapi/ipp/2021.10/lib:/soft/compilers/oneapi/2023.12.15.001/oneapi/ippcp/2021.9/lib:/soft/compilers/oneapi/2023.12.15.001/oneapi/dpl/2022.3/lib:/soft/compilers/oneapi/2023.12.15.001/oneapi/d\nebugger/2024.0/opt/debugger/lib:/soft/compilers/oneapi/2023.12.15.001/oneapi/ccl/2021.11/lib:/soft/compilers/oneapi/2023.12.15.001/oneapi/dal/2024.0/lib:/soft/compilers/oneapi/2023.12.15.001/oneapi/dnnl/2024.0/lib:/soft/compilers/oneapi/2\n023.12.15.001/oneapi/tbb/2021.11/lib/intel64/gcc4.8:/soft/compilers/oneapi/2023.12.15.001/oneapi/mkl/2024.0/lib:/soft/compilers/oneapi/2023.12.15.001/oneapi/compiler/2024.0/opt/compiler/lib:/soft/compilers/oneapi/2023.12.15.001/oneapi/com\npiler/2024.0/lib:/opt/cray/libfabric/1.15.2.0/lib64:/opt/cray/pe/gcc/11.2.0/snos/lib64; export http_proxy=http://proxy-01.pub.alcf.anl.gov:3128; export https_proxy=http://proxy.alcf.anl.gov:3128;  cd /lus/gecko/projects/Aurora_deployment/\nforemans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed; /home/foremans/miniconda3/envs/anl_release_q4/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJ4NDUwMmMxczBiMG4wLmhvc3RtZ210MjUwMi5jbS5hdXJvcmEuYWxjZi5hbmwuZ292IjogWzAsI\nDEsIDIsIDMsIDQsIDUsIDYsIDcsIDgsIDksIDEwLCAxMV0sICJ4NDUwMmMxczNiMG4wLmhvc3RtZ210MjUwMi5jbS5hdXJvcmEuYWxjZi5hbmwuZ292IjogWzAsIDEsIDIsIDMsIDQsIDUsIDYsIDcsIDgsIDksIDEwLCAxMV19 --node_rank=%n --master_addr=10.115.53.137 --master_port=29500 pre\ntrain_llama.py --tensor-model-parallel-size '1' --pipeline-model-parallel-size '1' --num-layers '32' --hidden-size '4096' --ffn-hidden-size '5504' --num-attention-heads '32' --micro-batch-size '1' --global-batch-size '24' --seq-length '20\n48' --max-position-embeddings '2048' --train-iters '250000' --save '/lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/checkpoints/LLAMA_7B_LLAMA_7B_z3_seqlen_mp1_pp1_sp24_nl32_hs4096_gb24_mb1'\n--load '/lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/checkpoints/LLAMA_7B_LLAMA_7B_z3_seqlen_mp1_pp1_sp24_nl32_hs4096_gb24_mb1' --data-path --data-impl 'mmap' --tokenizer-type 'GPTSentence\nPieceTokenizer' --tokenizer-model './tmp/tokenizer.model' --split '949,50,1' --distributed-backend 'ccl' --lr '3e-4' --lr-decay-style 'cosine' --min-lr '3e-5' --weight-decay '0.1' --clip-grad '1' --lr-warmup-iters '2000' --optimizer 'adam\n' --adam-beta1 '0.9' --adam-beta2 '0.95' --log-interval '1' --save-interval '10000' --eval-interval '1000' --eval-iters '10' --bf16 --no-query-key-layer-scaling --attention-dropout '0' --hidden-dropout '0' --use-rotary-position-embeddings\n --untie-embeddings-and-output-weights --swiglu --normalization 'rmsnorm' --disable-bias-linear --num-key-value-heads '4' --tensorboard-dir '/lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/ou\ntputs/LLAMA_7B_LLAMA_7B_z3_seqlen_mp1_pp1_sp24_nl32_hs4096_gb24_mb1/tensorboard' --log-timers-to-tensorboard --tensorboard-log-interval '1' --data-path '/lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-\nDeepSpeed/dataset/BookCorpusDataset_text_document' --vocab-file '/lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/dataset/gpt2-vocab.json' --merge-file '/lus/gecko/projects/Aurora_deployment/f\noremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/dataset/gpt2-merges.txt' --zero-stage=3 --deepspeed_config=/lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/deepspeed.json --deepspeed\nx4502c1s3b0n0: Warning: Permanently added 'x4502c1s3b0n0.hostmgmt2502.cm.aurora.alcf.anl.gov,10.115.53.138' (ECDSA) to the list of known hosts.\n\nx4502c1s0b0n0: /home/foremans/miniconda3/envs/anl_release_q4/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io\n`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\nx4502c1s0b0n0:   warn(\nx4502c1s3b0n0: /home/foremans/miniconda3/envs/anl_release_q4/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\nx4502c1s3b0n0:   warn(\nx4502c1s0b0n0: [2024-01-23 06:10:07,853] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to xpu (auto detect)\nx4502c1s0b0n0: [2024-01-23 06:10:08,419] [INFO] [launch.py:145:main] WORLD INFO DICT: {'x4502c1s0b0n0.hostmgmt2502.cm.aurora.alcf.anl.gov': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], 'x4502c1s3b0n0.hostmgmt2502.cm.aurora.alcf.anl.gov': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]}\nx4502c1s0b0n0: [2024-01-23 06:10:08,419] [INFO] [launch.py:151:main] nnodes=2, num_local_procs=12, node_rank=0\nx4502c1s0b0n0: [2024-01-23 06:10:08,419] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(&lt;class 'list'&gt;, {'x4502c1s0b0n0.hostmgmt2502.cm.aurora.alcf.anl.gov': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], 'x4502c1s3b0n0.hostmgmt2502.cm.aurora.alcf.anl.gov': [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]})\nx4502c1s0b0n0: [2024-01-23 06:10:08,419] [INFO] [launch.py:163:main] dist_world_size=24\nx4502c1s0b0n0: [2024-01-23 06:10:08,419] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7,8,9,10,11\nx4502c1s3b0n0: [2024-01-23 06:10:08,885] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to xpu (auto detect)\nx4502c1s3b0n0: [2024-01-23 06:10:09,452] [INFO] [launch.py:145:main] WORLD INFO DICT: {'x4502c1s0b0n0.hostmgmt2502.cm.aurora.alcf.anl.gov': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], 'x4502c1s3b0n0.hostmgmt2502.cm.aurora.alcf.anl.gov': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]}\nx4502c1s3b0n0: [2024-01-23 06:10:09,452] [INFO] [launch.py:151:main] nnodes=2, num_local_procs=12, node_rank=1\nx4502c1s3b0n0: [2024-01-23 06:10:09,452] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(&lt;class 'list'&gt;, {'x4502c1s0b0n0.hostmgmt2502.cm.aurora.alcf.anl.gov': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], 'x4502c1s3b0n0.hostmgmt2502.cm.aurora.alcf.anl.gov': [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]})\nx4502c1s3b0n0: [2024-01-23 06:10:09,452] [INFO] [launch.py:163:main] dist_world_size=24\nx4502c1s3b0n0: [2024-01-23 06:10:09,452] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7,8,9,10,11\nx4502c1s0b0n0: My guessed rank = 4\nx4502c1s0b0n0: My guessed rank = 9\nx4502c1s0b0n0: My guessed rank = 8\nx4502c1s0b0n0: My guessed rank = 0\nx4502c1s0b0n0: My guessed rank = 6\nx4502c1s0b0n0: My guessed rank = 7\nx4502c1s0b0n0: My guessed rank = 11\nx4502c1s0b0n0: My guessed rank = 5\nx4502c1s0b0n0: My guessed rank = 10\nx4502c1s0b0n0: My guessed rank = 3\nx4502c1s0b0n0: My guessed rank = 2\nx4502c1s0b0n0: My guessed rank = 1\nx4502c1s3b0n0: My guessed rank = 21\nx4502c1s3b0n0: My guessed rank = 18\nx4502c1s3b0n0: My guessed rank = 22\nx4502c1s3b0n0: My guessed rank = 20\nx4502c1s3b0n0: My guessed rank = 14\nx4502c1s3b0n0: My guessed rank = 12\nx4502c1s3b0n0: My guessed rank = 23\nx4502c1s3b0n0: My guessed rank = 16\nx4502c1s3b0n0: My guessed rank = 17\nx4502c1s3b0n0: My guessed rank = 19\nx4502c1s3b0n0: My guessed rank = 15\nx4502c1s3b0n0: My guessed rank = 13\nx4502c1s0b0n0: [2024-01-23 06:10:14,751] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to xpu (auto detect)\nx4502c1s0b0n0: [2024-01-23 06:10:19,225] [INFO] [comm.py:161:init_deepspeed_backend] Initialize ccl backend\nx4502c1s0b0n0: [2024-01-23 06:10:19,225] [INFO] [comm.py:637:init_distributed] cdb=None\nx4502c1s0b0n0: [2024-01-23 06:10:20,891] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend ccl\nx4502c1s0b0n0: [2024-01-23 06:10:21][INFO][dist:257] - DistInfo={\nx4502c1s0b0n0:     \"DEVICE\": \"xpu\",\nx4502c1s0b0n0:     \"DEVICE_ID\": \"xpu:0\",\nx4502c1s0b0n0:     \"DISTRIBUTED_BACKEND\": \"gloo\",\nx4502c1s0b0n0:     \"GPUS_PER_NODE\": 12,\nx4502c1s0b0n0:     \"HOSTFILE\": \"/var/spool/pbs/aux/604213.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\",\nx4502c1s0b0n0:     \"HOSTNAME\": \"x4502c1s0b0n0.hostmgmt2502.cm.aurora.alcf.anl.gov\",\nx4502c1s0b0n0:     \"HOSTS\": \"['x4502c1s0b0n0', 'x4502c1s3b0n0']\",\nx4502c1s0b0n0:     \"LOCAL_RANK\": 0,\nx4502c1s0b0n0:     \"MACHINE\": \"Aurora\",\nx4502c1s0b0n0:     \"NGPUS\": 24,\nx4502c1s0b0n0:     \"NODE_ID\": 0,\nx4502c1s0b0n0:     \"NUM_NODES\": 2,\nx4502c1s0b0n0:     \"RANK\": 0,\nx4502c1s0b0n0:     \"SCHEDULER\": \"PBS\",\nx4502c1s0b0n0:     \"WORLD_SIZE_IN_USE\": 1,\nx4502c1s0b0n0:     \"WORLD_SIZE_TOTAL\": 24\nx4502c1s0b0n0: }\nx4502c1s0b0n0: [2024-01-23 06:10:21,533] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to xpu (auto detect)\nx4502c1s0b0n0: --------------------------------------------------\nx4502c1s0b0n0: DeepSpeed C++/CUDA extension op report\nx4502c1s0b0n0: --------------------------------------------------\nx4502c1s0b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at\nx4502c1s0b0n0:       runtime if needed. Op compatibility means that your system\nx4502c1s0b0n0:       meet the required dependencies to JIT install the op.\nx4502c1s0b0n0: --------------------------------------------------\nx4502c1s0b0n0: JIT compiled ops requires ninja\nx4502c1s0b0n0: ninja .................. [OKAY]\nx4502c1s0b0n0: --------------------------------------------------\nx4502c1s0b0n0: op name ................ installed .. compatible\nx4502c1s0b0n0: --------------------------------------------------\nx4502c1s0b0n0: [2024-01-23 06:10:21][INFO][spawn:38] - gcc -pthread -B /home/foremans/miniconda3/envs/anl_release_q4/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/foremans/miniconda3/envs/anl_release_q4/include -fPIC -O2 -isystem /home/foremans/miniconda3/envs/anl_release_q4/include -fPIC -c /tmp/tmptqyph55g/test.c -o /tmp/tmptqyph55g/test.o\nx4502c1s3b0n0: [2024-01-23 06:10:21,671] [INFO] [comm.py:161:init_deepspeed_backend] Initialize ccl backend\nx4502c1s3b0n0: [2024-01-23 06:10:21,672] [INFO] [comm.py:637:init_distributed] cdb=None\n\n[...]\nx4502c1s0b0n0: &gt;fused kernel is only supported in cuda, skip loading fused kernel\nx4502c1s0b0n0: 2024:01:23-06:12:16:(153241) |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi\nx4502c1s0b0n0: 2024:01:23-06:12:16:(153241) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL\nx4502c1s0b0n0: 2024:01:23-06:12:16:(153241) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nx4502c1s0b0n0: to get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\nx4502c1s0b0n0: 2024:01:23-06:12:16:(153242) |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi\nx4502c1s0b0n0: 2024:01:23-06:12:16:(153242) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL\nx4502c1s0b0n0: 2024:01:23-06:12:16:(153242) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nx4502c1s0b0n0: to get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\nx4502c1s3b0n0:  &gt; padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\nx4502c1s0b0n0: 2024:01:23-06:12:16:(153237) |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi\nx4502c1s0b0n0: 2024:01:23-06:12:16:(153237) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL\nx4502c1s0b0n0: 2024:01:23-06:12:16:(153237) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nx4502c1s0b0n0: to get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\nx4502c1s0b0n0:  &gt; padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\nx4502c1s0b0n0:  &gt; padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\nx4502c1s3b0n0: 2024:01:23-06:12:16:(129554) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL\nx4502c1s3b0n0: 2024:01:23-06:12:16:(129554) |CCL_ERROR| global.cpp:150 getenv_local_coord: condition global_data::env().ze_ipc_exchange == ccl::ze::ipc_exchange_mode::sockets failed\nx4502c1s3b0n0: to get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\nx4502c1s3b0n0: RuntimeError: oneCCL: global.cpp:150 getenv_local_coord: EXCEPTION: to get local_idx/count from ATL, set CCL_ZE_IPC_EXCHANGE=sockets explicitly\nx4502c1s3b0n0: Traceback (most recent call last):\nx4502c1s3b0n0:   File \"/lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/pretrain_llama.py\", line 583, in &lt;module&gt;\nx4502c1s3b0n0:     model = main()\nx4502c1s3b0n0:   File \"/lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/pretrain_llama.py\", line 561, in main\nx4502c1s3b0n0:     model = pretrain(\nx4502c1s3b0n0:   File \"/lus/gecko/projects/Aurora_deployment/foremans/anl_24_release_q4/llm.devkit/Megatron-DeepSpeed/megatron/training.py\", line 136, in pretrain\nx4502c1s3b0n0:     torch.distributed.all_reduce(start_time_tensor,\nx4502c1s3b0n0:   File \"/home/foremans/miniconda3/envs/anl_release_q4/lib/python3.9/site-packages/torch/distributed/c10d_logger.py\", line 47, in wrapper\nx4502c1s3b0n0:     return func(*args, **kwargs)\nx4502c1s3b0n0:   File \"/home/foremans/miniconda3/envs/anl_release_q4/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py\", line 2050, in all_reduce\nx4502c1s3b0n0:     work = group.allreduce([tensor], opts)"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "",
    "section": "🧑🏻‍💻 About Me",
    "text": "🧑🏻‍💻 About Me\n\n🧪 Interested in {AI, HPC} for science1\n💻 computational scientist at Argonne National Laboratory (ALCF)2\n\n\n\n\n🚀 usually working on scaling large (language, vision, multi-modal) models across thousands of GPUs3"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSo far, for: {Lattice QCD, Quantum Mechanics, Biology (Protein Generation, Drug Discovery), and Climate Modeling / Weather Forecasting}↩︎\nMostly trying to get supercomputers to stop yelling at each other 🤬↩︎\nIf this sounds like something you’d be interested in doing, please feel free to reach out to me!↩︎\nAnd resulted in a patent !!↩︎\n⚡ powered by ezpz 🍋↩︎\nForked from   saforem2/opinionated↩︎"
  },
  {
    "objectID": "qmd/dope-slides/index.html#quarto-reveal.js",
    "href": "qmd/dope-slides/index.html#quarto-reveal.js",
    "title": "How to Make Dope Slides",
    "section": "Quarto 🤝 Reveal.js",
    "text": "Quarto 🤝 Reveal.js\nSo, after making a promise some time ago on twitter 1, and having many questions following my talk on Parallel Training Techniques last week, I’m finally getting around to writing this up.\nThe slides are written using Quarto, a flavor of Markdown, and uses the built-in Quarto + Reveal.js functionality.\nFor this post, I’ll focus on the slides I presented at last years Lattice 2023, shown below:\n\n\n\n\n\n\n🪧 MLMC: Machine Learning Monte Carlo @ Lattice 2023 [07/2023]\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\n🏃‍♂️ Follow Along…\n\n\n\n\n\nOnce you’ve Installed Quarto, you can build these slides yourself by:\n\ngit clone  saforem2/lattice23\ncd lattice23 && quarto preview\n\nThis will create a docs/ directory with the following structure:\n📂 docs/\n├── 📂 assets/\n├── 📂 css/\n├── 📄 index.html\n├── 📄 lattice23.md\n├── 📄 search.json\n└── 📂 site_libs/\nOnce you’ve created this, and the docs/index.html file looks how you want, you can add the docs/ directory to your GitHub repo:\n$ git add docs\n$ git commit -m 'Create site'\n$ git push\nOnce you’ve enabled the GitHub page, the site will be automatically built and updated alongside the repo."
  },
  {
    "objectID": "qmd/dope-slides/index.html#getting-started",
    "href": "qmd/dope-slides/index.html#getting-started",
    "title": "How to Make Dope Slides",
    "section": "Getting Started",
    "text": "Getting Started\nWhenever I give a talk, my workflow is typically:\n\nCreate new GitHub repo for it\nHunt down the GitHub repo from my last talk and2:\n$ cp -r old_talk/{_quarto.yml,index.qmd,references.bib,css/*} new_talk/\n\nHonestly, other than that, 90% of the work is done automatically by Quarto. The remaining 10% consists of figuring out why my css is broken (see CSS).\nThe best place to start for learning to make slides with Quarto and Reveal.js is the official documentation:\n\nQuarto / Presentations / Revealjs:\n\nReveal Basics\nPresenting Slides\nAdvanced Reveal\nReveal Themes\n\n\n\nThe slides are written in markdown Quarto (.qmd)3, a pandoc-compliant based markup language.\nFor a single slide deck, the content will be placed in index.qmd and our directory structure will look something like:\n📂 lattice23/\n├── 📂 assets/            # for images, etc.\n│   └── 🖼️ thumbnail.png  # can be used as social preview image\n├── 📂 css/\n│   ├── 📄 callouts.css\n│   ├── 📄 dark.scss\n│   └── 📄 default.css\n├── 🛠️ _quarto.yml        # Configuration goes here\n├── 📄 index.qmd          # Quarto document containing slides content\n└── 📜 references.bib     # BibTex references\nEquations are rendered using $ delimiters for inline math and $$ for display math4.\nWe can use Divs and Spans from Pandoc.\n\n&lt;span&gt;’s: are created by wrapping text in square brackets, and will be treated as a &lt;span&gt; with attributes if it is followed immediately by attributes, e.g.:\n\nExample: [This is *some text*]{.class key=\"val\"}\nidk what I’m doing really, so I mostly find myself doing things like [blue   text]{style=\"color:#1E88E5;\"} which produces blue text.\n\n&lt;div&gt;’s: are created by wrapping text with a line consisting of at least three colons :::.\n\nExample:\n::: {#special .sidebar}\nHere is a paragraph.\n\nAnd another.\n:::\nWe can use either attributes in curly braces or a single unbraced word, which will be treated as a class name.\n\n\n\n\n🎁 Install Extensions\nFind the full list of available extensions at Quarto Extensions\nTo install various icon sets used in the example slides, we can install the following extensions:\n$ quarto install extension mcanouil/quarto-iconify      # https://icones.js.org/ [&lt;-- Contains rest of icon sets ??]\n$ quarto install extension shafayetShafee/bsicons       # bootstrap icons\n$ quarto install extension schochastics/academicicons   # OrcID, Google Scholar, ...\n$ quarto install extension quarto-ext/fontawesome       # Font Awesome icons\nnote that these aren’t necessary for functionality, but provide additional icons that I like to use 🤷🏻‍♂️"
  },
  {
    "objectID": "qmd/dope-slides/index.html#metadata",
    "href": "qmd/dope-slides/index.html#metadata",
    "title": "How to Make Dope Slides",
    "section": "Metadata",
    "text": "Metadata\nThe first section of our index.qmd contains the YAML metadata for the Quarto document.\nExplicitly, we see this consists of:\n\n\nExpand for yaml\n\n---\nformat:\n  revealjs:\n    title-block-style: none\n    slide-number: c\n    title-slide-style: default\n    chalkboard:\n      buttons: false\n    auto-animate: true\n    reference-location: section\n    touch: true\n    pause: false\n    footnotes-hover: true\n    citations-hover: true\n    preview-links: true\n    controls-tutorial: true\n    controls: false\n    logo: \"https://raw.githubusercontent.com/saforem2/anl-job-talk/main/docs/assets/anl.svg\"\n    history: false\n    theme: [dark, css/dark.scss]\n    css: [css/default.css, css/callouts.css]\n    self-contained: false\n    embed-resources: false\n    self-contained-math: false\n    center: true\n    highlight-style: \"atom-one\"\n    default-image-extension: svg\n    code-line-numbers: true\n    code-overflow: scroll\n    html-math-method: katex\n    fig-align: center\n    mermaid:\n      theme: dark\n  gfm:\n    output-file: \"lattice23.md\"\n---\n\nThe complete list of Reveal.js options are listed, with descriptions at: Quarto – Revealjs Options"
  },
  {
    "objectID": "qmd/dope-slides/index.html#title-slide",
    "href": "qmd/dope-slides/index.html#title-slide",
    "title": "How to Make Dope Slides",
    "section": "Title Slide",
    "text": "Title Slide\n\nStarting with the title slide5:\n\n\n\n\n\n\nFigure 1: Title Slide\n\n\n\n\nThe full slide contents are included below:\n\n\nExpand for quarto\n\n# {.title-slide .centeredslide background-iframe=\"https://saforem2.github.io/grid-worms-animation/\" loading=\"lazy\"}\n\n::: {style=\"background-color: rgba(22,22,22,0.75); border-radius: 10px; text-align:center; padding: 0px; padding-left: 1.5em; padding-right: 1.5em; max-width: min-content; min-width: max-content; margin-left: auto; margin-right: auto; padding-top: 0.2em; padding-bottom: 0.2em; line-height: 1.5em!important;\"}\n\n[MLMC: Machine Learning Monte Carlo]{.style=\"color:#939393; font-size:1.5em; font-weight:bold;}  \n[for Lattice Gauge Theory]{style=\"color:#777777; font-size:1.2em; font-weight: bold;\"}\n[&lt;br&gt;&nbsp;]{style=\"padding-bottom: 0.5rem;\"}  \n[](https://samforeman.me) Sam Foreman  \n[Xiao-Yong Jin, James C. Osborn]{.dim-text style=\"font-size:0.8em;\"}  \n[[[ `saforem2/`](https://github.com/saforem2/)]{style=\"border-bottom: 0.5px solid #00ccff;\"}`{`[[`lattice23`](https://github.com/saforem2/lattice23)]{style=\"border-bottom: 0.5px solid #00ccff;\"}, [[`l2hmc-qcd`](https://github.com/saforem2/l2hmc-qcd)]{style=\"border-bottom: 0.5px solid #00ccff;\"}`}`]{style=\"font-size:0.8em;\"}\n\n:::\n\n::: footer\n[2023-07-31 @ [Lattice 2023](https://indico.fnal.gov/event/57249/contributions/271305/)]{.dim-text style=\"text-align:left;'}\n:::\n\nFor the background, I made a simple animation  saforem2/grid-worms-animation that is hosted on GitHub pages as a simple html website\nThis static GitHub page is then used as an IFrame Background natively in Quarto with Reveal.js\nThis is as simple as:\n# {.title-slide .centeredslide background-iframe=\"https://saforem2.github.io/grid-worms-animation/\" loading=\"lazy\"}"
  },
  {
    "objectID": "qmd/dope-slides/index.html#single-column-slides",
    "href": "qmd/dope-slides/index.html#single-column-slides",
    "title": "How to Make Dope Slides",
    "section": "Single-Column Slides",
    "text": "Single-Column Slides\nOther than the title slide, the remainder of the slides are all relatively straightforward to construct.\nFor single-column slides, constructing the content is as simple as writing it in Markdown:\n\n\n\nCodeSlide\n\n\n# Overview\n\n1. [Background: `{MCMC,HMC}`](#markov-chain-monte-carlo-mcmc)\n    - [Leapfrog Integrator](#leapfrog-integrator-hmc)\n    - [Issues with HMC](#sec-issues-with-hmc)\n    - [Can we do better?](#sec-can-we-do-better)\n\n2. [L2HMC: Generalizing MD](#sec-l2hmc)\n    - [4D $SU(3)$ Model](#sec-su3)\n    - [Results](#sec-results)\n3. [References](#sec-references)\n4. [Extras](#sec-extras)\n\n\n\n\n\n\n\n\n\nFigure 2: Overview Slide"
  },
  {
    "objectID": "qmd/dope-slides/index.html#centered-slides",
    "href": "qmd/dope-slides/index.html#centered-slides",
    "title": "How to Make Dope Slides",
    "section": "Centered Slides",
    "text": "Centered Slides\nWe can center all the text on a slide by adding the {.centeredslide} class to the slide header, e.g.\n\nindex.qmdstyle.scss\n\n\n---\nformat:\n  revealjs:\n    theme: [style.scss]\n---\n\n# Title {.centeredslide}\n\n\n.centeredslide {\n  text-align: center;\n}"
  },
  {
    "objectID": "qmd/dope-slides/index.html#multi-column-slides",
    "href": "qmd/dope-slides/index.html#multi-column-slides",
    "title": "How to Make Dope Slides",
    "section": "Multi-Column Slides",
    "text": "Multi-Column Slides\nSide-by-side content (either text or images)\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Markov Chain Monte Carlo\n\n\n\n\n\nThis slide is horizontally centered6 and the content consists of two rows split as follows:\n\n\n\n\n\n\nA\nB\n\n\nC\nC\n\n\n\n\n\nTable 1: Slide layout. First row split into two columns, second row spans full width.\n\n\n\n\nIn panel A, we have a ::: {.callout-note} block followed by a single list element containing a LaTeX equation.\nIn panel B we have a standard markdown image\n![](./asets/mcmc.png)\nIn panel C we have normal text + math with LaTeX7 syntax.\n\n\n\n\n\nNote that we additionally have a ::: footer element included at the bottom of the slide.\nThe code used to generate the slide above is included below:\n\n\nExpand forquarto\n\n# Markov Chain Monte Carlo (MCMC) {.centeredslide}\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n::: {.callout-note title=\"Goal\" style=\"text-align:left;!important\"}\nGenerate **independent** samples $\\{x_{i}\\}$, such that[^notation]\n$$\\{x_{i}\\} \\sim p(x) \\propto e^{-S(x)}$$\nwhere $S(x)$ is the _action_ (or potential energy)\n:::\n\n- Want to calculate observables $\\mathcal{O}$:  \n  $\\left\\langle \\mathcal{O}\\right\\rangle \\propto \\int \\left[\\mathcal{D}x\\right]\\hspace{4pt} {\\mathcal{O}(x)\\, p(x)}$\n\n:::\n\n::: {.column width=\"49%\"}\n![](https://raw.githubusercontent.com/saforem2/deep-fridays/main/assets/normal_distribution.dark.svg)\n:::\n\n::::\n\nIf these were [independent]{.style=\"color:#00CCFF;\"}, we could approximate:\n$\\left\\langle\\mathcal{O}\\right\\rangle \\simeq \\frac{1}{N}\\sum^{N}_{n=1}\\mathcal{O}(x_{n})$\n$$\\sigma_{\\mathcal{O}}^{2} = \\frac{1}{N}\\mathrm{Var}{\\left[\\mathcal{O} (x) \\right]}\\Longrightarrow\n\\sigma_{\\mathcal{O}} \\propto \\frac{1}{\\sqrt{N}}$$\n\n[^notation]: Here, $\\sim$ means \"is distributed according to\"\n\n::: footer\n[ `saforem2/lattice23`](https://saforem2.github.io/lattice23)\n:::"
  },
  {
    "objectID": "qmd/dope-slides/index.html#css",
    "href": "qmd/dope-slides/index.html#css",
    "title": "How to Make Dope Slides",
    "section": "💅 CSS",
    "text": "💅 CSS\nMy web developer friend laughs at me, but when something is broken / doesn’t look right / I want it to look different, I:\n\nPull up Chrome Tools ( ⌘ + ⌥ + I )\nInspect element of interest ( ⌘ + ⇧ + C )\nMake changes to the CSS\nSave the new rule to my .scss file 🤷🏻‍♂️\n\nI’m guessing this might be obvious to some people, but it took me a while to figure out how things worked so maybe its helpful for others.\n\n\nExpand for css\n\n\n\n\n\n\n\nFigure 4: Example of selecting an element and making a change to the CSS."
  },
  {
    "objectID": "qmd/dope-slides/index.html#github-page",
    "href": "qmd/dope-slides/index.html#github-page",
    "title": "How to Make Dope Slides",
    "section": "📃 GitHub Page",
    "text": "📃 GitHub Page\nTo enable your GitHub page, you can do the following:\n\n\n\n\n\n\nFigure 5: Instructions for building a GitHub page using the docs/ directory off the main branch.\n\n\n\nIn this case, the repo is:\n saforem2/lattice23\nand the site is published at\nhttps://saforem2.github.io/lattice23"
  },
  {
    "objectID": "qmd/dope-slides/index.html#references",
    "href": "qmd/dope-slides/index.html#references",
    "title": "How to Make Dope Slides",
    "section": "📓 References",
    "text": "📓 References\n\nReveal Themes\nUsing Pandoc fenced divs\nSlidecraft 101: Colors and Fonts\nBeautiful Reports and Presentations with Quarto\n Quarto Clean Theme\n\n\n\n\n\n\n\n❤️‍🩹 Status\n\n\n\n\n\n\n\nLast Updated: 05/14/2024 @ 09:08:49\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Title Slide\nFigure 2: Overview Slide\nFigure 3: Markov Chain Monte Carlo\nFigure 4: Example of selecting an element and making a change to the CSS.\nFigure 5: Instructions for building a GitHub page using the docs/ directory off the main branch."
  },
  {
    "objectID": "qmd/dope-slides/index.html#footnotes",
    "href": "qmd/dope-slides/index.html#footnotes",
    "title": "How to Make Dope Slides",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAnd countless other people IRL↩︎\nOne thing I’ve been meaning to do, is clean up all my css/* files and move them all to a single repository, but I’ll save that for another day.↩︎\nAn open-source scientific and technical publishing system↩︎\nEquations↩︎\nQuarto comes with lightbox support, so you can click on images to display them full screen.↩︎\nBy adding the {.centeredslide} class to the slide header↩︎\nText surrounded by $ will be rendered with LaTeX↩︎"
  },
  {
    "objectID": "qmd/ezpz/ezpz.html#initialization-times",
    "href": "qmd/ezpz/ezpz.html#initialization-times",
    "title": "Starting Up Distributed Training",
    "section": "Initialization Times",
    "text": "Initialization Times\n\n\n\n\n\n\n\n   Application Startup Time\n\n\n\n\n\n\nFrom Tanima:\n\nHi Sam and Corey,\nThanks for your comments on measuring the application start up time last week.\nTypically, we report the throughput performance after the start-up and warm-up during the “steady” state of the training.\nWe have a few follow-up questions so that we establish a methodology to address the issue brought up by Argonne.\n\nWe can set a few timestamps in the model scripts and job scripts used for the queue submission: Job script:\nTime stamp A:  \n&lt;actual python command using mpiexec&gt;\n\nInside the model script:  \nmain()  \nTimestamp B:  \n[...]\nTimestamp C:  \nFirst training steps and onwards.  \nBy startup time, do you mean measuring time difference between A and C or B and C?\n\n\n\n\nWill the measurement methodology be the same for distributed training?\nFor examples, we can measure the start-up time for the rank0?\n\n\n\n\nIf we need to report the startup time for the DL applications, do we need to collect measurements using the actual Aurora NRE workloads or some small benchmarking test cases?\nFor example, we can try to recreate the typical start-up scenarios, like library imports, and measure those separately as shown below.\nJob script:\nTime stamp A:\n&lt;actual python command using mpiexec&gt;\n\nTime stamp B:\n import torch\nTime stamp C\nimport IPEX\nTime stamp D\nEtc...\nIf you have any other scenarios, please feel free to suggest.\n\nThanks, Tanima.\n\n\n\n\n\n\nIn Measuring / Calculating Startup Time,I provide a summary of how the startup time is identified and calculated.\nI’m not sure exactly I understand\n\nWill the measurement methodology be the same for distributed training? For examples, we can measure the start-up time for the rank0?\n\nThe startup time is being measured for distributed training (logs only created on RANK = 0)\nI discuss in Minimal Working Example a minimal example that can be used to measure the startup times.\n\nThis is using a library I’ve been working on, ezpz that is designed to help simplify the process of setting up / initializing distributed training across many GPUs.\n\n\n\nMeasuring / Calculating Startup Time\n\nThe startup timing was identified by parsing the logfiles from existing runs and calculating the difference \\delta t = t_{1} - t_{0},\n\nt_{0} is the time stamp at the very beginning of the shell script (defined here) which then launches mpiexec &lt;mpi-args&gt; python3 [...].\n\nt_{0} appears in the logfile as:\nJob started at: 2023-11-02-183323 on x3004c0s13b0n0\n\nt_{1} is identified as the timestamp associated with the completion of the first training step\n\nt_{1} appears in the logfile as:\n[2023-11-02 18:34:13,122] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.999), (0.9, 0.999)]\n\n\nBelow is an example of the bash script use to parse the logfiles and identify these timestamps:\n  $ for f in $(tail -5 logfiles) ; do echo $f; cat $f | grep -E \"Job started|step=0\\,\" | uniq ; echo \"\\n\" ; done\n  /lus/grand/projects/datascience/foremans/locations/polaris/projects/argonne-lcf/Megatron-DeepSpeed/outputs/gpt_actCkpt_GPT1T_4L_z1_seqlen2048_mp8_pp2_sp1_nl4_hs25600_gb16_mb1/logs/foremans-x3004c0s13b0n0-nhosts4-ngpu16-2023-11-02-183323.log\n  Job started at: 2023-11-02-183323 on x3004c0s13b0n0\n  [2023-11-02 18:34:13,122] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.999), (0.9, 0.999)]\n\n  /lus/grand/projects/datascience/foremans/locations/polaris/projects/argonne-lcf/Megatron-DeepSpeed/outputs/gpt_SP_actCkpt_GPT125M_z0_seqlen2048_mp16_pp1_sp1_nl12_hs768_gb1_mb1/logs/foremans-x3015c0s37b0n0-nhosts4-ngpu16-2023-11-02-184240.log\n  Job started at: 2023-11-02-184240 on x3015c0s37b0n0\n\n  /lus/grand/projects/datascience/foremans/locations/polaris/projects/argonne-lcf/Megatron-DeepSpeed/outputs/gpt_SP_actCkpt_GPT125M_z0_seqlen2048_mp16_pp1_sp1_nl12_hs768_gb1_mb1/logs/foremans-x3015c0s37b0n0-nhosts4-ngpu16-2023-11-02-184259.log\n  Job started at: 2023-11-02-184259 on x3015c0s37b0n0\n  [2023-11-02 18:43:23,385] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.999), (0.9, 0.999)]\n\n  /lus/grand/projects/datascience/foremans/locations/polaris/projects/argonne-lcf/Megatron-DeepSpeed/outputs/gpt_SP_actCkpt_GPT125M_z0_seqlen2048_mp16_pp1_sp1_nl12_hs768_gb1_mb1/logs/foremans-x3004c0s13b0n0-nhosts4-ngpu16-2023-11-02-184407.log\n  Job started at: 2023-11-02-184407 on x3004c0s13b0n0\n  [2023-11-02 18:44:32,804] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.999), (0.9, 0.999)]\n\n  /lus/grand/projects/datascience/foremans/locations/polaris/projects/argonne-lcf/Megatron-DeepSpeed/outputs/gpt_actCkpt_GPT1T_4L_z1_seqlen2048_mp8_pp2_sp1_nl4_hs25600_gb16_mb2/logs/foremans-x3108c0s25b1n0-nhosts2-ngpu8-2023-11-02-192739.log\n  Job started at: 2023-11-02-192739 on x3108c0s25b1n0\n\n\n\n\n\n\n\n   Startup Times (Perlmutter)\n\n\n\n\n\n\n\n\nTable 1: Startup times on Perlmutter\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n****\nmodel_size\nworld_size\nstart\nstop\nt0\nt1\ndt\n\n\n\n\nforemans-nid008217-nhosts2-ngpu8-2023-10-05-191101.log\nGPT1T_1L\n8\n2023-10-05-191101\n2023-10-05-191215\n191101\n191215\n114\n\n\nforemans-nid008217-nhosts2-ngpu8-2023-10-05-191400.log\nGPT1T_1L\n8\n2023-10-05-191400\n2023-10-05-191511\n191400\n191511\n111\n\n\nforemans-nid008217-nhosts2-ngpu8-2023-10-05-191707.log\nGPT1T_1L\n8\n2023-10-05-191707\n2023-10-05-191817\n191707\n191817\n110\n\n\nforemans-nid008553-nhosts2-ngpu8-2023-10-15-114506.log\nGPT1T_2L\n8\n2023-10-15-114506\n2023-10-15-114616\n114506\n114616\n110\n\n\nforemans-nid008572-nhosts2-ngpu8-2023-10-15-133531.log\nGPT2_7B\n8\n2023-10-15-133531\n2023-10-15-133745\n133531\n133745\n214\n\n\nforemans-nid008572-nhosts2-ngpu8-2023-10-15-135041.log\nGPT2_7B\n8\n2023-10-15-135041\n2023-10-15-135255\n135041\n135255\n214\n\n\nforemans-nid008572-nhosts2-ngpu8-2023-10-15-140806.log\nGPT2_7B\n8\n2023-10-15-140806\n2023-10-15-141236\n140806\n141236\n430\n\n\nforemans-nid008572-nhosts2-ngpu8-2023-10-15-143120.log\nGPT2_7B\n8\n2023-10-15-143120\n2023-10-15-143655\n143120\n143655\n535\n\n\nforemans-nid008268-nhosts2-ngpu8-2023-10-15-154337.log\nGPT2_7B\n8\n2023-10-15-154337\n2023-10-15-154446\n154337\n154446\n109\n\n\nforemans-nid008268-nhosts2-ngpu8-2023-10-15-154943.log\nGPT1T_1L\n8\n2023-10-15-154943\n2023-10-15-155317\n154943\n155317\n374\n\n\nforemans-nid008268-nhosts2-ngpu8-2023-10-15-162315.log\nGPT1T_1L\n8\n2023-10-15-162315\n2023-10-15-162441\n162315\n162441\n126\n\n\nforemans-login12-nhosts2-ngpu8-2023-10-15-180714.log\nGPT2_7B\n8\n2023-10-15-180714\n2023-10-15-180805\n180714\n180805\n91\n\n\nforemans-login12-nhosts2-ngpu8-2023-10-15-181733.log\nGPT2_7B\n8\n2023-10-15-181733\n2023-10-15-181834\n181733\n181834\n101\n\n\nforemans-login12-nhosts2-ngpu8-2023-10-15-182228.log\nGPT1T_1L\n8\n2023-10-15-182228\n2023-10-15-183031\n182228\n183031\n803\n\n\nforemans-login12-nhosts2-ngpu8-2023-10-15-183345.log\nGPT1T_2L\n8\n2023-10-15-183345\n2023-10-15-183750\n183345\n183750\n405\n\n\nforemans-login12-nhosts2-ngpu8-2023-10-15-184442.log\nGPT1T_2L\n8\n2023-10-15-184442\n2023-10-15-184727\n184442\n184727\n285\n\n\nforemans-login12-nhosts2-ngpu8-2023-10-15-185952.log\nGPT1T_1L\n8\n2023-10-15-185952\n2023-10-15-190046\n185952\n190046\n4094\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-15-191508.log\nGPT2_7B\n8\n2023-10-15-191508\n2023-10-15-191608\n191508\n191608\n100\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-15-192404.log\nGPT2_7B\n8\n2023-10-15-192404\n2023-10-15-192504\n192404\n192504\n100\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-15-193041.log\nGPT2_7B\n8\n2023-10-15-193041\n2023-10-15-193137\n193041\n193137\n96\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-15-193448.log\nGPT2_7B\n8\n2023-10-15-193448\n2023-10-15-193540\n193448\n193540\n92\n\n\nforemans-login12-nhosts4-ngpu16-2023-10-15-195802.log\nGPT1T_1L\n16\n2023-10-15-195802\n2023-10-15-195904\n195802\n195904\n102\n\n\nforemans-login12-nhosts4-ngpu16-2023-10-15-200019.log\nGPT2_7B\n16\n2023-10-15-200019\n2023-10-15-200258\n200019\n200258\n239\n\n\nforemans-login12-nhosts4-ngpu16-2023-10-15-200902.log\nGPT2_7B\n16\n2023-10-15-200902\n2023-10-15-201239\n200902\n201239\n337\n\n\nforemans-login12-nhosts4-ngpu16-2023-10-15-201524.log\nGPT2_7B\n16\n2023-10-15-201524\n2023-10-15-201612\n201524\n201612\n88\n\n\nforemans-login12-nhosts4-ngpu16-2023-10-15-201834.log\nGPT2_7B\n16\n2023-10-15-201834\n2023-10-15-201923\n201834\n201923\n89\n\n\nforemans-login12-nhosts4-ngpu16-2023-10-15-202402.log\nGPT2_7B\n16\n2023-10-15-202402\n2023-10-15-202501\n202402\n202501\n99\n\n\nforemans-login12-nhosts4-ngpu16-2023-10-15-202606.log\nGPT2_7B\n16\n2023-10-15-202606\n2023-10-15-202713\n202606\n202713\n107\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-16-084033.log\nGPT1T_1L\n8\n2023-10-16-084033\n2023-10-16-084212\n84033\n84212\n179\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-16-084628.log\nGPT1T_1L\n8\n2023-10-16-084628\n2023-10-16-084728\n84628\n84728\n100\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-16-085401.log\nGPT1T_1L\n8\n2023-10-16-085401\n2023-10-16-085505\n85401\n85505\n104\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-16-090142.log\nGPT1T_1L\n8\n2023-10-16-090142\n2023-10-16-090305\n90142\n90305\n163\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-16-093404.log\nactCkpt_GPT13B\n8\n2023-10-16-093404\n2023-10-16-093504\n93404\n93504\n100\n\n\nforemans-nid008572-nhosts4-ngpu16-2023-10-16-101437.log\nGPT1T_1L\n16\n2023-10-16-101437\n2023-10-16-101549\n101437\n101549\n112\n\n\nforemans-nid008396-nhosts4-ngpu16-2023-10-16-101512.log\nGPT1T_1L\n16\n2023-10-16-101512\n2023-10-16-101615\n101512\n101615\n103\n\n\nforemans-nid008396-nhosts4-ngpu16-2023-10-16-102217.log\nactCkpt_GPT25B\n16\n2023-10-16-102217\n2023-10-16-102452\n102217\n102452\n235\n\n\nforemans-nid008396-nhosts4-ngpu16-2023-10-16-102750.log\nactCkpt_GPT25B\n16\n2023-10-16-102750\n2023-10-16-103243\n102750\n103243\n493\n\n\nforemans-nid008572-nhosts4-ngpu16-2023-10-16-103113.log\nactCkpt_GPT25B\n16\n2023-10-16-103113\n2023-10-16-103237\n103113\n103237\n124\n\n\nforemans-nid008396-nhosts4-ngpu16-2023-10-16-104037.log\nactCkpt_GPT25B\n16\n2023-10-16-104037\n2023-10-16-104148\n104037\n104148\n111\n\n\nforemans-nid008396-nhosts4-ngpu16-2023-10-16-104819.log\nactCkpt_GPT25B\n16\n2023-10-16-104819\n2023-10-16-110002\n104819\n110002\n5183\n\n\nforemans-nid008396-nhosts4-ngpu16-2023-10-16-110119.log\nactCkpt_GPT25B\n16\n2023-10-16-110119\n2023-10-16-110225\n110119\n110225\n106\n\n\nforemans-nid008701-nhosts4-ngpu16-2023-10-16-113715.log\nactCkpt_GPT25B\n16\n2023-10-16-113715\n2023-10-16-113824\n113715\n113824\n109\n\n\nforemans-nid008701-nhosts4-ngpu16-2023-10-16-114236.log\nGPT1T_1L\n16\n2023-10-16-114236\n2023-10-16-114338\n114236\n114338\n102\n\n\nforemans-nid008701-nhosts4-ngpu16-2023-10-16-114610.log\nGPT1T_1L\n16\n2023-10-16-114610\n2023-10-16-114711\n114610\n114711\n101\n\n\nforemans-nid008701-nhosts4-ngpu16-2023-10-16-114819.log\nGPT1T_2L\n16\n2023-10-16-114819\n2023-10-16-114953\n114819\n114953\n134\n\n\nforemans-nid008701-nhosts4-ngpu16-2023-10-16-131058.log\nGPT1T_2L\n16\n2023-10-16-131058\n2023-10-16-131203\n131058\n131203\n145\n\n\nforemans-nid008576-nhosts1-ngpu4-2023-10-16-151427.log\nGPT1T_1L\n4\n2023-10-16-151427\n2023-10-16-151600\n151427\n151600\n173\n\n\nforemans-nid008576-nhosts1-ngpu4-2023-10-16-152528.log\nGPT1T_1L\n4\n2023-10-16-152528\n2023-10-16-152640\n152528\n152640\n112\n\n\nforemans-nid008224-nhosts1-ngpu4-2023-10-16-175717.log\nGPT1T_1L\n4\n2023-10-16-175717\n2023-10-16-175829\n175717\n175829\n112\n\n\nforemans-nid008224-nhosts1-ngpu4-2023-10-16-180457.log\nGPT1T_1L\n4\n2023-10-16-180457\n2023-10-16-180605\n180457\n180605\n148\n\n\nforemans-nid008224-nhosts1-ngpu4-2023-10-16-183116.log\nGPT1T_1L\n4\n2023-10-16-183116\n2023-10-16-183216\n183116\n183216\n100\n\n\nforemans-nid008224-nhosts1-ngpu4-2023-10-16-183921.log\nGPT1T_1L\n4\n2023-10-16-183921\n2023-10-16-184033\n183921\n184033\n112\n\n\nforemans-nid008237-nhosts1-ngpu4-2023-10-16-215614.log\nGPT1T_1L\n4\n2023-10-16-215614\n2023-10-16-215815\n215614\n215815\n201\n\n\nforemans-nid008385-nhosts1-ngpu4-2023-10-17-052944.log\nGPT1T_1L\n4\n2023-10-17-052944\n2023-10-17-053139\n52944\n53139\n195\n\n\nforemans-nid008385-nhosts1-ngpu4-2023-10-17-053529.log\nGPT1T_1L\n4\n2023-10-17-053529\n2023-10-17-053650\n53529\n53650\n121\n\n\nforemans-nid008385-nhosts1-ngpu4-2023-10-17-053910.log\nGPT1T_1L\n4\n2023-10-17-053910\n2023-10-17-054120\n53910\n54120\n210\n\n\nforemans-nid008385-nhosts1-ngpu4-2023-10-17-054238.log\nGPT2_7B\n4\n2023-10-17-054238\n2023-10-17-054346\n54238\n54346\n108\n\n\nforemans-nid008385-nhosts1-ngpu4-2023-10-17-060418.log\nGPT1T_1L\n4\n2023-10-17-060418\n2023-10-17-060600\n60418\n60600\n182\n\n\nforemans-nid008385-nhosts1-ngpu4-2023-10-17-061514.log\nGPT1T_1L\n4\n2023-10-17-061514\n2023-10-17-061653\n61514\n61653\n139\n\n\nforemans-nid008385-nhosts1-ngpu4-2023-10-17-062102.log\nGPT1T_1L\n4\n2023-10-17-062102\n2023-10-17-062252\n62102\n62252\n150\n\n\nforemans-nid008385-nhosts1-ngpu4-2023-10-17-062445.log\nGPT1T_1L\n4\n2023-10-17-062445\n2023-10-17-062720\n62445\n62720\n275\n\n\nforemans-nid008333-nhosts2-ngpu8-2023-10-17-064643.log\nGPT1T_1L\n8\n2023-10-17-064643\n2023-10-17-064848\n64643\n64848\n205\n\n\nforemans-nid008333-nhosts2-ngpu8-2023-10-17-065806.log\nGPT1T_2L\n8\n2023-10-17-065806\n2023-10-17-070003\n65806\n70003\n4197\n\n\nforemans-nid008333-nhosts2-ngpu8-2023-10-17-075152.log\nGPT1T_2L\n8\n2023-10-17-075152\n2023-10-17-075502\n75152\n75502\n350\n\n\nforemans-nid008333-nhosts2-ngpu8-2023-10-17-080059.log\nGPT1T_2L\n8\n2023-10-17-080059\n2023-10-17-080434\n80059\n80434\n375\n\n\nforemans-nid008333-nhosts2-ngpu8-2023-10-17-081404.log\nGPT1T_2L\n8\n2023-10-17-081404\n2023-10-17-081920\n81404\n81920\n516\n\n\nforemans-nid008228-nhosts1-ngpu4-2023-10-17-090344.log\nGPT1T_1L\n4\n2023-10-17-090344\n2023-10-17-090714\n90344\n90714\n370\n\n\nforemans-nid008228-nhosts1-ngpu4-2023-10-17-100759.log\nGPT1T_1L\n4\n2023-10-17-100759\n2023-10-17-100957\n100759\n100957\n198\n\n\nforemans-nid008404-nhosts4-ngpu16-2023-10-17-182501.log\nGPT1T_1L\n16\n2023-10-17-182501\n2023-10-17-184001\n182501\n184001\n1500\n\n\nforemans-nid008404-nhosts4-ngpu16-2023-10-17-193736.log\nGPT1T_1L\n16\n2023-10-17-193736\n2023-10-17-193856\n193736\n193856\n120\n\n\nforemans-nid008404-nhosts4-ngpu16-2023-10-17-195432.log\nGPT1T_1L\n16\n2023-10-17-195432\n2023-10-17-195536\n195432\n195536\n104\n\n\nforemans-nid008404-nhosts4-ngpu16-2023-10-17-201659.log\nGPT1T_2L\n16\n2023-10-17-201659\n2023-10-17-201823\n201659\n201823\n164\n\n\nforemans-nid008404-nhosts4-ngpu16-2023-10-17-202949.log\nGPT1T_2L\n16\n2023-10-17-202949\n2023-10-17-203054\n202949\n203054\n105\n\n\nforemans-nid008404-nhosts4-ngpu16-2023-10-17-205848.log\nGPT1T_1L\n16\n2023-10-17-205848\n2023-10-17-205952\n205848\n205952\n104\n\n\nforemans-nid008577-nhosts8-ngpu32-2023-10-17-213244.log\nGPT1T_1L\n32\n2023-10-17-213244\n2023-10-17-213406\n213244\n213406\n162\n\n\nforemans-nid008577-nhosts8-ngpu32-2023-10-17-213558.log\nGPT1T_1L\n32\n2023-10-17-213558\n2023-10-17-213720\n213558\n213720\n162\n\n\nforemans-nid008577-nhosts8-ngpu32-2023-10-17-214900.log\nGPT1T_2L\n32\n2023-10-17-214900\n2023-10-17-214959\n214900\n214959\n59\n\n\nforemans-nid008577-nhosts8-ngpu32-2023-10-17-215201.log\nGPT1T_2L\n32\n2023-10-17-215201\n2023-10-17-215309\n215201\n215309\n108\n\n\nforemans-nid008577-nhosts8-ngpu32-2023-10-17-215612.log\nGPT1T_2L\n32\n2023-10-17-215612\n2023-10-17-215726\n215612\n215726\n114\n\n\nforemans-nid008577-nhosts8-ngpu32-2023-10-17-215938.log\nGPT1T_2L\n32\n2023-10-17-215938\n2023-10-17-220044\n215938\n220044\n4106\n\n\nforemans-nid008529-nhosts8-ngpu32-2023-10-18-110001.log\nGPT1T_4L\n32\n2023-10-18-110001\n2023-10-18-110143\n110001\n110143\n142\n\n\nforemans-nid008529-nhosts8-ngpu32-2023-10-18-110424.log\nGPT1T_8L\n32\n2023-10-18-110424\n2023-10-18-110550\n110424\n110550\n126\n\n\nforemans-nid008244-nhosts4-ngpu16-2023-10-18-110821.log\nGPT1T_8L\n16\n2023-10-18-110821\n2023-10-18-110952\n110821\n110952\n131\n\n\nforemans-nid008529-nhosts8-ngpu32-2023-10-18-111345.log\nGPT1T_8L\n32\n2023-10-18-111345\n2023-10-18-111458\n111345\n111458\n113\n\n\nforemans-nid008197-nhosts16-ngpu64-2023-10-18-112531.log\nGPT1T_16L\n64\n2023-10-18-112531\n2023-10-18-112728\n112531\n112728\n197\n\n\nforemans-nid008456-nhosts16-ngpu64-2023-10-18-113119.log\nGPT1T_16L\n64\n2023-10-18-113119\n2023-10-18-113343\n113119\n113343\n224\n\n\nforemans-nid008244-nhosts4-ngpu16-2023-10-18-113131.log\nGPT1T_4L\n16\n2023-10-18-113131\n2023-10-18-113257\n113131\n113257\n126\n\n\nforemans-nid008244-nhosts4-ngpu16-2023-10-18-113920.log\nGPT1T_4L\n16\n2023-10-18-113920\n2023-10-18-114157\n113920\n114157\n237\n\n\nforemans-nid008197-nhosts16-ngpu64-2023-10-18-114549.log\nGPT1T_16L\n64\n2023-10-18-114549\n2023-10-18-114721\n114549\n114721\n172\n\n\nforemans-nid008456-nhosts16-ngpu64-2023-10-18-114636.log\nGPT1T_16L\n64\n2023-10-18-114636\n2023-10-18-114805\n114636\n114805\n169\n\n\nforemans-nid008244-nhosts4-ngpu16-2023-10-18-115808.log\nGPT1T_4L\n16\n2023-10-18-115808\n2023-10-18-120146\n115808\n120146\n4338\n\n\nforemans-nid008456-nhosts16-ngpu64-2023-10-18-123039.log\nGPT1T_16L\n64\n2023-10-18-123039\n2023-10-18-123221\n123039\n123221\n182\n\n\nforemans-nid008389-nhosts2-ngpu8-2023-10-18-123135.log\nGPT1T_4L\n8\n2023-10-18-123135\n2023-10-18-123300\n123135\n123300\n165\n\n\nforemans-nid008244-nhosts4-ngpu16-2023-10-18-123206.log\nGPT1T_4L\n16\n2023-10-18-123206\n2023-10-18-123352\n123206\n123352\n146\n\n\nforemans-nid008456-nhosts16-ngpu64-2023-10-18-125022.log\nGPT1T_16L\n64\n2023-10-18-125022\n2023-10-18-125146\n125022\n125146\n124\n\n\nforemans-nid008256-nhosts8-ngpu32-2023-10-22-122736.log\nGPT1T_8L\n32\n2023-10-22-122736\n2023-10-22-122844\n122736\n122844\n108\n\n\nforemans-nid008256-nhosts8-ngpu32-2023-10-22-123824.log\nGPT1T_8L\n32\n2023-10-22-123824\n2023-10-22-123945\n123824\n123945\n121\n\n\nforemans-nid008256-nhosts8-ngpu32-2023-10-22-130148.log\nGPT1T_8L\n32\n2023-10-22-130148\n2023-10-22-130256\n130148\n130256\n108\n\n\nforemans-nid008256-nhosts8-ngpu32-2023-10-22-131746.log\nGPT1T_8L\n32\n2023-10-22-131746\n2023-10-22-131909\n131746\n131909\n163\n\n\nforemans-nid008256-nhosts8-ngpu32-2023-10-22-132700.log\nGPT1T_8L\n32\n2023-10-22-132700\n2023-10-22-132817\n132700\n132817\n117\n\n\nforemans-nid008256-nhosts8-ngpu32-2023-10-22-133459.log\nGPT1T_8L\n32\n2023-10-22-133459\n2023-10-22-133708\n133459\n133708\n249\n\n\nforemans-nid008380-nhosts4-ngpu16-2023-10-22-175049.log\nactCkpt_GPT25B\n16\n2023-10-22-175049\n2023-10-22-175230\n175049\n175230\n181\n\n\nforemans-nid008649-nhosts4-ngpu16-2023-10-22-192352.log\nGPT1T_4L\n16\n2023-10-22-192352\n2023-10-22-192530\n192352\n192530\n178\n\n\nforemans-nid008212-nhosts16-ngpu64-2023-10-23-081527.log\nGPT1T_8L\n64\n2023-10-23-081527\n2023-10-23-081702\n81527\n81702\n175\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-23-091436.log\nGPT1T_2L\n8\n2023-10-23-091436\n2023-10-23-091610\n91436\n91610\n174\n\n\nforemans-nid008197-nhosts32-ngpu128-2023-10-24-102617.log\nGPT1T_32L\n128\n2023-10-24-102617\n2023-10-24-102826\n102617\n102826\n209\n\n\nforemans-nid008192-nhosts64-ngpu256-2023-10-24-191748.log\nGPT1T_64L\n256\n2023-10-24-191748\n2023-10-24-192021\n191748\n192021\n273\n\n\nforemans-nid008192-nhosts128-ngpu512-2023-10-24-201243.log\nGPT1T_128L\n512\n2023-10-24-201243\n2023-10-24-201629\n201243\n201629\n386\n\n\nforemans-nid008192-nhosts128-ngpu512-2023-10-26-005401.log\nGPT1T_128L\n512\n2023-10-26-005401\n2023-10-26-005811\n5401\n5811\n410\n\n\nforemans-nid008192-nhosts32-ngpu128-2023-10-26-082710.log\nGPT1T_32L\n128\n2023-10-26-082710\n2023-10-26-083049\n82710\n83049\n339\n\n\nforemans-nid008585-nhosts2-ngpu8-2023-10-31-044203.log\nGPT1T_2L\n8\n2023-10-31-044203\n2023-10-31-044533\n44203\n44533\n330\n\n\nforemans-nid008272-nhosts4-ngpu16-2023-10-31-072717.log\nGPT1T_4L\n16\n2023-10-31-072717\n2023-10-31-073131\n72717\n73131\n414\n\n\nforemans-nid008221-nhosts8-ngpu32-2023-10-31-083055.log\nGPT1T_8L\n32\n2023-10-31-083055\n2023-10-31-083545\n83055\n83545\n490\n\n\nforemans-nid008196-nhosts16-ngpu64-2023-10-31-100336.log\nGPT1T_16L\n64\n2023-10-31-100336\n2023-10-31-100848\n100336\n100848\n512\n\n\nforemans-nid008285-nhosts2-ngpu8-2023-11-01-200430.log\nGPT1T_2L\n8\n2023-11-01-200430\n2023-11-01-200829\n200430\n200829\n399\n\n\nforemans-nid008193-nhosts8-ngpu32-2023-11-01-201702.log\nGPT1T_8L\n32\n2023-11-01-201702\n2023-11-01-202131\n201702\n202131\n429\n\n\nforemans-nid008240-nhosts16-ngpu64-2023-11-01-210454.log\nGPT1T_16L\n64\n2023-11-01-210454\n2023-11-01-211007\n210454\n211007\n553\n\n\nforemans-nid008321-nhosts2-ngpu8-2023-11-02-154438.log\nGPT1T_2L\n8\n2023-11-02-154438\n2023-11-02-154949\n154438\n154949\n511\n\n\nforemans-nid008192-nhosts128-ngpu512-2023-11-04-001717.log\nGPT1T_128L\n512\n2023-11-04-001717\n2023-11-04-002124\n1717\n2124\n407"
  },
  {
    "objectID": "qmd/ezpz/ezpz.html#minimal-working-example",
    "href": "qmd/ezpz/ezpz.html#minimal-working-example",
    "title": "Starting Up Distributed Training",
    "section": "Minimal Working Example",
    "text": "Minimal Working Example\n\nAs for 3:\n\nIf we need to report the startup time for the DL applications, do we need to collect measurements using the actual Aurora NRE workloads or some small benchmarking test cases? For example, we can try to recreate the typical start-up scenarios, like library imports, and measure those separately as shown below.\n\n\nI’ve been working on a library to help simplify this:\n ezpz\nMinimal library that handles the initialization of distributed training\n\n  Working on Aurora, example:\n\nSetup / Install:\n# launch job\n$ qsub -q EarlyAppAccess -A Aurora_Deployment -l walltime=2:00:00 -l select=4 -I\n\n# load frameworks\n$ module use -a /soft/modulefiles ; module --ignore_cache load frameworks\n$ module load frameworks/.2023.12.15.001\n\n# install `ezpz`\n$ git clone https://github.com/saforem2/ezpz\n$ cd ezpz\n$ mkdir -p venvs/aurora/2023.12.15.001\n$ python3 -m venv venvs/aurora/2023.12.15.001 --system-site-packages\n$ source venvs/aurora/2023.12.15.001/bin/activate\n$ python3 -m pip install -e .\n\n# print job info and define `launch` alias\n$ source ezpz/src/ezpz/bin/savejobenv\n┌──────────────────────────────────────────────────────────────────\n│ [Hosts]:\n│     • x4415c6s5b0n0.hostmgmt2415.cm.aurora.alcf.anl.gov\nx4415c6s6b0n0.hostmgmt2415.cm.aurora.alcf.anl.gov\nx4415c6s7b0n0.hostmgmt2415.cm.aurora.alcf.anl.gov\nx4415c7s0b0n0.hostmgmt2415.cm.aurora.alcf.anl.gov\n└──────────────────────────────────────────────────────────────────\n┌──────────────────────────────────────────────────────────────────\n│ [DIST INFO]:\n│     • Loading job env from: /home/foremans/.pbsenv\n│     • HOSTFILE: /var/spool/pbs/aux/297306.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\n│     • NHOSTS: 4\n│     • NGPU_PER_HOST: 12\n│     • NGPUS (NHOSTS x NGPU_PER_HOST): 48\n│     • DIST_LAUNCH: mpiexec --verbose --envall -n 48 -ppn 12 --hostfile /var/spool/pbs/aux/297306.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\n│     • Defining alias: launch: aliased to mpiexec --verbose --envall -n 48 -ppn 12 --hostfile /var/spool/pbs/aux/297306.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\n└──────────────────────────────────────────────────────────────────\nLaunch with framework=pytorch, backend=DDP:\n# ----------------------------------------------------------\n# launch + startup on all workers with\n# • `framework` ∈ {`pytorch`, `tensorflow`}\n# • `backend` ∈ {`horovod`, `deepspeed`, `DDP`}\n# where `deepspeed` and `DDP` only available for `pytorch`\n# ----------------------------------------------------------\n$ launch python3 -m ezpz framework=pytorch backend=DDP\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:26][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:26][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:26][INFO][dist.py:243] - Using DDP for distributed training\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:28][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:28][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:29][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:29][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:29][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:30][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:30][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:30][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:30][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:30][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:30][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:34][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 1 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 2 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 3 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 4 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 0 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 5 / 47\n[2023-12-19 13:33:35][INFO][__main__.py:49] - {\n    \"_target_\": \"ezpz.configs.TrainConfig\",\n    \"framework\": \"pytorch\",\n    \"backend\": \"DDP\",\n    \"ds_config_path\": null,\n    \"port\": null,\n    \"seed\": null,\n    \"use_wandb\": true,\n    \"wandb_project_name\": null,\n    \"precision\": null,\n    \"ngpus\": null\n}\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 9 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 10 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 11 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 7 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 8 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 6 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 12 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 13 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 14 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 15 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 18 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 19 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 20 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 21 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 22 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 23 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 24 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 25 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 26 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 27 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 30 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 16 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 17 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 28 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 32 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 33 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 36 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 37 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 38 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 39 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 43 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 46 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 29 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 47 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 31 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 34 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 35 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 42 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 41 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 44 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 45 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 40 / 47\n[2023-12-19 13:33:47][INFO][dist.py:415] - Setting up wandb from rank: 0\n[2023-12-19 13:33:47][INFO][dist.py:416] - Using: WB PROJECT: ezpz\n[2023-12-19 13:33:58][INFO][dist.py:448] - W&B RUN: [flowing-wood-8](https://wandb.ai/l2hmc-qcd/ezpz/runs/uya29gm5)\n[2023-12-19 13:33:58][INFO][dist.py:490] - Running on x4415c6s5b0n0.hostmgmt2415.cm.aurora.alcf.anl.gov\n[2023-12-19 13:33:58][INFO][dist.py:506] - Reading hosts from /var/spool/pbs/aux/297306.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\n[2023-12-19 13:33:58][INFO][__main__.py:57] - Output dir: /lus/gecko/projects/Aurora_deployment/foremans/projects/saforem2/ezpz/src/ezpz/outputs/runs/pytorch/DDP/2023-12-19/13-33-17\n[2023-12-19 13:33:58][CRITICAL][dist.py:519] - 🚀 flowing-wood-8\n[2023-12-19 13:33:58][CRITICAL][dist.py:520] - 🔗 https://wandb.ai/l2hmc-qcd/ezpz/runs/uya29gm5\n[2023-12-19 13:33:58][CRITICAL][dist.py:521] - 📂/: /lus/gecko/projects/Aurora_deployment/foremans/projects/saforem2/ezpz/src/ezpz/outputs/runs/pytorch/DDP/2023-12-19/13-33-17/wandb/run-20231219_133354-uya29gm5/files\n[2023-12-19 13:33:58][INFO][dist.py:563] - Adding /lus/gecko/projects/Aurora_deployment/foremans/projects/saforem2/ezpz/src/ezpz/ezpz-pt-DDP-xpu.log to W&B artifact...\n[2023-12-19 13:33:58][INFO][dist.py:563] - Adding /lus/gecko/projects/Aurora_deployment/foremans/projects/saforem2/ezpz/src/ezpz/outputs/runs/pytorch/DDP/2023-12-19/13-33-17/__main__.log to W&B artifact...\n[2023-12-19 13:33:58][INFO][dist.py:563] - Adding /lus/gecko/projects/Aurora_deployment/foremans/projects/saforem2/ezpz/src/ezpz/outputs/runs/pytorch/DDP/2023-12-19/13-33-17/main_debug.log to W&B artifact...\n[2023-12-19 13:33:58][INFO][dist.py:563] - Adding /lus/gecko/projects/Aurora_deployment/foremans/projects/saforem2/ezpz/src/ezpz/outputs/runs/pytorch/DDP/2023-12-19/13-33-16/__main__.log to W&B artifact..."
  },
  {
    "objectID": "qmd/flash-attn-sunspot/index.html#impact-on-loss-bug",
    "href": "qmd/flash-attn-sunspot/index.html#impact-on-loss-bug",
    "title": "📸 flash-attn on Sunspot",
    "section": "🐛 Impact on Loss [Bug?]",
    "text": "🐛 Impact on Loss [Bug?]\nIn the q4-drop, it was observed that toggling flash-attn on / off seemed to produce different loss curves (with otherwise identical configs)\n\n\nshared-config.yaml\n\nTP: 1\nPP: 1\nGAS: 1\nOPT: adamw\ndtype: bf16\nNLAYERS: 10\nMICRO_BATCH: 2\nWORLD_SIZE: 24\n\nThis can be seen clearly in the figure below:\n\nThis was identified, and to be addressed in upcoming release."
  },
  {
    "objectID": "qmd/flash-attn-sunspot/index.html#llm-framework-release",
    "href": "qmd/flash-attn-sunspot/index.html#llm-framework-release",
    "title": "📸 flash-attn on Sunspot",
    "section": "📦 LLM Framework Release",
    "text": "📦 LLM Framework Release\nOn 05/14/2024, Intel dropped their new LLM frameworks release:\n\n\n🎁 frameworks_2024_5_v2 Announcement:\n\nHi Venkat,\nWe have shared the official Q2 release in two different forms :\nManual Setup: /gila/Aurora_deployment/anl_24_q2_release.tar.gz\nand\nModule:\nmodule use -a /home/jmitche1/anl_release/2024/q2\nmodule load frameworks_2024_5_v2\n Instructions on how to use modules with Q2 build are anl_24_q2_release/README\n\nThe release includes :\n\nMegatron-DeepSpeed 0.14.2 (with patch)\nIntel® Extension for PyTorch* v2.1.30+xpu\nTorchCCL 2.1.300\nONEAPI 2024.1.0.596.PUBLIC_IDP_2024.1.0_723\nAgama driver: 803.29\n\nThe release provides following key features:\n\nScaleup Performance improvement from the TorchCCl prototype feature enabled by TORCH_LLM_ALLREDUCE=1  details\nAuto TP inference support for more workloads\nFlash Attention V2 improvement for 256 head dimension support; MiCS support.\nLatest Features and Optimizations from DeepSpeed 0.14.2 and Intel® Extension for PyTorch* 2.1.30.\n\n\nThanks, \nJerome\n\n\n📸 flash 🤝 📷 no-flash\nWith this new release, Intel observed that the loss curves agreed exactly for flash / no-flash, using the learning rate settings below:\nlr: 0.00015\nlr_warmup_frac: 0.01\nlr_decay_iters: 320000\nTesting with Jerome’s new release:\nmodule use -a /home/jmitche1/anl_release/2024/q2\nmodule load frameworks_2024_5_v2\nI was able to independently confirm these results, shown in 📸 flash 🤝 📷 no-flash below.\n\n\n🔗 wandb links:\n\n\n[📸 flash] W&B Run: youthful-river-1832\n[📷 no-flash] W&B Run: earthy-wave-1830\n\n\n\n\n📸 flash vs. 📷 no-flash\n\n\n\n\nflash 📸 🤝 📷 no-flash\n\n\n\n\n\n🚧 Broken MPI1\nFor whatever reason, things seemed to have spontaneously broken on the night of 2024-04-14 ??\nWhen trying to run experiments the following day (05/15/2024) I was met with this[^]:\nAbort(15): Fatal error in internal_Init_thread: Other MPI error\nwhich was discussed further in this thread on slack.\nIt seems Subrata also encountered a similar issue [see: slack thread]\n\n\n✅ mpi4py fix\n\n\nTo resolve this\nAbort(15): Fatal error in internal_Init_thread: Other MPI error\nissue we can simply load the correct modules:\nmodule use -a /home/jmitche1/anl_release/2024/q2\nmodule load frameworks_2024_5_v2\nmodule use /home/ftartagl/graphics-compute-runtime/modulefiles\nmodule load graphics-compute-runtime/agama-ci-devel-803.29 \nmodule load spack-pe-gcc/0.6.1-23.275.2 gcc/12.2.0\nmodule use /soft/preview-modulefiles/24.086.0\nmodule load oneapi/release/2024.04.15.001\nFor full details see mpi4py-reproducer, and this [slack thread]."
  },
  {
    "objectID": "qmd/flash-attn-sunspot/index.html#framework-comparison",
    "href": "qmd/flash-attn-sunspot/index.html#framework-comparison",
    "title": "📸 flash-attn on Sunspot",
    "section": "🕵🏻‍ Framework Comparison ??",
    "text": "🕵🏻‍ Framework Comparison ??\nAs I was re-building MPI, and after talking to Jerome, I realized that most of the dependencies are already present in the provided frameworks/ modules on Sunspot.\nAs a simple test, I tried building a new environment built on the base conda environment2 provided by theframeworks/2023.12.15.001 module, which worked without modification and had ) most of what I needed already installed:\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; torch.__version__\n'2.1.0a0+cxx11.abi'\n&gt;&gt;&gt; import intel_extension_for_pytorch as ipex\n&gt;&gt;&gt; ipex.__version__\n'2.1.10+xpu'\n&gt;&gt;&gt; from mpi4py import MPI\nThe remaining dependencies were installed according to the instructions from the new release frameworks_2024_5_v2.\nDetails included below.\n\n\n📦 pip Install Dependencies\n\nUnfortunately, the frameworks/** don’t appear to provide DeepSpeed.\nWe can create a virtual environment on top of the base conda by\n$ module use frameworks/2023.12.15.001\n$ export PBS_O_WORKDIR=$(pwd) ; source ALCF/helpers.sh && setup_venv_from_conda\nOnce the venv has been created and activated, we can install the remaining dependencies:\nTo build / install DeepSpeed, along with its required dependencies:\n\nintel-extension-for-deepspeed:\npython3 -m pip install intel_extension_for_pytorch_deepspeed\\=\\=2.1.30 -f \"https://pytorch-extension.intel.com/release-whl/stable/xpu/us/intel-extension-for-pytorch-deepspeed/\"\nDeepSpeed:\necho \"build deepspeed\"\ngit clone https://github.com/microsoft/DeepSpeed.git\ncd DeepSpeed\ngit remote add yizhou_ds https://github.com/YizhouZ/DeepSpeed.git\ngit fetch yizhou_ds\ngit checkout yizhou/kernel_path\npip install -r requirements/requirements.txt\npython setup.py develop |& tee build.log\nExtras:\npython3 -m pip install transformers datasets python-etcd tensorboardX packaging sentencepiece bitsandbytes tiktoken neural-speed einops intel-extension-for-transformers\n\n\nLooking around the available modules a bit, I noticed a newer frameworks release (frameworks/2024.04.15.002) that had a newer version of both torch and ipex:\nmodule use /soft/preview-modulefiles/24.086.0\nmodule load frameworks/2024.04.15.002.lua\npython3 -c 'from mpi4py import MPI; print(MPI.__file__)'\n# /soft/datascience/aurora_nre_models_frameworks-2024.1_preview_u1/lib/python3.9/site-packages/mpi4py/MPI.cpython-39-x86_64-linux-gnu.so\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; torch.__version__\n'2.1.0.post2+cxx11.abi'\n&gt;&gt;&gt; import intel_extension_for_pytorch as ipex\n&gt;&gt;&gt; ipex.__version__\n'2.1.30+xpu'\n&gt;&gt;&gt; from mpi4py import MPI; print(MPI.__file__)\n/soft/datascience/aurora_nre_models_frameworks-2024.1_preview_u1/lib/python3.9/site-packages/mpi4py/MPI.cpython-39-x86_64-linux-gnu.so\nThe remaining dependencies were installed identically to what was just done previously for the frameworks/2023.12.15.001 module.\nNOTE: In the figures below, we denote these two environments as:\n\n2024.0:\n\nmodule load frameworks/2023.12.15.001\n\n2024.1:\n\nmodule use /soft/preview-modulefiles/24.086.0\nmodule load frameworks/2024.04.15.002.lua\n\nanl_24_q2_release:\n\neval \"$(~/miniconda3/bin/conda shell.zsh hook)\"\nconda activate anl_24_q2_release\n\n\n\n🥸 Fix in Disguise\nArmed now with functional environment(s) for argonne-lcf/Megatron-DeepSpeed, I was able to resume my previous experiments.\nFrom the discussion with Intel, it was hard to understand / reason about why the flash-attn fix would have any dependence on the learning rate schedule (warmup + decay).\nIf the flash-attn fix works for a particular learning rate schedule, you would reasonably expect that it should work for any learning rate schedule.\nAn additional source of confusion for me was that the discrepancy in the loss curves (seemingly) disappeared when using the learning rate settings provided by Intel3, but not when using the ALCF defaults4.\nAfter thinking about it for a bit and trying to reason about possible causes, I wondered if it might not be a mix of multiple different factors:\n\nSmall learning rate\nVery long decay\n[maybe ?] somehow dependent on the learning rate warmup fraction\n\npreliminary experiments seemed to suggest this was not the case\n\n\nSo, I was curious what would happen if I used the (larger) learning rate value from the ALCF defaults (lr=0.003) with the very long lr-decay-iters: 320000 from Intel.\nThese results are shown below.\nIn particular, for all three experiments the following learning rate settings were used:\nlr: 0.0003\nlr-warmup-frac: 0.05\nlr-decay-iters: 320000\n Looking at this figure ^, it appears that up until the very very end, all three loss curves agree identically.\nHowever, if we look closely at the very end, it looks like there might be a slight difference beginning to appear between the 2024.0 (brown line) and {anl_24_q2_release, 2024.1} ({dark, light} blue lines, respectively).\nThinking that I might be onto something, I then tried again with a smaller lr-decay-iters: 5000.\nThis result is shown below:\n In particular, we can now more clearly see the difference beginning to appear between the 2024.0 and 2024.1 loss curves.\nContinuing on, we see this effect become increasingly dramatic with even smaller values of lr-decay-iters:\n \n In each of these experiments, it appears that:\n\n2024.0:\n\nNot impacted by this lr-decay-iters dependence\nContinue to decrease for the duration of training\n\n2024.1:\n\nImpacted by the lr-decay-iters dependence\nPlateaus towards the end of training\n\n\n\n\nOlder Figs\n\n \n\n\n\n✅ 2024.0 Fix\nEverything seems to work with\nmodule load frameworks/2023.12.15.001\n \n\n\n📊 lr-decay-iters Comparison\n\n2024.0:\n\n\n\n2024.1:"
  },
  {
    "objectID": "qmd/flash-attn-sunspot/index.html#lr-decay-iters-dependence",
    "href": "qmd/flash-attn-sunspot/index.html#lr-decay-iters-dependence",
    "title": "📸 flash-attn on Sunspot",
    "section": "📈 lr-decay-iters dependence",
    "text": "📈 lr-decay-iters dependence"
  },
  {
    "objectID": "qmd/flash-attn-sunspot/index.html#performance-improvement-in-2024.1",
    "href": "qmd/flash-attn-sunspot/index.html#performance-improvement-in-2024.1",
    "title": "📸 flash-attn on Sunspot",
    "section": "🏎️ Performance Improvement in 2024.1",
    "text": "🏎️ Performance Improvement in 2024.1\n\n\nlr: 0.0003\nlr-warmup-frac: 0.05\nlr-decay-iters: null\n\n\n\nflash 📸 🤝 📷 no-flash"
  },
  {
    "objectID": "qmd/flash-attn-sunspot/index.html#footnotes",
    "href": "qmd/flash-attn-sunspot/index.html#footnotes",
    "title": "📸 flash-attn on Sunspot",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGremlins, likely↩︎\nExplicitly, aurora_nre_models_frameworks-2024.0, abbreviated as 2024.0↩︎\nIntel used the following learning rate schedule in their experiments yml   lr: 0.00015   lr-warmup-frac: 0.01   lr-decay-iters: 320000↩︎\nALCF used the following learning rate schedule in their experiments↩︎"
  },
  {
    "objectID": "qmd/l2hmc-qcd/diffusion/diffusion.html",
    "href": "qmd/l2hmc-qcd/diffusion/diffusion.html",
    "title": "MCMC + Diffusion Sampling",
    "section": "",
    "text": "2D U(1)\nfrom l2hmc.configs import dict_to_list_of_overrides\n\nseed = np.random.randint(0, 2**32)\nconsole.print(f\"seed = {seed}\")\n\noverrides = {\n    \"seed\": f\"{seed}\",\n    \"precision\": \"float32\",\n    \"init_wandb\": False,\n    \"init_aim\": False,\n    \"use_wandb\": False,\n    \"dynamics\": {\n        \"latvolume\": [32, 32],\n        \"nleapfrog\": 10,\n        \"nchains\": 16,\n        \"eps\": 0.05,\n    },\n    \"network\": {\n        \"use_batch_norm\": False,\n    },\n    'annealing_schedule': {\n        'beta_init': 6.0,\n        'beta_final': 6.0,\n    },\n\n}\nOVERRIDES = dict_to_list_of_overrides(overrides)\n\nseed = 1675333995\nfrom pathlib import Path\nfrom l2hmc.common import get_timestamp\nfrom enrich.console import get_theme, Console\nconsole = Console(theme=get_theme())\n\nOUTDIR = Path(\n    'l2hmc-diffusion-2dU1'\n).joinpath(get_timestamp(\"%Y-%m-%d\"))\nOUTDIR.mkdir(exist_ok=True, parents=True)\nconsole.print(f\"OUTDIR: {OUTDIR}\")\n\ndate = get_timestamp('%Y-%m-%d')\nPLOTS_DIR = OUTDIR.joinpath('plots')\nPLOTS_DIR.mkdir(exist_ok=True, parents=True)\nconsole.print(f\"Saving figures to: {PLOTS_DIR}\")\n\nOUTDIR: l2hmc-diffusion-2dU1/2023-09-21\n\n\n\nSaving figures to: l2hmc-diffusion-2dU1/2023-09-21/plots\n#os.environ['MASTER_PORT'] = '5436'\n\nexp = build_experiment(\n    overrides=[\n        *OVERRIDES,\n        'framework=pytorch',\n        'backend=DDP'\n    ]\n)\n\n[09/21/23 12:23:55][INFO][dist.py:226] - Caught MASTER_PORT:5561 from environment!\n[09/21/23 12:23:55][INFO][dist.py:338] - Global Rank: 0 / 0\n[09/21/23 12:23:58][INFO][experiment.py:251] - Creating outputs/2023-09-21-122358/pytorch/train\n[09/21/23 12:23:58][INFO][experiment.py:251] - Creating outputs/2023-09-21-122358/pytorch/eval\n[09/21/23 12:23:58][INFO][experiment.py:251] - Creating outputs/2023-09-21-122358/pytorch/hmc\n[09/21/23 12:23:58][INFO][dist.py:226] - Caught MASTER_PORT:5561 from environment!\n[09/21/23 12:23:58][INFO][dist.py:226] - Caught MASTER_PORT:5561 from environment!\n[09/21/23 12:24:06][INFO][trainer.py:441] - Looking for checkpoints in:\n /Users/samforeman/projects/saforem2/l2hmc-qcd/src/l2hmc/checkpoints/U1/2-32-32/nlf-10/xsplit-True/sepnets-True/merge-True/conv-8-16-32-64-128_5-3-3-3-2_2-2-2-2-2/net-16-16-16-16_dp-0.2_bn-False/pytorch\n[09/21/23 12:24:06][WARNING][trainer.py:437] - No checkpoints found to load from\n[09/21/23 12:24:06][WARNING][trainer.py:437] - Restoring global step from ckpt! self._gstep: 0\n[09/21/23 12:24:06][WARNING][trainer.py:437] - Using `torch.optim.Adam` optimizer\n[09/21/23 12:24:06][INFO][trainer.py:284] - num_params in model: 958628260\n[09/21/23 12:24:09][WARNING][trainer.py:250] - logging with freq 50 for wandb.watch\nstate = exp.trainer.dynamics.random_state(6.0)\nxdim = state.x.flatten().shape[0]\n\ndim = xdim\nlow_bound = (-np.pi) * np.ones(dim)\nhigh_bound = (np.pi) * np.ones(dim)\nsigma = 0.15\nretrains = 10\nsamples_per_retrain = 100\ndiffusion_prob = 0.1\nsns.set_context('notebook')\n\noutputs = {}\noutputs['hmc'] = exp.trainer.eval(\n    job_type='hmc',\n    beta=6.0,\n    nprint=100,\n    nchains=16,\n    eval_steps=1000\n)\n#hdset = exp.save_dataset(job_type='hmc', nchains=1)\n\n[09/21/23 12:24:21][WARNING][trainer.py:437] - Step size `eps` not specified for HMC! Using default: 0.1000 for generic HMC\n[09/21/23 12:24:21][WARNING][trainer.py:437] - x.shape (original): torch.Size([16, 2, 32, 32])\n[09/21/23 12:24:21][WARNING][trainer.py:437] - x[:nchains].shape: torch.Size([16, 2, 32, 32])\n[09/21/23 12:24:21][INFO][trainer.py:1058] - eps=0.1\nbeta=6.0\nnlog=10\ntable=&lt;rich.table.Table object at 0x2e1b98520&gt;\nnprint=100\neval_steps=1000\nnleapfrog=20\n\n\n\n\n\n\n\n\n\n[09/21/23 12:24:24][INFO][trainer.py:1188] - hstep=0 dt=0.024 beta=6.000 loss=3.410 dQsin=0.125 dQint=0.000 energy=1586.502 logprob=1586.502 logdet=0.000 acc=0.472 sumlogdet=0.000 acc_mask=0.500 plaqs=0.909 intQ=0.000 sinQ=0.051\n[09/21/23 12:24:27][INFO][trainer.py:1188] - hstep=100 dt=0.026 beta=6.000 loss=2.876 dQsin=0.163 dQint=0.000 energy=1555.800 logprob=1555.800 logdet=0.000 acc=0.593 sumlogdet=0.000 acc_mask=0.688 plaqs=0.912 intQ=-0.125 sinQ=-0.159\n[09/21/23 12:24:31][INFO][trainer.py:1188] - hstep=200 dt=0.025 beta=6.000 loss=4.678 dQsin=0.088 dQint=0.063 energy=1569.994 logprob=1569.994 logdet=0.000 acc=0.451 sumlogdet=0.000 acc_mask=0.250 plaqs=0.912 intQ=-0.187 sinQ=-0.149\n[09/21/23 12:24:34][INFO][trainer.py:1188] - hstep=300 dt=0.024 beta=6.000 loss=14.041 dQsin=0.094 dQint=0.000 energy=1554.118 logprob=1554.118 logdet=0.000 acc=0.438 sumlogdet=0.000 acc_mask=0.438 plaqs=0.914 intQ=-0.125 sinQ=-0.114\n[09/21/23 12:24:38][INFO][trainer.py:1188] - hstep=400 dt=0.024 beta=6.000 loss=-0.739 dQsin=0.199 dQint=0.000 energy=1566.516 logprob=1566.516 logdet=0.000 acc=0.509 sumlogdet=0.000 acc_mask=0.562 plaqs=0.912 intQ=-0.437 sinQ=-0.452\n[09/21/23 12:24:41][INFO][trainer.py:1188] - hstep=500 dt=0.045 beta=6.000 loss=1.545 dQsin=0.100 dQint=0.000 energy=1570.837 logprob=1570.837 logdet=0.000 acc=0.448 sumlogdet=0.000 acc_mask=0.562 plaqs=0.911 intQ=0.125 sinQ=0.189\n[09/21/23 12:24:45][INFO][trainer.py:1188] - hstep=600 dt=0.025 beta=6.000 loss=3.780 dQsin=0.094 dQint=0.000 energy=1568.012 logprob=1568.012 logdet=0.000 acc=0.463 sumlogdet=0.000 acc_mask=0.500 plaqs=0.913 intQ=0.438 sinQ=0.466\n[09/21/23 12:24:50][INFO][trainer.py:1188] - hstep=700 dt=0.023 beta=6.000 loss=-0.902 dQsin=0.113 dQint=0.000 energy=1563.778 logprob=1563.778 logdet=0.000 acc=0.475 sumlogdet=0.000 acc_mask=0.375 plaqs=0.913 intQ=0.688 sinQ=0.628\n[09/21/23 12:24:53][INFO][trainer.py:1188] - hstep=800 dt=0.024 beta=6.000 loss=11.416 dQsin=0.061 dQint=0.000 energy=1561.427 logprob=1561.427 logdet=0.000 acc=0.339 sumlogdet=0.000 acc_mask=0.438 plaqs=0.913 intQ=0.813 sinQ=0.755\n[09/21/23 12:24:57][INFO][trainer.py:1188] - hstep=900 dt=0.028 beta=6.000 loss=1.114 dQsin=0.127 dQint=0.000 energy=1564.465 logprob=1564.465 logdet=0.000 acc=0.699 sumlogdet=0.000 acc_mask=0.625 plaqs=0.913 intQ=0.938 sinQ=0.893\n# %matplotlib inline\nfrom l2hmc.common import plot_dataset\nsns.set_context('notebook')\nhdataset = outputs['hmc']['history'].get_dataset()\nplot_dataset(hdataset, outdir=PLOTS_DIR, job_type='HMC')\n\n[09/21/23 12:25:06][INFO][plot_helpers.py:1049] - Saving figure to: l2hmc-diffusion-2dU1/2023-09-21/plots/ridgeplots/svgs/energy_ridgeplot.svg\n[09/21/23 12:25:09][INFO][plot_helpers.py:1049] - Saving figure to: l2hmc-diffusion-2dU1/2023-09-21/plots/ridgeplots/svgs/logprob_ridgeplot.svg\n[09/21/23 12:25:11][INFO][plot_helpers.py:1049] - Saving figure to: l2hmc-diffusion-2dU1/2023-09-21/plots/ridgeplots/svgs/logdet_ridgeplot.svg\nimport torch\n\ninitial_states = []\nstate_init = exp.trainer.dynamics.random_state(6.0)\nx = state_init.x\nbeta = state_init.beta\n\nNSAMPLES = 1000\nfor idx in range(NSAMPLES + int(0.1 * NSAMPLES)):\n    if idx % 100 == 0:\n        console.print(f\"step: {idx}\")\n        \n    x, metrics = exp.trainer.hmc_step((x, beta))\n    if idx &gt; int((0.1 * NSAMPLES)):\n        initial_states.append(x)\n\ninitial_states = torch.stack(initial_states).squeeze()\ninitial_states_np = initial_states.detach().cpu().numpy()\n\nstep: 0\n\n\n\nstep: 100\n\n\n\nstep: 200\n\n\n\nstep: 300\n\n\n\nstep: 400\n\n\n\nstep: 500\n\n\n\nstep: 600\n\n\n\nstep: 700\n\n\n\nstep: 800\n\n\n\nstep: 900\n\n\n\nstep: 1000\ninitial_states_np.shape\n\n(999, 16, 2048)\nx_ = initial_states_np.reshape(-1, 16, 2, 32, 32)\ntmp_ = x_[:, 0, ...]\nconsole.print(f'{x_.shape}')\nconsole.print(f'{tmp_.shape}')\n\n(999, 16, 2, 32, 32)\n\n\n\n(999, 2, 32, 32)\nfrom l2hmc.common import savefig\n\n#x_ = initial_states_np[:100].reshape(-1, 2, 32, 32)\ntmp_ = x_[:, 0, ...]\nfig, ax = plt.subplots()\nsns.kdeplot(\n    x=tmp_[-100:, 0].flatten(),\n    y=tmp_[-100:, 1].flatten(),\n    # ax=ax,\n    cmap='viridis',\n    # ax=axes[0],\n    # cmap=\"Blues\",\n    shade=False,\n    # bw_adjust=0.5,\n    thresh=0\n)\nax.set_xlim((-4, 4))\nax.set_ylim((-4, 4))\nsavefig(\n    f'hmc_samples-{NSAMPLES}',\n    Path(PLOTS_DIR),\n    tstamp=True,\n)\n\nSaving hmc_samples-1000-2023-09-21-122840 to l2hmc-diffusion-2dU1/2023-09-21/plots\nclass Diffusion:\n    def __init__(\n            self,\n            noise_steps: int = 1000,\n            beta_start: float = 1e-4,\n            beta_end: float = 0.02,\n            nchannels: int = 2,\n            img_size: int = 256,\n            device: str = \"cuda\"\n    ):\n        self.noise_steps = noise_steps\n        self.beta_start = beta_start\n        self.beta_end = beta_end\n        self.img_size = img_size\n        self.device = device\n        self.nchannels = nchannels\n\n        self.beta = self.prepare_noise_schedule().to(device)\n        self.alpha = 1. - self.beta\n        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n\n    def prepare_noise_schedule(self):\n        return torch.linspace(self.beta_start, self.beta_end, self.noise_steps)\n\n    def noise_images(self, x, t):\n        sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:, None, None, None]\n        sqrt_one_minus_alpha_hat = torch.sqrt(\n            1 - self.alpha_hat[t]\n        )[:, None, None, None]\n        eps = torch.randn_like(x)\n        return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * eps, eps\n\n    def sample_timesteps(self, n):\n        return torch.randint(low=1, high=self.noise_steps, size=(n,))\n\n    def sample(self, model, n):\n        # console.print(f\"Sampling {n} new images....\")\n        model.eval()\n        with torch.no_grad():\n            x = torch.randn(\n                (n, self.nchannels, self.img_size, self.img_size)\n            ).to(self.device)\n            sample_bar = tqdm(\n                reversed(range(1, self.noise_steps)),\n                position=0,\n                total=self.noise_steps - 1,\n                dynamic_ncols=True,\n            )\n            for i in sample_bar:\n                t = (torch.ones(n) * i).long().to(self.device)\n                predicted_noise = model(x, t)\n                alpha = self.alpha[t][:, None, None, None]\n                alpha_hat = self.alpha_hat[t][:, None, None, None]\n                beta = self.beta[t][:, None, None, None]\n                if i &gt; 1:\n                    noise = torch.randn_like(x)\n                else:\n                    noise = torch.zeros_like(x)\n                x = (\n                    (1 / torch.sqrt(alpha))\n                    * (\n                        x \n                        - ((1 - alpha) / (torch.sqrt(1 - alpha_hat)))\n                        * predicted_noise\n                    ) \n                    + (torch.sqrt(beta) * noise)\n                )\n        model.train()\n        x = (x + np.pi) % (2 * np.pi) - np.pi\n        return x\ninitial_states.shape\n\ntorch.Size([999, 16, 2048])\nTrain Diffusion Model\nimport torchvision\nimport os\nimport random\nfrom pathlib import Path\nimport torch\nimport torchvision\nimport torchvision.transforms as T\nimport numpy as np\nfrom PIL import Image\n#from fastdownload import FastDownload\nfrom torch.utils.data import DataLoader\n\ndef save_images(images, path, **kwargs):\n    grid = torchvision.utils.make_grid(images, **kwargs)\n    ndarr = grid.permute(1, 2, 0).to('cpu').numpy()\n    im = Image.fromarray(ndarr)\n    im.save(path)\nBuild Diffusion Model with UNet Architecure\nfrom torch import nn\nfrom torch import optim\nimport torch.nn.functional as F\nfrom tqdm.auto import tqdm\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import TensorDataset\n\nfrom l2hmc.common import savefig\nfrom l2hmc.diffusion.modules import NoiseScheduler, UNet\nfrom l2hmc.diffusion import ddpm\n\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nconfig = {\n    'channels_in': 2,\n    'channels_out': 2,\n    'train_batch_size': 5,\n    'learning_rate': 0.001,\n    'num_epochs': 1,\n    'noise_steps': 100,\n    'beta': 6.0,\n    'img_size': 32,\n    'retrains': 10,\n    'samples_per_retrain': 500,\n    'diffusion_prob': 0.1,\n}\n\nmodel = UNet(c_in=2, c_out=2)\n\ndataset = TensorDataset(initial_states.reshape(-1, 2, 32, 32))\ndataloader = DataLoader(\n    dataset,\n    batch_size=config[\"train_batch_size\"],\n    shuffle=False,\n    drop_last=True\n)\n\n\noptimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'])\nmse = nn.MSELoss()\ndiffusion = Diffusion(\n    noise_steps=100,\n    img_size=32,\n    device=DEVICE,\n    nchannels=2,\n)\n#logger = SummaryWriter(os.path.join(\"runs\", args.run_name))\nl = len(dataloader)\n\nrun_name = 'diffusion2dU1'\nPerform initial training on HMC samples\nfrom torch import optim\ndevice = 'cpu'\n#dataloader = get_data(args)\n#model = UNet().to(device)\n\nsampled_images_history = []\n\nfor epoch in range(config['num_epochs']):\n    console.print(f\"Starting epoch {epoch}:\")\n    pbar = tqdm(dataloader)\n    for i, images in enumerate(pbar):\n        if isinstance(images, (tuple, list)) and len(images) == 1:\n            images = images[0]\n        t = diffusion.sample_timesteps(images.shape[0]).to(device)\n        x_t, noise = diffusion.noise_images(images, t)\n        predicted_noise = model(x_t, t)\n        loss = mse(noise, predicted_noise)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        pbar.set_postfix({'epoch': epoch, 'batch': i, 'MSE': loss.item()})\n    console.print(f'epoch: {epoch}, loss: {loss.item()}')\n    sampled_images = diffusion.sample(model, n=images.shape[0])\n    sampled_images_history.append(sampled_images)\n    sns.set_context('notebook')\n    #tmp = initial_states.reshape(-1, 2, 32, 32)\n    fig, ax = plt.subplots(ncols=2)\n    _ = ax[0].imshow(sampled_images[0, 0, :, :])\n    _ = ax[1].imshow(sampled_images[0, 1, :, :])\n    _ = ax[0].set_xticklabels([])\n    _ = ax[1].set_xticklabels([])\n    _ = ax[0].set_yticklabels([])\n    _ = ax[1].set_yticklabels([])\n    _ = ax[0].set_title(r\"$U_{0}$\", loc='center')\n    _ = ax[1].set_title(r\"$U_{1}$\", loc='center')\n    _ = fig.suptitle('Diffusion Samples', y=0.8)\n    plt.show()\n    savefig(fname=f'sampled_image_epoch{epoch}', outdir=PLOTS_DIR, tstamp=True)\n    MODEL_FILE = OUTDIR.joinpath(\"models\", f\"unet-diffusion-epoch{epoch}.pt\")\n    MODEL_FILE.parent.mkdir(exist_ok=True, parents=True)\n    console.print(f\"Saving model checkpoint to: {MODEL_FILE}\")\n    torch.save(model.state_dict(), MODEL_FILE)\n\nStarting epoch 0:\n\n\n\n{\"model_id\":\"19b415c346b24bef8b60336d7f7bc355\",\"version_major\":2,\"version_minor\":0}\n\n\nepoch: 0, loss: 0.6023472547531128\n\n\n\n{\"model_id\":\"eea24504754f4cb9ab4d9925a6225c10\",\"version_major\":2,\"version_minor\":0}\n\n\n\n\n\n\n\n\n\nSaving sampled_image_epoch0-2023-09-21-124506 to l2hmc-diffusion-2dU1/2023-09-21/plots\n\n\nSaving model checkpoint to: l2hmc-diffusion-2dU1/2023-09-21/models/unet-diffusion-epoch0.pt\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\nsns.set_context('notebook')\ntmp = initial_states.reshape(-1, 2, 32, 32)\nfig, ax = plt.subplots(ncols=2)\n_ = ax[0].imshow(tmp[0, 0, :, :])\n_ = ax[1].imshow(tmp[0, 1, :, :])\n_ = ax[0].set_title(r\"$U_{0}$\", loc='center')\n_ = ax[0].set_xticklabels([])\n_ = ax[1].set_xticklabels([])\n_ = ax[0].set_yticklabels([])\n_ = ax[1].set_yticklabels([])\n_ = ax[1].set_title(r\"$U_{1}$\", loc='center')\n_ = fig.suptitle('HMC Samples', y=0.8)\nsampled_images_history_ = torch.stack(sampled_images_history)\nsampled_images_history_.shape\n\ntorch.Size([1, 5, 2, 32, 32])\nsns.set_context('notebook')\nfig, ax = plt.subplots(ncols=2)\n_ = ax[0].imshow(sampled_images_history_[0][0][0])\n_ = ax[1].imshow(sampled_images_history_[0][0][1])\n_ = ax[0].set_xticklabels([])\n_ = ax[1].set_xticklabels([])\n_ = ax[0].set_yticklabels([])\n_ = ax[1].set_yticklabels([])\n_ = ax[0].set_title(r\"$U_{0}$\", loc='center')\n_ = ax[1].set_title(r\"$U_{1}$\", loc='center')\n_ = fig.suptitle('Diffusion Samples', y=0.85)\nfor idx in range(sampled_images_history_.shape[0]):\n    q = exp.trainer.lattice.charges(x=sampled_images_history_[idx])\n    console.print(f'{idx}: {q}')\n\n0: Charges(intQ=tensor([ 5.0000e+00, -4.0000e+00, -6.0000e+00, -4.5535e-07,  1.0000e+00]), sinQ=tensor([ 1.6426, -1.7244, -4.4651,  0.5680,  0.7046]))\nHMC Sampling with Diffusion\n#for retrain_iter in range(config['retrains']):\nstate = exp.trainer.dynamics.random_state(config['beta'])\nx = state.x\n\nhistories = {}\nsamples = []\nhmc_samples = []\ndiffusion_samples = []\n\nglobal_step = 0\nwatcher = {}\nupdate_types = []\ncombined_samples = {}\nglobal_step\n\n0\nfor retrain_iter in range(2):\n    console.print(f'retrain_iter: {retrain_iter}')\n    ndiff_acc = 0\n    ndiff_proposed = 0\n    histories[retrain_iter] = {\n        'diffusion': [],\n        'hmc': [],\n    }\n    #for idx in range(config['samples_per_retrain']):\n    sbar = tqdm(range(10))\n    for idx in sbar:\n        t0_ = time.perf_counter()\n        if idx % 100 == 0:\n            console.print(f'sample idx: {idx}')\n        rand = np.random.uniform()\n        if (retrain_iter &gt;= 1) and rand &lt; diffusion_prob:\n            console.print(f'rand: {rand} &lt; {diffusion_prob}')\n            # Sample from diffusion model\n            x_ = diffusion.sample(model, n=x.shape[0])\n            ll_ = exp.trainer.dynamics.potential_energy(x_, config['beta'])\n            ll = exp.trainer.dynamics.potential_energy(x, config['beta'])\n            ratio = ll_ / ll\n            a = torch.min(torch.ones_like(ratio), ratio)\n            u = torch.rand(a.shape)\n            #u = np.random.uniform()\n            #for jdx in range(u.shape[0]):\n            #    if u[jdx] &lt; a[jdx]:\n            #        samples.append(x_[jdx])\n            #        diffusion_samples.append(x_[jdx])\n            #x = torch.where((u &lt; a), x_, x.reshape_as(x_)).reshape_as(x)\n            x = torch.where((u &lt; a)[:, None, None, None], x_, x.reshape_as(x_))\n            samples.append(x)\n            diffusion_samples.append(x)\n            combined_samples[global_step] = x\n            watcher[global_step] = 'diffusion'\n            #diffusion_samples.extend(x)\n            #samples.extend(x)\n            #ndiff_acc += \n            #if u &lt; a:\n            #    console.print('Accepted diffusion sample!')\n            #    console.print(f'{ndiff_acc} / {ndiff_proposed}')\n            #    ndiff_acc += 1\n            #    x = x_\n            #    diffusion_samples.append(x)\n            #    samples.append(x)\n        else:\n            # Oherwise, HMC\n            x, metrics = exp.trainer.hmc_step((x, config['beta']))\n            hmc_samples.append(x)\n            samples.append(x)\n            combined_samples[global_step] = x\n            watcher[global_step] = 'HMC'\n        smetrics = {\n            'idx': idx,\n            'global_step': global_step,\n            'dt': time.perf_counter() - t0_,\n        }\n        global_step += 1\n        #smetrics |= {\n        #    f'{k}': {torch.tensor(v).mean().item()} for k, v in metrics.items()\n        #}\n        sbar.set_postfix(smetrics)\n    # Train loop\n    dataset = TensorDataset(\n        torch.stack(hmc_samples).reshape(-1, 2, 32, 32)\n    )\n    dataloader = DataLoader(\n        dataset,\n        shuffle=False,\n        drop_last=True,\n        batch_size=config[\"train_batch_size\"],\n    )\n    pbar = tqdm(dataloader)\n    for i, batch in enumerate(pbar):\n        if i == 0:\n            console.print('Retraining...')\n        if isinstance(batch, (tuple, list)) and len(batch) == 1:\n            batch, = batch\n        batch = batch.reshape(-1, 2, 32, 32)\n        t0 = time.time()\n        t = diffusion.sample_timesteps(batch.shape[0]).to(device)\n        x_t, noise = diffusion.noise_images(batch, t)\n        predicted_noise = model(x_t, t)\n        loss = mse(noise, predicted_noise)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        t1 = time.time()\n        pbar.set_postfix(\n            {\n                'global_step': global_step,\n                'retrain_iter': retrain_iter,\n                'batch': i,\n                'dt': t1 - t0,\n                'MSE': loss.item()\n            }\n        )\n\nretrain_iter: 0\n\n\n\n{\"model_id\":\"17132d7ca8624fa387ee9467e4f1fa4d\",\"version_major\":2,\"version_minor\":0}\n\n\nsample idx: 0\n\n\n\n{\"model_id\":\"0ed1080fdebd4f7b9aae80db0d36b96b\",\"version_major\":2,\"version_minor\":0}\n\n\nRetraining...\n\n\n\nretrain_iter: 1\n\n\n\n{\"model_id\":\"d0346019e21b4d2a9b624dc59e84015b\",\"version_major\":2,\"version_minor\":0}\n\n\nsample idx: 0\n\n\n\nrand: 0.05506106760134255 &lt; 0.1\n\n\n\n{\"model_id\":\"c02b09d53ada46a194a47921f0ab3cba\",\"version_major\":2,\"version_minor\":0}\n\n\nrand: 0.07860283644524213 &lt; 0.1\n\n\n\n{\"model_id\":\"184df3f1c9714ece9756866b2617ed02\",\"version_major\":2,\"version_minor\":0}\n\n\n{\"model_id\":\"eaa0d84229c04618b7a2bffe2a4b1739\",\"version_major\":2,\"version_minor\":0}\n\n\nRetraining...\nconsole.print('\\n'.join([f\"{i.shape}\" for i in samples[:100]]))\n\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2, 32, 32])\ntorch.Size([16, 2048])\ntorch.Size([16, 2, 32, 32])\nsamples_ = torch.stack([i.reshape(-1, 2, 32, 32) for i in samples])\nsamples_.shape\n\ntorch.Size([30, 16, 2, 32, 32])\nlen(hmc_samples)\n\n28\nlen(diffusion_samples)\n\n2\nhmc_samples_ = torch.stack([i.reshape(-1, 2, 32, 32) for i in hmc_samples])\ndiffusion_samples_ = torch.stack(\n    [i.reshape(-1, 2, 32, 32) for i in diffusion_samples]\n)\nhmc_samples_.shape\n\ntorch.Size([28, 16, 2, 32, 32])\ndiffusion_samples_.shape\n\ntorch.Size([2, 16, 2, 32, 32])\nsamples_.shape\n\ntorch.Size([30, 16, 2, 32, 32])\ndef calc_plaqs(x):\n    return torch.stack([\n        exp.trainer.lattice.plaqs(\n            x[:, idx]\n        ) for idx in range(x.shape[1])\n    ], -1)\n\ndef calc_intQ(x):\n    return torch.stack([\n        exp.trainer.lattice.int_charges(\n            x[:, idx]\n        ) for idx in range(x.shape[1])\n    ], -1)\n    \ndef calc_sinQ(x):\n    return torch.stack([\n        exp.trainer.lattice.sin_charges(\n            x[:, idx]\n        ) for idx in range(x.shape[1])\n    ], -1)\nsamples_init_ = initial_states.reshape(-1, initial_states.shape[1], 2, 32, 32)\nsamples_init_.shape\n\ntorch.Size([999, 16, 2, 32, 32])\nmetrics_init_ = {\n    'plaqs': calc_plaqs(samples_init_),\n    'intQ': calc_intQ(samples_init_),\n    'sinQ': calc_sinQ(samples_init_)\n}\n    \nmetrics_ = {\n    'plaqs': calc_plaqs(samples_),\n    'intQ': calc_intQ(samples_),\n    'sinQ': calc_sinQ(samples_)\n}\n\nmetrics_hmc_ = {\n    'plaqs': calc_plaqs(hmc_samples_),\n    'intQ': calc_intQ(hmc_samples_),\n    'sinQ': calc_sinQ(hmc_samples_)\n}\n\nmetrics_diffusion_ = {\n    'plaqs': calc_plaqs(diffusion_samples_),\n    'intQ': calc_intQ(diffusion_samples_),\n    'sinQ': calc_sinQ(diffusion_samples_)\n}\nmetrics_['plaqs'].shape\n\ntorch.Size([30, 16])\nconsole.print('\\n'.join([f\"{k}: {v}\" for k, v in watcher.items()]))\n\n0: HMC\n1: HMC\n2: HMC\n3: HMC\n4: HMC\n5: HMC\n6: HMC\n7: HMC\n8: HMC\n9: HMC\n10: HMC\n11: HMC\n12: HMC\n13: HMC\n14: HMC\n15: HMC\n16: HMC\n17: HMC\n18: HMC\n19: HMC\n20: HMC\n21: HMC\n22: HMC\n23: HMC\n24: HMC\n25: HMC\n26: HMC\n27: diffusion\n28: HMC\n29: diffusion\nfig, ax = plt.subplots()\n\n_ = ax.plot(metrics_['plaqs'][:, 0], label='Combined')\n_ = ax.plot(metrics_hmc_['plaqs'][:, 0], label='HMC')\n_ = ax.plot(metrics_diffusion_['plaqs'][:, 0], label='Diffusion')\n#_ = ax.plot(metrics_hmc1['plaqs'], label='HMC 1')\n#_ = ax.plot(metrics_diff_['plaqs'], label='Diffusion')\n_ = ax.legend(loc='upper left', bbox_to_anchor=(1.05, 1.00))\nfig, ax = plt.subplots(ncols=3, figsize=(14, 4))\nfor idx, (key, val) in enumerate(metrics_init_.items()):\n    _ = ax[idx].plot(val[:, 0], label='HMC (Initial Samples)')\n    _ = ax[idx].set_ylabel(key, loc='center')\n    _ = ax[idx].set_xlabel('Draw', loc='center')\n    #_ = ax[idx].legend(loc='best', frameon=True, edgecolor=\"#838383\")\n\n_ = fig.suptitle(f\"Initial HMC States\", y=0.92)\nfig, ax = plt.subplots(ncols=3, figsize=(14, 4))\nfor idx, (key, val) in enumerate(metrics_.items()):\n    _ = ax[idx].plot(val[:, 0], label='Combined')\n    _ = ax[idx].set_ylabel(key, loc='center')\n    _ = ax[idx].set_xlabel('Draw', loc='center')\n    #_ = ax[idx].legend(loc='best', frameon=True, edgecolor=\"#838383\")\n\n_ = fig.suptitle(f\"Combined Samples\", y=0.92)\nfig, ax = plt.subplots(ncols=3, figsize=(14, 4))\nfor idx, (key, val) in enumerate(metrics_hmc_.items()):\n    _ = ax[idx].plot(val[:, 0], label='HMC')\n    _ = ax[idx].set_ylabel(key, loc='center')\n    _ = ax[idx].set_xlabel('Draw', loc='center')\n_ = fig.suptitle(f\"Generated HMC States\", y=0.92)\nfig, ax = plt.subplots(ncols=3, figsize=(14, 4))\nfor idx, (key, val) in enumerate(metrics_diffusion_.items()):\n    _ = ax[idx].plot(val[:, 0], label='Diffusion')\n    _ = ax[idx].set_ylabel(key, loc='center')\n    _ = ax[idx].set_xlabel('Draw', loc='center')\n_ = fig.suptitle(f\"Generated Diffusion States\", y=0.92)\nfrom l2hmc.lattice.u1.pytorch.lattice import plaq_exact\nplaq_exact(torch.tensor(6.0))\n\ntensor(0.9124)\nfig, ax = plt.subplots()\n#_ = plt.hist(metrics_['intQ'].flatten(), color='C0', alpha=0.6, label='Combined', edgecolor='none')\n_ = ax.hist(\n    metrics_diffusion_['intQ'].flatten(),\n    color='C0',\n    alpha=0.6,\n    edgecolor='none',\n    label='Diffusion',\n    density=True,\n)\n_ = ax.hist(\n    metrics_hmc_['intQ'].flatten(),\n    color='C1',\n    alpha=0.6,\n    edgecolor='none',\n    label='HMC',\n    density=True,\n)\n_ = ax.legend(loc='best', frameon=True, edgecolor='#666666')\n_ = ax.set_xlabel(r\"$Q$\", loc='center')\n_ = ax.set_title('Topological Charge ($Q$) Distribution', loc='center')\nfig, ax = plt.subplots()\n_ = plt.plot(metrics_['plaqs'][:, 0], color='C0', label='Diffusion')\n_ = plt.plot(metrics_hmc_['plaqs'][:, 0], color='C1', label='HMC')\n_ = ax.legend(loc='best', frameon=True, edgecolor='#666666', ncols=2)\n_ = ax.set_ylabel(r\"$\\left\\langle U_{\\mu\\nu}\\right\\rangle $\", loc='center')\n_ = ax.set_xlabel(f\"Draw\", loc='center')\nwloops = {\n    'hmc': [\n        exp.trainer.lattice.wilson_loops(i) for i in hmc_samples_\n    ],\n    'diffusion': [\n        exp.trainer.lattice.wilson_loops(i) for i in diffusion_samples_\n    ],\n}\n\nplaqs = {\n    'hmc': [\n        exp.trainer.lattice.plaqs(i) for i in hmc_samples_\n    ],\n    'diffusion': [\n        exp.trainer.lattice.plaqs(i) for i in diffusion_samples_\n    ],\n}\nwlhmc = torch.stack(wloops['hmc']).squeeze()\nwldiff = torch.stack(wloops['diffusion']).squeeze()\nwlhmc.shape\n\ntorch.Size([28, 16, 32, 32])\n_ = plt.tight_layout()\nfor idx in range(2):\n    fig, ax = plt.subplots(ncols=2)\n    _ = ax[0].imshow(wlhmc[idx, 0])\n    _ = ax[0].set_title(\"HMC\", loc='center')\n    _ = ax[1].imshow(wldiff[idx, 0])\n    _ = ax[1].set_title(\"Diffusion\", loc='center')\n    _ = fig.suptitle(r\"$U_{\\mu\\nu}$\", y=0.8)\n    for ax_ in ax:\n        _ = ax_.set_xticklabels([])\n        _ = ax_.set_yticklabels([])\n\n&lt;Figure size 640x480 with 0 Axes&gt;\nqhmc = metrics_hmc_['intQ']\nqdiff = metrics_diffusion_['intQ']\nqhmc.shape\n\ntorch.Size([28, 16])\nphmc = torch.stack(plaqs['hmc']).squeeze()\npdiff = torch.stack(plaqs['diffusion']).squeeze()\nphmc.shape\n\ntorch.Size([28, 16])\npdiff.shape\n\ntorch.Size([2, 16])\nfig, ax = plt.subplots()\n\n_ = ax.hist(\n    metrics_['plaqs'].flatten(),\n    color='C1',\n    histtype='step',\n    stacked=True,\n    density=True,\n    label='HMC',\n    linewidth=1.5\n)\n_ = ax.hist(\n    metrics_diffusion_['plaqs'].flatten(),\n    color='C0',\n    histtype='step',\n    stacked=True,\n    density=True,\n    label='Diffusion',\n    linewidth=1.5\n)\n_ = ax.hist(\n    metrics_hmc_['plaqs'].flatten(),\n    color='C2',\n    histtype='step',\n    stacked=True,\n    density=True,\n    label='Combined',\n    linewidth=1.5\n)\n_ = ax.set_xlabel(r\"$U_{\\mu\\nu}$\", loc='center')\n_ = ax.legend(\n    loc='upper left',\n    frameon=True,\n    #ncols=2,\n    bbox_to_anchor=(0.55, 1.00),\n    edgecolor=\"#838383\",\n)\n_ = ax.set_title('Plaquette Distribution', loc='center')\nfig, ax = plt.subplots()\n\n_ = ax.hist(\n    metrics_['intQ'].flatten(),\n    color='C1',\n    histtype='step',\n    stacked=True,\n    density=True,\n    label='HMC',\n    linewidth=1.5\n)\n_ = ax.hist(\n    metrics_diffusion_['intQ'].flatten(),\n    color='C0',\n    histtype='step',\n    stacked=True,\n    density=True,\n    label='Diffusion',\n    linewidth=1.5\n)\n_ = ax.hist(\n    metrics_hmc_['intQ'].flatten(),\n    color='C2',\n    histtype='step',\n    stacked=True,\n    density=True,\n    label='Combined',\n    linewidth=1.5\n)\n_ = ax.set_xlabel('$Q_{\\mathbb{Z}}$', loc='center')\n_ = ax.legend(\n    loc='upper left',\n    frameon=True,\n    #ncols=2,\n    bbox_to_anchor=(0.55, 1.00),\n    edgecolor=\"#838383\",\n)\n_ = ax.set_title('Charge Distribution', loc='center')\nglobal_step = 0\nframes = []\nlosses = []\nprint(\"Training model...\")\nfor epoch in range(config[\"num_epochs\"]):\n    model.train()\n    progress_bar = tqdm(total=len(dataloader))\n    progress_bar.set_description(f\"Epoch {epoch}\")\n    for step, batch in enumerate(dataloader):\n        t = diffusion.sample_timesteps(images.shape[0]).to(device)\n\n        noise = torch.randn(batch.shape)\n        timesteps = torch.randint(\n            0, noise_scheduler.num_timesteps, (batch.shape[0],)\n        ).long()\n\n        #noisy = noise_scheduler.add_noise(batch, noise, timesteps)\n        noisy = noise_scheduler.noise_images(batch, timesteps)\n        noise_pred = model(noisy, timesteps)\n        loss = F.mse_loss(noise_pred, noise)\n        loss.backward(loss)\n\n        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        optimizer.zero_grad()\n\n        progress_bar.update(1)\n        logs = {\"loss\": loss.detach().item(), \"step\": global_step}\n        losses.append(loss.detach().item())\n        progress_bar.set_postfix(**logs)\n        global_step += 1\n    progress_bar.close()\n\n    if epoch % config[\"save_images_step\"] == 0 or epoch == config[\"num_epochs\"] - 1:\n        # generate data with the model to later visualize the learning process\n        model.eval()\n        sample = torch.randn(config[\"eval_batch_size\"], 2)\n        timesteps = list(range(len(noise_scheduler)))[::-1]\n        for i, t in enumerate(tqdm(timesteps)):\n            t = torch.from_numpy(np.repeat(t, config[\"eval_batch_size\"])).long()\n            with torch.no_grad():\n                residual = model(sample, t)\n            sample = noise_scheduler.step(residual, t[0], sample)\n        frames.append(sample.numpy())\ndataset[6]\nlen(dataloader)\neval_batch_size = 10\nnum_timesteps = 50\nplot_step = 5\nnoise_scheduler = ddpm.NoiseScheduler(num_timesteps=num_timesteps)\nsample = torch.randn(eval_batch_size, 2)\ntimesteps = list(range(num_timesteps))[::-1]\nsamples = []\nsteps = []\n\nretrains = 10\ndiffusion_prob = 0.3\nsamples_per_retrain = 100\neval_batch_size = 10\nt = torch.from_numpy(np.repeat(timesteps[0], eval_batch_size)).long()\nwith torch.no_grad():\n    residual = model(sample, t)\nsample_ = noise_scheduler.step(residual, t[0], sample)\nsample.shape\nresidual.shape\nsample_.shape\ndiffusion_samples = []\nhmc_samples = []\nbeta = 1.\nfor retrain_iter in range(retrains):\n    console.print(f'retrain_iter: {retrain_iter}')\n    ndiff_acc = 0\n    ndiff_proposed = 0\n    for idx in range(samples_per_retrain):\n        console.print(f'sample idx: {idx}')\n        rand = np.random.uniform()\n        if rand &lt; diffusion_prob:\n            ndiff_proposed += 1\n            rand_pick = randrange(len(dataloader))\n            #theta_prime = dataset[rand_pick]\n            t = torch.from_numpy(np.repeat(t, eval_batch_size)).long()\n            with torch.no_grad():\n                residual = model(sample, t)\n            sample_ = noise_scheduler.step(residual, t[0], sample)\n            ratio = (\n                log_likelihood_2dU1(sample_, 2)\n                / log_likelihood_2dU1(sample, 2)\n            )\n            a = min(1, ratio)\n            u = np.random.uniform()\n            if u &lt; a:\n                ndiff_acc += 1\n                sample = sample_\n                diffusion_samples.append(sample)\n        else:\n            sample_, metrics = exp.trainer.hmc_step((sample_, beta))\n            hmc_samples.append(sample)\nfor i, t in enumerate(tqdm(timesteps)):\n    t = torch.from_numpy(np.repeat(t, eval_batch_size)).long()\n    with torch.no_grad():\n        residual = model(sample, t)\n    sample = noise_scheduler.step(residual, t[0], sample)\n    if (i + 1) % plot_step == 0:\n        samples.append(sample.numpy())\n        steps.append(i + 1)\nAlternate\ndiffusion_ = DiffusionAlt(img_size=64, device='cpu')\nunet\nimage = torch.rand(1, 2, 64, 64)\nt = diffusion_.sample_timesteps(image.shape[0]).to('cpu')\nunet(image, t)\ndiffusion_.sample("
  },
  {
    "objectID": "qmd/l2hmc-qcd/diffusion/diffusion.html#denoising-diffusion-probabilistic-models",
    "href": "qmd/l2hmc-qcd/diffusion/diffusion.html#denoising-diffusion-probabilistic-models",
    "title": "MCMC + Diffusion Sampling",
    "section": "Denoising Diffusion Probabilistic Models",
    "text": "Denoising Diffusion Probabilistic Models"
  },
  {
    "objectID": "qmd/l2hmc-qcd/diffusion/diffusion.html#imports--setup",
    "href": "qmd/l2hmc-qcd/diffusion/diffusion.html#imports--setup",
    "title": "MCMC + Diffusion Sampling",
    "section": "Imports / Setup",
    "text": "Imports / Setup\n\nfrom __future__ import absolute_import, print_function, annotations, division\nfrom dataclasses import dataclass\n\nimport sys\nimport os\nimport math\nimport numpy as np\nimport scipy\nimport time\nfrom random import randrange\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n\nfrom ezpz.dist import setup_torch\n\nport = np.random.randint(5000, 6000)\nprint(f\"Using port: {port}\")\n\nRANK = setup_torch(\n    backend=\"DDP\",\n    port=f\"{port}\"\n)\n\n\n    Using port: 5561\n\n\nUsing DDP for distributed training\n\n\n\nGlobal Rank: 0 / 0\n\n\n:::\n\n%matplotlib inline\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n\nfrom l2hmc.main import build_experiment\nfrom l2hmc.utils.rich import get_console\nfrom l2hmc.utils.plot_helpers import set_plot_style\n\nimport opinionated\nfrom l2hmc.diffusion.diffusion import PureMH, MH_Diffusion\nfrom l2hmc.utils.plot_helpers import set_plot_style\n\nfrom pandas.io.formats import style\nimport scipy\nimport time\nfrom random import randrange\nfrom l2hmc.diffusion.diffusion import PureMH, MH_Diffusion\n\nset_plot_style()\nconsole = get_console()\nprint(console.is_jupyter)\nif console.is_jupyter:\n    console.is_jupyter = False\nprint(console.is_jupyter)\n\nUsing device: cpu\n\n\n\nFailed to download font: Source Sans Pro, skipping! Failed to download font: Titillium WebRoboto Condensed, skipping!\n\n\nTrue\n\n\n\nFalse\n\n\n\nplt.style.use(opinionated.STYLES['opinionated_min'])\nsns.set_context('notebook')"
  },
  {
    "objectID": "qmd/l2hmc-qcd/diffusion/diffusion.html#2d-u1",
    "href": "qmd/l2hmc-qcd/diffusion/diffusion.html#2d-u1",
    "title": "MCMC + Diffusion Sampling",
    "section": "2D U(1)",
    "text": "2D U(1)"
  },
  {
    "objectID": "qmd/l2hmc-qcd/diffusion/diffusion.html#train-diffusion-model",
    "href": "qmd/l2hmc-qcd/diffusion/diffusion.html#train-diffusion-model",
    "title": "MCMC + Diffusion Sampling",
    "section": "Train Diffusion Model",
    "text": "Train Diffusion Model"
  },
  {
    "objectID": "qmd/l2hmc-qcd/diffusion/diffusion.html#build-diffusion-model-with-unet-architecure",
    "href": "qmd/l2hmc-qcd/diffusion/diffusion.html#build-diffusion-model-with-unet-architecure",
    "title": "MCMC + Diffusion Sampling",
    "section": "Build Diffusion Model with UNet Architecure",
    "text": "Build Diffusion Model with UNet Architecure"
  },
  {
    "objectID": "qmd/l2hmc-qcd/diffusion/diffusion.html#hmc-sampling-with-diffusion",
    "href": "qmd/l2hmc-qcd/diffusion/diffusion.html#hmc-sampling-with-diffusion",
    "title": "MCMC + Diffusion Sampling",
    "section": "HMC Sampling with Diffusion",
    "text": "HMC Sampling with Diffusion"
  },
  {
    "objectID": "qmd/l2hmc-qcd/diffusion/diffusion.html#alternate",
    "href": "qmd/l2hmc-qcd/diffusion/diffusion.html#alternate",
    "title": "MCMC + Diffusion Sampling",
    "section": "Alternate",
    "text": "Alternate"
  },
  {
    "objectID": "qmd/posts.html",
    "href": "qmd/posts.html",
    "title": "🔥 Hot off the Press 📰",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nMay 29, 2024\n\n\n📸 flash-attn on Sunspot\n\n\nSam Foreman \n\n\n\n\nMay 25, 2024\n\n\n🐛 mpi4py bug on Sunspot\n\n\nSam Foreman \n\n\n\n\nMay 14, 2024\n\n\nHow to Make Dope Slides\n\n\nSam Foreman \n\n\n\n\nApr 15, 2024\n\n\nMCMC + Diffusion Sampling\n\n\nSam Foreman \n\n\n\n\nApr 15, 2024\n\n\nRecent Talks\n\n\nSam Foreman \n\n\n\n\nApr 3, 2024\n\n\nMegatron DeepSpeed on xpu\n\n\nSam Foreman \n\n\n\n\nMar 21, 2024\n\n\nStarting Up Distributed Training\n\n\nSam Foreman \n\n\n\n\nMar 17, 2024\n\n\nPosts\n\n\nSam Foreman \n\n\n\n\nFeb 12, 2024\n\n\nLoooooooong Sequence Lengths\n\n\nSam Foreman \n\n\n\n\nOct 5, 2023\n\n\nProjects\n\n\nSam Foreman \n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{foreman2024,\n  author = {Foreman, Sam},\n  title = {Personal {Website}},\n  date = {2024-05-29},\n  url = {https://samforeman.me},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nForeman, Sam. 2024. “Personal Website.” May 29, 2024. https://samforeman.me."
  },
  {
    "objectID": "qmd/slides.html#section",
    "href": "qmd/slides.html#section",
    "title": "Recent Talks",
    "section": "📆 2024",
    "text": "📆 2024\n\n\n\n\n\n\n🪧 Parallel Training Techniques @ AI-4-Science Training Series [03/2024]\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\n🪧 LLMs from Scratch @ LLM Tutorial Workshop (2) [02/2024]\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;"
  },
  {
    "objectID": "qmd/slides.html#section-1",
    "href": "qmd/slides.html#section-1",
    "title": "Recent Talks",
    "section": "📆 2023",
    "text": "📆 2023\n\n\n\n\n\n\n🪧 Creating Small(-ish) LLMs @ LLM Tutorial Workshop (1) [11/2023]\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n🪧 Exascale Science on Aurora @ Intel oneAPI Workshop @ UIC [10/2023]\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\n🪧 LLM Lunch Talk @ ALCF Hands On HPC Workshop [10/2023]\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n&lt;p&gt;\n\n\n\n\n\n\n\n\n\n🪧 Scaling LLMs for Science and Ongoing Collaborations @ Data-Intensive Computing and AI/ML at Scale [08/2023]\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n&lt;p&gt;\n\n\n\n\n\n\n\n\n\n🪧 MLMC: Machine Learning Monte Carlo @ Lattice 2023 [07/2023]\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n&lt;p&gt;\n\n\n\n\n\n\n\n\n\n🪧 Generative Modeling and Efficient Sampling @ PASC23 [07/2023]\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n&lt;p&gt;\n\n\n\n\n\n\n\n\n\n🪧 Efficient Sampling for Lattice Gauge Theory @ Deep Fridays @ U. Bologna [04/2023]\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n&lt;p&gt;"
  },
  {
    "objectID": "qmd/slides.html#section-2",
    "href": "qmd/slides.html#section-2",
    "title": "Recent Talks",
    "section": "📆 2022",
    "text": "📆 2022\n\n\n\n\n\n\n🪧 Large Scale Training @ Introduction to AI4Science on Supercomputers (ALCF) [11/2022]\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n&lt;p&gt;\n\n\n\n\n\n\n\n\n\n🪧 Hyperparameter Management @ ALCF Simulation, Data, and Learning Workshop [10/2022]\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n&lt;p&gt;\n\n\n\n\n\n\n\n\n\n🪧 Statistical Learning @ ATPESC 2022 [08/2022]\n\n\n\n\n\n\n📕 accompanying notebook\n\n&lt;p&gt;\n&lt;p&gt;Your browser does not support iframes.&lt;/p&gt;\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\n\n🪧 Scientific Data Science: An Emerging Symbiosis @ ANL [05/2022]\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n&lt;p&gt;\n\n\n\n\n\n\n\n\n\n🪧 Machine Learning in HEP @ UNC Greensboro [03/2022]\n\n\n\n\n\n\nMachine Learning in HEP, at UNC Greensboro, March 2022\n\n&lt;p&gt;\n&lt;p&gt;Your browser does not support iframes.&lt;/p&gt;\n&lt;/p&gt;"
  },
  {
    "objectID": "qmd/slides.html#section-3",
    "href": "qmd/slides.html#section-3",
    "title": "Recent Talks",
    "section": "📆 2021",
    "text": "📆 2021\n\n\n\n\n\n\n🪧 Accelerated Sampling Methods for Lattice Gauge Theory, @ BNL / DWQ @ 25 [12/2021]\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n&lt;p&gt;\n\n\n\n\n\n\n\n\n\n🪧 Training Topological Samplers for Lattice Gauge Theory @ ML4HEP, ECT* Trento [09/2021]\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n&lt;p&gt;\n\n\n\n\n\n\n\n\n\n🪧 l2hmc-qcd @ MIT Lattice Group Seminar [2021]\n\n\n\n\n\nl2hmc-qcd at the MIT Lattice Group Seminar, 2021\n\n\n\n\n\n\n\n\n\n🪧 Deep Learning HMC for Improved Gauge Generation @ ML in LQCD Workshop [2021]\n\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;"
  },
  {
    "objectID": "qmd/slides.html#section-4",
    "href": "qmd/slides.html#section-4",
    "title": "Recent Talks",
    "section": "📆 2020",
    "text": "📆 2020\n\n\n\n\n\n\n🪧 Machine Learning for Lattice QCD @ U. Iowa (2020)\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;"
  }
]