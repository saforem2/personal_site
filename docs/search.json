[
  {
    "objectID": "posts/ai-for-physics/diffusion/index.html",
    "href": "posts/ai-for-physics/diffusion/index.html",
    "title": "🎲 MCMC + Diffusion Sampling",
    "section": "",
    "text": "2D U(1)\nfrom l2hmc.configs import dict_to_list_of_overrides\n\nseed = np.random.randint(0, 2**32)\nconsole.print(f\"seed = {seed}\")\n\noverrides = {\n    \"seed\": f\"{seed}\",\n    \"precision\": \"float32\",\n    \"init_wandb\": False,\n    \"init_aim\": False,\n    \"use_wandb\": False,\n    \"dynamics\": {\n        \"latvolume\": [32, 32],\n        \"nleapfrog\": 10,\n        \"nchains\": 16,\n        \"eps\": 0.05,\n    },\n    \"network\": {\n        \"use_batch_norm\": False,\n    },\n    'annealing_schedule': {\n        'beta_init': 6.0,\n        'beta_final': 6.0,\n    },\n\n}\nOVERRIDES = dict_to_list_of_overrides(overrides)\n\nseed = 1675333995\nfrom pathlib import Path\nfrom l2hmc.common import get_timestamp\nfrom enrich.console import get_theme, Console\nconsole = Console(theme=get_theme())\n\nOUTDIR = Path(\n    'l2hmc-diffusion-2dU1'\n).joinpath(get_timestamp(\"%Y-%m-%d\"))\nOUTDIR.mkdir(exist_ok=True, parents=True)\nconsole.print(f\"OUTDIR: {OUTDIR}\")\n\ndate = get_timestamp('%Y-%m-%d')\nPLOTS_DIR = OUTDIR.joinpath('plots')\nPLOTS_DIR.mkdir(exist_ok=True, parents=True)\nconsole.print(f\"Saving figures to: {PLOTS_DIR}\")\n\nOUTDIR: l2hmc-diffusion-2dU1/2023-09-21\n\n\n\nSaving figures to: l2hmc-diffusion-2dU1/2023-09-21/plots\n#os.environ['MASTER_PORT'] = '5436'\n\nexp = build_experiment(\n    overrides=[\n        *OVERRIDES,\n        'framework=pytorch',\n        'backend=DDP'\n    ]\n)\n\n[09/21/23 12:23:55][INFO][dist.py:226] - Caught MASTER_PORT:5561 from environment!\n[09/21/23 12:23:55][INFO][dist.py:338] - Global Rank: 0 / 0\n[09/21/23 12:23:58][INFO][experiment.py:251] - Creating outputs/2023-09-21-122358/pytorch/train\n[09/21/23 12:23:58][INFO][experiment.py:251] - Creating outputs/2023-09-21-122358/pytorch/eval\n[09/21/23 12:23:58][INFO][experiment.py:251] - Creating outputs/2023-09-21-122358/pytorch/hmc\n[09/21/23 12:23:58][INFO][dist.py:226] - Caught MASTER_PORT:5561 from environment!\n[09/21/23 12:23:58][INFO][dist.py:226] - Caught MASTER_PORT:5561 from environment!\n[09/21/23 12:24:06][INFO][trainer.py:441] - Looking for checkpoints in:\n /Users/samforeman/projects/saforem2/l2hmc-qcd/src/l2hmc/checkpoints/U1/2-32-32/nlf-10/xsplit-True/sepnets-True/merge-True/conv-8-16-32-64-128_5-3-3-3-2_2-2-2-2-2/net-16-16-16-16_dp-0.2_bn-False/pytorch\n[09/21/23 12:24:06][WARNING][trainer.py:437] - No checkpoints found to load from\n[09/21/23 12:24:06][WARNING][trainer.py:437] - Restoring global step from ckpt! self._gstep: 0\n[09/21/23 12:24:06][WARNING][trainer.py:437] - Using `torch.optim.Adam` optimizer\n[09/21/23 12:24:06][INFO][trainer.py:284] - num_params in model: 958628260\n[09/21/23 12:24:09][WARNING][trainer.py:250] - logging with freq 50 for wandb.watch\nstate = exp.trainer.dynamics.random_state(6.0)\nxdim = state.x.flatten().shape[0]\n\ndim = xdim\nlow_bound = (-np.pi) * np.ones(dim)\nhigh_bound = (np.pi) * np.ones(dim)\nsigma = 0.15\nretrains = 10\nsamples_per_retrain = 100\ndiffusion_prob = 0.1\nsns.set_context('notebook')\n\noutputs = {}\noutputs['hmc'] = exp.trainer.eval(\n    job_type='hmc',\n    beta=6.0,\n    nprint=100,\n    nchains=16,\n    eval_steps=1000\n)\n#hdset = exp.save_dataset(job_type='hmc', nchains=1)\n\n[09/21/23 12:24:21][WARNING][trainer.py:437] - Step size `eps` not specified for HMC! Using default: 0.1000 for generic HMC\n[09/21/23 12:24:21][WARNING][trainer.py:437] - x.shape (original): torch.Size([16, 2, 32, 32])\n[09/21/23 12:24:21][WARNING][trainer.py:437] - x[:nchains].shape: torch.Size([16, 2, 32, 32])\n[09/21/23 12:24:21][INFO][trainer.py:1058] - eps=0.1\nbeta=6.0\nnlog=10\ntable=&lt;rich.table.Table object at 0x2e1b98520&gt;\nnprint=100\neval_steps=1000\nnleapfrog=20\n\n\n\n\n\n\n\n\n\n[09/21/23 12:24:24][INFO][trainer.py:1188] - hstep=0 dt=0.024 beta=6.000 loss=3.410 dQsin=0.125 dQint=0.000 energy=1586.502 logprob=1586.502 logdet=0.000 acc=0.472 sumlogdet=0.000 acc_mask=0.500 plaqs=0.909 intQ=0.000 sinQ=0.051\n[09/21/23 12:24:27][INFO][trainer.py:1188] - hstep=100 dt=0.026 beta=6.000 loss=2.876 dQsin=0.163 dQint=0.000 energy=1555.800 logprob=1555.800 logdet=0.000 acc=0.593 sumlogdet=0.000 acc_mask=0.688 plaqs=0.912 intQ=-0.125 sinQ=-0.159\n[09/21/23 12:24:31][INFO][trainer.py:1188] - hstep=200 dt=0.025 beta=6.000 loss=4.678 dQsin=0.088 dQint=0.063 energy=1569.994 logprob=1569.994 logdet=0.000 acc=0.451 sumlogdet=0.000 acc_mask=0.250 plaqs=0.912 intQ=-0.187 sinQ=-0.149\n[09/21/23 12:24:34][INFO][trainer.py:1188] - hstep=300 dt=0.024 beta=6.000 loss=14.041 dQsin=0.094 dQint=0.000 energy=1554.118 logprob=1554.118 logdet=0.000 acc=0.438 sumlogdet=0.000 acc_mask=0.438 plaqs=0.914 intQ=-0.125 sinQ=-0.114\n[09/21/23 12:24:38][INFO][trainer.py:1188] - hstep=400 dt=0.024 beta=6.000 loss=-0.739 dQsin=0.199 dQint=0.000 energy=1566.516 logprob=1566.516 logdet=0.000 acc=0.509 sumlogdet=0.000 acc_mask=0.562 plaqs=0.912 intQ=-0.437 sinQ=-0.452\n[09/21/23 12:24:41][INFO][trainer.py:1188] - hstep=500 dt=0.045 beta=6.000 loss=1.545 dQsin=0.100 dQint=0.000 energy=1570.837 logprob=1570.837 logdet=0.000 acc=0.448 sumlogdet=0.000 acc_mask=0.562 plaqs=0.911 intQ=0.125 sinQ=0.189\n[09/21/23 12:24:45][INFO][trainer.py:1188] - hstep=600 dt=0.025 beta=6.000 loss=3.780 dQsin=0.094 dQint=0.000 energy=1568.012 logprob=1568.012 logdet=0.000 acc=0.463 sumlogdet=0.000 acc_mask=0.500 plaqs=0.913 intQ=0.438 sinQ=0.466\n[09/21/23 12:24:50][INFO][trainer.py:1188] - hstep=700 dt=0.023 beta=6.000 loss=-0.902 dQsin=0.113 dQint=0.000 energy=1563.778 logprob=1563.778 logdet=0.000 acc=0.475 sumlogdet=0.000 acc_mask=0.375 plaqs=0.913 intQ=0.688 sinQ=0.628\n[09/21/23 12:24:53][INFO][trainer.py:1188] - hstep=800 dt=0.024 beta=6.000 loss=11.416 dQsin=0.061 dQint=0.000 energy=1561.427 logprob=1561.427 logdet=0.000 acc=0.339 sumlogdet=0.000 acc_mask=0.438 plaqs=0.913 intQ=0.813 sinQ=0.755\n[09/21/23 12:24:57][INFO][trainer.py:1188] - hstep=900 dt=0.028 beta=6.000 loss=1.114 dQsin=0.127 dQint=0.000 energy=1564.465 logprob=1564.465 logdet=0.000 acc=0.699 sumlogdet=0.000 acc_mask=0.625 plaqs=0.913 intQ=0.938 sinQ=0.893\n# %matplotlib inline\nfrom l2hmc.common import plot_dataset\nsns.set_context('notebook')\nhdataset = outputs['hmc']['history'].get_dataset()\nplot_dataset(hdataset, outdir=PLOTS_DIR, job_type='HMC')\n\n[09/21/23 12:25:06][INFO][plot_helpers.py:1049] - Saving figure to: l2hmc-diffusion-2dU1/2023-09-21/plots/ridgeplots/svgs/energy_ridgeplot.svg\n[09/21/23 12:25:09][INFO][plot_helpers.py:1049] - Saving figure to: l2hmc-diffusion-2dU1/2023-09-21/plots/ridgeplots/svgs/logprob_ridgeplot.svg\n[09/21/23 12:25:11][INFO][plot_helpers.py:1049] - Saving figure to: l2hmc-diffusion-2dU1/2023-09-21/plots/ridgeplots/svgs/logdet_ridgeplot.svg\nimport torch\n\ninitial_states = []\nstate_init = exp.trainer.dynamics.random_state(6.0)\nx = state_init.x\nbeta = state_init.beta\n\nNSAMPLES = 1000\nfor idx in range(NSAMPLES + int(0.1 * NSAMPLES)):\n    if idx % 100 == 0:\n        console.print(f\"step: {idx}\")\n        \n    x, metrics = exp.trainer.hmc_step((x, beta))\n    if idx &gt; int((0.1 * NSAMPLES)):\n        initial_states.append(x)\n\ninitial_states = torch.stack(initial_states).squeeze()\ninitial_states_np = initial_states.detach().cpu().numpy()\n\nstep: 0\n\n\n\nstep: 100\n\n\n\nstep: 200\n\n\n\nstep: 300\n\n\n\nstep: 400\n\n\n\nstep: 500\n\n\n\nstep: 600\n\n\n\nstep: 700\n\n\n\nstep: 800\n\n\n\nstep: 900\n\n\n\nstep: 1000\ninitial_states_np.shape\n\n(999, 16, 2048)\nx_ = initial_states_np.reshape(-1, 16, 2, 32, 32)\ntmp_ = x_[:, 0, ...]\nconsole.print(f'{x_.shape}')\nconsole.print(f'{tmp_.shape}')\n\n(999, 16, 2, 32, 32)\n\n\n\n(999, 2, 32, 32)\nfrom l2hmc.common import savefig\n\n#x_ = initial_states_np[:100].reshape(-1, 2, 32, 32)\ntmp_ = x_[:, 0, ...]\nfig, ax = plt.subplots()\nsns.kdeplot(\n    x=tmp_[-100:, 0].flatten(),\n    y=tmp_[-100:, 1].flatten(),\n    # ax=ax,\n    cmap='viridis',\n    # ax=axes[0],\n    # cmap=\"Blues\",\n    shade=False,\n    # bw_adjust=0.5,\n    thresh=0\n)\nax.set_xlim((-4, 4))\nax.set_ylim((-4, 4))\nsavefig(\n    f'hmc_samples-{NSAMPLES}',\n    Path(PLOTS_DIR),\n    tstamp=True,\n)\n\nSaving hmc_samples-1000-2023-09-21-122840 to l2hmc-diffusion-2dU1/2023-09-21/plots\nclass Diffusion:\n    def __init__(\n            self,\n            noise_steps: int = 1000,\n            beta_start: float = 1e-4,\n            beta_end: float = 0.02,\n            nchannels: int = 2,\n            img_size: int = 256,\n            device: str = \"cuda\"\n    ):\n        self.noise_steps = noise_steps\n        self.beta_start = beta_start\n        self.beta_end = beta_end\n        self.img_size = img_size\n        self.device = device\n        self.nchannels = nchannels\n\n        self.beta = self.prepare_noise_schedule().to(device)\n        self.alpha = 1. - self.beta\n        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n\n    def prepare_noise_schedule(self):\n        return torch.linspace(self.beta_start, self.beta_end, self.noise_steps)\n\n    def noise_images(self, x, t):\n        sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:, None, None, None]\n        sqrt_one_minus_alpha_hat = torch.sqrt(\n            1 - self.alpha_hat[t]\n        )[:, None, None, None]\n        eps = torch.randn_like(x)\n        return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * eps, eps\n\n    def sample_timesteps(self, n):\n        return torch.randint(low=1, high=self.noise_steps, size=(n,))\n\n    def sample(self, model, n):\n        # console.print(f\"Sampling {n} new images....\")\n        model.eval()\n        with torch.no_grad():\n            x = torch.randn(\n                (n, self.nchannels, self.img_size, self.img_size)\n            ).to(self.device)\n            sample_bar = tqdm(\n                reversed(range(1, self.noise_steps)),\n                position=0,\n                total=self.noise_steps - 1,\n                dynamic_ncols=True,\n            )\n            for i in sample_bar:\n                t = (torch.ones(n) * i).long().to(self.device)\n                predicted_noise = model(x, t)\n                alpha = self.alpha[t][:, None, None, None]\n                alpha_hat = self.alpha_hat[t][:, None, None, None]\n                beta = self.beta[t][:, None, None, None]\n                if i &gt; 1:\n                    noise = torch.randn_like(x)\n                else:\n                    noise = torch.zeros_like(x)\n                x = (\n                    (1 / torch.sqrt(alpha))\n                    * (\n                        x \n                        - ((1 - alpha) / (torch.sqrt(1 - alpha_hat)))\n                        * predicted_noise\n                    ) \n                    + (torch.sqrt(beta) * noise)\n                )\n        model.train()\n        x = (x + np.pi) % (2 * np.pi) - np.pi\n        return x\ninitial_states.shape\n\ntorch.Size([999, 16, 2048])\nTrain Diffusion Model\nimport torchvision\nimport os\nimport random\nfrom pathlib import Path\nimport torch\nimport torchvision\nimport torchvision.transforms as T\nimport numpy as np\nfrom PIL import Image\n#from fastdownload import FastDownload\nfrom torch.utils.data import DataLoader\n\ndef save_images(images, path, **kwargs):\n    grid = torchvision.utils.make_grid(images, **kwargs)\n    ndarr = grid.permute(1, 2, 0).to('cpu').numpy()\n    im = Image.fromarray(ndarr)\n    im.save(path)\nBuild Diffusion Model with UNet Architecure\nfrom torch import nn\nfrom torch import optim\nimport torch.nn.functional as F\nfrom tqdm.auto import tqdm\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import TensorDataset\n\nfrom l2hmc.common import savefig\nfrom l2hmc.diffusion.modules import NoiseScheduler, UNet\nfrom l2hmc.diffusion import ddpm\n\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nconfig = {\n    'channels_in': 2,\n    'channels_out': 2,\n    'train_batch_size': 5,\n    'learning_rate': 0.001,\n    'num_epochs': 1,\n    'noise_steps': 100,\n    'beta': 6.0,\n    'img_size': 32,\n    'retrains': 10,\n    'samples_per_retrain': 500,\n    'diffusion_prob': 0.1,\n}\n\nmodel = UNet(c_in=2, c_out=2)\n\ndataset = TensorDataset(initial_states.reshape(-1, 2, 32, 32))\ndataloader = DataLoader(\n    dataset,\n    batch_size=config[\"train_batch_size\"],\n    shuffle=False,\n    drop_last=True\n)\n\n\noptimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'])\nmse = nn.MSELoss()\ndiffusion = Diffusion(\n    noise_steps=100,\n    img_size=32,\n    device=DEVICE,\n    nchannels=2,\n)\n#logger = SummaryWriter(os.path.join(\"runs\", args.run_name))\nl = len(dataloader)\n\nrun_name = 'diffusion2dU1'\nPerform initial training on HMC samples\nfrom torch import optim\ndevice = 'cpu'\n#dataloader = get_data(args)\n#model = UNet().to(device)\n\nsampled_images_history = []\n\nfor epoch in range(config['num_epochs']):\n    console.print(f\"Starting epoch {epoch}:\")\n    pbar = tqdm(dataloader)\n    for i, images in enumerate(pbar):\n        if isinstance(images, (tuple, list)) and len(images) == 1:\n            images = images[0]\n        t = diffusion.sample_timesteps(images.shape[0]).to(device)\n        x_t, noise = diffusion.noise_images(images, t)\n        predicted_noise = model(x_t, t)\n        loss = mse(noise, predicted_noise)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        pbar.set_postfix({'epoch': epoch, 'batch': i, 'MSE': loss.item()})\n    console.print(f'epoch: {epoch}, loss: {loss.item()}')\n    sampled_images = diffusion.sample(model, n=images.shape[0])\n    sampled_images_history.append(sampled_images)\n    sns.set_context('notebook')\n    #tmp = initial_states.reshape(-1, 2, 32, 32)\n    fig, ax = plt.subplots(ncols=2)\n    _ = ax[0].imshow(sampled_images[0, 0, :, :])\n    _ = ax[1].imshow(sampled_images[0, 1, :, :])\n    _ = ax[0].set_xticklabels([])\n    _ = ax[1].set_xticklabels([])\n    _ = ax[0].set_yticklabels([])\n    _ = ax[1].set_yticklabels([])\n    _ = ax[0].set_title(r\"$U_{0}$\", loc='center')\n    _ = ax[1].set_title(r\"$U_{1}$\", loc='center')\n    _ = fig.suptitle('Diffusion Samples', y=0.8)\n    plt.show()\n    savefig(fname=f'sampled_image_epoch{epoch}', outdir=PLOTS_DIR, tstamp=True)\n    MODEL_FILE = OUTDIR.joinpath(\"models\", f\"unet-diffusion-epoch{epoch}.pt\")\n    MODEL_FILE.parent.mkdir(exist_ok=True, parents=True)\n    console.print(f\"Saving model checkpoint to: {MODEL_FILE}\")\n    torch.save(model.state_dict(), MODEL_FILE)\n\nStarting epoch 0:\n\n\n\n{\"model_id\":\"19b415c346b24bef8b60336d7f7bc355\",\"version_major\":2,\"version_minor\":0}\n\n\nepoch: 0, loss: 0.6023472547531128\n\n\n\n{\"model_id\":\"eea24504754f4cb9ab4d9925a6225c10\",\"version_major\":2,\"version_minor\":0}\n\n\n\n\n\n\n\n\n\nSaving sampled_image_epoch0-2023-09-21-124506 to l2hmc-diffusion-2dU1/2023-09-21/plots\n\n\nSaving model checkpoint to: l2hmc-diffusion-2dU1/2023-09-21/models/unet-diffusion-epoch0.pt\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\nsns.set_context('notebook')\ntmp = initial_states.reshape(-1, 2, 32, 32)\nfig, ax = plt.subplots(ncols=2)\n_ = ax[0].imshow(tmp[0, 0, :, :])\n_ = ax[1].imshow(tmp[0, 1, :, :])\n_ = ax[0].set_title(r\"$U_{0}$\", loc='center')\n_ = ax[0].set_xticklabels([])\n_ = ax[1].set_xticklabels([])\n_ = ax[0].set_yticklabels([])\n_ = ax[1].set_yticklabels([])\n_ = ax[1].set_title(r\"$U_{1}$\", loc='center')\n_ = fig.suptitle('HMC Samples', y=0.8)\nsampled_images_history_ = torch.stack(sampled_images_history)\nsampled_images_history_.shape\n\ntorch.Size([1, 5, 2, 32, 32])\nsns.set_context('notebook')\nfig, ax = plt.subplots(ncols=2)\n_ = ax[0].imshow(sampled_images_history_[0][0][0])\n_ = ax[1].imshow(sampled_images_history_[0][0][1])\n_ = ax[0].set_xticklabels([])\n_ = ax[1].set_xticklabels([])\n_ = ax[0].set_yticklabels([])\n_ = ax[1].set_yticklabels([])\n_ = ax[0].set_title(r\"$U_{0}$\", loc='center')\n_ = ax[1].set_title(r\"$U_{1}$\", loc='center')\n_ = fig.suptitle('Diffusion Samples', y=0.85)\nfor idx in range(sampled_images_history_.shape[0]):\n    q = exp.trainer.lattice.charges(x=sampled_images_history_[idx])\n    console.print(f'{idx}: {q}')\n\n0: Charges(intQ=tensor([ 5.0000e+00, -4.0000e+00, -6.0000e+00, -4.5535e-07,  1.0000e+00]), sinQ=tensor([ 1.6426, -1.7244, -4.4651,  0.5680,  0.7046]))\nHMC Sampling with Diffusion\n#for retrain_iter in range(config['retrains']):\nstate = exp.trainer.dynamics.random_state(config['beta'])\nx = state.x\n\nhistories = {}\nsamples = []\nhmc_samples = []\ndiffusion_samples = []\n\nglobal_step = 0\nwatcher = {}\nupdate_types = []\ncombined_samples = {}\nglobal_step\n\n0\nfor retrain_iter in range(2):\n    console.print(f'retrain_iter: {retrain_iter}')\n    ndiff_acc = 0\n    ndiff_proposed = 0\n    histories[retrain_iter] = {\n        'diffusion': [],\n        'hmc': [],\n    }\n    #for idx in range(config['samples_per_retrain']):\n    sbar = tqdm(range(10))\n    for idx in sbar:\n        t0_ = time.perf_counter()\n        if idx % 100 == 0:\n            console.print(f'sample idx: {idx}')\n        rand = np.random.uniform()\n        if (retrain_iter &gt;= 1) and rand &lt; diffusion_prob:\n            console.print(f'rand: {rand} &lt; {diffusion_prob}')\n            # Sample from diffusion model\n            x_ = diffusion.sample(model, n=x.shape[0])\n            ll_ = exp.trainer.dynamics.potential_energy(x_, config['beta'])\n            ll = exp.trainer.dynamics.potential_energy(x, config['beta'])\n            ratio = ll_ / ll\n            a = torch.min(torch.ones_like(ratio), ratio)\n            u = torch.rand(a.shape)\n            #u = np.random.uniform()\n            #for jdx in range(u.shape[0]):\n            #    if u[jdx] &lt; a[jdx]:\n            #        samples.append(x_[jdx])\n            #        diffusion_samples.append(x_[jdx])\n            #x = torch.where((u &lt; a), x_, x.reshape_as(x_)).reshape_as(x)\n            x = torch.where((u &lt; a)[:, None, None, None], x_, x.reshape_as(x_))\n            samples.append(x)\n            diffusion_samples.append(x)\n            combined_samples[global_step] = x\n            watcher[global_step] = 'diffusion'\n            #diffusion_samples.extend(x)\n            #samples.extend(x)\n            #ndiff_acc += \n            #if u &lt; a:\n            #    console.print('Accepted diffusion sample!')\n            #    console.print(f'{ndiff_acc} / {ndiff_proposed}')\n            #    ndiff_acc += 1\n            #    x = x_\n            #    diffusion_samples.append(x)\n            #    samples.append(x)\n        else:\n            # Oherwise, HMC\n            x, metrics = exp.trainer.hmc_step((x, config['beta']))\n            hmc_samples.append(x)\n            samples.append(x)\n            combined_samples[global_step] = x\n            watcher[global_step] = 'HMC'\n        smetrics = {\n            'idx': idx,\n            'global_step': global_step,\n            'dt': time.perf_counter() - t0_,\n        }\n        global_step += 1\n        #smetrics |= {\n        #    f'{k}': {torch.tensor(v).mean().item()} for k, v in metrics.items()\n        #}\n        sbar.set_postfix(smetrics)\n    # Train loop\n    dataset = TensorDataset(\n        torch.stack(hmc_samples).reshape(-1, 2, 32, 32)\n    )\n    dataloader = DataLoader(\n        dataset,\n        shuffle=False,\n        drop_last=True,\n        batch_size=config[\"train_batch_size\"],\n    )\n    pbar = tqdm(dataloader)\n    for i, batch in enumerate(pbar):\n        if i == 0:\n            console.print('Retraining...')\n        if isinstance(batch, (tuple, list)) and len(batch) == 1:\n            batch, = batch\n        batch = batch.reshape(-1, 2, 32, 32)\n        t0 = time.time()\n        t = diffusion.sample_timesteps(batch.shape[0]).to(device)\n        x_t, noise = diffusion.noise_images(batch, t)\n        predicted_noise = model(x_t, t)\n        loss = mse(noise, predicted_noise)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        t1 = time.time()\n        pbar.set_postfix(\n            {\n                'global_step': global_step,\n                'retrain_iter': retrain_iter,\n                'batch': i,\n                'dt': t1 - t0,\n                'MSE': loss.item()\n            }\n        )\n\nretrain_iter: 0\n\n\n\n{\"model_id\":\"17132d7ca8624fa387ee9467e4f1fa4d\",\"version_major\":2,\"version_minor\":0}\n\n\nsample idx: 0\n\n\n\n{\"model_id\":\"0ed1080fdebd4f7b9aae80db0d36b96b\",\"version_major\":2,\"version_minor\":0}\n\n\nRetraining...\n\n\n\nretrain_iter: 1\n\n\n\n{\"model_id\":\"d0346019e21b4d2a9b624dc59e84015b\",\"version_major\":2,\"version_minor\":0}\n\n\nsample idx: 0\n\n\n\nrand: 0.05506106760134255 &lt; 0.1\n\n\n\n{\"model_id\":\"c02b09d53ada46a194a47921f0ab3cba\",\"version_major\":2,\"version_minor\":0}\n\n\nrand: 0.07860283644524213 &lt; 0.1\n\n\n\n{\"model_id\":\"184df3f1c9714ece9756866b2617ed02\",\"version_major\":2,\"version_minor\":0}\n\n\n{\"model_id\":\"eaa0d84229c04618b7a2bffe2a4b1739\",\"version_major\":2,\"version_minor\":0}\n\n\nRetraining...\nconsole.print('\\n'.join([f\"{i.shape}\" for i in samples[:100]]))\n\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2048])\ntorch.Size([16, 2, 32, 32])\ntorch.Size([16, 2048])\ntorch.Size([16, 2, 32, 32])\nsamples_ = torch.stack([i.reshape(-1, 2, 32, 32) for i in samples])\nsamples_.shape\n\ntorch.Size([30, 16, 2, 32, 32])\nlen(hmc_samples)\n\n28\nlen(diffusion_samples)\n\n2\nhmc_samples_ = torch.stack([i.reshape(-1, 2, 32, 32) for i in hmc_samples])\ndiffusion_samples_ = torch.stack(\n    [i.reshape(-1, 2, 32, 32) for i in diffusion_samples]\n)\nhmc_samples_.shape\n\ntorch.Size([28, 16, 2, 32, 32])\ndiffusion_samples_.shape\n\ntorch.Size([2, 16, 2, 32, 32])\nsamples_.shape\n\ntorch.Size([30, 16, 2, 32, 32])\ndef calc_plaqs(x):\n    return torch.stack([\n        exp.trainer.lattice.plaqs(\n            x[:, idx]\n        ) for idx in range(x.shape[1])\n    ], -1)\n\ndef calc_intQ(x):\n    return torch.stack([\n        exp.trainer.lattice.int_charges(\n            x[:, idx]\n        ) for idx in range(x.shape[1])\n    ], -1)\n    \ndef calc_sinQ(x):\n    return torch.stack([\n        exp.trainer.lattice.sin_charges(\n            x[:, idx]\n        ) for idx in range(x.shape[1])\n    ], -1)\nsamples_init_ = initial_states.reshape(-1, initial_states.shape[1], 2, 32, 32)\nsamples_init_.shape\n\ntorch.Size([999, 16, 2, 32, 32])\nmetrics_init_ = {\n    'plaqs': calc_plaqs(samples_init_),\n    'intQ': calc_intQ(samples_init_),\n    'sinQ': calc_sinQ(samples_init_)\n}\n    \nmetrics_ = {\n    'plaqs': calc_plaqs(samples_),\n    'intQ': calc_intQ(samples_),\n    'sinQ': calc_sinQ(samples_)\n}\n\nmetrics_hmc_ = {\n    'plaqs': calc_plaqs(hmc_samples_),\n    'intQ': calc_intQ(hmc_samples_),\n    'sinQ': calc_sinQ(hmc_samples_)\n}\n\nmetrics_diffusion_ = {\n    'plaqs': calc_plaqs(diffusion_samples_),\n    'intQ': calc_intQ(diffusion_samples_),\n    'sinQ': calc_sinQ(diffusion_samples_)\n}\nmetrics_['plaqs'].shape\n\ntorch.Size([30, 16])\nconsole.print('\\n'.join([f\"{k}: {v}\" for k, v in watcher.items()]))\n\n0: HMC\n1: HMC\n2: HMC\n3: HMC\n4: HMC\n5: HMC\n6: HMC\n7: HMC\n8: HMC\n9: HMC\n10: HMC\n11: HMC\n12: HMC\n13: HMC\n14: HMC\n15: HMC\n16: HMC\n17: HMC\n18: HMC\n19: HMC\n20: HMC\n21: HMC\n22: HMC\n23: HMC\n24: HMC\n25: HMC\n26: HMC\n27: diffusion\n28: HMC\n29: diffusion\nfig, ax = plt.subplots()\n\n_ = ax.plot(metrics_['plaqs'][:, 0], label='Combined')\n_ = ax.plot(metrics_hmc_['plaqs'][:, 0], label='HMC')\n_ = ax.plot(metrics_diffusion_['plaqs'][:, 0], label='Diffusion')\n#_ = ax.plot(metrics_hmc1['plaqs'], label='HMC 1')\n#_ = ax.plot(metrics_diff_['plaqs'], label='Diffusion')\n_ = ax.legend(loc='upper left', bbox_to_anchor=(1.05, 1.00))\nfig, ax = plt.subplots(ncols=3, figsize=(14, 4))\nfor idx, (key, val) in enumerate(metrics_init_.items()):\n    _ = ax[idx].plot(val[:, 0], label='HMC (Initial Samples)')\n    _ = ax[idx].set_ylabel(key, loc='center')\n    _ = ax[idx].set_xlabel('Draw', loc='center')\n    #_ = ax[idx].legend(loc='best', frameon=True, edgecolor=\"#838383\")\n\n_ = fig.suptitle(f\"Initial HMC States\", y=0.92)\nfig, ax = plt.subplots(ncols=3, figsize=(14, 4))\nfor idx, (key, val) in enumerate(metrics_.items()):\n    _ = ax[idx].plot(val[:, 0], label='Combined')\n    _ = ax[idx].set_ylabel(key, loc='center')\n    _ = ax[idx].set_xlabel('Draw', loc='center')\n    #_ = ax[idx].legend(loc='best', frameon=True, edgecolor=\"#838383\")\n\n_ = fig.suptitle(f\"Combined Samples\", y=0.92)\nfig, ax = plt.subplots(ncols=3, figsize=(14, 4))\nfor idx, (key, val) in enumerate(metrics_hmc_.items()):\n    _ = ax[idx].plot(val[:, 0], label='HMC')\n    _ = ax[idx].set_ylabel(key, loc='center')\n    _ = ax[idx].set_xlabel('Draw', loc='center')\n_ = fig.suptitle(f\"Generated HMC States\", y=0.92)\nfig, ax = plt.subplots(ncols=3, figsize=(14, 4))\nfor idx, (key, val) in enumerate(metrics_diffusion_.items()):\n    _ = ax[idx].plot(val[:, 0], label='Diffusion')\n    _ = ax[idx].set_ylabel(key, loc='center')\n    _ = ax[idx].set_xlabel('Draw', loc='center')\n_ = fig.suptitle(f\"Generated Diffusion States\", y=0.92)\nfrom l2hmc.lattice.u1.pytorch.lattice import plaq_exact\nplaq_exact(torch.tensor(6.0))\n\ntensor(0.9124)\nfig, ax = plt.subplots()\n#_ = plt.hist(metrics_['intQ'].flatten(), color='C0', alpha=0.6, label='Combined', edgecolor='none')\n_ = ax.hist(\n    metrics_diffusion_['intQ'].flatten(),\n    color='C0',\n    alpha=0.6,\n    edgecolor='none',\n    label='Diffusion',\n    density=True,\n)\n_ = ax.hist(\n    metrics_hmc_['intQ'].flatten(),\n    color='C1',\n    alpha=0.6,\n    edgecolor='none',\n    label='HMC',\n    density=True,\n)\n_ = ax.legend(loc='best', frameon=True, edgecolor='#666666')\n_ = ax.set_xlabel(r\"$Q$\", loc='center')\n_ = ax.set_title('Topological Charge ($Q$) Distribution', loc='center')\nfig, ax = plt.subplots()\n_ = plt.plot(metrics_['plaqs'][:, 0], color='C0', label='Diffusion')\n_ = plt.plot(metrics_hmc_['plaqs'][:, 0], color='C1', label='HMC')\n_ = ax.legend(loc='best', frameon=True, edgecolor='#666666', ncols=2)\n_ = ax.set_ylabel(r\"$\\left\\langle U_{\\mu\\nu}\\right\\rangle $\", loc='center')\n_ = ax.set_xlabel(f\"Draw\", loc='center')\nwloops = {\n    'hmc': [\n        exp.trainer.lattice.wilson_loops(i) for i in hmc_samples_\n    ],\n    'diffusion': [\n        exp.trainer.lattice.wilson_loops(i) for i in diffusion_samples_\n    ],\n}\n\nplaqs = {\n    'hmc': [\n        exp.trainer.lattice.plaqs(i) for i in hmc_samples_\n    ],\n    'diffusion': [\n        exp.trainer.lattice.plaqs(i) for i in diffusion_samples_\n    ],\n}\nwlhmc = torch.stack(wloops['hmc']).squeeze()\nwldiff = torch.stack(wloops['diffusion']).squeeze()\nwlhmc.shape\n\ntorch.Size([28, 16, 32, 32])\n_ = plt.tight_layout()\nfor idx in range(2):\n    fig, ax = plt.subplots(ncols=2)\n    _ = ax[0].imshow(wlhmc[idx, 0])\n    _ = ax[0].set_title(\"HMC\", loc='center')\n    _ = ax[1].imshow(wldiff[idx, 0])\n    _ = ax[1].set_title(\"Diffusion\", loc='center')\n    _ = fig.suptitle(r\"$U_{\\mu\\nu}$\", y=0.8)\n    for ax_ in ax:\n        _ = ax_.set_xticklabels([])\n        _ = ax_.set_yticklabels([])\n\n&lt;Figure size 640x480 with 0 Axes&gt;\nqhmc = metrics_hmc_['intQ']\nqdiff = metrics_diffusion_['intQ']\nqhmc.shape\n\ntorch.Size([28, 16])\nphmc = torch.stack(plaqs['hmc']).squeeze()\npdiff = torch.stack(plaqs['diffusion']).squeeze()\nphmc.shape\n\ntorch.Size([28, 16])\npdiff.shape\n\ntorch.Size([2, 16])\nfig, ax = plt.subplots()\n\n_ = ax.hist(\n    metrics_['plaqs'].flatten(),\n    color='C1',\n    histtype='step',\n    stacked=True,\n    density=True,\n    label='HMC',\n    linewidth=1.5\n)\n_ = ax.hist(\n    metrics_diffusion_['plaqs'].flatten(),\n    color='C0',\n    histtype='step',\n    stacked=True,\n    density=True,\n    label='Diffusion',\n    linewidth=1.5\n)\n_ = ax.hist(\n    metrics_hmc_['plaqs'].flatten(),\n    color='C2',\n    histtype='step',\n    stacked=True,\n    density=True,\n    label='Combined',\n    linewidth=1.5\n)\n_ = ax.set_xlabel(r\"$U_{\\mu\\nu}$\", loc='center')\n_ = ax.legend(\n    loc='upper left',\n    frameon=True,\n    #ncols=2,\n    bbox_to_anchor=(0.55, 1.00),\n    edgecolor=\"#838383\",\n)\n_ = ax.set_title('Plaquette Distribution', loc='center')\nfig, ax = plt.subplots()\n\n_ = ax.hist(\n    metrics_['intQ'].flatten(),\n    color='C1',\n    histtype='step',\n    stacked=True,\n    density=True,\n    label='HMC',\n    linewidth=1.5\n)\n_ = ax.hist(\n    metrics_diffusion_['intQ'].flatten(),\n    color='C0',\n    histtype='step',\n    stacked=True,\n    density=True,\n    label='Diffusion',\n    linewidth=1.5\n)\n_ = ax.hist(\n    metrics_hmc_['intQ'].flatten(),\n    color='C2',\n    histtype='step',\n    stacked=True,\n    density=True,\n    label='Combined',\n    linewidth=1.5\n)\n_ = ax.set_xlabel('$Q_{\\mathbb{Z}}$', loc='center')\n_ = ax.legend(\n    loc='upper left',\n    frameon=True,\n    #ncols=2,\n    bbox_to_anchor=(0.55, 1.00),\n    edgecolor=\"#838383\",\n)\n_ = ax.set_title('Charge Distribution', loc='center')\nglobal_step = 0\nframes = []\nlosses = []\nprint(\"Training model...\")\nfor epoch in range(config[\"num_epochs\"]):\n    model.train()\n    progress_bar = tqdm(total=len(dataloader))\n    progress_bar.set_description(f\"Epoch {epoch}\")\n    for step, batch in enumerate(dataloader):\n        t = diffusion.sample_timesteps(images.shape[0]).to(device)\n\n        noise = torch.randn(batch.shape)\n        timesteps = torch.randint(\n            0, noise_scheduler.num_timesteps, (batch.shape[0],)\n        ).long()\n\n        #noisy = noise_scheduler.add_noise(batch, noise, timesteps)\n        noisy = noise_scheduler.noise_images(batch, timesteps)\n        noise_pred = model(noisy, timesteps)\n        loss = F.mse_loss(noise_pred, noise)\n        loss.backward(loss)\n\n        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        optimizer.zero_grad()\n\n        progress_bar.update(1)\n        logs = {\"loss\": loss.detach().item(), \"step\": global_step}\n        losses.append(loss.detach().item())\n        progress_bar.set_postfix(**logs)\n        global_step += 1\n    progress_bar.close()\n\n    if epoch % config[\"save_images_step\"] == 0 or epoch == config[\"num_epochs\"] - 1:\n        # generate data with the model to later visualize the learning process\n        model.eval()\n        sample = torch.randn(config[\"eval_batch_size\"], 2)\n        timesteps = list(range(len(noise_scheduler)))[::-1]\n        for i, t in enumerate(tqdm(timesteps)):\n            t = torch.from_numpy(np.repeat(t, config[\"eval_batch_size\"])).long()\n            with torch.no_grad():\n                residual = model(sample, t)\n            sample = noise_scheduler.step(residual, t[0], sample)\n        frames.append(sample.numpy())\ndataset[6]\nlen(dataloader)\neval_batch_size = 10\nnum_timesteps = 50\nplot_step = 5\nnoise_scheduler = ddpm.NoiseScheduler(num_timesteps=num_timesteps)\nsample = torch.randn(eval_batch_size, 2)\ntimesteps = list(range(num_timesteps))[::-1]\nsamples = []\nsteps = []\n\nretrains = 10\ndiffusion_prob = 0.3\nsamples_per_retrain = 100\neval_batch_size = 10\nt = torch.from_numpy(np.repeat(timesteps[0], eval_batch_size)).long()\nwith torch.no_grad():\n    residual = model(sample, t)\nsample_ = noise_scheduler.step(residual, t[0], sample)\nsample.shape\nresidual.shape\nsample_.shape\ndiffusion_samples = []\nhmc_samples = []\nbeta = 1.\nfor retrain_iter in range(retrains):\n    console.print(f'retrain_iter: {retrain_iter}')\n    ndiff_acc = 0\n    ndiff_proposed = 0\n    for idx in range(samples_per_retrain):\n        console.print(f'sample idx: {idx}')\n        rand = np.random.uniform()\n        if rand &lt; diffusion_prob:\n            ndiff_proposed += 1\n            rand_pick = randrange(len(dataloader))\n            #theta_prime = dataset[rand_pick]\n            t = torch.from_numpy(np.repeat(t, eval_batch_size)).long()\n            with torch.no_grad():\n                residual = model(sample, t)\n            sample_ = noise_scheduler.step(residual, t[0], sample)\n            ratio = (\n                log_likelihood_2dU1(sample_, 2)\n                / log_likelihood_2dU1(sample, 2)\n            )\n            a = min(1, ratio)\n            u = np.random.uniform()\n            if u &lt; a:\n                ndiff_acc += 1\n                sample = sample_\n                diffusion_samples.append(sample)\n        else:\n            sample_, metrics = exp.trainer.hmc_step((sample_, beta))\n            hmc_samples.append(sample)\nfor i, t in enumerate(tqdm(timesteps)):\n    t = torch.from_numpy(np.repeat(t, eval_batch_size)).long()\n    with torch.no_grad():\n        residual = model(sample, t)\n    sample = noise_scheduler.step(residual, t[0], sample)\n    if (i + 1) % plot_step == 0:\n        samples.append(sample.numpy())\n        steps.append(i + 1)\nAlternate\ndiffusion_ = DiffusionAlt(img_size=64, device='cpu')\nunet\nimage = torch.rand(1, 2, 64, 64)\nt = diffusion_.sample_timesteps(image.shape[0]).to('cpu')\nunet(image, t)\ndiffusion_.sample(",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "🪧 Posts",
      "⚛️ AI for Physics",
      "🎲 MCMC + Diffusion Sampling"
    ]
  },
  {
    "objectID": "posts/ai-for-physics/diffusion/index.html#denoising-diffusion-probabilistic-models",
    "href": "posts/ai-for-physics/diffusion/index.html#denoising-diffusion-probabilistic-models",
    "title": "🎲 MCMC + Diffusion Sampling",
    "section": "Denoising Diffusion Probabilistic Models",
    "text": "Denoising Diffusion Probabilistic Models",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "🪧 Posts",
      "⚛️ AI for Physics",
      "🎲 MCMC + Diffusion Sampling"
    ]
  },
  {
    "objectID": "posts/ai-for-physics/diffusion/index.html#imports--setup",
    "href": "posts/ai-for-physics/diffusion/index.html#imports--setup",
    "title": "🎲 MCMC + Diffusion Sampling",
    "section": "Imports / Setup",
    "text": "Imports / Setup\n\nfrom __future__ import absolute_import, print_function, annotations, division\nfrom dataclasses import dataclass\n\nimport sys\nimport os\nimport math\nimport numpy as np\nimport scipy\nimport time\nfrom random import randrange\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n\nfrom ezpz.dist import setup_torch\n\nport = np.random.randint(5000, 6000)\nprint(f\"Using port: {port}\")\n\nRANK = setup_torch(\n    backend=\"DDP\",\n    port=f\"{port}\"\n)\n\n\n    Using port: 5561\n\n\nUsing DDP for distributed training\n\n\n\nGlobal Rank: 0 / 0\n\n\n:::\n\n%matplotlib inline\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n\nfrom l2hmc.main import build_experiment\nfrom l2hmc.utils.rich import get_console\nfrom l2hmc.utils.plot_helpers import set_plot_style\n\nimport opinionated\nfrom l2hmc.diffusion.diffusion import PureMH, MH_Diffusion\nfrom l2hmc.utils.plot_helpers import set_plot_style\n\nfrom pandas.io.formats import style\nimport scipy\nimport time\nfrom random import randrange\nfrom l2hmc.diffusion.diffusion import PureMH, MH_Diffusion\n\nset_plot_style()\nconsole = get_console()\nprint(console.is_jupyter)\nif console.is_jupyter:\n    console.is_jupyter = False\nprint(console.is_jupyter)\n\nUsing device: cpu\n\n\n\nFailed to download font: Source Sans Pro, skipping! Failed to download font: Titillium WebRoboto Condensed, skipping!\n\n\nTrue\n\n\n\nFalse\n\n\n\nplt.style.use(opinionated.STYLES['opinionated_min'])\nsns.set_context('notebook')",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "🪧 Posts",
      "⚛️ AI for Physics",
      "🎲 MCMC + Diffusion Sampling"
    ]
  },
  {
    "objectID": "posts/ai-for-physics/diffusion/index.html#2d-u1",
    "href": "posts/ai-for-physics/diffusion/index.html#2d-u1",
    "title": "🎲 MCMC + Diffusion Sampling",
    "section": "2D U(1)",
    "text": "2D U(1)",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "🪧 Posts",
      "⚛️ AI for Physics",
      "🎲 MCMC + Diffusion Sampling"
    ]
  },
  {
    "objectID": "posts/ai-for-physics/diffusion/index.html#train-diffusion-model",
    "href": "posts/ai-for-physics/diffusion/index.html#train-diffusion-model",
    "title": "🎲 MCMC + Diffusion Sampling",
    "section": "Train Diffusion Model",
    "text": "Train Diffusion Model",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "🪧 Posts",
      "⚛️ AI for Physics",
      "🎲 MCMC + Diffusion Sampling"
    ]
  },
  {
    "objectID": "posts/ai-for-physics/diffusion/index.html#build-diffusion-model-with-unet-architecure",
    "href": "posts/ai-for-physics/diffusion/index.html#build-diffusion-model-with-unet-architecure",
    "title": "🎲 MCMC + Diffusion Sampling",
    "section": "Build Diffusion Model with UNet Architecure",
    "text": "Build Diffusion Model with UNet Architecure",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "🪧 Posts",
      "⚛️ AI for Physics",
      "🎲 MCMC + Diffusion Sampling"
    ]
  },
  {
    "objectID": "posts/ai-for-physics/diffusion/index.html#hmc-sampling-with-diffusion",
    "href": "posts/ai-for-physics/diffusion/index.html#hmc-sampling-with-diffusion",
    "title": "🎲 MCMC + Diffusion Sampling",
    "section": "HMC Sampling with Diffusion",
    "text": "HMC Sampling with Diffusion",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "🪧 Posts",
      "⚛️ AI for Physics",
      "🎲 MCMC + Diffusion Sampling"
    ]
  },
  {
    "objectID": "posts/ai-for-physics/diffusion/index.html#alternate",
    "href": "posts/ai-for-physics/diffusion/index.html#alternate",
    "title": "🎲 MCMC + Diffusion Sampling",
    "section": "Alternate",
    "text": "Alternate",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "🪧 Posts",
      "⚛️ AI for Physics",
      "🎲 MCMC + Diffusion Sampling"
    ]
  },
  {
    "objectID": "talks/llms-at-scale/index.html#ai-compute-historical",
    "href": "talks/llms-at-scale/index.html#ai-compute-historical",
    "title": "Training LLMs at Scale",
    "section": "AI 🤝 Compute [Historical]",
    "text": "AI 🤝 Compute [Historical]\n\n\n\n\n\n\n\nFirst Era:\n\n[1960 – 2012]\n2 year doubling (Moore’s law)\n\n\\simeq 7\\times increase\n\n\n\n \n\nModern Era:\n\n[2012 – present]\n3.4 month doubling\n\n\\simeq \\mathbf{300,000}\\times increase\n\n\n\n\n\n\n\n\nSource."
  },
  {
    "objectID": "talks/llms-at-scale/index.html#ai-compute-historical-1",
    "href": "talks/llms-at-scale/index.html#ai-compute-historical-1",
    "title": "Training LLMs at Scale",
    "section": "AI 🤝 Compute [Historical]",
    "text": "AI 🤝 Compute [Historical]\n\n\n\n\n\n\n\nFirst Era:\n\n[1960 – 2012]\n2 year doubling (Moore’s law)\n\n\\simeq 7\\times increase\n\n\n\n \n\nModern Era:\n\n[2012 – present]\n3.4 month doubling\n\n\\simeq \\mathbf{300,000}\\times increase\n\n\n\n\n\n\n\n\nSource."
  },
  {
    "objectID": "talks/llms-at-scale/index.html#data-parallel-training",
    "href": "talks/llms-at-scale/index.html#data-parallel-training",
    "title": "Training LLMs at Scale",
    "section": "Data Parallel Training",
    "text": "Data Parallel Training\n\n\n\nRelatively simple to get up and running (minor modifications to code)\n saforem2/ezpz\nPyTorch – DDP\n DeepSpeed\n\n\n\n\n\n\n\n\nFigure 6: Data Parallelism\n\n\n\n\n\n\nAlso see: 🎬 “Parallel Training Techniques”_"
  },
  {
    "objectID": "talks/llms-at-scale/index.html#deal-with-data",
    "href": "talks/llms-at-scale/index.html#deal-with-data",
    "title": "Training LLMs at Scale",
    "section": "Deal with Data",
    "text": "Deal with Data\n\nAt each training step, we want to ensure that each worker receives unique data\nThis can be done in one of two ways:\n\nManually partition data (ahead of time) and assign different sections to different workers\n\nEach worker can only see their local portion of the data\n\nFrom each worker, randomly select a mini-batch\n\nEach worker can see the full dataset\n\n\n\n\n\n\n⚠️ Warning\n\n\nDon’t forget your seed!\nWhen randomly selecting, it is important that each worker uses different seeds to ensure they receive unique data"
  },
  {
    "objectID": "talks/llms-at-scale/index.html#broadcast-initial-state",
    "href": "talks/llms-at-scale/index.html#broadcast-initial-state",
    "title": "Training LLMs at Scale",
    "section": "Broadcast Initial State",
    "text": "Broadcast Initial State\n\nAt the start of training (or when loading from a checkpoint), we want all of our workers to be initialized consistently\n\nBroadcast the model and optimizer states from rank() == 0 worker\n\n\n\n\n\n\n\n  flowchart TD\n    0[\"GPU0\"] --&gt; 1[\"GPU 1\"]\n    0 --&gt; 2[\"GPU 2\"]\n    0 --&gt;|Model + Optimizer State| 3[\"GPU 3\"]\n    0 --&gt; ...\n    0 --&gt; N[\"GPU N\"]"
  },
  {
    "objectID": "talks/llms-at-scale/index.html#best-practices",
    "href": "talks/llms-at-scale/index.html#best-practices",
    "title": "Training LLMs at Scale",
    "section": "Best Practices",
    "text": "Best Practices\n\n\n\n\n🤝 Keeping things in Sync\n\n\nComputation stalls during communication !!\nKeeping the communication to computation ratio small is important for effective scaling.\n\n\n\n\n\nUse parallel IO whenever possible\n\nFeed each rank from different files\nUse MPI IO to have each rank read its own batch from a file\nUse several ranks to read data, MPI to scatter to remaining ranks\n\nMost practical in big at-scale training\n\n\nTake advantage of data storage\n\nUse striping on lustre\nUse the right optimizations for Aurora, Polaris, etc.\n\nPreload data when possible\n\nOffloading to a GPU frees CPU cycles for loading the next batch of data\n\nminimize IO latency this way"
  },
  {
    "objectID": "talks/llms-at-scale/index.html#why-distributed-training",
    "href": "talks/llms-at-scale/index.html#why-distributed-training",
    "title": "Training LLMs at Scale",
    "section": "Why Distributed Training?",
    "text": "Why Distributed Training?\n\nSplitting data across workers \\longrightarrow larger batch size1\n\n[micro_batch_size = 1] \\times [N GPUs] \\rightarrow [global_batch_size = N]\n\nSmooth loss landscape\nImproved gradient estimators\nLess iterations needed for same number of epochs\n\nMay need to train for more epochs if another change is not made\ne.g. scaling learning rate\n\nSee Large Batch Training of Convolutional Networks\n\nmicro_batch_size = batch_size per GPU"
  },
  {
    "objectID": "talks/llms-at-scale/index.html#recent-progress",
    "href": "talks/llms-at-scale/index.html#recent-progress",
    "title": "Training LLMs at Scale",
    "section": "Recent Progress",
    "text": "Recent Progress\n\n\n\nTable 1: Batch-Size-Scaling\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear\nAuthor\nBatch Size\nProcessor\n# Processors\nTime\nAccuracy\n\n\n\n\n2016\nHe\n256\nP100\n8\n29 Hour\n75.30%\n\n\n2019\nYamazaki\n81,920\nV100\n2048\n1.2 Min\n75.08%"
  },
  {
    "objectID": "talks/llms-at-scale/index.html#model-parallel-training",
    "href": "talks/llms-at-scale/index.html#model-parallel-training",
    "title": "Training LLMs at Scale",
    "section": "Model Parallel Training",
    "text": "Model Parallel Training\n\n\n\n\n\n\n\nSplit up network over multiple workers\n\nEach receives disjoint subset\nAll communication associated with subsets are distributed\n\nCommunication whenever dataflow between two subsets\nTypically more complicated to implement than data parallel training\nSuitable when the model is too large to fit onto a single device (CPU / GPU)\n argonne-lcf/Megatron-DeepSpeed\n🤗 huggingface/nanotron\n\n\n\n\n\n\n\n\n\nFigure 7"
  },
  {
    "objectID": "talks/llms-at-scale/index.html#tensor-model-parallelismefficient-large-scale",
    "href": "talks/llms-at-scale/index.html#tensor-model-parallelismefficient-large-scale",
    "title": "Training LLMs at Scale",
    "section": "Tensor (Model) Parallelism1",
    "text": "Tensor (Model) Parallelism1\n\nIn Tensor Paralleism each GPU processes only a slice of a tensor and only aggregates the full tensor for operations that require the whole thing.\n\nThe main building block of any transformer is a fully connected nn.Linear followed by a nonlinear activation GeLU.\n\nY = GeLU(XA), where X and Y are the input and output vectors, and A is the weight matrix.\n\nIf we look at the computation in matrix form, it’s easy to see how the matrix multiplication can be split between multiple GPUs:\n\n\nEfficient Large-Scale Language Model Training on GPU Clusters"
  },
  {
    "objectID": "talks/llms-at-scale/index.html#tensor-parallelism-.center-0.9em-background-colorfff",
    "href": "talks/llms-at-scale/index.html#tensor-parallelism-.center-0.9em-background-colorfff",
    "title": "Training LLMs at Scale",
    "section": "Tensor Parallelism {.center 0.9em;” background-color=“#FFF”}",
    "text": "Tensor Parallelism {.center 0.9em;” background-color=“#FFF”}\n\n\n\n\n\n\nFigure 8: Tensor Parallel GEMM. This information is based on (the much more in-depth) TP Overview by @anton-l"
  },
  {
    "objectID": "talks/llms-at-scale/index.html#d-parallelism",
    "href": "talks/llms-at-scale/index.html#d-parallelism",
    "title": "Training LLMs at Scale",
    "section": "3D Parallelism",
    "text": "3D Parallelism\n\nDP + TP + PP (3D) Parallelism\n\n\n\n\n\n\n\nFigure 9: Figure taken from 3D parallelism: Scaling to trillion-parameter models"
  },
  {
    "objectID": "talks/llms-at-scale/index.html#collective-operations",
    "href": "talks/llms-at-scale/index.html#collective-operations",
    "title": "Training LLMs at Scale",
    "section": "Collective Operations",
    "text": "Collective Operations\n\n\n\n\n⌛ Timeouts\n\n\n\nCollective operations have to be called for each rank to form a complete collective operation.\n\nFailure to do so will result in other ranks waiting indefinitely"
  },
  {
    "objectID": "talks/llms-at-scale/index.html#emergent-abilities",
    "href": "talks/llms-at-scale/index.html#emergent-abilities",
    "title": "Training LLMs at Scale",
    "section": "Emergent Abilities",
    "text": "Emergent Abilities\n\n\nEmergent abilities of Large Language Models Yao et al. (2023)"
  },
  {
    "objectID": "talks/llms-at-scale/index.html#training-llms",
    "href": "talks/llms-at-scale/index.html#training-llms",
    "title": "Training LLMs at Scale",
    "section": "Training LLMs",
    "text": "Training LLMs\n\nModern parallelism techniques1 enable the training of large language models\n\n\n\n\n\n\n\n\nFigure 16: It’s hungry! Wei et al. (2022)\n\n\n\n\n\n\n\n\n\n\nFigure 17: Visualization from Yang et al. (2023)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSee my slides on Parallel Training Techniques for additional details"
  },
  {
    "objectID": "talks/llms-at-scale/index.html#life-cycle-of-the-llm",
    "href": "talks/llms-at-scale/index.html#life-cycle-of-the-llm",
    "title": "Training LLMs at Scale",
    "section": "Life-Cycle of the LLM",
    "text": "Life-Cycle of the LLM\n\n\n\n\n\n\n\nData collection + preprocessing\nPre-training\n\nArchitecture decisions:\n{model_size, hyperparameters,\nparallelism, lr_schedule, ...}\n\nSupervised Fine-Tuning\n\nInstruction Tuning\nAlignment\n\nDeploy (+ monitor, re-evaluate, etc.)\n\n\n\n\n\n\n\n\n\nFigure 18: Pre-training: Virtually all of the compute used during pretraining phase1.\n\n\n\n\n\n\nFigure from The Illustrated Transformer"
  },
  {
    "objectID": "talks/llms-at-scale/index.html#forward-pass",
    "href": "talks/llms-at-scale/index.html#forward-pass",
    "title": "Training LLMs at Scale",
    "section": "Forward Pass",
    "text": "Forward Pass\n\n\n\n\n\n\n\nFigure 19: Language Model trained for causal language modeling. Video from: 🤗 Generation with LLMs"
  },
  {
    "objectID": "talks/llms-at-scale/index.html#generating-text",
    "href": "talks/llms-at-scale/index.html#generating-text",
    "title": "Training LLMs at Scale",
    "section": "Generating Text",
    "text": "Generating Text\n\n\n\n\n\n\n\nFigure 20: Language Model trained for causal language modeling. Video from: 🤗 Generation with LLMs"
  },
  {
    "objectID": "talks/llms-at-scale/index.html#life-cycle-of-the-llm-pre-training",
    "href": "talks/llms-at-scale/index.html#life-cycle-of-the-llm-pre-training",
    "title": "Training LLMs at Scale",
    "section": "Life-Cycle of the LLM: Pre-training",
    "text": "Life-Cycle of the LLM: Pre-training\n\n\n\n\n\n\nFigure 21: Pre-training: Virtually all of the compute used during pretraining phase"
  },
  {
    "objectID": "talks/llms-at-scale/index.html#life-cycle-of-the-llm-fine-tuning",
    "href": "talks/llms-at-scale/index.html#life-cycle-of-the-llm-fine-tuning",
    "title": "Training LLMs at Scale",
    "section": "Life-Cycle of the LLM: Fine-Tuning",
    "text": "Life-Cycle of the LLM: Fine-Tuning\n\n\n\n\n\n\nFigure 22: Fine-tuning1: Fine-tuning actually updates the model’s weights to make the model better at a certain task.\n\n\n\nFigure from The Illustrated Transformer"
  },
  {
    "objectID": "talks/llms-at-scale/index.html#assistant-models",
    "href": "talks/llms-at-scale/index.html#assistant-models",
    "title": "Training LLMs at Scale",
    "section": "Assistant Models",
    "text": "Assistant Models\n\n\n\n\n\n\nFigure 23"
  },
  {
    "objectID": "talks/llms-at-scale/index.html#clone-repos",
    "href": "talks/llms-at-scale/index.html#clone-repos",
    "title": "Training LLMs at Scale",
    "section": "Clone Repo(s)",
    "text": "Clone Repo(s)\n\n#[⭐][07:33:08 AM][foremans@x3101c0s13b0n0][~/tmp]\n$ mkdir ~/tmp/polaris-talk\n\n#[⭐][07:33:21 AM][foremans@x3101c0s13b0n0][~/tmp]\n$ cd ~/tmp/polaris-talk\n\n#[⭐][07:33:25 AM][foremans@x3101c0s13b0n0][~/tmp/polaris-talk]\n$ NOW=$(tstamp) && mkdir \"${NOW}\" && cd \"${NOW}\" # && mkdir \"core-dumps-${NOW}\" && mv -v **core\\.** \"core-dumps-${NOW}\" && mv \"core-dumps-${NOW}\" core-dumps\n\n#[⭐][07:33:27 AM][foremans@x3101c0s13b0n0][~/tmp/polaris-talk/2024-07-17-073327]\n$ pwd\n/home/foremans/tmp/polaris-talk/2024-07-17-073327\n\n#[⭐][07:33:31 AM][foremans@x3101c0s13b0n0][~/tmp/polaris-talk/2024-07-17-073327]\n$ git clone https://github.com/saforem2/ezpz ezpz && git clone https://github.com/saforem2/wordplay wordplay\nCloning into 'ezpz'...\nremote: Enumerating objects: 2134, done.`\nremote: Counting objects: 100% (363/363), done.\nremote: Compressing objects: 100% (169/169), done.\nremote: Total 2134 (delta 197), reused 265 (delta 141), pack-reused 1771\nReceiving objects: 100% (2134/2134), 4.27 MiB | 25.01 MiB/s, done.\nResolving deltas: 100% (1117/1117), done.\nCloning into 'wordplay'...\nremote: Enumerating objects: 869, done.\nremote: Counting objects: 100% (72/72), done.\nremote: Compressing objects: 100% (37/37), done.\nremote: Total 869 (delta 29), reused 56 (delta 23), pack-reused 797\nReceiving objects: 100% (869/869), 14.36 MiB | 46.54 MiB/s, done.\nResolving deltas: 100% (395/395), done."
  },
  {
    "objectID": "talks/llms-at-scale/index.html#setup-python-.scrollable",
    "href": "talks/llms-at-scale/index.html#setup-python-.scrollable",
    "title": "Training LLMs at Scale",
    "section": "Setup Python {.scrollable}",
    "text": "Setup Python {.scrollable}\n\n#[⭐][07:33:53 AM][foremans@x3101c0s13b0n0][~/tmp/polaris-talk/2024-07-17-073327]\n$ source ezpz/src/ezpz/bin/utils.sh && ezpz_setup_python && ezpz_setup_alcf\nUnable to detect PBS or SLURM working directory info...\nUsing /home/foremans/tmp/polaris-talk/2024-07-17-073327 as working directory...\nUsing WORKING_DIR: /home/foremans/tmp/polaris-talk/2024-07-17-073327\nNo conda_prefix OR virtual_env found in environment...\nSetting up conda...\nLmod is automatically replacing \"nvhpc/23.9\" with \"gcc-native/12.3\".\nLmod is automatically replacing \"PrgEnv-nvhpc/8.5.0\" with \"PrgEnv-gnu/8.5.0\".\nDue to MODULEPATH changes, the following have been reloaded:\n  1) cray-mpich/8.1.28\nFound conda at: /soft/applications/conda/2024-04-29/mconda3\nNo VIRTUAL_ENV found in environment!\n    - Trying to setup from /soft/applications/conda/2024-04-29/mconda3\n    - Using VENV_DIR=/home/foremans/tmp/polaris-talk/2024-07-17-073327/venvs/2024-04-29\n    - Creating a new virtual env on top of 2024-04-29 in /home/foremans/tmp/polaris-talk/2024-07-17-073327/venvs/2024-04-29\n[python] Using /home/foremans/tmp/polaris-talk/2024-07-17-073327/venvs/2024-04-29/bin/python3\n\n[ezpz/bin/utils.sh]\n\n[2024-07-17-073407]\n    • USER=foremans\n    • MACHINE=polaris\n    • HOST=x3101c0s13b0n0\n\n[ezpz_setup_host]\n    • Using hostfile: /var/spool/pbs/aux/2024084.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov\n    • Found in environment:\n        • HOSTFILE: /var/spool/pbs/aux/2024084.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov\n        • Writing PBS vars to: /home/foremans/.pbsenv\n\n[ezpz_save_pbs_env]\n    • Setting:\n        • HOSTFILE: /var/spool/pbs/aux/2024084.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov\n        • JOBENV_FILE: /home/foremans/.pbsenv\n\n[HOSTS]\n    • [host:0] - x3101c0s13b0n0.hsn.cm.polaris.alcf.anl.gov\n\n[DIST INFO]\n    • HOSTFILE=/var/spool/pbs/aux/2024084.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov\n    • NHOSTS=1\n    • NGPU_PER_HOST=4\n    • NGPUS=4\n    • DIST_LAUNCH=mpiexec --verbose --envall -n 4 -ppn 4 --hostfile /var/spool/pbs/aux/2024084.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov --cpu-bind depth -d 16\n\n[LAUNCH]:\n    • To launch across all available GPUs, use: launch\n      launch = mpiexec --verbose --envall -n 4 -ppn 4 --hostfile /var/spool/pbs/aux/2024084.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov --cpu-bind depth -d 16"
  },
  {
    "objectID": "talks/llms-at-scale/index.html#install-ezpz-wordplay",
    "href": "talks/llms-at-scale/index.html#install-ezpz-wordplay",
    "title": "Training LLMs at Scale",
    "section": "Install {ezpz, wordplay}",
    "text": "Install {ezpz, wordplay}\n\n#[⭐][07:34:13 AM][foremans@x3101c0s13b0n0][~/tmp/polaris-talk/2024-07-17-073327]\n$ python3 -m pip install -e ezpz wordplay --require-virtualenv\nLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\nObtaining file:///home/foremans/tmp/polaris-talk/2024-07-17-073327/ezpz\n  Installing build dependencies ... done\n  Checking if build backend supports build_editable ... done\n  Getting requirements to build editable ... done\n  Installing backend dependencies ... done\n  Preparing editable metadata (pyproject.toml) ... done\n\n# ...[clipped]...\n\nSuccessfully built ezpz\nInstalling collected packages: enum34, wordplay, pyinstrument, ezpz\n  Attempting uninstall: ezpz\n    Found existing installation: ezpz 0.1\n    Not uninstalling ezpz at /home/foremans/.local/polaris/conda/2024-04-29/lib/python3.11/site-packages, outside environment /home/foremans/tmp/polaris-talk/2024-07-17-073327/venvs/2024-04-29\n    Cant uninstall 'ezpz'. No files were found to uninstall.\nSuccessfully installed enum34-1.1.10 ezpz pyinstrument-4.6.2 wordplay-1.0.0a4\n[notice] A new release of pip is available: 24.0 -&gt; 24.1.2\n[notice] To update, run: pip install --upgrade pip\n9.62s user 1.11s system 61% cpu 17.505s total\n\n#[⭐][07:34:53 AM][foremans@x3101c0s13b0n0][~/tmp/polaris-talk/2024-07-17-073327]\n$ python3 -m pip install --upgrade wandb\nLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\nRequirement already satisfied: wandb in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (0.16.6)\nCollecting wandb\n  Downloading wandb-0.17.4-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\nDownloading wandb-0.17.4-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.9 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.9/6.9 MB 2.1 MB/s eta 0:00:00\nInstalling collected packages: wandb\n  Attempting uninstall: wandb\n    Found existing installation: wandb 0.16.6\n    Not uninstalling wandb at /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages, outside environment /home/foremans/tmp/polaris-talk/2024-07-17-073327/venvs/2024-04-29\n    Cant uninstall 'wandb'. No files were found to uninstall.\nSuccessfully installed wandb-0.17.4\n[notice] A new release of pip is available: 24.0 -&gt; 24.1.2\n[notice] To update, run: pip install --upgrade pip"
  },
  {
    "objectID": "talks/llms-at-scale/index.html#launch-ezpz.test_dist",
    "href": "talks/llms-at-scale/index.html#launch-ezpz.test_dist",
    "title": "Training LLMs at Scale",
    "section": "Launch ezpz.test_dist",
    "text": "Launch ezpz.test_dist\n\n#(👻 2024-04-29)\n#[⭐][07:34:07 AM][foremans@x3101c0s13b0n0][~/tmp/polaris-talk/2024-07-17-073327][⏱ 7s]\n$ which launch\nlaunch: aliased to mpiexec --verbose --envall -n 4 -ppn 4 --hostfile /var/spool/pbs/aux/2024084.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov --cpu-bind depth -d 16\n\n#(👻 2024-04-29)\n#[⭐][07:34:11 AM][foremans@x3101c0s13b0n0][~/tmp/polaris-talk/2024-07-17-073327]\n$ which python3\n/home/foremans/tmp/polaris-talk/2024-07-17-073327/venvs/2024-04-29/bin/python3\n\n#(👻 2024-04-29)\n#[⭐][07:35:21 AM][foremans@x3101c0s13b0n0][~/tmp/polaris-talk/2024-07-17-073327][⏱ 14s]\n$ launch python3 -m ezpz.test_dist | tee ezpz-test-dist-DDP.log\nConnected to tcp://x3101c0s13b0n0.hsn.cm.polaris.alcf.anl.gov:7919\nFound executable /home/foremans/tmp/polaris-talk/2024-07-17-073327/venvs/2024-04-29/bin/python3\nLaunching application cff755ee-557e-4df2-a987-db85a8b7dbe7\n[2024-07-17 07:35:30.304306][INFO][__init__:156] - Setting logging level to 'INFO' on 'RANK == 0'\n[2024-07-17 07:35:30.307036][INFO][__init__:157] - Setting logging level to 'CRITICAL' on all others 'RANK != 0'\n[2024-07-17 07:35:30.307494][INFO][__init__:160] - To disable this behavior, and log from ALL ranks (not recommended), set: 'export LOG_FROM_ALL_RANKS=1'  in your environment, and re-run.\n[2024-07-17 07:35:32.116037][INFO][dist:358] - [device='cuda'][rank=2/3][local_rank=2/3][node=0/0]\n[2024-07-17 07:35:32.116089][INFO][dist:358] - [device='cuda'][rank=3/3][local_rank=3/3][node=0/0]\n[2024-07-17 07:35:32.116940][INFO][dist:358] - [device='cuda'][rank=1/3][local_rank=1/3][node=0/0]\n[2024-07-17 07:35:32.122726][INFO][dist:95] -\n[dist_info]:\n  • DEVICE=cuda\n  • DEVICE_ID=cuda:0\n  • DISTRIBUTED_BACKEND=nccl\n  • GPUS_PER_NODE=4\n  • HOSTS=['x3101c0s13b0n0.hsn.cm.polaris.alcf.anl.gov']\n  • HOSTFILE=/var/spool/pbs/aux/2024084.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov\n  • HOSTNAME=x3101c0s13b0n0.hsn.cm.polaris.alcf.anl.gov\n  • LOCAL_RANK=0\n  • MACHINE=Polaris\n  • NUM_NODES=1\n  • NGPUS=4\n  • NGPUS_AVAILABLE=4\n  • NODE_ID=0\n  • RANK=0\n  • SCHEDULER=PBS\n  • WORLD_SIZE_TOTAL=4\n  • WORLD_SIZE_IN_USE=4\n  • LAUNCH_CMD=mpiexec --verbose --envall -n 4 -ppn 4 --hostfile /var/spool/pbs/aux/2024084.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov --cpu-bind depth -d 16\n[2024-07-17 07:35:32.124800][INFO][dist:725] - [0/4] Using device='cuda' with backend='DDP' + 'nccl' for distributed training.\n[2024-07-17 07:35:32.129169][INFO][dist:358] - [device='cuda'][rank=0/3][local_rank=0/3][node=0/0]\n[2024-07-17 07:35:32.129674][WARNING][dist:364] - Using [4 / 4] available \"cuda\" devices !!\n[2024-07-17 07:35:32.130219][INFO][dist:874] - Setting up wandb from rank: 0\n[2024-07-17 07:35:32.130638][INFO][dist:875] - Using: WB PROJECT: ezpz.test_dist\nwandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\nwandb: Currently logged in as: foremans (aurora_gpt). Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.17.4\nwandb: Run data is saved locally in /home/foremans/tmp/polaris-talk/2024-07-17-073327/wandb/run-20240717_073532-p49rzxtv\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run vibrant-river-284\nwandb: ⭐️ View project at https://wandb.ai/aurora_gpt/ezpz.test_dist\nwandb: 🚀 View run at https://wandb.ai/aurora_gpt/ezpz.test_dist/runs/p49rzxtv\n[2024-07-17 07:35:33.171085][INFO][dist:905] - W&B RUN: [vibrant-river-284](https://wandb.ai/aurora_gpt/ezpz.test_dist/runs/p49rzxtv)\n[2024-07-17 07:35:33.182307][INFO][dist:312] - Updating wandb.run: vibrant-river-284 config with \"DIST_INFO\"\n[2024-07-17 07:35:33.186499][INFO][dist:938] - Running on machine='Polaris'\n[2024-07-17 07:35:33.187790][INFO][dist:95] -\n[timers_import]:\n  • os=1.082196831703186e-06\n  • logging=4.507601261138916e-07\n  • typing=2.9457733035087585e-06\n  • pathlib=1.3122335076332092e-06\n  • ezpz=6.109476089477539e-07\n  • torch=2.9457733035087585e-06\n  • torch_ddp=2.314336597919464e-06\n  • wandb=1.842435449361801e-05\n  • total=3.0086375772953033e-05\n\n[2024-07-17 07:35:33.188979][INFO][dist:95] -\n\n[CONFIG]:\n  • warmup=0\n  • log_freq=1\n  • batch_size=64\n  • input_size=128\n  • output_size=128\n  • dtype=torch.float32\n  • device=cuda\n  • world_size=4\n  • train_iters=100\n\n[2024-07-17 07:35:34.761945][INFO][test_dist:183] - model=Network(\n  (layers): Sequential(\n    (0): Linear(in_features=128, out_features=1024, bias=True)\n    (1): Linear(in_features=1024, out_features=512, bias=True)\n    (2): Linear(in_features=512, out_features=256, bias=True)\n    (3): Linear(in_features=256, out_features=128, bias=True)\n    (4): Linear(in_features=128, out_features=128, bias=True)\n  )\n)\n[2024-07-17 07:35:36.943300][INFO][test_dist:274] - iter=1, loss=2152.41, sps=1.697e+04, dt=0.00377066, dtf=0.001003, dtb=0.002768\n[2024-07-17 07:35:36.948048][INFO][test_dist:274] - iter=2, loss=1577.24, sps=3.611e+04, dt=0.00177221, dtf=0.0005256, dtb=0.001247\n[2024-07-17 07:35:36.952085][INFO][test_dist:274] - iter=3, loss=1201.25, sps=3.59e+04, dt=0.00178271, dtf=0.0004875, dtb=0.001295\n[2024-07-17 07:35:36.956071][INFO][test_dist:274] - iter=4, loss=1034.03, sps=3.704e+04, dt=0.0017279, dtf=0.0005082, dtb=0.00122\n[2024-07-17 07:35:36.959944][INFO][test_dist:274] - iter=5, loss=875.796, sps=3.825e+04, dt=0.00167313, dtf=0.0005121, dtb=0.001161\n[2024-07-17 07:35:36.963806][INFO][test_dist:274] - iter=6, loss=817.544, sps=3.804e+04, dt=0.00168248, dtf=0.0004651, dtb=0.001217\n[2024-07-17 07:35:36.967806][INFO][test_dist:274] - iter=7, loss=734.838, sps=3.536e+04, dt=0.0018099, dtf=0.0004969, dtb=0.001313\n[2024-07-17 07:35:36.971741][INFO][test_dist:274] - iter=8, loss=741.583, sps=3.682e+04, dt=0.00173809, dtf=0.0004537, dtb=0.001284\n[2024-07-17 07:35:36.975672][INFO][test_dist:274] - iter=9, loss=738.157, sps=3.717e+04, dt=0.0017217, dtf=0.0004635, dtb=0.001258\n[2024-07-17 07:35:36.979537][INFO][test_dist:274] - iter=10, loss=727.255, sps=3.857e+04, dt=0.00165911, dtf=0.0004897, dtb=0.001169\n[2024-07-17 07:35:36.983367][INFO][test_dist:274] - iter=11, loss=715.534, sps=3.979e+04, dt=0.00160845, dtf=0.0004246, dtb=0.001184\n[2024-07-17 07:35:36.987262][INFO][test_dist:274] - iter=12, loss=693.96, sps=3.791e+04, dt=0.00168827, dtf=0.0004543, dtb=0.001234\n[2024-07-17 07:35:36.991156][INFO][test_dist:274] - iter=13, loss=693.518, sps=3.815e+04, dt=0.00167748, dtf=0.0004182, dtb=0.001259\n[2024-07-17 07:35:36.994942][INFO][test_dist:274] - iter=14, loss=675.289, sps=4.003e+04, dt=0.00159879, dtf=0.0004048, dtb=0.001194\n[2024-07-17 07:35:36.999681][INFO][test_dist:274] - iter=15, loss=677.706, sps=4.062e+04, dt=0.0015755, dtf=0.0004248, dtb=0.001151\n[2024-07-17 07:35:37.003599][INFO][test_dist:274] - iter=16, loss=671.639, sps=3.754e+04, dt=0.00170499, dtf=0.000416, dtb=0.001289\n[2024-07-17 07:35:37.007565][INFO][test_dist:274] - iter=17, loss=652.219, sps=3.704e+04, dt=0.00172777, dtf=0.0004208, dtb=0.001307\n[2024-07-17 07:35:37.011753][INFO][test_dist:274] - iter=18, loss=633.308, sps=3.191e+04, dt=0.00200554, dtf=0.0004193, dtb=0.001586\n[2024-07-17 07:35:37.015595][INFO][test_dist:274] - iter=19, loss=635.459, sps=3.845e+04, dt=0.0016645, dtf=0.0004236, dtb=0.001241\n[2024-07-17 07:35:37.019356][INFO][test_dist:274] - iter=20, loss=626.979, sps=4.033e+04, dt=0.00158685, dtf=0.0004225, dtb=0.001164\n[2024-07-17 07:35:37.023081][INFO][test_dist:274] - iter=21, loss=612.352, sps=4.105e+04, dt=0.00155914, dtf=0.0004169, dtb=0.001142\n[2024-07-17 07:35:37.026861][INFO][test_dist:274] - iter=22, loss=609.89, sps=4.004e+04, dt=0.00159827, dtf=0.0004155, dtb=0.001183\n[2024-07-17 07:35:37.030555][INFO][test_dist:274] - iter=23, loss=602.673, sps=4.258e+04, dt=0.00150295, dtf=0.0004166, dtb=0.001086\n[2024-07-17 07:35:37.034382][INFO][test_dist:274] - iter=24, loss=613.106, sps=3.918e+04, dt=0.00163367, dtf=0.0004164, dtb=0.001217\n[2024-07-17 07:35:37.038129][INFO][test_dist:274] - iter=25, loss=644.755, sps=4.173e+04, dt=0.00153368, dtf=0.0004175, dtb=0.001116\n[2024-07-17 07:35:37.041943][INFO][test_dist:274] - iter=26, loss=789.106, sps=4.049e+04, dt=0.00158053, dtf=0.0004397, dtb=0.001141\n[2024-07-17 07:35:37.045705][INFO][test_dist:274] - iter=27, loss=691.36, sps=4.166e+04, dt=0.00153641, dtf=0.0004157, dtb=0.001121\n[2024-07-17 07:35:37.049496][INFO][test_dist:274] - iter=28, loss=657.228, sps=4.018e+04, dt=0.00159288, dtf=0.0004209, dtb=0.001172\n[2024-07-17 07:35:37.053229][INFO][test_dist:274] - iter=29, loss=633.212, sps=4.19e+04, dt=0.0015274, dtf=0.0004288, dtb=0.001099\n[2024-07-17 07:35:37.057013][INFO][test_dist:274] - iter=30, loss=640.29, sps=4.012e+04, dt=0.00159538, dtf=0.0004144, dtb=0.001181\n[2024-07-17 07:35:37.060722][INFO][test_dist:274] - iter=31, loss=604.287, sps=4.21e+04, dt=0.00152018, dtf=0.000398, dtb=0.001122\n[2024-07-17 07:35:37.064489][INFO][test_dist:274] - iter=32, loss=640.15, sps=4.079e+04, dt=0.00156912, dtf=0.0004007, dtb=0.001168\n[2024-07-17 07:35:37.068206][INFO][test_dist:274] - iter=33, loss=585.789, sps=4.238e+04, dt=0.00151007, dtf=0.0004199, dtb=0.00109\n[2024-07-17 07:35:37.071974][INFO][test_dist:274] - iter=34, loss=591.99, sps=4.053e+04, dt=0.00157917, dtf=0.000434, dtb=0.001145\n[2024-07-17 07:35:37.075702][INFO][test_dist:274] - iter=35, loss=618.223, sps=4.168e+04, dt=0.00153538, dtf=0.0004152, dtb=0.00112\n[2024-07-17 07:35:37.079496][INFO][test_dist:274] - iter=36, loss=572.365, sps=3.998e+04, dt=0.0016008, dtf=0.0004108, dtb=0.00119\n[2024-07-17 07:35:37.083250][INFO][test_dist:274] - iter=37, loss=573.749, sps=4.276e+04, dt=0.00149675, dtf=0.0004123, dtb=0.001084\n[2024-07-17 07:35:37.086969][INFO][test_dist:274] - iter=38, loss=580.662, sps=4.136e+04, dt=0.00154751, dtf=0.0004129, dtb=0.001135\n[2024-07-17 07:35:37.090636][INFO][test_dist:274] - iter=39, loss=568.836, sps=4.311e+04, dt=0.0014847, dtf=0.000409, dtb=0.001076\n[2024-07-17 07:35:37.094396][INFO][test_dist:274] - iter=40, loss=551.294, sps=4.145e+04, dt=0.00154388, dtf=0.0004118, dtb=0.001132\n[2024-07-17 07:35:37.098103][INFO][test_dist:274] - iter=41, loss=573.647, sps=4.352e+04, dt=0.00147048, dtf=0.0003977, dtb=0.001073\n[2024-07-17 07:35:37.101867][INFO][test_dist:274] - iter=42, loss=545.584, sps=4.257e+04, dt=0.00150354, dtf=0.000433, dtb=0.001071\n[2024-07-17 07:35:37.105639][INFO][test_dist:274] - iter=43, loss=544.877, sps=4.322e+04, dt=0.00148085, dtf=0.0004117, dtb=0.001069\n[2024-07-17 07:35:37.109471][INFO][test_dist:274] - iter=44, loss=559.886, sps=4.028e+04, dt=0.00158879, dtf=0.0004254, dtb=0.001163\n[2024-07-17 07:35:37.113186][INFO][test_dist:274] - iter=45, loss=534.895, sps=4.311e+04, dt=0.00148444, dtf=0.0004153, dtb=0.001069\n[2024-07-17 07:35:37.116972][INFO][test_dist:274] - iter=46, loss=536.457, sps=4.099e+04, dt=0.00156151, dtf=0.0004113, dtb=0.00115\n[2024-07-17 07:35:37.120710][INFO][test_dist:274] - iter=47, loss=548.508, sps=4.183e+04, dt=0.00152993, dtf=0.0004151, dtb=0.001115\n[2024-07-17 07:35:37.124552][INFO][test_dist:274] - iter=48, loss=532.186, sps=4.051e+04, dt=0.0015798, dtf=0.0004379, dtb=0.001142\n[2024-07-17 07:35:37.128266][INFO][test_dist:274] - iter=49, loss=519.254, sps=4.272e+04, dt=0.0014981, dtf=0.0004164, dtb=0.001082\n[2024-07-17 07:35:37.131975][INFO][test_dist:274] - iter=50, loss=535.535, sps=4.16e+04, dt=0.00153862, dtf=0.0004304, dtb=0.001108\n[2024-07-17 07:35:37.135717][INFO][test_dist:274] - iter=51, loss=520.722, sps=4.136e+04, dt=0.00154757, dtf=0.0004158, dtb=0.001132\n[2024-07-17 07:35:37.139451][INFO][test_dist:274] - iter=52, loss=513.063, sps=4.147e+04, dt=0.00154317, dtf=0.0004138, dtb=0.001129\n[2024-07-17 07:35:37.143231][INFO][test_dist:274] - iter=53, loss=514.546, sps=4.038e+04, dt=0.0015848, dtf=0.0004149, dtb=0.00117\n[2024-07-17 07:35:37.146971][INFO][test_dist:274] - iter=54, loss=506.488, sps=4.137e+04, dt=0.00154701, dtf=0.0004132, dtb=0.001134\n[2024-07-17 07:35:37.150659][INFO][test_dist:274] - iter=55, loss=503.01, sps=4.319e+04, dt=0.0014817, dtf=0.000415, dtb=0.001067\n[2024-07-17 07:35:37.154441][INFO][test_dist:274] - iter=56, loss=506.116, sps=4.06e+04, dt=0.00157637, dtf=0.0004211, dtb=0.001155\n[2024-07-17 07:35:37.158180][INFO][test_dist:274] - iter=57, loss=485.523, sps=4.287e+04, dt=0.00149301, dtf=0.000414, dtb=0.001079\n[2024-07-17 07:35:37.161931][INFO][test_dist:274] - iter=58, loss=489.076, sps=4.185e+04, dt=0.00152915, dtf=0.0004162, dtb=0.001113\n[2024-07-17 07:35:37.165759][INFO][test_dist:274] - iter=59, loss=484.844, sps=4.134e+04, dt=0.00154802, dtf=0.0004119, dtb=0.001136\n[2024-07-17 07:35:37.169483][INFO][test_dist:274] - iter=60, loss=496.104, sps=4.209e+04, dt=0.00152069, dtf=0.0003993, dtb=0.001121\n[2024-07-17 07:35:37.173190][INFO][test_dist:274] - iter=61, loss=467.599, sps=4.221e+04, dt=0.00151621, dtf=0.0004142, dtb=0.001102\n[2024-07-17 07:35:37.176950][INFO][test_dist:274] - iter=62, loss=480.055, sps=4.187e+04, dt=0.00152868, dtf=0.0004138, dtb=0.001115\n[2024-07-17 07:35:37.181194][INFO][test_dist:274] - iter=63, loss=483.146, sps=3.656e+04, dt=0.00175062, dtf=0.0006253, dtb=0.001125\n[2024-07-17 07:35:37.185018][INFO][test_dist:274] - iter=64, loss=479.273, sps=4.099e+04, dt=0.00156151, dtf=0.0004447, dtb=0.001117\n[2024-07-17 07:35:37.188752][INFO][test_dist:274] - iter=65, loss=464.753, sps=4.189e+04, dt=0.00152781, dtf=0.0004161, dtb=0.001112\n[2024-07-17 07:35:37.192464][INFO][test_dist:274] - iter=66, loss=462.583, sps=4.188e+04, dt=0.00152824, dtf=0.0004138, dtb=0.001114\n[2024-07-17 07:35:37.196126][INFO][test_dist:274] - iter=67, loss=461.665, sps=4.272e+04, dt=0.00149801, dtf=0.0004293, dtb=0.001069\n[2024-07-17 07:35:37.199838][INFO][test_dist:274] - iter=68, loss=465.25, sps=4.118e+04, dt=0.00155412, dtf=0.0004298, dtb=0.001124\n[2024-07-17 07:35:37.203602][INFO][test_dist:274] - iter=69, loss=460.897, sps=4.01e+04, dt=0.00159593, dtf=0.0004131, dtb=0.001183\n[2024-07-17 07:35:37.207372][INFO][test_dist:274] - iter=70, loss=456.136, sps=4.106e+04, dt=0.00155887, dtf=0.00041, dtb=0.001149\n[2024-07-17 07:35:37.211089][INFO][test_dist:274] - iter=71, loss=447.565, sps=4.158e+04, dt=0.00153923, dtf=0.0004113, dtb=0.001128\n[2024-07-17 07:35:37.214861][INFO][test_dist:274] - iter=72, loss=444.733, sps=4.05e+04, dt=0.00158026, dtf=0.0004127, dtb=0.001168\n[2024-07-17 07:35:37.218601][INFO][test_dist:274] - iter=73, loss=459.152, sps=4.123e+04, dt=0.00155234, dtf=0.0004201, dtb=0.001132\n[2024-07-17 07:35:37.222334][INFO][test_dist:274] - iter=74, loss=444.6, sps=4.226e+04, dt=0.00151444, dtf=0.0004371, dtb=0.001077\n[2024-07-17 07:35:37.226042][INFO][test_dist:274] - iter=75, loss=439.884, sps=4.29e+04, dt=0.001492, dtf=0.0004154, dtb=0.001077\n[2024-07-17 07:35:37.229838][INFO][test_dist:274] - iter=76, loss=438.578, sps=4.086e+04, dt=0.00156632, dtf=0.0004418, dtb=0.001125\n[2024-07-17 07:35:37.233560][INFO][test_dist:274] - iter=77, loss=431.993, sps=4.327e+04, dt=0.00147909, dtf=0.0004096, dtb=0.00107\n[2024-07-17 07:35:37.237367][INFO][test_dist:274] - iter=78, loss=422.338, sps=4.057e+04, dt=0.00157754, dtf=0.0004468, dtb=0.001131\n[2024-07-17 07:35:37.241117][INFO][test_dist:274] - iter=79, loss=427.973, sps=4.288e+04, dt=0.00149254, dtf=0.000415, dtb=0.001077\n[2024-07-17 07:35:37.244895][INFO][test_dist:274] - iter=80, loss=418.703, sps=4.06e+04, dt=0.00157617, dtf=0.0004137, dtb=0.001162\n[2024-07-17 07:35:37.248740][INFO][test_dist:274] - iter=81, loss=427.645, sps=4.031e+04, dt=0.00158766, dtf=0.000415, dtb=0.001173\n[2024-07-17 07:35:37.252447][INFO][test_dist:274] - iter=82, loss=417.629, sps=4.227e+04, dt=0.00151406, dtf=0.0004149, dtb=0.001099\n[2024-07-17 07:35:37.256190][INFO][test_dist:274] - iter=83, loss=411.667, sps=4.189e+04, dt=0.00152778, dtf=0.0004357, dtb=0.001092\n[2024-07-17 07:35:37.259935][INFO][test_dist:274] - iter=84, loss=409.366, sps=4.144e+04, dt=0.0015445, dtf=0.0004575, dtb=0.001087\n[2024-07-17 07:35:37.263677][INFO][test_dist:274] - iter=85, loss=409.511, sps=4.232e+04, dt=0.00151228, dtf=0.0004035, dtb=0.001109\n[2024-07-17 07:35:37.267463][INFO][test_dist:274] - iter=86, loss=409.593, sps=4.101e+04, dt=0.00156049, dtf=0.0004028, dtb=0.001158\n[2024-07-17 07:35:37.271174][INFO][test_dist:274] - iter=87, loss=408.794, sps=4.3e+04, dt=0.00148828, dtf=0.0004006, dtb=0.001088\n[2024-07-17 07:35:37.274926][INFO][test_dist:274] - iter=88, loss=403.151, sps=4.091e+04, dt=0.00156441, dtf=0.000415, dtb=0.001149\n[2024-07-17 07:35:37.278633][INFO][test_dist:274] - iter=89, loss=402.182, sps=4.26e+04, dt=0.00150243, dtf=0.0004147, dtb=0.001088\n[2024-07-17 07:35:37.282372][INFO][test_dist:274] - iter=90, loss=387.829, sps=4.216e+04, dt=0.00151793, dtf=0.0004411, dtb=0.001077\n[2024-07-17 07:35:37.286102][INFO][test_dist:274] - iter=91, loss=393.108, sps=4.308e+04, dt=0.00148558, dtf=0.0004167, dtb=0.001069\n[2024-07-17 07:35:37.289904][INFO][test_dist:274] - iter=92, loss=389.039, sps=4.103e+04, dt=0.00155996, dtf=0.0004359, dtb=0.001124\n[2024-07-17 07:35:37.293618][INFO][test_dist:274] - iter=93, loss=383.54, sps=4.322e+04, dt=0.00148092, dtf=0.0004147, dtb=0.001066\n[2024-07-17 07:35:37.297401][INFO][test_dist:274] - iter=94, loss=384.459, sps=4.1e+04, dt=0.00156106, dtf=0.0004164, dtb=0.001145\n[2024-07-17 07:35:37.301172][INFO][test_dist:274] - iter=95, loss=376.397, sps=4.191e+04, dt=0.0015272, dtf=0.0004129, dtb=0.001114\n[2024-07-17 07:35:37.304924][INFO][test_dist:274] - iter=96, loss=389.544, sps=4.091e+04, dt=0.00156433, dtf=0.0004139, dtb=0.00115\n[2024-07-17 07:35:37.308641][INFO][test_dist:274] - iter=97, loss=365.041, sps=4.343e+04, dt=0.00147362, dtf=0.0004165, dtb=0.001057\n[2024-07-17 07:35:37.312398][INFO][test_dist:274] - iter=98, loss=358.427, sps=4.134e+04, dt=0.00154796, dtf=0.0004143, dtb=0.001134\n[2024-07-17 07:35:37.561881][INFO][test_dist:274] - iter=99, loss=375.596, sps=258.9, dt=0.247161, dtf=0.1969, dtb=0.05026\n\n                            train/dt [2024-07-17-073537]\n     ┌─────────────────────────────────────────────────────────────────────────┐\n0.247┤                                                                        ▝│\n     │                                                                         │\n     │                                                                         │\n0.206┤                                                                         │\n     │                                                                         │\n     │                                                                         │\n0.165┤                                                                         │\n     │                                                                         │\n0.124┤                                                                         │\n     │                                                                         │\n     │                                                                         │\n0.083┤                                                                         │\n     │                                                                         │\n     │                                                                         │\n0.042┤                                                                         │\n     │                                                                         │\n     │                                                                         │\n0.001┤▄▗▖▄▗▖▄▗▖▄▗▖▄▗▖▄▗▖▄▖▄▗▖▄▗▖▄▗▖▄▗▖▄▗▖▄▗▄▗▖▄▗▖▄▗▖▄▗▖▄▗▖▄▗▄▗▖▄▗▖▄▗▖▄▗▖▄▗▖▄▗▖▖│\n     └┬─────────────────┬─────────────────┬─────────────────┬─────────────────┬┘\n     1.0              25.5              50.0              74.5             99.0\ntrain/dt                                iter\n[2024-07-17 07:35:37.589287][INFO][plot:156] - Appending plot to: /home/foremans/tmp/polaris-talk/2024-07-17-073327/test-dist-plots/train/dt.txt\ntext saved in /home/foremans/tmp/polaris-talk/2024-07-17-073327/test-dist-plots/train/dt.txt\n                            train/dtf [2024-07-17-073537]\n     ┌─────────────────────────────────────────────────────────────────────────┐\n0.197┤                                                                        ▝│\n     │                                                                         │\n     │                                                                         │\n0.164┤                                                                         │\n     │                                                                         │\n     │                                                                         │\n0.131┤                                                                         │\n     │                                                                         │\n0.099┤                                                                         │\n     │                                                                         │\n     │                                                                         │\n0.066┤                                                                         │\n     │                                                                         │\n     │                                                                         │\n0.033┤                                                                         │\n     │                                                                         │\n     │                                                                         │\n0.000┤▄▗▖▄▗▖▄▗▖▄▗▖▄▗▖▄▗▖▄▖▄▗▖▄▗▖▄▗▖▄▗▖▄▗▖▄▗▄▗▖▄▗▖▄▗▖▄▗▖▄▗▖▄▗▄▗▖▄▗▖▄▗▖▄▗▖▄▗▖▄▗▖▖│\n     └┬─────────────────┬─────────────────┬─────────────────┬─────────────────┬┘\n     1.0              25.5              50.0              74.5             99.0\ntrain/dtf                               iter\n[2024-07-17 07:35:37.603242][INFO][plot:156] - Appending plot to: /home/foremans/tmp/polaris-talk/2024-07-17-073327/test-dist-plots/train/dtf.txt\ntext saved in /home/foremans/tmp/polaris-talk/2024-07-17-073327/test-dist-plots/train/dtf.txt\n                             train/dtb [2024-07-17-073537]\n      ┌────────────────────────────────────────────────────────────────────────┐\n0.0503┤                                                                       ▝│\n      │                                                                        │\n      │                                                                        │\n0.0421┤                                                                        │\n      │                                                                        │\n      │                                                                        │\n0.0339┤                                                                        │\n      │                                                                        │\n0.0257┤                                                                        │\n      │                                                                        │\n      │                                                                        │\n0.0175┤                                                                        │\n      │                                                                        │\n      │                                                                        │\n0.0093┤                                                                        │\n      │                                                                        │\n      │                                                                        │\n0.0011┤▚▗▖▄▗▖▄▗▖▄▖▄▗▖▄▗▖▄▖▄▗▖▄▗▖▄▗▄▗▖▄▗▖▄▗▖▄▖▄▗▖▄▗▖▄▖▄▗▖▄▗▖▄▗▄▗▖▄▗▖▄▗▄▗▖▄▗▖▄▗▖▖│\n      └┬─────────────────┬─────────────────┬────────────────┬─────────────────┬┘\n      1.0              25.5              50.0             74.5             99.0\ntrain/dtb                                iter\n[2024-07-17 07:35:37.615896][INFO][plot:156] - Appending plot to: /home/foremans/tmp/polaris-talk/2024-07-17-073327/test-dist-plots/train/dtb.txt\ntext saved in /home/foremans/tmp/polaris-talk/2024-07-17-073327/test-dist-plots/train/dtb.txt\n                            train/loss [2024-07-17-073537]\n      ┌────────────────────────────────────────────────────────────────────────┐\n2152.4┤▘                                                                       │\n      │                                                                        │\n      │                                                                        │\n1853.4┤                                                                        │\n      │                                                                        │\n      │▗                                                                       │\n1554.4┤                                                                        │\n      │                                                                        │\n1255.4┤                                                                        │\n      │ ▗                                                                      │\n      │                                                                        │\n 956.4┤  ▘                                                                     │\n      │   ▖                                                                    │\n      │   ▝              ▖                                                     │\n 657.4┤    ▝▘▀▝▘▚▖▄     ▗ ▄                                                    │\n      │            ▝▘▀▝▘▘  ▝▘▀▗▘▚▗▄▗▖▄▗ ▗                                      │\n      │                                ▘▘▝▘▀▘▀▝▘▞▗▘▄▖▄▗▖▄▗▖▄▗▄                 │\n 358.4┤                                                       ▝▘▀▝▘▀▝▀▝▘▀▝▖▚▝▖▄│\n      └┬─────────────────┬─────────────────┬────────────────┬─────────────────┬┘\n      1.0              25.5              50.0             74.5             99.0\ntrain/loss                               iter\n[2024-07-17 07:35:37.655339][INFO][plot:156] - Appending plot to: /home/foremans/tmp/polaris-talk/2024-07-17-073327/test-dist-plots/train/loss.txt\ntext saved in /home/foremans/tmp/polaris-talk/2024-07-17-073327/test-dist-plots/train/loss.txt\n                           train/iter [2024-07-17-073537]\n    ┌──────────────────────────────────────────────────────────────────────────┐\n99.0┤                                                                      ▗▗▖▀│\n    │                                                                   ▄▝▘▘   │\n    │                                                              ▗▖▞▝▘       │\n82.7┤                                                          ▄▗▘▀            │\n    │                                                      ▖▄▝▘                │\n    │                                                 ▗▗▖▀▝                    │\n66.3┤                                              ▄▝▘▘                        │\n    │                                         ▗▖▞▝▘                            │\n50.0┤                                     ▄▗▘▀                                 │\n    │                                 ▖▄▝▘                                     │\n    │                            ▗▗▖▀▝                                         │\n33.7┤                         ▄▝▘▘                                             │\n    │                    ▗▖▞▝▘                                                 │\n    │                ▄▗▘▀                                                      │\n17.3┤            ▖▄▝▘                                                          │\n    │       ▗▗▖▀▝                                                              │\n    │    ▄▝▘▘                                                                  │\n 1.0┤▖▞▝▘                                                                      │\n    └┬─────────────────┬──────────────────┬─────────────────┬─────────────────┬┘\n    1.0              25.5               50.0              74.5             99.0\ntrain/iter                              iter\n[2024-07-17 07:35:37.669214][INFO][plot:156] - Appending plot to: /home/foremans/tmp/polaris-talk/2024-07-17-073327/test-dist-plots/train/iter.txt\ntext saved in /home/foremans/tmp/polaris-talk/2024-07-17-073327/test-dist-plots/train/iter.txt\n                             train/sps [2024-07-17-073537]\n       ┌───────────────────────────────────────────────────────────────────────┐\n43523.3┤                ▖▗  ▖▗ ▖▗ ▖▝ ▚▘▝ ▖▗    ▘▗▖▗▖▖ ▖▄    ▗▖▝ ▖ ▗▖▗ ▘▗▞ ▘▗ ▘ │\n       │       ▖ ▗▘  ▗▝▖  ▀▗ ▖▝▝ ▖▝ ▘  ▖▝ ▘▝▀▗▘▝ ▝   ▝  ▘▞▝▘▘ ▘▝ ▚ ▝ ▘▝  ▝ ▘▝ ▘│\n       │  ▖▀ ▖▞ ▞  ▄ ▘  ▝                                                      │\n36312.5┤▝▝  ▗                                       ▝                          │\n       │            ▖                                                          │\n       │                                                                       │\n29101.8┤                                                                       │\n       │                                                                       │\n21891.1┤                                                                       │\n       │                                                                       │\n       │▖                                                                      │\n14680.4┤                                                                       │\n       │                                                                       │\n       │                                                                       │\n 7469.7┤                                                                       │\n       │                                                                       │\n       │                                                                       │\n  258.9┤                                                                      ▗│\n       └┬─────────────────┬────────────────┬─────────────────┬────────────────┬┘\n       1.0              25.5             50.0              74.5            99.0\ntrain/sps                                iter\n[2024-07-17 07:35:37.681268][INFO][plot:156] - Appending plot to: /home/foremans/tmp/polaris-talk/2024-07-17-073327/test-dist-plots/train/sps.txt\ntext saved in /home/foremans/tmp/polaris-talk/2024-07-17-073327/test-dist-plots/train/sps.txt"
  },
  {
    "objectID": "talks/llms-at-scale/index.html#pyinstrument-profile",
    "href": "talks/llms-at-scale/index.html#pyinstrument-profile",
    "title": "Training LLMs at Scale",
    "section": "PyInstrument Profile",
    "text": "PyInstrument Profile\n\nRecorded: 07:35:34  Samples:  2227\nDuration: 2.948     CPU time: 5.441\nPyInstrument: v4.6.2\nProgram: /home/foremans/tmp/polaris-talk/2024-07-17-073327/ezpz/src/ezpz/test_dist.py\n2.948 &lt;module&gt;  ezpz/test_dist.py:1\n└─ 2.946 main  ezpz/test_dist.py:217\n   ├─ 2.043 build_model_and_optimizer  ezpz/test_dist.py:171\n   │  └─ 2.011 Adam.__init__  torch/optim/adam.py:15\n   │        [129 frames hidden]  torch, wandb, transformers, jax, func...\n   ├─ 0.326 _forward_step  ezpz/test_dist.py:231\n   │  ├─ 0.279 DistributedDataParallel._wrapped_call_impl  torch/nn/modules/module.py:1528\n   │  │     [13 frames hidden]  torch, wandb, &lt;built-in&gt;\n   │  │        0.273 Network._call_impl  torch/nn/modules/module.py:1534\n   │  │        └─ 0.076 Network.forward  ezpz/test_dist.py:164\n   │  │           └─ 0.076 Sequential._wrapped_call_impl  torch/nn/modules/module.py:1528\n   │  │                 [7 frames hidden]  torch, &lt;built-in&gt;\n   │  └─ 0.046 calc_loss  ezpz/test_dist.py:168\n   ├─ 0.254 _backward_step  ezpz/test_dist.py:236\n   │  ├─ 0.177 Tensor.backward  torch/_tensor.py:466\n   │  │     [4 frames hidden]  torch, &lt;built-in&gt;\n   │  └─ 0.077 wrapper  torch/optim/optimizer.py:374\n   │        [5 frames hidden]  torch\n   ├─ 0.119 tplot_dict  ezpz/plot.py:136\n   │  └─ 0.069 show  plotext/_core.py:292\n   │        [5 frames hidden]  plotext\n   ├─ 0.102 Logger.info  logging/__init__.py:1479\n   │     [6 frames hidden]  logging, rich\n   │        0.102 RichHandler.emit  rich/logging.py:126\n   │        └─ 0.100 Console.print  ezpz/log/console.py:79\n   │           └─ 0.100 Console.print  rich/console.py:1624\n   │                 [5 frames hidden]  rich\n   └─ 0.099 Run.wrapper  wandb/sdk/wandb_run.py:418\n         [13 frames hidden]  wandb, json\n[2024-07-17 07:35:37.876629][INFO][profile:115] - Saving pyinstrument profile output to: /home/foremans/tmp/polaris-talk/2024-07-17-073327/ezpz_pyinstrument_profiles\n[2024-07-17 07:35:37.877255][INFO][profile:123] - PyInstrument profile saved (as html) to:  /home/foremans/tmp/polaris-talk/2024-07-17-073327/ezpz_pyinstrument_profiles/pyinstrument-profile-2024-07-17-073537.html\n[2024-07-17 07:35:37.877936][INFO][profile:131] - PyInstrument profile saved (as text) to:  /home/foremans/tmp/polaris-talk/2024-07-17-073327/ezpz_pyinstrument_profiles/pyinstrument-profile-2024-07-17-073537.txt\n[2024-07-17 07:35:38.391628][INFO][profile:143] - Finished with pyinstrument profiler. Took: 2.94768s\n[2024-07-17 07:35:38.392519][INFO][test_dist:318] - [0] runtime=8.075730s\nwandb: 🚀 View run vibrant-river-284 at: https://wandb.ai/aurora_gpt/ezpz.test_dist/runs/p49rzxtv\nwandb: Find logs at: wandb/run-20240717_073532-p49rzxtv/logs\nApplication cff755ee resources: utime=25s stime=23s maxrss=1434396KB inblock=32 oublock=4320 minflt=670179 majflt=864 nvcsw=195893 nivcsw=1331214"
  },
  {
    "objectID": "talks/llms-at-scale/index.html#example-ezpz",
    "href": "talks/llms-at-scale/index.html#example-ezpz",
    "title": "Training LLMs at Scale",
    "section": "Example: ezpz 🍋",
    "text": "Example: ezpz 🍋\n\nLink[^ez-video] to video\n\n\nExample: using 🍋 ezpz.test_dist to train a small model using DDP"
  },
  {
    "objectID": "talks/llms-at-scale/index.html#prepare-data",
    "href": "talks/llms-at-scale/index.html#prepare-data",
    "title": "Training LLMs at Scale",
    "section": "Prepare Data",
    "text": "Prepare Data\n\n#[⭐][07:41:20 AM][foremans@x3101c0s13b0n0][~/tmp/polaris-talk/2024-07-17-073327][⏱ 29s]\n$ python3 wordplay/data/shakespeare_char/prepare.py\nUsing HF_DATASETS_CACHE=/home/foremans/tmp/polaris-talk/2024-07-17-073327/wordplay/data/shakespeare_char/.cache/huggingface\nlength of dataset in characters: 1,115,394\nall the unique characters:\n !$&\\',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\nvocab size: 65\ntrain has 1,003,854 tokens\nval has 111,540 tokens"
  },
  {
    "objectID": "talks/llms-at-scale/index.html#launch-training-ddp",
    "href": "talks/llms-at-scale/index.html#launch-training-ddp",
    "title": "Training LLMs at Scale",
    "section": "Launch Training (DDP)",
    "text": "Launch Training (DDP)\n\n#(👻 2024-04-29)\n#[⭐][07:42:02 AM][foremans@x3101c0s13b0n0][~/tmp/polaris-talk/2024-07-17-073327]\n$ launch python3 -m wordplay train.backend=DDP train.eval_interval=100 data=shakespeare train.dtype=bf16 model.batch_size=64 model.block_size=1024 train.max_iters=1000 train.log_interval=10 train.compile=false | tee wordplay-gpt2-DDP.log\n[2024-07-17 07:42:11.746540][INFO][__init__:156] - Setting logging level to 'INFO' on 'RANK == 0'\n[2024-07-17 07:42:11.748763][INFO][__init__:157] - Setting logging level to 'CRITICAL' on all others 'RANK != 0'\n[2024-07-17 07:42:11.749453][INFO][__init__:160] - To disable this behavior, and log from ALL ranks (not recommended), set: 'export LOG_FROM_ALL_RANKS=1'  in your environment, and re-run.\n[2024-07-17 07:42:11.772718][INFO][configs:81] - Setting HF_DATASETS_CACHE to /home/foremans/tmp/polaris-talk/2024-07-17-073327/wordplay/.cache/huggingface/datasets\n[2024-07-17 07:42:15.341532][INFO][dist:358] - [device='cuda'][rank=2/3][local_rank=2/3][node=0/0]\n[2024-07-17 07:42:15.342381][INFO][dist:358] - [device='cuda'][rank=1/3][local_rank=1/3][node=0/0]\n[2024-07-17 07:42:15.342430][INFO][dist:358] - [device='cuda'][rank=3/3][local_rank=3/3][node=0/0]\n[2024-07-17 07:42:15.348657][INFO][dist:95] -\n\n[dist_info]:\n  • DEVICE=cuda\n  • DEVICE_ID=cuda:0\n  • DISTRIBUTED_BACKEND=nccl\n  • GPUS_PER_NODE=4\n  • HOSTS=['x3101c0s13b0n0.hsn.cm.polaris.alcf.anl.gov']\n  • HOSTFILE=/var/spool/pbs/aux/2024084.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov\n  • HOSTNAME=x3101c0s13b0n0.hsn.cm.polaris.alcf.anl.gov\n  • LOCAL_RANK=0\n  • MACHINE=Polaris\n  • NUM_NODES=1\n  • NGPUS=4\n  • NGPUS_AVAILABLE=4\n  • NODE_ID=0\n  • RANK=0\n  • SCHEDULER=PBS\n  • WORLD_SIZE_TOTAL=4\n  • WORLD_SIZE_IN_USE=4\n  • LAUNCH_CMD=mpiexec --verbose --envall -n 4 -ppn 4 --hostfile /var/spool/pbs/aux/2024084.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov --cpu-bind depth -d 16\n\n\n[2024-07-17 07:42:15.351446][INFO][dist:725] - [0/4] Using device='cuda' with backend='DDP' + 'nccl' for distributed training.\n[2024-07-17 07:42:15.356169][INFO][dist:358] - [device='cuda'][rank=0/3][local_rank=0/3][node=0/0]\n[2024-07-17 07:42:15.356692][WARNING][dist:364] - Using [4 / 4] available \"cuda\" devices !!\n[2024-07-17 07:42:15.359571][INFO][configs:317] - Loading val from /home/foremans/tmp/polaris-talk/2024-07-17-073327/wordplay/data/shakespeare_char/val.bin\n[2024-07-17 07:42:15.360138][INFO][configs:317] - Loading train from /home/foremans/tmp/polaris-talk/2024-07-17-073327/wordplay/data/shakespeare_char/train.bin\n[2024-07-17 07:42:15.361154][INFO][configs:442] - Tokens per iteration: 262,144\n[2024-07-17 07:42:15.361574][INFO][configs:465] - Using self.ptdtype=torch.float16 on self.device_type='cuda'\n[2024-07-17 07:42:15.362002][INFO][configs:471] - Initializing a new model from scratch\n[2024-07-17 07:42:15.362529][INFO][dist:874] - Setting up wandb from rank: 0\n[2024-07-17 07:42:15.362896][INFO][dist:875] - Using: WB PROJECT: WordPlay\n[2024-07-17 07:42:16.451786][INFO][dist:905] - W&B RUN: [still-frog-17](https://wandb.ai/aurora_gpt/WordPlay/runs/6by9vpcj)\n[2024-07-17 07:42:16.464106][INFO][dist:312] - Updating wandb.run: still-frog-17 config with \"DIST_INFO\"\n[2024-07-17 07:42:16.469424][INFO][dist:938] - Running on machine='Polaris'\n[2024-07-17 07:42:16.471151][WARNING][__main__:89] - {\n    \"train\": {\n        \"framework\": \"pytorch\",\n        \"backend\": \"DDP\",\n        \"device\": null,\n        \"seed\": null,\n        \"port\": null,\n        \"ds_config_path\": null,\n        \"precision\": null,\n        \"ngpus\": null,\n        \"use_wandb\": true,\n        \"eval_interval\": 100,\n        \"log_interval\": 10,\n        \"eval_iters\": 200,\n        \"eval_only\": false,\n        \"always_save_checkpoint\": false,\n        \"init_from\": \"scratch\",\n        \"wandb_project\": \"WordPlay\",\n        \"max_iters\": 1000,\n        \"warmup_iters\": 100,\n        \"dtype\": \"bf16\",\n        \"compile\": false\n    },\n    \"model\": {\n        \"n_layer\": 12,\n        \"n_head\": 12,\n        \"n_embd\": 768,\n        \"batch_size\": 64,\n        \"block_size\": 1024,\n        \"activation\": \"gelu\",\n        \"dropout\": 0.0,\n        \"bias\": false,\n        \"vocab_size\": 65\n    },\n    \"data\": {\n        \"dataset\": \"shakespeare_char\",\n        \"out_dir\": \"out-shakespeare-char\",\n        \"root_path\": null\n    },\n    \"optimizer\": {\n        \"gas\": 1,\n        \"name\": \"AdamW\",\n        \"learning_rate\": 0.0006,\n        \"weight_decay\": 0.1,\n        \"beta1\": 0.9,\n        \"beta2\": 0.95,\n        \"grad_clip\": 1.0,\n        \"decay_lr\": true,\n        \"lr_decay_iters\": 600000,\n        \"min_lr\": 6e-05\n    }\n}\n[2024-07-17 07:42:16.474305][WARNING][__main__:90] - Output dir: /home/foremans/tmp/polaris-talk/outputs/runs/pytorch/DDP/2024-07-17/07-42-13\n[2024-07-17 07:42:16.474922][INFO][trainer:246] - Initializing a new model from scratch\n[2024-07-17 07:42:17.258904][INFO][model:255] - number of parameters: 85.00M\n[2024-07-17 07:42:17.290004][INFO][trainer:264] - Model size: num_params=85003776\n[2024-07-17 07:42:17.292626][INFO][model:445] - num decayed parameter tensors: 50, with 85,771,008 parameters\n[2024-07-17 07:42:17.293296][INFO][model:449] - num non-decayed parameter tensors: 25, with 19,200 parameters\n[2024-07-17 07:42:17.515324][CRITICAL][trainer:316] - \"devid='cuda:1'\"\n[2024-07-17 07:42:17.515340][CRITICAL][trainer:316] - \"devid='cuda:2'\"\n[2024-07-17 07:42:17.515465][CRITICAL][trainer:316] - \"devid='cuda:3'\"\n[2024-07-17 07:42:18.431814][INFO][model:465] - using fused AdamW: True\n[2024-07-17 07:42:18.432620][CRITICAL][trainer:316] - \"devid='cuda:0'\"\n[2024-07-17 07:42:19.951020][INFO][trainer:356] - • self.model=GPT(\n  (transformer): ModuleDict(\n    (wte): Embedding(65, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.0, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x Block(\n        (ln_1): LayerNorm()\n        (attn): CausalSelfAttention(\n          (c_attn): Linear(in_features=768, out_features=2304, bias=False)\n          (c_proj): Linear(in_features=768, out_features=768, bias=False)\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (ln_2): LayerNorm()\n        (mlp): MLP(\n          (c_fc): Linear(in_features=768, out_features=3072, bias=False)\n          (act_fn): GELU(approximate='none')\n          (c_proj): Linear(in_features=3072, out_features=768, bias=False)\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm()\n  )\n  (lm_head): Linear(in_features=768, out_features=65, bias=False)\n)\n[2024-07-17 07:42:19.955340][INFO][trainer:357] - • self.grad_scaler=&lt;torch.cuda.amp.grad_scaler.GradScaler object at 0x145a38f0f090&gt;\n[2024-07-17 07:42:19.956897][INFO][trainer:358] - • self.model_engine=DistributedDataParallel(\n  (module): GPT(\n    (transformer): ModuleDict(\n      (wte): Embedding(65, 768)\n      (wpe): Embedding(1024, 768)\n      (drop): Dropout(p=0.0, inplace=False)\n      (h): ModuleList(\n        (0-11): 12 x Block(\n          (ln_1): LayerNorm()\n          (attn): CausalSelfAttention(\n            (c_attn): Linear(in_features=768, out_features=2304, bias=False)\n            (c_proj): Linear(in_features=768, out_features=768, bias=False)\n            (attn_dropout): Dropout(p=0.0, inplace=False)\n            (resid_dropout): Dropout(p=0.0, inplace=False)\n          )\n          (ln_2): LayerNorm()\n          (mlp): MLP(\n            (c_fc): Linear(in_features=768, out_features=3072, bias=False)\n            (act_fn): GELU(approximate='none')\n            (c_proj): Linear(in_features=3072, out_features=768, bias=False)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n      )\n      (ln_f): LayerNorm()\n    )\n    (lm_head): Linear(in_features=768, out_features=65, bias=False)\n  )\n)\n[2024-07-17 07:42:19.961066][INFO][trainer:359] - • self.optimizer=AdamW (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.95)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: True\n    lr: 0.0006\n    maximize: False\n    weight_decay: 0.1\n\nParameter Group 1\n    amsgrad: False\n    betas: (0.9, 0.95)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: True\n    lr: 0.0006\n    maximize: False\n    weight_decay: 0.0\n)\n[2024-07-17 07:42:19.988827][INFO][trainer:802] - Startup time: 6.7125\n                Training Legend\n┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃    abbr     ┃ desc                           ┃\n┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│    step     │ Current training iteration     │\n│    loss     │ Loss value                     │\n│     dt      │ Elapsed time per training step │\n│     dtf     │ Elapsed time per forward step  │\n│     dtb     │ Elapsed time per backward step │\n│     sps     │ Samples per second             │\n│ sps_per_gpu │ Samples per second (per GPU)   │\n│     tps     │ Tokens per second              │\n│ tps_per_gpu │ Tokens per second (per GPU)    │\n│     mfu     │ Model flops utilization        │\n│ train_loss  │ Training loss value            │\n│  val_loss   │ Validation loss value          │\n└─────────────┴────────────────────────────────┘\n[2024-07-17 07:42:21.451865][INFO][trainer:820] - ['prompt']: 'What is an LLM?'\n[2024-07-17 07:42:21.452667][INFO][trainer:824] - ['response']:\nWhat is an LLM?eelEl\\'$nltPwBSWal,;PWw bbu\\'HiyP\\'FWwF &AhW:ygrn kk-\\'\\'KFlMwnlEfflkc,elpWaWtgml$Pgglhllw lglhFllzczPAFHpeAAPPSltgkrWPPhlEMgcrN ggPWt-WPSSzHSkkrzzk.FFrtSSkgMll&gFXr,hghaueaVPW-pHFF-gg,,,FF,,kbApgg gg\\'aWWzzkk\\'a\\'CggHl$bGeA,FFk,,SF;UF,,aZ ;gglee$,k.US&kg:S,,zVzzc\n[2024-07-17 07:43:01.573073][INFO][trainer:885] - step=10 loss=3.154310 dt=0.282833 dtf=0.005247 dtb=0.011417 sps=14.142633 sps_per_gpu=3.535658 tps=926851.609409 tps_per_gpu=231712.902352 mfu=46.288281 train_loss=4.125778 val_loss=4.128809\n[2024-07-17 07:43:04.402750][INFO][trainer:885] - step=20 loss=2.660851 dt=0.306263 dtf=0.005233 dtb=0.011419 sps=13.060678 sps_per_gpu=3.265170 tps=855944.613638 tps_per_gpu=213986.153409 mfu=45.934162 train_loss=4.125778 val_loss=4.128809\n[2024-07-17 07:43:07.237507][INFO][trainer:885] - step=30 loss=2.543283 dt=0.283021 dtf=0.005238 dtb=0.011245 sps=14.133211 sps_per_gpu=3.533303 tps=926234.088226 tps_per_gpu=231558.522057 mfu=45.966490 train_loss=4.125778 val_loss=4.128809\n[2024-07-17 07:43:10.077248][INFO][trainer:885] - step=40 loss=2.503963 dt=0.285001 dtf=0.005213 dtb=0.011471 sps=14.035061 sps_per_gpu=3.508765 tps=919801.749941 tps_per_gpu=229950.437485 mfu=45.963461 train_loss=4.125778 val_loss=4.128809\n[2024-07-17 07:43:12.917039][INFO][trainer:885] - step=50 loss=2.477469 dt=0.283532 dtf=0.005166 dtb=0.011294 sps=14.107763 sps_per_gpu=3.526941 tps=924566.380009 tps_per_gpu=231141.595002 mfu=45.984530 train_loss=4.125778 val_loss=4.128809\n[2024-07-17 07:43:15.760749][INFO][trainer:885] - step=60 loss=2.471083 dt=0.284630 dtf=0.005140 dtb=0.011224 sps=14.053326 sps_per_gpu=3.513332 tps=920998.786204 tps_per_gpu=230249.696551 mfu=45.985675 train_loss=4.125778 val_loss=4.128809\n[2024-07-17 07:43:18.602785][INFO][trainer:885] - step=70 loss=2.458894 dt=0.283926 dtf=0.005219 dtb=0.010383 sps=14.088155 sps_per_gpu=3.522039 tps=923281.352698 tps_per_gpu=230820.338174 mfu=45.998106 train_loss=4.125778 val_loss=4.128809\n[2024-07-17 07:43:21.451433][INFO][trainer:885] - step=80 loss=2.489088 dt=0.285537 dtf=0.005183 dtb=0.011373 sps=14.008683 sps_per_gpu=3.502171 tps=918073.060430 tps_per_gpu=229518.265108 mfu=45.983282 train_loss=4.125778 val_loss=4.128809\n[2024-07-17 07:43:24.302241][INFO][trainer:885] - step=90 loss=2.471990 dt=0.300767 dtf=0.005445 dtb=0.010290 sps=13.299337 sps_per_gpu=3.324834 tps=871585.359388 tps_per_gpu=217896.339847 mfu=45.737774 train_loss=4.125778 val_loss=4.128809\n[2024-07-17 07:43:27.153275][INFO][trainer:885] - step=100 loss=2.445556 dt=0.285869 dtf=0.005182 dtb=0.011251 sps=13.992403 sps_per_gpu=3.498101 tps=917006.151328 tps_per_gpu=229251.537832 mfu=45.743655 train_loss=4.125778 val_loss=4.128809\n[2024-07-17 07:43:28.182553][INFO][trainer:820] - ['prompt']: 'What is an LLM?'\n[2024-07-17 07:43:28.183179][INFO][trainer:824] - ['response']:\n\nWhat is an LLM?\n\nGoupay my winghimithell bls ger t bon sinthard ht omind be,\nAnd lereind h py balithand frd oforondof wimon me hageas thinero mand,\nThacanes,\nAn frift ghik med d herthecke ntore thack couthen ale, t thit ang d m t h chy me fache ag, wit my hathan glat ng\n[2024-07-17 07:44:06.025837][INFO][trainer:760] - Saving checkpoint to: /home/foremans/tmp/polaris-talk/outputs/runs/pytorch/DDP/2024-07-17/07-42-13\n[2024-07-17 07:44:06.026607][INFO][trainer:761] - Saving model to: /home/foremans/tmp/polaris-talk/outputs/runs/pytorch/DDP/2024-07-17/07-42-13/model.pth\n[2024-07-17 07:44:07.682968][INFO][configs:141] - Appending /home/foremans/tmp/polaris-talk/outputs/runs/pytorch/DDP/2024-07-17/07-42-13 to /home/foremans/tmp/polaris-talk/2024-07-17-073327/wordplay/src/ckpts/checkpoints.log\n[2024-07-17 07:44:10.519506][INFO][trainer:885] - step=110 loss=2.433923 dt=0.285038 dtf=0.005757 dtb=0.011762 sps=14.033209 sps_per_gpu=3.508302 tps=919680.367894 tps_per_gpu=229920.091974 mfu=45.762304 train_loss=2.439494 val_loss=2.478951\n[2024-07-17 07:44:13.362148][INFO][trainer:885] - step=120 loss=2.429014 dt=0.284445 dtf=0.005222 dtb=0.011486 sps=14.062460 sps_per_gpu=3.515615 tps=921597.361532 tps_per_gpu=230399.340383 mfu=45.788661 train_loss=2.439494 val_loss=2.478951\n[2024-07-17 07:44:16.210694][INFO][trainer:885] - step=130 loss=2.402059 dt=0.285559 dtf=0.005199 dtb=0.011765 sps=14.007633 sps_per_gpu=3.501908 tps=918004.211586 tps_per_gpu=229501.052897 mfu=45.794438 train_loss=2.439494 val_loss=2.478951\n[2024-07-17 07:44:19.061546][INFO][trainer:885] - step=140 loss=2.374062 dt=0.285476 dtf=0.005239 dtb=0.011453 sps=14.011662 sps_per_gpu=3.502916 tps=918268.297093 tps_per_gpu=229567.074273 mfu=45.800956 train_loss=2.439494 val_loss=2.478951\n[2024-07-17 07:44:21.917283][INFO][trainer:885] - step=150 loss=2.365385 dt=0.285846 dtf=0.005125 dtb=0.011320 sps=13.993568 sps_per_gpu=3.498392 tps=917082.475791 tps_per_gpu=229270.618948 mfu=45.800900 train_loss=2.439494 val_loss=2.478951\n[2024-07-17 07:44:24.771924][INFO][trainer:885] - step=160 loss=2.317337 dt=0.280788 dtf=0.005173 dtb=0.011249 sps=14.245602 sps_per_gpu=3.561401 tps=933599.792506 tps_per_gpu=233399.948127 mfu=45.883340 train_loss=2.439494 val_loss=2.478951\n[2024-07-17 07:44:27.626812][INFO][trainer:885] - step=170 loss=2.256231 dt=0.284973 dtf=0.005141 dtb=0.011299 sps=14.036416 sps_per_gpu=3.509104 tps=919890.544506 tps_per_gpu=229972.636126 mfu=45.889069 train_loss=2.439494 val_loss=2.478951\n[2024-07-17 07:44:30.480952][INFO][trainer:885] - step=180 loss=2.216419 dt=0.286555 dtf=0.005180 dtb=0.011402 sps=13.958906 sps_per_gpu=3.489726 tps=914810.852170 tps_per_gpu=228702.713043 mfu=45.868857 train_loss=2.439494 val_loss=2.478951\n[2024-07-17 07:44:33.337342][INFO][trainer:885] - step=190 loss=2.145123 dt=0.291456 dtf=0.005409 dtb=0.019347 sps=13.724205 sps_per_gpu=3.431051 tps=899429.467247 tps_per_gpu=224857.366812 mfu=45.773849 train_loss=2.439494 val_loss=2.478951\n[2024-07-17 07:44:36.194584][INFO][trainer:885] - step=200 loss=2.068149 dt=0.285703 dtf=0.005153 dtb=0.011286 sps=14.000555 sps_per_gpu=3.500139 tps=917540.393411 tps_per_gpu=229385.098353 mfu=45.778791 train_loss=2.439494 val_loss=2.478951\n[2024-07-17 07:44:37.224149][INFO][trainer:820] - ['prompt']: 'What is an LLM?'\n[2024-07-17 07:44:37.224745][INFO][trainer:824] - ['response']:\n\nWhat is an LLM?\n\nLORTESS LA:\nNo, sighappat selace? don downd sourciceans note cancen up sof liond\nThis and my man, werame, of re thee\nThise not will I on land brond sul me a fingore?\n\nFLER:\nTisint your not nare lame o igen,-to brorst.\n\nSamERS:\nSin:\nI\\'l hell she lor hen w\n[2024-07-17 07:45:14.409129][INFO][trainer:760] - Saving checkpoint to: /home/foremans/tmp/polaris-talk/outputs/runs/pytorch/DDP/2024-07-17/07-42-13\n[2024-07-17 07:45:14.409820][INFO][trainer:761] - Saving model to: /home/foremans/tmp/polaris-talk/outputs/runs/pytorch/DDP/2024-07-17/07-42-13/model.pth\n[2024-07-17 07:45:16.366935][INFO][configs:141] - Appending /home/foremans/tmp/polaris-talk/outputs/runs/pytorch/DDP/2024-07-17/07-42-13 to /home/foremans/tmp/polaris-talk/2024-07-17-073327/wordplay/src/ckpts/checkpoints.log\n[2024-07-17 07:45:19.245061][INFO][trainer:885] - step=210 loss=1.982169 dt=0.283305 dtf=0.005223 dtb=0.011284 sps=14.119042 sps_per_gpu=3.529760 tps=925305.515083 tps_per_gpu=231326.378771 mfu=45.822019 train_loss=2.045786 val_loss=2.148510\n[2024-07-17 07:45:22.092430][INFO][trainer:885] - step=220 loss=1.897731 dt=0.284759 dtf=0.005217 dtb=0.011187 sps=14.046945 sps_per_gpu=3.511736 tps=920580.608106 tps_per_gpu=230145.152026 mfu=45.837327 train_loss=2.045786 val_loss=2.148510\n[2024-07-17 07:45:24.942639][INFO][trainer:885] - step=230 loss=1.817213 dt=0.285266 dtf=0.005208 dtb=0.011446 sps=14.022003 sps_per_gpu=3.505501 tps=918945.985503 tps_per_gpu=229736.496376 mfu=45.842940 train_loss=2.045786 val_loss=2.148510\n[2024-07-17 07:45:27.797910][INFO][trainer:885] - step=240 loss=1.779287 dt=0.285465 dtf=0.005189 dtb=0.011220 sps=14.012250 sps_per_gpu=3.503062 tps=918306.793546 tps_per_gpu=229576.698387 mfu=45.844800 train_loss=2.045786 val_loss=2.148510\n[2024-07-17 07:45:30.653597][INFO][trainer:885] - step=250 loss=1.704220 dt=0.289284 dtf=0.005471 dtb=0.010346 sps=13.827253 sps_per_gpu=3.456813 tps=906182.836379 tps_per_gpu=226545.709095 mfu=45.785926 train_loss=2.045786 val_loss=2.148510\n[2024-07-17 07:45:33.512769][INFO][trainer:885] - step=260 loss=1.671318 dt=0.287679 dtf=0.005125 dtb=0.011250 sps=13.904380 sps_per_gpu=3.476095 tps=911237.442617 tps_per_gpu=227809.360654 mfu=45.758182 train_loss=2.045786 val_loss=2.148510\n[2024-07-17 07:45:36.373461][INFO][trainer:885] - step=270 loss=1.650952 dt=0.298661 dtf=0.005118 dtb=0.011520 sps=13.393107 sps_per_gpu=3.348277 tps=877730.651421 tps_per_gpu=219432.662855 mfu=45.565875 train_loss=2.045786 val_loss=2.148510\n[2024-07-17 07:45:39.236930][INFO][trainer:885] - step=280 loss=1.573242 dt=0.285970 dtf=0.005171 dtb=0.011290 sps=13.987477 sps_per_gpu=3.496869 tps=916683.279847 tps_per_gpu=229170.819962 mfu=45.587333 train_loss=2.045786 val_loss=2.148510\n[2024-07-17 07:45:42.100605][INFO][trainer:885] - step=290 loss=1.533265 dt=0.286487 dtf=0.005432 dtb=0.011288 sps=13.962259 sps_per_gpu=3.490565 tps=915030.617828 tps_per_gpu=228757.654457 mfu=45.598392 train_loss=2.045786 val_loss=2.148510\n[2024-07-17 07:45:44.964424][INFO][trainer:885] - step=300 loss=1.492064 dt=0.288480 dtf=0.005355 dtb=0.011480 sps=13.865774 sps_per_gpu=3.466443 tps=908707.340870 tps_per_gpu=227176.835218 mfu=45.576766 train_loss=2.045786 val_loss=2.148510\n[2024-07-17 07:45:45.995833][INFO][trainer:820] - ['prompt']: 'What is an LLM?'\n[2024-07-17 07:45:45.996497][INFO][trainer:824] - ['response']:\n\nWhat is an LLM?\n\nRICHMORD:\nChar stire? how in those are name the range hone.\n\nGLOUCESTER:\nNay, in lond's time the palt are worder more\nThat wilt in the purpose be a pey\nAnd thou thine onter hands, and the which broth.\n\nELBOWINCA:\nAt lie my lord with the me an arms be a s\n[2024-07-17 07:46:23.549987][INFO][trainer:760] - Saving checkpoint to: /home/foremans/tmp/polaris-talk/outputs/runs/pytorch/DDP/2024-07-17/07-42-13\n[2024-07-17 07:46:23.550696][INFO][trainer:761] - Saving model to: /home/foremans/tmp/polaris-talk/outputs/runs/pytorch/DDP/2024-07-17/07-42-13/model.pth\n[2024-07-17 07:46:25.496559][INFO][configs:141] - Appending /home/foremans/tmp/polaris-talk/outputs/runs/pytorch/DDP/2024-07-17/07-42-13 to /home/foremans/tmp/polaris-talk/2024-07-17-073327/wordplay/src/ckpts/checkpoints.log\n[2024-07-17 07:46:28.374854][INFO][trainer:885] - step=310 loss=1.444200 dt=0.299907 dtf=0.005333 dtb=0.010637 sps=13.337481 sps_per_gpu=3.334370 tps=874085.133345 tps_per_gpu=218521.283336 mfu=45.384395 train_loss=1.495372 val_loss=1.713714\n[2024-07-17 07:46:31.223079][INFO][trainer:885] - step=320 loss=1.429350 dt=0.285238 dtf=0.005245 dtb=0.011485 sps=14.023353 sps_per_gpu=3.505838 tps=919034.479880 tps_per_gpu=229758.619970 mfu=45.435743 train_loss=1.495372 val_loss=1.713714\n[2024-07-17 07:46:34.074957][INFO][trainer:885] - step=330 loss=1.362220 dt=0.285027 dtf=0.005165 dtb=0.011407 sps=14.033736 sps_per_gpu=3.508434 tps=919714.904826 tps_per_gpu=229928.726207 mfu=45.485355 train_loss=1.495372 val_loss=1.713714\n[2024-07-17 07:46:36.929464][INFO][trainer:885] - step=340 loss=1.350888 dt=0.284436 dtf=0.005199 dtb=0.011287 sps=14.062893 sps_per_gpu=3.515723 tps=921625.744709 tps_per_gpu=230406.436177 mfu=45.539549 train_loss=1.495372 val_loss=1.713714"
  },
  {
    "objectID": "talks/llms-at-scale/index.html#wordplay",
    "href": "talks/llms-at-scale/index.html#wordplay",
    "title": "Training LLMs at Scale",
    "section": "wordplay 🎮💬",
    "text": "wordplay 🎮💬\n\nLink1 to video\n\n\nExample: Training a LLM to talk like Shakespeare using saforem2/wordplay 🎮💬\nidk why it doesn’t render correctly in the slide (seems like refreshing helps?)"
  },
  {
    "objectID": "talks/llms-at-scale/index.html#bibliography",
    "href": "talks/llms-at-scale/index.html#bibliography",
    "title": "Training LLMs at Scale",
    "section": "Bibliography",
    "text": "Bibliography\n\n\nWei, Jason, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, et al. 2022. “Emergent Abilities of Large Language Models.” https://arxiv.org/abs/2206.07682.\n\n\nYang, Jingfeng, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Bing Yin, and Xia Hu. 2023. “Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond.” https://arxiv.org/abs/2304.13712.\n\n\nYao, Shunyu, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. “Tree of Thoughts: Deliberate Problem Solving with Large Language Models.” https://arxiv.org/abs/2305.10601."
  },
  {
    "objectID": "talks/llms-on-polaris/index.html#transformer-architecture",
    "href": "talks/llms-on-polaris/index.html#transformer-architecture",
    "title": "LLMs on Polaris",
    "section": "Transformer Architecture",
    "text": "Transformer Architecture\n\n\nFigure 11: Vaswani et al. (2017)",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "🦜 Talks",
      "Polaris Overview + LLMs"
    ]
  },
  {
    "objectID": "posts/dope-slides/index.html#quarto-reveal.js",
    "href": "posts/dope-slides/index.html#quarto-reveal.js",
    "title": "💅 How to Make Dope Slides",
    "section": "Quarto 🤝 Reveal.js",
    "text": "Quarto 🤝 Reveal.js\nSo, after making a promise some time ago on twitter 1, and having many questions following my talk on Parallel Training Techniques last week, I’m finally getting around to writing this up.\nThe slides are written using Quarto, a flavor of Markdown, and uses the built-in Quarto + Reveal.js functionality.\nFor this post, I’ll focus on the slides I presented at last years Lattice 2023, shown below:\n\n\n\n\n\n\n\n🪧 MLMC: Machine Learning Monte Carlo @ Lattice 2023 [07/2023]\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\n🏃‍♂️ Follow Along…\n\n\n\n\n\nOnce you’ve Installed Quarto, you can build these slides yourself by:\n\ngit clone  saforem2/lattice23\ncd lattice23 && quarto preview\n\nThis will create a docs/ directory with the following structure:\n📂 docs/\n├── 📂 assets/\n├── 📂 css/\n├── 📄 index.html\n├── 📄 lattice23.md\n├── 📄 search.json\n└── 📂 site_libs/\nOnce you’ve created this, and the docs/index.html file looks how you want, you can add the docs/ directory to your GitHub repo:\n$ git add docs\n$ git commit -m 'Create site'\n$ git push\nOnce you’ve enabled the GitHub page, the site will be automatically built and updated alongside the repo.",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "🪧 Posts",
      "💅 How to Make Dope Slides"
    ]
  },
  {
    "objectID": "posts/dope-slides/index.html#getting-started",
    "href": "posts/dope-slides/index.html#getting-started",
    "title": "💅 How to Make Dope Slides",
    "section": "Getting Started",
    "text": "Getting Started\nWhenever I give a talk, my workflow is typically:\n\nCreate new GitHub repo for it\nHunt down the GitHub repo from my last talk and2:\n$ cp -r old_talk/{_quarto.yml,index.qmd,references.bib,css/*} new_talk/\n\nHonestly, other than that, 90% of the work is done automatically by Quarto. The remaining 10% consists of figuring out why my css is broken (see CSS).\nThe best place to start for learning to make slides with Quarto and Reveal.js is the official documentation:\n\nQuarto / Presentations / Revealjs:\n\nReveal Basics\nPresenting Slides\nAdvanced Reveal\nReveal Themes\n\n\n\nThe slides are written in markdown Quarto (.qmd)3, a pandoc-compliant based markup language.\nFor a single slide deck, the content will be placed in index.qmd and our directory structure will look something like:\n\n📂 lattice23/\n├── 📂 assets/            # for images, etc.\n│   └── 🖼️ thumbnail.png  # can be used as social preview image\n├── 📂 css/\n│   ├── 📄 callouts.css\n│   ├── 📄 dark.scss\n│   └── 📄 default.css\n├── 🛠️ _quarto.yml        # Configuration goes here\n├── 📄 index.qmd          # Quarto document containing slides content\n└── 📜 references.bib     # BibTex references\n\nEquations are rendered using $ delimiters for inline math and $$ for display math4.\nWe can use Divs and Spans from Pandoc.\n\n&lt;span&gt;’s: are created by wrapping text in square brackets, and will be treated as a &lt;span&gt; with attributes if it is followed immediately by attributes, e.g.:\n\nExample: [This is *some text*]{.class key=\"val\"}\nidk what I’m doing really, so I mostly find myself doing things like [blue text]{style=\"color:#1E88E5;\"} which produces blue text.\n\n&lt;div&gt;’s: are created by wrapping text with a line consisting of at least three colons :::.\n\nExample:\n::: {#special .sidebar}\nHere is a paragraph.\n\nAnd another.\n:::\nWe can use either attributes in curly braces or a single unbraced word, which will be treated as a class name.\n\n\n\n\n🎁 Install Extensions\nFind the full list of available extensions at Quarto Extensions\nTo install various icon sets used in the example slides, we can install the following extensions:\n$ quarto install extension mcanouil/quarto-iconify      # https://icones.js.org/ [&lt;-- Contains rest of icon sets ??]\n$ quarto install extension shafayetShafee/bsicons       # bootstrap icons\n$ quarto install extension schochastics/academicicons   # OrcID, Google Scholar, ...\n$ quarto install extension quarto-ext/fontawesome       # Font Awesome icons\nnote that these aren’t necessary for functionality, but provide additional icons that I like to use 🤷🏻‍♂️",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "🪧 Posts",
      "💅 How to Make Dope Slides"
    ]
  },
  {
    "objectID": "posts/dope-slides/index.html#metadata",
    "href": "posts/dope-slides/index.html#metadata",
    "title": "💅 How to Make Dope Slides",
    "section": "Metadata",
    "text": "Metadata\nThe first section of our index.qmd contains the YAML metadata for the Quarto document.\nExplicitly, we see this consists of:\n\n\nExpand for yaml\n\n---\nformat:\n  revealjs:\n    title-block-style: none\n    slide-number: c\n    title-slide-style: default\n    chalkboard:\n      buttons: false\n    auto-animate: true\n    reference-location: section\n    touch: true\n    pause: false\n    footnotes-hover: true\n    citations-hover: true\n    preview-links: true\n    controls-tutorial: true\n    controls: false\n    logo: \"https://raw.githubusercontent.com/saforem2/anl-job-talk/main/docs/assets/anl.svg\"\n    history: false\n    theme: [dark, css/dark.scss]\n    css: [css/default.css, css/callouts.css]\n    self-contained: false\n    embed-resources: false\n    self-contained-math: false\n    center: true\n    highlight-style: \"atom-one\"\n    default-image-extension: svg\n    code-line-numbers: true\n    code-overflow: scroll\n    html-math-method: katex\n    fig-align: center\n    mermaid:\n      theme: dark\n  gfm:\n    output-file: \"lattice23.md\"\n---\n\nThe complete list of Reveal.js options are listed, with descriptions at: Quarto – Revealjs Options",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "🪧 Posts",
      "💅 How to Make Dope Slides"
    ]
  },
  {
    "objectID": "posts/dope-slides/index.html#title-slide",
    "href": "posts/dope-slides/index.html#title-slide",
    "title": "💅 How to Make Dope Slides",
    "section": "Title Slide",
    "text": "Title Slide\n\nStarting with the title slide5:\n\n\n\n\n\n\nFigure 1: Title Slide\n\n\n\n\nThe full slide contents are included below:\n\n\nExpand for quarto\n\n# {.title-slide .centeredslide background-iframe=\"https://saforem2.github.io/grid-worms-animation/\" loading=\"lazy\"}\n\n::: {style=\"background-color: rgba(22,22,22,0.75); border-radius: 10px; text-align:center; padding: 0px; padding-left: 1.5em; padding-right: 1.5em; max-width: min-content; min-width: max-content; margin-left: auto; margin-right: auto; padding-top: 0.2em; padding-bottom: 0.2em; line-height: 1.5em!important;\"}\n\n[MLMC: Machine Learning Monte Carlo]{.style=\"color:#939393; font-size:1.5em; font-weight:bold;}  \n[for Lattice Gauge Theory]{style=\"color:#777777; font-size:1.2em; font-weight: bold;\"}\n[&lt;br&gt;&nbsp;]{style=\"padding-bottom: 0.5rem;\"}  \n[](https://samforeman.me) Sam Foreman  \n[Xiao-Yong Jin, James C. Osborn]{.dim-text style=\"font-size:0.8em;\"}  \n[[[ `saforem2/`](https://github.com/saforem2/)]{style=\"border-bottom: 0.5px solid #00ccff;\"}`{`[[`lattice23`](https://github.com/saforem2/lattice23)]{style=\"border-bottom: 0.5px solid #00ccff;\"}, [[`l2hmc-qcd`](https://github.com/saforem2/l2hmc-qcd)]{style=\"border-bottom: 0.5px solid #00ccff;\"}`}`]{style=\"font-size:0.8em;\"}\n\n:::\n\n::: footer\n[2023-07-31 @ [Lattice 2023](https://indico.fnal.gov/event/57249/contributions/271305/)]{.dim-text style=\"text-align:left;'}\n:::\n\nFor the background, I made a simple animation  saforem2/grid-worms-animation that is hosted on GitHub pages as a simple html website\nThis static GitHub page is then used as an IFrame Background natively in Quarto with Reveal.js\nThis is as simple as:\n# {.title-slide .centeredslide background-iframe=\"https://saforem2.github.io/grid-worms-animation/\" loading=\"lazy\"}",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "🪧 Posts",
      "💅 How to Make Dope Slides"
    ]
  },
  {
    "objectID": "posts/dope-slides/index.html#single-column-slides",
    "href": "posts/dope-slides/index.html#single-column-slides",
    "title": "💅 How to Make Dope Slides",
    "section": "Single-Column Slides",
    "text": "Single-Column Slides\nOther than the title slide, the remainder of the slides are all relatively straightforward to construct.\nFor single-column slides, constructing the content is as simple as writing it in Markdown:\n\nCodeSlide\n\n\n# Overview\n\n1. [Background: `{MCMC,HMC}`](#markov-chain-monte-carlo-mcmc)\n    - [Leapfrog Integrator](#leapfrog-integrator-hmc)\n    - [Issues with HMC](#sec-issues-with-hmc)\n    - [Can we do better?](#sec-can-we-do-better)\n\n2. [L2HMC: Generalizing MD](#sec-l2hmc)\n    - [4D $SU(3)$ Model](#sec-su3)\n    - [Results](#sec-results)\n3. [References](#sec-references)\n4. [Extras](#sec-extras)\n\n\n\n\n\n\n\n\n\nFigure 2: Overview Slide",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "🪧 Posts",
      "💅 How to Make Dope Slides"
    ]
  },
  {
    "objectID": "posts/dope-slides/index.html#centered-slides",
    "href": "posts/dope-slides/index.html#centered-slides",
    "title": "💅 How to Make Dope Slides",
    "section": "Centered Slides",
    "text": "Centered Slides\nWe can center all the text on a slide by adding the {.centeredslide} class to the slide header, e.g.\n\nindex.qmdstyle.scss\n\n\n---\nformat:\n  revealjs:\n    theme: [style.scss]\n---\n\n# Title {.centeredslide}\n\n\n.centeredslide {\n  text-align: center;\n}",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "🪧 Posts",
      "💅 How to Make Dope Slides"
    ]
  },
  {
    "objectID": "posts/dope-slides/index.html#multi-column-slides",
    "href": "posts/dope-slides/index.html#multi-column-slides",
    "title": "💅 How to Make Dope Slides",
    "section": "Multi-Column Slides",
    "text": "Multi-Column Slides\nSide-by-side content (either text or images)\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Markov Chain Monte Carlo\n\n\n\n\n\nThis slide is horizontally centered6 and the content consists of two rows split as follows:\n\n\n\n\n\n\nA\nB\n\n\nC\nC\n\n\n\n\n\nTable 1: Slide layout. First row split into two columns, second row spans full width.\n\n\n\n\nIn panel A, we have a ::: {.callout-note} block followed by a single list element containing a LaTeX equation.\nIn panel B we have a standard markdown image\n![](./asets/mcmc.png)\nIn panel C we have normal text + math with LaTeX7 syntax.\n\n\n\n\n\nNote that we additionally have a ::: footer element included at the bottom of the slide.\nThe code used to generate the slide above is included below:\n\n\nExpand forquarto\n\n# Markov Chain Monte Carlo (MCMC) {.centeredslide}\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n::: {.callout-note title=\"Goal\" style=\"text-align:left;!important\"}\nGenerate **independent** samples $\\{x_{i}\\}$, such that[^notation]\n$$\\{x_{i}\\} \\sim p(x) \\propto e^{-S(x)}$$\nwhere $S(x)$ is the _action_ (or potential energy)\n:::\n\n- Want to calculate observables $\\mathcal{O}$:  \n  $\\left\\langle \\mathcal{O}\\right\\rangle \\propto \\int \\left[\\mathcal{D}x\\right]\\hspace{4pt} {\\mathcal{O}(x)\\, p(x)}$\n\n:::\n\n::: {.column width=\"49%\"}\n![](https://raw.githubusercontent.com/saforem2/deep-fridays/main/assets/normal_distribution.dark.svg)\n:::\n\n::::\n\nIf these were [independent]{.style=\"color:#00CCFF;\"}, we could approximate:\n$\\left\\langle\\mathcal{O}\\right\\rangle \\simeq \\frac{1}{N}\\sum^{N}_{n=1}\\mathcal{O}(x_{n})$\n$$\\sigma_{\\mathcal{O}}^{2} = \\frac{1}{N}\\mathrm{Var}{\\left[\\mathcal{O} (x) \\right]}\\Longrightarrow\n\\sigma_{\\mathcal{O}} \\propto \\frac{1}{\\sqrt{N}}$$\n\n[^notation]: Here, $\\sim$ means \"is distributed according to\"\n\n::: footer\n[ `saforem2/lattice23`](https://saforem2.github.io/lattice23)\n:::",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "🪧 Posts",
      "💅 How to Make Dope Slides"
    ]
  },
  {
    "objectID": "posts/dope-slides/index.html#css",
    "href": "posts/dope-slides/index.html#css",
    "title": "💅 How to Make Dope Slides",
    "section": "💅 CSS",
    "text": "💅 CSS\nMy web developer friend laughs at me, but when something is broken / doesn’t look right / I want it to look different, I:\n\nPull up Chrome Tools ( ⌘ + ⌥ + I )\nInspect element of interest ( ⌘ + ⇧ + C )\nMake changes to the CSS\nSave the new rule to my .scss file 🤷🏻‍♂️\n\nI’m guessing this might be obvious to some people, but it took me a while to figure out how things worked so maybe its helpful for others.\n\n\nExpand for css\n\n\n\n\n\n\n\nFigure 4: Example of selecting an element and making a change to the CSS.",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "🪧 Posts",
      "💅 How to Make Dope Slides"
    ]
  },
  {
    "objectID": "posts/dope-slides/index.html#github-page",
    "href": "posts/dope-slides/index.html#github-page",
    "title": "💅 How to Make Dope Slides",
    "section": "📃 GitHub Page",
    "text": "📃 GitHub Page\nTo enable your GitHub page, you can do the following:\n\n\n\n\n\n\nFigure 5: Instructions for building a GitHub page using the docs/ directory off the main branch.\n\n\n\nIn this case, the repo is:\n saforem2/lattice23\nand the site is published at\nhttps://saforem2.github.io/lattice23",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "🪧 Posts",
      "💅 How to Make Dope Slides"
    ]
  },
  {
    "objectID": "posts/dope-slides/index.html#references",
    "href": "posts/dope-slides/index.html#references",
    "title": "💅 How to Make Dope Slides",
    "section": "📓 References",
    "text": "📓 References\n\nReveal Themes\nUsing Pandoc fenced divs\nSlidecraft 101: Colors and Fonts\nBeautiful Reports and Presentations with Quarto\n Quarto Clean Theme\n\n\n\n\n\n\n\n❤️‍🩹 Status\n\n\n\n\n\n\n\nLast Updated: 08/13/2024 @ 11:48:37",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "🪧 Posts",
      "💅 How to Make Dope Slides"
    ]
  },
  {
    "objectID": "posts/dope-slides/index.html#footnotes",
    "href": "posts/dope-slides/index.html#footnotes",
    "title": "💅 How to Make Dope Slides",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAnd countless other people IRL↩︎\nOne thing I’ve been meaning to do, is clean up all my css/* files and move them all to a single repository, but I’ll save that for another day.↩︎\nAn open-source scientific and technical publishing system↩︎\nEquations↩︎\nQuarto comes with lightbox support, so you can click on images to display them full screen.↩︎\nBy adding the {.centeredslide} class to the slide header↩︎\nText surrounded by $ will be rendered with LaTeX↩︎",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "🪧 Posts",
      "💅 How to Make Dope Slides"
    ]
  },
  {
    "objectID": "talks/index.html",
    "href": "talks/index.html",
    "title": "🦜 Recent Talks",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nAug 9, 2024\n\n\nTraining LLMs at Scale\n\n\nSam Foreman \n\n\n\n\nAug 9, 2024\n\n\nTest Rendering on Mobile\n\n\nSam Foreman \n\n\n\n\nJul 17, 2024\n\n\nLLMs on Polaris\n\n\nSam Foreman \n\n\n\n\nJul 31, 2023\n\n\nMLMC: Machine Learning Monte Carlo\n\n\nSam Foreman\n\n\n\n\n\nNo matching items\n\n\n\n📆 2024\n\n\n\n\n\n\nTraining LLMs at Scale @ ATPESC, 2024 [08/2024]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLLMs on Polaris @ Center for Scientific Foundation Models, Summer School 24’ [07/2024]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParallel Training Techniques @ AI-4-Science Training Series [03/2024]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLLMs from Scratch @ LLM Tutorial Workshop [02/2024]\n\n\n\n\n\n\n\n\n\n\n\n📆 2023\n\n\n\n\n\n\nCreating Small(-ish) LLMs @ LLM Tutorial Workshop (1) [11/2023]\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\nExascale Science on Aurora @ Intel oneAPI Workshop @ UIC [10/2023]\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\nLLM Lunch Talk @ ALCF Hands On HPC Workshop [10/2023]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScaling LLMs for Science @ Data-Intensive Computing + AI/ML at Scale [08/2023]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMLMC: Machine Learning Monte Carlo @ Lattice 2023 [07/2023]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative Modeling and Efficient Sampling @ PASC23 [07/2023]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEfficient Sampling for LGT @ Deep Fridays @ U. Bologna [04/2023]\n\n\n\n\n\n\n\n\n\n\n\n📆 2022\n\n\n\n\n\n\nLarge Scale Training @ AI4Science on Supercomputers (ALCF) [11/2022]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHyperparameter Management @ ALCF SDL Workshop [10/2022]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical Learning @ ATPESC 2022 [08/2022]\n\n\n\n\n\n\n📕 accompanying notebook\n\n\n\n\n\n\n\n\n\n\n\nScientific Data Science: An Emerging Symbiosis @ ANL (05/2022)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMachine Learning in HEP @ UNC Greensboro [03/2022]\n\n\n\n\n\n\nMachine Learning in HEP, at UNC Greensboro, March 2022\n\n\n\n\n\n\n\n📆 2021\n\n\n\n\n\n\nAccelerated Sampling Methods for LGT, @ DWQ @ 25 [BNL] [12/2021]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTraining Topological Samplers for LGT @ ML4HEP, ECT* Trento [09/2021]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nl2hmc-qcd @ MIT Lattice Group Seminar [2021]\n\n\n\n\n\nl2hmc-qcd at the MIT Lattice Group Seminar, 2021\n\n\n\n\n\n\n\n\n\nDeep Learning HMC for Improved Gauge Generation @ ML in LQCD Workshop [2021]\n\n\n\n\n\n \n\n\n\n\n\n📆 2020\n\n\n\n\n\n\nMachine Learning for Lattice QCD @ U. Iowa [2020]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{foreman2024,\n  author = {Foreman, Sam},\n  title = {🦜 {Recent} {Talks}},\n  date = {2024-08-13},\n  url = {https://samforeman.me},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nForeman, Sam. 2024. “🦜 Recent Talks.” August 13, 2024. https://samforeman.me.",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "🦜 Talks",
      "📢 All Talks"
    ]
  },
  {
    "objectID": "talks/lattice23/index.html#hamiltonian-monte-carlo-hmc-1",
    "href": "talks/lattice23/index.html#hamiltonian-monte-carlo-hmc-1",
    "title": "MLMC: Machine Learning Monte Carlo",
    "section": "Hamiltonian Monte Carlo (HMC)",
    "text": "Hamiltonian Monte Carlo (HMC)\n\n\n\nIdea: Evolve the (\\dot{x}, \\dot{v}) system to get new states \\{x_{i}\\}❗\nWrite the joint distribution p(x, v): \np(x, v) \\propto e^{-S[x]} e^{-\\frac{1}{2}v^{T} v} = e^{-H(x, v)}\n\n\n\n\n\n\n\n\n\n\nHamiltonian Dynamics\n\n\nH = S[x] + \\frac{1}{2} v^{T} v \\Longrightarrow \\dot{x} = +\\partial_{v} H,\n\\,\\,\\dot{v} = -\\partial_{x} H\n\n\n\n\n\n\n\nFigure 1: Overview of HMC algorithm"
  },
  {
    "objectID": "talks/lattice23/index.html#sec-leapfrog",
    "href": "talks/lattice23/index.html#sec-leapfrog",
    "title": "MLMC: Machine Learning Monte Carlo",
    "section": "Leapfrog Integrator (HMC)",
    "text": "Leapfrog Integrator (HMC)\n\n\n\n\n\n\n\n\n\nHamiltonian Dynamics\n\n\n\\left(\\dot{x}, \\dot{v}\\right) = \\left(\\partial_{v} H, -\\partial_{x} H\\right)\n\n\n\n\n\n\n\n\n\n\n\nLeapfrog Step\n\n\ninput \\,\\left(x, v\\right) \\rightarrow \\left(x', v'\\right)\\, output\n\\begin{align*}\n\\tilde{v} &:= \\textcolor{#F06292}{\\Gamma}(x, v)\\hspace{2.2pt} = v - \\frac{\\varepsilon}{2} \\partial_{x} S(x) \\\\\nx' &:= \\textcolor{#FD971F}{\\Lambda}(x, \\tilde{v}) \\, =  x + \\varepsilon \\, \\tilde{v} \\\\\nv' &:= \\textcolor{#F06292}{\\Gamma}(x', \\tilde{v}) = \\tilde{v} - \\frac{\\varepsilon}{2} \\partial_{x} S(x')\n\\end{align*}\n\n\n\n\n\n\n\n\n\n\n\nWarning!\n\n\n\nResample v_{0} \\sim \\mathcal{N}(0, \\mathbb{1})\nat the beginning of each trajectory\n\n\n\n\n\n\nNote: \\partial_{x} S(x) is the force"
  },
  {
    "objectID": "talks/lattice23/index.html#hmc-update",
    "href": "talks/lattice23/index.html#hmc-update",
    "title": "MLMC: Machine Learning Monte Carlo",
    "section": "HMC Update",
    "text": "HMC Update\n\n\n\nWe build a trajectory of N_{\\mathrm{LF}} leapfrog steps1 \\begin{equation*}\n(x_{0}, v_{0})%\n\\rightarrow (x_{1}, v_{1})\\rightarrow \\cdots%\n\\rightarrow (x', v')\n\\end{equation*}\nAnd propose x' as the next state in our chain\n\n\\begin{align*}\n  \\textcolor{#F06292}{\\Gamma}: (x, v) \\textcolor{#F06292}{\\rightarrow} v' &:= v - \\frac{\\varepsilon}{2} \\partial_{x} S(x) \\\\\n  \\textcolor{#FD971F}{\\Lambda}: (x, v) \\textcolor{#FD971F}{\\rightarrow} x' &:= x + \\varepsilon v\n\\end{align*}\n\nWe then accept / reject x' using Metropolis-Hastings criteria,\nA(x'|x) = \\min\\left\\{1, \\frac{p(x')}{p(x)}\\left|\\frac{\\partial x'}{\\partial x}\\right|\\right\\}\n\n\n\n\n\n\n\n\nWe always start by resampling the momentum, v_{0} \\sim\n\\mathcal{N}(0, \\mathbb{1})↩︎"
  },
  {
    "objectID": "talks/lattice23/index.html#hmc-demo",
    "href": "talks/lattice23/index.html#hmc-demo",
    "title": "MLMC: Machine Learning Monte Carlo",
    "section": "HMC Demo",
    "text": "HMC Demo\n\n\n\n\n\n\n\nFigure 2: HMC Demo"
  },
  {
    "objectID": "talks/lattice23/index.html#l2hmc-leapfrog-layer",
    "href": "talks/lattice23/index.html#l2hmc-leapfrog-layer",
    "title": "MLMC: Machine Learning Monte Carlo",
    "section": "L2HMC: Leapfrog Layer",
    "text": "L2HMC: Leapfrog Layer"
  },
  {
    "objectID": "talks/lattice23/index.html#l2hmc-update",
    "href": "talks/lattice23/index.html#l2hmc-update",
    "title": "MLMC: Machine Learning Monte Carlo",
    "section": "L2HMC Update",
    "text": "L2HMC Update\n\n\n\n\n\n\nAlgorithm\n\n\n\ninput: x\n\nResample: \\textcolor{#07B875}{v} \\sim \\mathcal{N}(0, \\mathbb{1}); \\,\\,{d\\sim\\mathcal{U}(\\pm)}\nConstruct initial state: \\textcolor{#939393}{\\xi} =(\\textcolor{#AE81FF}{x}, \\textcolor{#07B875}{v}, {\\pm})\n\nforward: Generate proposal \\xi' by passing initial \\xi through N_{\\mathrm{LF}} leapfrog layers\n\\textcolor{#939393} \\xi \\hspace{1pt}\\xrightarrow[]{\\tiny{\\mathrm{LF} \\text{ layer}}}\\xi_{1} \\longrightarrow\\cdots \\longrightarrow \\xi_{N_{\\mathrm{LF}}} = \\textcolor{#f8f8f8}{\\xi'} := (\\textcolor{#AE81FF}{x''}, \\textcolor{#07B875}{v''})\n\nAccept / Reject: \\begin{equation*}\nA({\\textcolor{#f8f8f8}{\\xi'}}|{\\textcolor{#939393}{\\xi}})=\n\\mathrm{min}\\left\\{1,\n\\frac{\\pi(\\textcolor{#f8f8f8}{\\xi'})}{\\pi(\\textcolor{#939393}{\\xi})} \\left| \\mathcal{J}\\left(\\textcolor{#f8f8f8}{\\xi'},\\textcolor{#939393}{\\xi}\\right)\\right| \\right\\}\n\\end{equation*}\n\nbackward (if training):\n\nEvaluate the loss function1 \\mathcal{L}\\gets \\mathcal{L}_{\\theta}(\\textcolor{#f8f8f8}{\\xi'}, \\textcolor{#939393}{\\xi}) and backprop\n\nreturn: \\textcolor{#AE81FF}{x}_{i+1}\nEvaluate MH criteria (1) and return accepted config, \\textcolor{#AE81FF}{{x}_{i+1}}\\gets\n  \\begin{cases}\n  \\textcolor{#f8f8f8}{\\textcolor{#AE81FF}{x''}} \\small{\\text{ w/ prob }} A(\\textcolor{#f8f8f8}{\\xi''}|\\textcolor{#939393}{\\xi}) \\hspace{26pt} ✅ \\\\\n  \\textcolor{#939393}{\\textcolor{#AE81FF}{x}} \\hspace{5pt}\\small{\\text{ w/ prob }} 1 - A(\\textcolor{#f8f8f8}{\\xi''}|{\\textcolor{#939393}{\\xi}}) \\hspace{10pt} 🚫\n  \\end{cases}\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Leapfrog Layer used in generalized MD update\n\n\n\n\n\n\n\n\nFor simple \\mathbf{x} \\in \\mathbb{R}^{2} example, \\mathcal{L}_{\\theta} =\nA(\\xi^{\\ast}|\\xi)\\cdot \\left(\\mathbf{x}^{\\ast} - \\mathbf{x}\\right)^{2}↩︎"
  },
  {
    "objectID": "talks/lattice23/index.html#sec-hmcsu3",
    "href": "talks/lattice23/index.html#sec-hmcsu3",
    "title": "MLMC: Machine Learning Monte Carlo",
    "section": "HMC: 4D SU(3)",
    "text": "HMC: 4D SU(3)\nHamiltonian: H[P, U] = \\frac{1}{2} P^{2} + S[U] \\Longrightarrow\n\n\n\n\n\n\n\nU update: \\frac{d\\omega^{k}}{dt} = \\frac{\\partial H}{\\partial P^{k}} \\frac{d\\omega^{k}}{dt}\\lambda^{k} = P^{k}\\lambda^{k} \\Longrightarrow \\frac{dQ}{dt} = P \\begin{align*}\nQ(\\textcolor{#FFEE58}{\\varepsilon}) &= Q(0) + \\textcolor{#FFEE58}{\\varepsilon} P(0)\\Longrightarrow\\\\\n-i\\, \\log U(\\textcolor{#FFEE58}{\\varepsilon}) &= -i\\, \\log U(0) + \\textcolor{#FFEE58}{\\varepsilon} P(0) \\\\\nU(\\textcolor{#FFEE58}{\\varepsilon}) &= e^{i\\,\\textcolor{#FFEE58}{\\varepsilon} P(0)} U(0)\\Longrightarrow \\\\\n&\\hspace{1pt}\\\\\n\\textcolor{#FD971F}{\\Lambda}:\\,\\, U \\longrightarrow U' &:= e^{i\\varepsilon P'} U\n\\end{align*}\n\n\n\n\n\n\n\n\n\n\n\n\nP update: \\frac{dP^{k}}{dt} = - \\frac{\\partial H}{\\partial \\omega^{k}} \\frac{dP^{k}}{dt} = - \\frac{\\partial H}{\\partial \\omega^{k}}\n= -\\frac{\\partial H}{\\partial Q} = -\\frac{dS}{dQ}\\Longrightarrow \\begin{align*}\nP(\\textcolor{#FFEE58}{\\varepsilon}) &= P(0) - \\textcolor{#FFEE58}{\\varepsilon} \\left.\\frac{dS}{dQ}\\right|_{t=0} \\\\\n&= P(0) - \\textcolor{#FFEE58}{\\varepsilon} \\,\\textcolor{#E599F7}{F[U]} \\\\\n&\\hspace{1pt}\\\\\n\\textcolor{#F06292}{\\Gamma}:\\,\\, P \\longrightarrow P' &:= P - \\frac{\\varepsilon}{2} F[U]\n\\end{align*}\n\n\n\n\n\n\n\n\n\\textcolor{#FFEE58}{\\varepsilon} is the step size\n\n\\textcolor{#E599F7}{F[U]} is the force term"
  },
  {
    "objectID": "talks/lattice23/index.html#hmc-4d-su3",
    "href": "talks/lattice23/index.html#hmc-4d-su3",
    "title": "MLMC: Machine Learning Monte Carlo",
    "section": "HMC: 4D SU(3)",
    "text": "HMC: 4D SU(3)\n\n\n\nMomentum Update: \\textcolor{#F06292}{\\Gamma}: P \\longrightarrow P' := P - \\frac{\\varepsilon}{2} F[U]\nLink Update: \\textcolor{#FD971F}{\\Lambda}: U \\longrightarrow U' := e^{i\\varepsilon P'} U\\quad\\quad\nWe maintain a batch of Nb lattices, all updated in parallel\n\nU.dtype = complex128\nU.shape\n= [Nb, 4, Nt, Nx, Ny, Nz, 3, 3]"
  },
  {
    "objectID": "talks/lattice23/index.html#p-network-pt.-1",
    "href": "talks/lattice23/index.html#p-network-pt.-1",
    "title": "MLMC: Machine Learning Monte Carlo",
    "section": "P-Network (pt. 1)",
    "text": "P-Network (pt. 1)\n\n\n\n\n\n\ninput1: \\hspace{7pt}\\left(U, F\\right) := (e^{i Q}, F) \\begin{align*}\nh_{0} &= \\sigma\\left( w_{Q} Q + w_{F} F + b \\right) \\\\\nh_{1} &= \\sigma\\left( w_{1} h_{0} + b_{1} \\right) \\\\\n&\\vdots \\\\\nh_{n} &= \\sigma\\left(w_{n-1} h_{n-2} + b_{n}\\right) \\\\\n\\textcolor{#FF5252}{z} & := \\sigma\\left(w_{n} h_{n-1} + b_{n}\\right) \\longrightarrow \\\\\n\\end{align*}\n\n\n\noutput2: \\hspace{7pt} (s_{P}, t_{P}, q_{P})\n\ns_{P} = \\lambda_{s} \\tanh(w_s \\textcolor{#FF5252}{z} + b_s)\nt_{P} = w_{t} \\textcolor{#FF5252}{z} + b_{t}\nq_{P} = \\lambda_{q} \\tanh(w_{q} \\textcolor{#FF5252}{z} + b_{q})\n\n\n\n\n\n\n\n\\sigma(\\cdot) denotes an activation function↩︎\n\\lambda_{s}, \\lambda_{q} \\in \\mathbb{R}, trainable parameters↩︎"
  },
  {
    "objectID": "talks/lattice23/index.html#p-network-pt.-2",
    "href": "talks/lattice23/index.html#p-network-pt.-2",
    "title": "MLMC: Machine Learning Monte Carlo",
    "section": "P-Network (pt. 2)",
    "text": "P-Network (pt. 2)\n\n\n\n\nUse (s_{P}, t_{P}, q_{P}) to update \\Gamma^{\\pm}: (U, P) \\rightarrow\n\\left(U, P_{\\pm}\\right)1:\n\nforward (d = \\textcolor{#FF5252}{+}): \\Gamma^{\\textcolor{#FF5252}{+}}(U, P) := P_{\\textcolor{#FF5252}{+}} = P \\cdot e^{\\frac{\\varepsilon}{2} s_{P}} - \\frac{\\varepsilon}{2}\\left[ F \\cdot e^{\\varepsilon q_{P}} + t_{P} \\right]\nbackward (d = \\textcolor{#1A8FFF}{-}): \\Gamma^{\\textcolor{#1A8FFF}{-}}(U, P) := P_{\\textcolor{#1A8FFF}{-}} = e^{-\\frac{\\varepsilon}{2} s_{P}} \\left\\{P + \\frac{\\varepsilon}{2}\\left[ F \\cdot e^{\\varepsilon q_{P}} + t_{P} \\right]\\right\\}\n\n\n\n\n\n\nNote that \\left(\\Gamma^{+}\\right)^{-1} = \\Gamma^{-}, i.e. \\Gamma^{+}\\left[\\Gamma^{-}(U, P)\\right] = \\Gamma^{-}\\left[\\Gamma^{+}(U, P)\\right] = (U, P)↩︎"
  },
  {
    "objectID": "talks/lattice23/index.html#sec-interpretation",
    "href": "talks/lattice23/index.html#sec-interpretation",
    "title": "MLMC: Machine Learning Monte Carlo",
    "section": "Interpretation",
    "text": "Interpretation\n\n\nDeviation in x_{P}\n\nTopological charge mixing\n\nArtificial influx of energy\n\n\n\n\n\n\n\nFigure 8: Illustration of how different observables evolve over a single L2HMC trajectory."
  },
  {
    "objectID": "talks/lattice23/index.html#interpretation",
    "href": "talks/lattice23/index.html#interpretation",
    "title": "MLMC: Machine Learning Monte Carlo",
    "section": "Interpretation",
    "text": "Interpretation\n\n\n\n\n\n\n\n\nAverage plaquette: \\langle x_{P}\\rangle vs LF step\n\n\n\n\n\n\n\nAverage energy: H - \\sum\\log|\\mathcal{J}|\n\n\n\n\n\n\nFigure 9: The trained model artifically increases the energy towards the middle of the trajectory, allowing the sampler to tunnel between isolated sectors."
  },
  {
    "objectID": "talks/lattice23/index.html#d-su3-results-delta-u_munu",
    "href": "talks/lattice23/index.html#d-su3-results-delta-u_munu",
    "title": "MLMC: Machine Learning Monte Carlo",
    "section": "4D SU(3) Results: \\delta U_{\\mu\\nu}",
    "text": "4D SU(3) Results: \\delta U_{\\mu\\nu}\n\n\n\n\n\n\nFigure 13: The difference in the average plaquette \\left|\\delta U_{\\mu\\nu}\\right|^{2} between the trained model and HMC"
  },
  {
    "objectID": "talks/lattice23/index.html#d-su3-results-delta-u_munu-1",
    "href": "talks/lattice23/index.html#d-su3-results-delta-u_munu-1",
    "title": "MLMC: Machine Learning Monte Carlo",
    "section": "4D SU(3) Results: \\delta U_{\\mu\\nu}",
    "text": "4D SU(3) Results: \\delta U_{\\mu\\nu}\n\n\n\n\n\n\nFigure 14: The difference in the average plaquette \\left|\\delta U_{\\mu\\nu}\\right|^{2} between the trained model and HMC"
  },
  {
    "objectID": "talks/lattice23/index.html#sec-thank-you",
    "href": "talks/lattice23/index.html#sec-thank-you",
    "title": "MLMC: Machine Learning Monte Carlo",
    "section": "Thank you!",
    "text": "Thank you!\n \n\n\n\n\n\n\n samforeman.me\n\n\n saforem2\n\n\n @saforem2\n\n\n foremans@anl.gov\n\n\n\n\n\n\n\n\n\n\nAcknowledgements\n\n\nThis research used resources of the Argonne Leadership Computing Facility,\nwhich is a DOE Office of Science User Facility supported under Contract DE-AC02-06CH11357."
  },
  {
    "objectID": "talks/lattice23/index.html#sec-acknowledgements",
    "href": "talks/lattice23/index.html#sec-acknowledgements",
    "title": "MLMC: Machine Learning Monte Carlo",
    "section": "Acknowledgements",
    "text": "Acknowledgements\n\n\n\nLinks:\n\n Link to github\n reach out!\n\nReferences:\n\nLink to slides\n\n link to github with slides\n\n (Foreman et al. 2022; Foreman, Jin, and Osborn 2022, 2021)\n (Boyda et al. 2022; Shanahan et al. 2022)\n\n\n\n\nHuge thank you to:\n\nYannick Meurice\nNorman Christ\nAkio Tomiya\nNobuyuki Matsumoto\nRichard Brower\nLuchang Jin\nChulwoo Jung\nPeter Boyle\nTaku Izubuchi\nDenis Boyda\nDan Hackett\nECP-CSD group\nALCF Staff + Datascience Group"
  },
  {
    "objectID": "talks/lattice23/index.html#sec-references",
    "href": "talks/lattice23/index.html#sec-references",
    "title": "MLMC: Machine Learning Monte Carlo",
    "section": "",
    "text": "Links\n\n saforem2/l2hmc-qcd\n📊 slides (Github:  saforem2/lattice23)\n\n\nReferences\n\nTitle Slide Background (worms) animation\n\nGithub:  saforem2/grid-worms-animation\n\nLink to HMC demo"
  },
  {
    "objectID": "talks/lattice23/index.html#references-1",
    "href": "talks/lattice23/index.html#references-1",
    "title": "MLMC: Machine Learning Monte Carlo",
    "section": "References",
    "text": "References\n(I don’t know why this is broken 🤷🏻‍♂️ )\n\n\nBoyda, Denis et al. 2022. “Applications of Machine Learning to Lattice Quantum Field Theory.” In Snowmass 2021. https://arxiv.org/abs/2202.05838.\n\n\nForeman, Sam, Taku Izubuchi, Luchang Jin, Xiao-Yong Jin, James C. Osborn, and Akio Tomiya. 2022. “HMC with Normalizing Flows.” PoS LATTICE2021: 073. https://doi.org/10.22323/1.396.0073.\n\n\nForeman, Sam, Xiao-Yong Jin, and James C. Osborn. 2021. “Deep Learning Hamiltonian Monte Carlo.” In 9th International Conference on Learning Representations. https://arxiv.org/abs/2105.03418.\n\n\n———. 2022. “LeapfrogLayers: A Trainable Framework for Effective Topological Sampling.” PoS LATTICE2021 (May): 508. https://doi.org/10.22323/1.396.0508.\n\n\nShanahan, Phiala et al. 2022. “Snowmass 2021 Computational Frontier CompF03 Topical Group Report: Machine Learning,” September. https://arxiv.org/abs/2209.07559."
  },
  {
    "objectID": "talks/lattice23/index.html#integrated-autocorrelation-time",
    "href": "talks/lattice23/index.html#integrated-autocorrelation-time",
    "title": "MLMC: Machine Learning Monte Carlo",
    "section": "Integrated Autocorrelation Time",
    "text": "Integrated Autocorrelation Time\n\n\n\n\n\n\nFigure 15: Plot of the integrated autocorrelation time for both the trained model (colored) and HMC (greyscale)."
  },
  {
    "objectID": "talks/lattice23/index.html#comparison",
    "href": "talks/lattice23/index.html#comparison",
    "title": "MLMC: Machine Learning Monte Carlo",
    "section": "Comparison",
    "text": "Comparison\n\n\n\n\n\n\n\n\n\n\n\n(a) Trained model\n\n\n\n\n\n\n\n\n\n\n\n(b) Generic HMC\n\n\n\n\n\n\n\nFigure 16: Comparison of \\langle \\delta Q\\rangle = \\frac{1}{N}\\sum_{i=k}^{N} \\delta Q_{i} for the trained model Figure 16 (a) vs. HMC Figure 16 (b)"
  },
  {
    "objectID": "talks/lattice23/index.html#plaquette-analysis-x_p",
    "href": "talks/lattice23/index.html#plaquette-analysis-x_p",
    "title": "MLMC: Machine Learning Monte Carlo",
    "section": "Plaquette analysis: x_{P}",
    "text": "Plaquette analysis: x_{P}\n\n\nDeviation from V\\rightarrow\\infty limit, x_{P}^{\\ast}\n\nAverage \\langle x_{P}\\rangle, with x_{P}^{\\ast} (dotted-lines)\n\n\n\n\n\n\n\nFigure 17: Plot showing how average plaquette, \\left\\langle x_{P}\\right\\rangle varies over a single trajectory for models trained at different \\beta, with varying trajectory lengths N_{\\mathrm{LF}}"
  },
  {
    "objectID": "talks/lattice23/index.html#loss-function",
    "href": "talks/lattice23/index.html#loss-function",
    "title": "MLMC: Machine Learning Monte Carlo",
    "section": "Loss Function",
    "text": "Loss Function\n\nWant to maximize the expected squared charge difference1: \\begin{equation*}\n\\mathcal{L}_{\\theta}\\left(\\xi^{\\ast}, \\xi\\right) =\n{\\mathbb{E}_{p(\\xi)}}\\big[-\\textcolor{#FA5252}{{\\delta Q}}^{2}\n\\left(\\xi^{\\ast}, \\xi \\right)\\cdot A(\\xi^{\\ast}|\\xi)\\big]\n\\end{equation*}\nWhere:\n\n\\delta Q is the tunneling rate: \\begin{equation*}\n\\textcolor{#FA5252}{\\delta Q}(\\xi^{\\ast},\\xi)=\\left|Q^{\\ast} - Q\\right|\n\\end{equation*}\nA(\\xi^{\\ast}|\\xi) is the probability2 of accepting the proposal \\xi^{\\ast}: \\begin{equation*}\nA(\\xi^{\\ast}|\\xi) = \\mathrm{min}\\left( 1,\n\\frac{p(\\xi^{\\ast})}{p(\\xi)}\\left|\\frac{\\partial \\xi^{\\ast}}{\\partial\n\\xi^{T}}\\right|\\right)\n\\end{equation*}\n\n\n\n\n\n\nWhere \\xi^{\\ast} is the proposed configuration (prior to Accept / Reject)↩︎\nAnd \\left|\\frac{\\partial \\xi^{\\ast}}{\\partial \\xi^{T}}\\right| is the Jacobian of the transformation from \\xi \\rightarrow \\xi^{\\ast}↩︎"
  },
  {
    "objectID": "talks/lattice23/index.html#networks-2d-u1",
    "href": "talks/lattice23/index.html#networks-2d-u1",
    "title": "MLMC: Machine Learning Monte Carlo",
    "section": "Networks 2D U(1)",
    "text": "Networks 2D U(1)\n\nStack gauge links as shape\\left(U_{\\mu}\\right)=[Nb, 2, Nt, Nx] \\in \\mathbb{C}\n x_{\\mu}(n) ≔ \\left[\\cos(x), \\sin(x)\\right]\nwith shape\\left(x_{\\mu}\\right)= [Nb, 2, Nt, Nx, 2] \\in \\mathbb{R}\nx-Network:\n\n\\psi_{\\theta}: (x, v) \\longrightarrow \\left(s_{x},\\, t_{x},\\, q_{x}\\right)\n\nv-Network:\n\n\\varphi_{\\theta}: (x, v) \\longrightarrow \\left(s_{v},\\, t_{v},\\, q_{v}\\right) \\hspace{2pt}\\longleftarrow lets look at this"
  },
  {
    "objectID": "talks/lattice23/index.html#v-updatereverse",
    "href": "talks/lattice23/index.html#v-updatereverse",
    "title": "MLMC: Machine Learning Monte Carlo",
    "section": "v-Update1",
    "text": "v-Update1\n\nforward (d = \\textcolor{#FF5252}{+}):\n\n\\Gamma^{\\textcolor{#FF5252}{+}}: (x, v) \\rightarrow v' := v \\cdot e^{\\frac{\\varepsilon}{2} s_{v}} - \\frac{\\varepsilon}{2}\\left[ F \\cdot e^{\\varepsilon q_{v}} + t_{v} \\right]\n\nbackward (d = \\textcolor{#1A8FFF}{-}):\n\n\\Gamma^{\\textcolor{#1A8FFF}{-}}: (x, v) \\rightarrow v' := e^{-\\frac{\\varepsilon}{2} s_{v}} \\left\\{v + \\frac{\\varepsilon}{2}\\left[ F \\cdot e^{\\varepsilon q_{v}} + t_{v} \\right]\\right\\}\n\n\n\n\nNote that \\left(\\Gamma^{+}\\right)^{-1} = \\Gamma^{-}, i.e. \\Gamma^{+}\\left[\\Gamma^{-}(x, v)\\right] = \\Gamma^{-}\\left[\\Gamma^{+}(x, v)\\right] = (x, v)↩︎"
  },
  {
    "objectID": "talks/lattice23/index.html#x-update",
    "href": "talks/lattice23/index.html#x-update",
    "title": "MLMC: Machine Learning Monte Carlo",
    "section": "x-Update",
    "text": "x-Update\n\nforward (d = \\textcolor{#FF5252}{+}):\n\n\\Lambda^{\\textcolor{#FF5252}{+}}(x, v) = x \\cdot e^{\\frac{\\varepsilon}{2} s_{x}} - \\frac{\\varepsilon}{2}\\left[ v \\cdot e^{\\varepsilon q_{x}} + t_{x} \\right]\n\nbackward (d = \\textcolor{#1A8FFF}{-}):\n\n\\Lambda^{\\textcolor{#1A8FFF}{-}}(x, v) = e^{-\\frac{\\varepsilon}{2} s_{x}} \\left\\{x + \\frac{\\varepsilon}{2}\\left[ v \\cdot e^{\\varepsilon q_{x}} + t_{x} \\right]\\right\\}"
  },
  {
    "objectID": "talks/lattice23/index.html#lattice-gauge-theory-2d-u1",
    "href": "talks/lattice23/index.html#lattice-gauge-theory-2d-u1",
    "title": "MLMC: Machine Learning Monte Carlo",
    "section": "Lattice Gauge Theory (2D U(1))",
    "text": "Lattice Gauge Theory (2D U(1))\n\n\n\n\n\n\n\n\n\n\nLink Variables\n\n\nU_{\\mu}(n) = e^{i x_{\\mu}(n)}\\in \\mathbb{C},\\quad \\text{where}\\quad x_{\\mu}(n) \\in [-\\pi,\\pi)\n\n\n\n\n\n\n\n\n\n\n\n\nWilson Action\n\n\nS_{\\beta}(x) = \\beta\\sum_{P} \\cos \\textcolor{#00CCFF}{x_{P}},\n\\textcolor{#00CCFF}{x_{P}} = \\left[x_{\\mu}(n) + x_{\\nu}(n+\\hat{\\mu})\n- x_{\\mu}(n+\\hat{\\nu})-x_{\\nu}(n)\\right]\n\n\n\n\nNote: \\textcolor{#00CCFF}{x_{P}} is the product of links around 1\\times 1 square, called a “plaquette”\n\n\n\n\n\n\n2D Lattice"
  },
  {
    "objectID": "talks/lattice23/index.html#section-1",
    "href": "talks/lattice23/index.html#section-1",
    "title": "MLMC: Machine Learning Monte Carlo",
    "section": "",
    "text": "Figure 18: Jupyter Notebook"
  },
  {
    "objectID": "talks/lattice23/index.html#annealing-schedule",
    "href": "talks/lattice23/index.html#annealing-schedule",
    "title": "MLMC: Machine Learning Monte Carlo",
    "section": "Annealing Schedule",
    "text": "Annealing Schedule\n\nIntroduce an annealing schedule during the training phase:\n\\left\\{ \\gamma_{t}  \\right\\}_{t=0}^{N} = \\left\\{\\gamma_{0}, \\gamma_{1},\n\\ldots, \\gamma_{N-1}, \\gamma_{N} \\right\\}\nwhere \\gamma_{0} &lt; \\gamma_{1} &lt; \\cdots &lt; \\gamma_{N} \\equiv 1, and \\left|\\gamma_{t+1} - \\gamma_{t}\\right| \\ll 1\nNote:\n\nfor \\left|\\gamma_{t}\\right| &lt; 1, this rescaling helps to reduce the height of the energy barriers \\Longrightarrow\neasier for our sampler to explore previously inaccessible regions of the phase space"
  },
  {
    "objectID": "talks/lattice23/index.html#networks-2d-u1-1",
    "href": "talks/lattice23/index.html#networks-2d-u1-1",
    "title": "MLMC: Machine Learning Monte Carlo",
    "section": "Networks 2D U(1)",
    "text": "Networks 2D U(1)\n\nStack gauge links as shape\\left(U_{\\mu}\\right)=[Nb, 2, Nt, Nx] \\in \\mathbb{C}\n x_{\\mu}(n) ≔ \\left[\\cos(x), \\sin(x)\\right]\nwith shape\\left(x_{\\mu}\\right)= [Nb, 2, Nt, Nx, 2] \\in \\mathbb{R}\nx-Network:\n\n\\psi_{\\theta}: (x, v) \\longrightarrow \\left(s_{x},\\, t_{x},\\, q_{x}\\right)\n\nv-Network:\n\n\\varphi_{\\theta}: (x, v) \\longrightarrow \\left(s_{v},\\, t_{v},\\, q_{v}\\right)"
  },
  {
    "objectID": "talks/lattice23/index.html#toy-example-gmm-in-mathbbr2",
    "href": "talks/lattice23/index.html#toy-example-gmm-in-mathbbr2",
    "title": "MLMC: Machine Learning Monte Carlo",
    "section": "Toy Example: GMM \\in \\mathbb{R}^{2}",
    "text": "Toy Example: GMM \\in \\mathbb{R}^{2}\n\n\nFigure 19"
  },
  {
    "objectID": "talks/lattice23/index.html#physical-quantities",
    "href": "talks/lattice23/index.html#physical-quantities",
    "title": "MLMC: Machine Learning Monte Carlo",
    "section": "Physical Quantities",
    "text": "Physical Quantities\n\nTo estimate physical quantities, we:\n\nCalculate physical observables at increasing spatial resolution\nPerform extrapolation to continuum limit\n\n\n\n\n\n\n\n\nFigure 20: Increasing the physical resolution (a \\rightarrow 0) allows us to make predictions about numerical values of physical quantities in the continuum limit."
  },
  {
    "objectID": "qmd/projects/index.html",
    "href": "qmd/projects/index.html",
    "title": "📦 Projects",
    "section": "",
    "text": "📊 GitHub Stats\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nEven More !!\n\n\n\nWakatime\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n📂 saforem2/\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{foreman2024,\n  author = {Foreman, Sam},\n  title = {📦 {Projects}},\n  date = {2024-08-10},\n  url = {https://samforeman.me},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nForeman, Sam. 2024. “📦 Projects.” August 10, 2024. https://samforeman.me.",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "📦 Projects",
      "📚 All Projects"
    ]
  },
  {
    "objectID": "posts/AuroraGPT/long-sequences/index.html",
    "href": "posts/AuroraGPT/long-sequences/index.html",
    "title": "🚂 Loooooooong Sequence Lengths",
    "section": "",
    "text": "The new Megatron-DeepSpeed release contains a variety of improvements / optimizations to enable pre-training Transformer based architectures with significantly longer sequences than was previously possible.",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "🪧 Posts",
      "🤖 AuroraGPT",
      "🚂 Loooooooong Sequence Lengths"
    ]
  },
  {
    "objectID": "posts/AuroraGPT/long-sequences/index.html#deepspeed4science-092023",
    "href": "posts/AuroraGPT/long-sequences/index.html#deepspeed4science-092023",
    "title": "🚂 Loooooooong Sequence Lengths",
    "section": "DeepSpeed4Science (09/2023)",
    "text": "DeepSpeed4Science (09/2023)\n\nNew Features\n\nEnabled Megatron-LM’s sequence parallel.\nEnabled rotary positional embedding.\nEnabled FlashAttention v1 and v2.\nEnabled new fused kernels from NVIDIA.\n\n\n\nNew optimizations\n\nEnabled attention map memory optimization, where we first generated attention mask on CPU memory and then moved it into GPU memory to avoid out-of-memory errors when training with very large sequence lengths.\nPosition embedding partitioning, where we split weights of position encoding across all GPUs when enabling sequence parallel to further reduce the memory footprint.\n\n\n\nInitial Results\n\n\n\nTable 1: Long sequence length support1 from microsoft/Megatron-DeepSpeed\n\n\n\n\n\n\n\n\n\n\nSequence Length\nOld Megatron-DeepSpeed (TFLOPS)\nNew Megatron-DeepSpeed (TFLOPS)\n\n\n\n\n2k\n25\n68\n\n\n4k\n28\n80\n\n\n8k\nOOM\n86\n\n\n16k\nOOM\n92\n\n\n32k\nOOM\n100\n\n\n64k\nOOM\n106\n\n\n128k\nOOM\n119\n\n\n256k\nOOM\n94\n\n\n\n\n\n\n\n\nData\ngpus = ('32', '64', '128')\n\ncolors = {\n    'Old Megatron-DS': '#FF5252',\n    'Megatron-LM': '#76b900',\n    'New Megatron-DS':  '#1A8FFF',\n}\n\ndata = {\n    '25B': {\n        'Old Megatron-DS': np.array([36, 42, 42]),\n        'Megatron-LM': np.array([26, 48, 52]),\n        'New Megatron-DS': np.array([192, 448, 512]),\n    },\n    '33B': {\n        'Old Megatron-DS': np.array([28, 32, 32]),\n        'Megatron-LM': np.array([14, 46, 52]),\n        'New Megatron-DS': np.array([128, 384, 448]),\n    },\n}\n\n\n\nMake the plots\nx = np.arange(len(gpus))\nwidth = 0.25\nmultiplier = 0\n\noutdir = Path(os.getcwd()).joinpath('assets')\noutdir.mkdir(exist_ok=True, parents=True)\n\nimprovement = {}\nfor idx, (model_size, d) in enumerate(data.items()):\n    multiplier = 0\n    figure, axes = plt.subplots(figsize=(7.5, 4))\n    fig = plt.gcf()\n    ax = plt.gca()\n    for label, value in d.items():\n        offset = width * multiplier\n        rects = ax.barh(\n          x + offset,\n          value,\n          width,\n          label=label,\n          color=colors[label],\n          alpha=0.8\n        )\n        ax.bar_label(\n          rects,\n          padding=3,\n          color=colors[label],\n          family='monospace',\n          weight='bold'\n        )\n        multiplier += 1\n    ax.set_ylabel(\n        'GPUs',\n        fontsize=18,\n        family='sans-serif',\n        loc='center',\n    )\n    ax.set_yticks(x + width, gpus)\n    plt.figtext(\n        0.005, 0.93, f\"{model_size}\", fontsize=24, fontweight='bold', ha='left'\n    )\n    ax.set_xlabel(\n        'Sequence Length (k)', fontsize=18, loc='center'\n    )\n    ax.legend(\n        bbox_to_anchor=(0.005, 1.04, 0.99, .098),\n        alignment='center',\n        edgecolor=\"#83838320\",\n        frameon=True,\n        ncols=3,\n        fontsize=13,\n        mode=\"expand\",\n        borderaxespad=0.01\n    )\n    save_figure(fname=f'{model_size}', outdir=outdir)\n    _ = plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nGPT-25B Model\n\n\n\n\n\n\n\nGPT-33B Model\n\n\n\n\n\n\n\nFigure 2: Pre-training with long sequence support across different model sizes and numbers of GPUs. In each case, the new (current) implementation significantly outperforms both NVIDIA/Megatron-LM as well as our previous implementation.",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "🪧 Posts",
      "🤖 AuroraGPT",
      "🚂 Loooooooong Sequence Lengths"
    ]
  },
  {
    "objectID": "posts/AuroraGPT/long-sequences/index.html#installation",
    "href": "posts/AuroraGPT/long-sequences/index.html#installation",
    "title": "🚂 Loooooooong Sequence Lengths",
    "section": "Installation",
    "text": "Installation\n\nUsing install.sh\n\n\n\n\n\n\nInstallation\n\n\n\n\n\nImportant To install, simply:\ngit clone https://github.com/ramanthanlab/GenSLM/\ncd GenSLM/examples/long-sequences/\n./install.sh\nExplicitly, ./install.sh will:\n\nAutomatically create a virtual environment on top of the latest conda module\nInstall (+ update2) / build all the required dependencies into this virtual environment\n\n\n\n\n\n\nStep-by-Step\nFor completeness, we describe below the steps for installing and building each of the dependencies.\n\nClone GitHub repo:\ngit clone https://github.com/ramanthanlab/GenSLM\nLoad conda module:\n\nThetaGPU:\n# ThetaGPU:\nif [[ \"$(hostname)==theta*\" ]]; then\n    export MACHINE=\"ThetaGPU\"\n    export CONDA_DATE=\"2023-01-10\"\n    module load conda/2023-01-11\n    conda activate base\nfi\nPolaris:\n# Polaris:\nif [[ \"$(hostname)==x3*\" ]]; then\n    export MACHINE=\"Polaris\"\n    export CONDA_DATE=\"2023-01-10\"\n    module load conda/2023-01-10-unstable\n    conda activate base\nfi\n\nSetup Virtual Environment3:\ncd ./genslm/examples/long-sequences\n# create a new virtual environment\nmkdir -p \"venvs/${MACHINE}/${CONDA_DATE}\"\npython3 -m venv \"venvs/${MACHINE}/${CONDA_DATE}\" --system-site-packages\nsource \"venvs/${MACHINE}/${CONDA_DATE}/bin/activate\"\nCreate a new folder (genslm/examples/long-sequences/deps/${MACHINE}) where we’ll installing dependencies locally:\nmkdir -p \"deps/${MACHINE}\"\ncd \"deps/${MACHINE}\"\n\n\nDependencies\nWe provide below the details needed to install each of the required dependencies.\n\n\n saforem2/ezpz\n\n\n saforem2/ezpz\npip install -e \"git+https://github.com/saforem2/ezpz.git#egg=ezpz\"\n\n\n\n\n Microsoft/DeepSpeed\n\n\n Microsoft/DeepSpeed\ngit clone https://github.com/microsoft/DeepSpeed.git\ncd DeepSpeed\npython3 -m pip install -e .\n\n\n\n\n Microsoft/Megatron-DeepSpeed\n\n\n Microsoft/Megatron-DeepSpeed:\ngit clone https://github.com/microsoft/Megatron-DeepSpeed.git\n\n\n\n\n NVIDIA/apex\n\n\n NVIDIA/apex\ngit clone https://github.com/NVIDIA/apex\ncd ../apex/\npip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" -e ./\n\n\n\n\n pybind/PyBind11\n\n\n pybind/PyBind11\npip install pybind11\n\n\n\n\n Dao-AILab/flash-attention\n\n\n Dao-AILab/flash-attention:\n\n\n\n\n\n\nFlash Attention\n\n\n\n\n\n\nThe new release supports three different implementations of FlashAttention: (v1.0.4, v2.x, triton)\nFlashAttention v2.x may have numerical instability issues. For the best performance, we recommend using FlashAttention + Triton\n\n\n\n\n\nv1.0.4:\npython3 -m pip install flash-attn==1.0.4\nv2.x:\ngit clone https://github.com/Dao-AILab/flash-attention\ncd flash-attention\npython3 setup.py install\nopenai/triton:\ngit clone -b legacy-backend https://github.com/openai/triton\ncd triton/python\npython3 -m pip install cmake\npython3 -m pip install .",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "🪧 Posts",
      "🤖 AuroraGPT",
      "🚂 Loooooooong Sequence Lengths"
    ]
  },
  {
    "objectID": "posts/AuroraGPT/long-sequences/index.html#running",
    "href": "posts/AuroraGPT/long-sequences/index.html#running",
    "title": "🚂 Loooooooong Sequence Lengths",
    "section": "Running",
    "text": "Running\nThe ALCF/ directory contains shell scripts for setting up the environment and specifying the options to be used when launching.\nVarious options can be specified dynamically at runtime by setting them in your environment, e.g.:\nMODEL_SIZE_KEY=\"GPT25B\" SEQ_LEN=128000 USE_FLASH_ATTN=1 MICRO_BATCH=1 GAS=1 SP_TYPE=\"megatron\" ZERO_STAGE=1 ./ALCF/train-gpt3.sh\nExplicitly:\n\nALCF/train-gpt3.sh: Main entry point for training\n\nThis script will automatically source the rest of the required ALCF/*.sh scripts below\n\nALCF/models.sh: Contains some example model architectures for GPT3-style models\nALCF/args.sh: Logic for parsing / setting up runtime options for Megatron and DeepSpeed\nALCF/setup.sh: Locate and activate virtual environment to be used, ensure MPI variables are set properly\nALCF/launch.sh: Identify available resources and build the command to be executed\n\ni.e. figure out how many: {nodes, GPUs per node, GPUs total}, to pass to mpi{run,exec}\nthen, use this to build mpiexec &lt;mpiexec-args&gt; python3 pretrain_gpt.py",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "🪧 Posts",
      "🤖 AuroraGPT",
      "🚂 Loooooooong Sequence Lengths"
    ]
  },
  {
    "objectID": "posts/AuroraGPT/long-sequences/index.html#zero-offloading",
    "href": "posts/AuroraGPT/long-sequences/index.html#zero-offloading",
    "title": "🚂 Loooooooong Sequence Lengths",
    "section": "ZeRO Offloading",
    "text": "ZeRO Offloading\n🚀 W&B Report: Looooooooong Sequences\nThese newly introduced optimizations, in combination with ZeRO-Offload allows us to go even further.\nBy employing ZeRO-Offloading, we are able to free up additional memory which can be used for even longer sequences.\nThough work is still ongoing, this is a promising direction that will allow us to consider significantly larger genomes than previously possible.\nWe use Weights & Biases to track these experiments, and have aggregated our initial results in the W&B Report below.\nWe can evaluate the performance of our model by looking at two different metrics for throughput: samples_per_sec and TFLOPS.\nExplicitly, we see that we are able to scale up to significantly longer sequences (420k / 128k ~ 3.3x) with only a minimal impact on throughput performance (81 / 105 ~ 77\\%)4.\n\n\n\nTable 2: Impact on TFLOPS as a function of increasing sequence length. Table from: throughput/TFLOPS\n\n\n\n\n\n\n\n\n\n\n\n\nName\nSequence Length (k)\n(seq_len / min_seq_len)\nTFLOPS\nTFLOPS (% of peak)\n\n\n\n\nGPT25B\n420\n3.28125\n81.77225\n77.867\n\n\nGPT25B\n400\n3.125\n90.62\n86.297\n\n\nGPT25B\n360\n2.8125\n81.6325\n77.7348\n\n\nGPT25B\n360\n2.8125\n82.6824\n78.7346\n\n\nGPT25B\n192\n1.5\n115.8228\n110.2927\n\n\nGPT25B\n128\n1\n106.672\n101.5788\n\n\nGPT25B\n128\n1\n105.014\n100.00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Weights & Biases Report",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "🪧 Posts",
      "🤖 AuroraGPT",
      "🚂 Loooooooong Sequence Lengths"
    ]
  },
  {
    "objectID": "posts/AuroraGPT/long-sequences/index.html#footnotes",
    "href": "posts/AuroraGPT/long-sequences/index.html#footnotes",
    "title": "🚂 Loooooooong Sequence Lengths",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe described experiments were performed on 4 NVIDIA DGX A100-40GB nodes, all using TPSIZE=32[^tpsize], connected through 8 HDR InfiniBand (200Gb/s per HDR).↩︎\n\n\ndeepspeed-0.10.3\npytorch==2.0.0+cu118\n\n↩︎\nWhere \"${MACHINE}\" \\in {\"ThetaGPU\", \"Polaris\"} and \"${CONDA_DATE}\" \\in {\"2023-01-10\", \"2023-01-11\"}↩︎\nthroughput/TFLOPS↩︎",
    "crumbs": [
      "[{{< iconify line-md github-loop >}}]{style='font-size: 1.15em;'}",
      "🪧 Posts",
      "🤖 AuroraGPT",
      "🚂 Loooooooong Sequence Lengths"
    ]
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "",
    "section": "🧑🏻‍💻 About Me",
    "text": "🧑🏻‍💻 About Me\n\n\n\n💻 Computational scientist at Argonne National Laboratory (ALCF)\n🧪 Interested in {AI, HPC} for science1\n\n🚀 working on scaling large (language, vision, multi-modal) models2 across thousands of GPUs\n\n\n\n\n\n\n\n\n\n\n🎤 Recent Talks\n\n\n\n\n\nlive: here ( + how I make them! )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n👀 If you’re curious\n\n\n\n\n🔥 What I work on\n\nAs a member of the AI / ML Group at ALCF, I work on3:\n\n\n\n🤖 🧪 AI + Science\n🎲 Building better sampling methods for Lattice QCD\n🧬 Genome-Scale Language Models\n\n GenSLM\n🥇 ACM Gordon Bell Special Prize\n\n\n\n\n\n🌍 Foundation models for long term climate forecasting\n🏃‍♂️ Scaling Large Language Models\n🏎️ Distributed training across thousands of GPUs\n\n\n\n\n\n\n📍 How I got here\n\nMy current research focuses on using deep generative modeling to help build better sampling algorithms in lattice gauge theory. In particular, I’m interested in building gauge equivariant neural network architectures and using inductive priors to incorporate physical symmetries into machine learning models.\nI received my PhD in Physics from the University of Iowa in 2019 and my thesis was on Learning Better Physics: A Machine Learning Approach to Lattice Gauge Theory.\nPrior to this, I completed two bachelors degrees (Engineering Physics and Applied Mathematics, 2015) at The University of Illinois at Urbana-Champaign. My undergraduate dissertation was titled Energy Storage in Quantum Resonators and was supervised by Professor Alfred Hübler within the Center for Complex Systems Research at UIUC4.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n📝 Work🦜 Talks📬 Posts📦 Projects🪖 Experience🎶 Music\n\n\n[NOTE]: You can find a full list of my publications on my Google Scholar.\n\nIntro to HPC Bootcamp: Engaging New Communities Through Energy Justice Projects\nJournal of Computational Science, 2024\nThorough Characterization and Analysis of Large Transformer Model Training At-Scale\nProc. ACM Meas. Anal. Comput. Syst. 03/2024\nMLMC: Machine Learning Monte Carlo for Lattice Gauge Theory\nS. Foreman et al. Lattice, 2023 (Proceedings), 12/2023\nProtein Generation via Genome-scale Language Models with Bio-physical Scoring\n@ SC’23, 11/2023\n DeepSpeed4Science Initiative: Enabling Large-Scale Scientific Discovery […]\n@ NeurIPS 2023 AI For Science Workshop, 10/2023\n\n DeepSpeed4Science.ai Blog Post\n Loooooooong Sequence Lengths\n\nComprehensive Performance Study of LLMs on Novel AI Accelerators\nM. Emani, S. Foreman, et al., IPDPS 2024, 10/2023\nExploratory Analysis of Climate Data with ClimRR\nS. Foreman, Intro to HPC Bootcamp @ NERSC, 08/2023\n🏆 GenSLMs: Genome-scale language models reveal SARS-Cov-2 evolutionary dynamics\n@ SC’22 10/2022\n\n🥇 ACM Gordon Bell Special Prize\n\nLattice QCD and Particle Physics\nA.S. Kronfeld et al., 07/2022\nApplications of ML to Lattice QFT\nD. Boyda, S. Calí, S. Foreman, et al., [arXiv:2202.05838], 02/2022\nLeapFrogLayers: Trainable Framework for Effective Sampling\nS. Foreman, X.Y. Jin, J.C. Osborn, Lattice, 2021\nHMC with Normalizing Flows [slides]\nS. Foreman et al., Lattice, 2021\nDeep Learning Hamiltonian Monte Carlo [+ poster]\nS. Foreman, X.Y. Jin, & J.C. Osborn, @ SimDL Workshop @ ICLR, 2021\nMachine Learning and Neural Networks for Field Theory\nS. Foreman, X.Y. Jin, & J.C. Osborn, SnowMass, 2020\nExamples of renormalization group transformations for image sets\nS. Foreman et al., Physical Review E., 2018\nRG inspired Machine Learning for lattice field theory\nS. Foreman et al., arXiv:1710.02079, 2017\nLarge Energy Density in Three-Plate Nanocapacitors due to Coulomb Blockade\nS. Foreman et al., J. Appl. Phys, 2018\n\n\n\n\n📆 2024\n\n\n\n\n\n\nTraining LLMs at Scale @ ATPESC, 2024 [08/2024]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLLMs on Polaris @ Center for Scientific Foundation Models, Summer School 24’ [07/2024]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParallel Training Techniques @ AI-4-Science Training Series [03/2024]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLLMs from Scratch @ LLM Tutorial Workshop [02/2024]\n\n\n\n\n\n\n\n\n\n\n\n📆 2023\n\n\n\n\n\n\nCreating Small(-ish) LLMs @ LLM Tutorial Workshop (1) [11/2023]\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\nExascale Science on Aurora @ Intel oneAPI Workshop @ UIC [10/2023]\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\nLLM Lunch Talk @ ALCF Hands On HPC Workshop [10/2023]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScaling LLMs for Science @ Data-Intensive Computing + AI/ML at Scale [08/2023]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMLMC: Machine Learning Monte Carlo @ Lattice 2023 [07/2023]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative Modeling and Efficient Sampling @ PASC23 [07/2023]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEfficient Sampling for LGT @ Deep Fridays @ U. Bologna [04/2023]\n\n\n\n\n\n\n\n\n\n\n\n📆 2022\n\n\n\n\n\n\nLarge Scale Training @ AI4Science on Supercomputers (ALCF) [11/2022]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHyperparameter Management @ ALCF SDL Workshop [10/2022]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical Learning @ ATPESC 2022 [08/2022]\n\n\n\n\n\n\n📕 accompanying notebook\n\n\n\n\n\n\n\n\n\n\n\nScientific Data Science: An Emerging Symbiosis @ ANL (05/2022)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMachine Learning in HEP @ UNC Greensboro [03/2022]\n\n\n\n\n\n\nMachine Learning in HEP, at UNC Greensboro, March 2022\n\n\n\n\n\n\n\n📆 2021\n\n\n\n\n\n\nAccelerated Sampling Methods for LGT, @ DWQ @ 25 [BNL] [12/2021]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTraining Topological Samplers for LGT @ ML4HEP, ECT* Trento [09/2021]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nl2hmc-qcd @ MIT Lattice Group Seminar [2021]\n\n\n\n\n\nl2hmc-qcd at the MIT Lattice Group Seminar, 2021\n\n\n\n\n\n\n\n\n\nDeep Learning HMC for Improved Gauge Generation @ ML in LQCD Workshop [2021]\n\n\n\n\n\n \n\n\n\n\n\n📆 2020\n\n\n\n\n\n\nMachine Learning for Lattice QCD @ U. Iowa [2020]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nAug 13, 2024\n\n\n💅 How to Make Dope Slides\n\n\nSam Foreman \n\n\n\n\nAug 12, 2024\n\n\n🍋 ezpz @ ALCF\n\n\nSam Foreman \n\n\n\n\nJun 17, 2024\n\n\n🎰 Deterministic flash-attn\n\n\nSam Foreman \n\n\n\n\nJun 17, 2024\n\n\n📸 flash-attn on Sunspot\n\n\nSam Foreman \n\n\n\n\nJun 15, 2024\n\n\n🏎️ Megatron-DeepSpeed on Intel XPU\n\n\nSam Foreman \n\n\n\n\nMay 25, 2024\n\n\n🐛 mpi4py bug on Sunspot\n\n\nSam Foreman \n\n\n\n\nApr 15, 2024\n\n\n🎲 MCMC + Diffusion Sampling\n\n\nSam Foreman \n\n\n\n\nMar 21, 2024\n\n\n⏰ Starting Up Distributed Training\n\n\nSam Foreman \n\n\n\n\nFeb 12, 2024\n\n\n🚂 Loooooooong Sequence Lengths\n\n\nSam Foreman \n\n\n\n\nFeb 12, 2024\n\n\nl2hmc Example: 2D U(1)\n\n\nSam Foreman \n\n\n\n\nDec 14, 2023\n\n\n🎢 l2hmc-qcd Example: 2D U(1)\n\n\nSam Foreman \n\n\n\n\nDec 6, 2023\n\n\n🔳 l2hmc-qcd Example: 4D SU(3)\n\n\nSam Foreman \n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\n\n\n\n\n\n📊 GitHub Stats\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nEven More !!\n\n\n\nWakatime\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n📂 saforem2/\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n🎪 Events\n\nOrganizer for:\n\nSC24 Workshop: High Performance Python for Science at Scale (HPPSS), November 2024\nSC23 Workshop: High Performance Python for Science at Scale (HPPSS), November 2023\nMachine Learning and Quantum Computing for Earth Sciences at 17th U. S. National Congress on Computational Mechanics, July 2023\n\n\n\n\n👔 Employment\n\n\n\nTable 1: 📟 Experience\n\n\n\n\n\nPosition\n@\nStart\nEnd\n\n\n\n\nAssistant Computational Scientist\nALCF\n2022\n–\n\n\nPostdoc\nALCF\n2019\n2022\n\n\nGraduate Researcher\nANL\n2018\n2019\n\n\n\n\n\n\n\n\n🍎 School\n\n\n\nTable 2: 🎓 Education\n\n\n\n\n\nDegree\nIn\n@\nEnd\n\n\n\n\nPhD\nPhysics\nUniversity of Iowa\n2019\n\n\nB.Sc\nPhysics\nUIUC\n2015\n\n\nB.Sc\nMath\nUIUC\n2015\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n🧾 Updated @\n\nimport datetime\nfrom rich import print\nnow = datetime.datetime.now()\nday = now.strftime(\"%Y-%m-%d\")\ntime = now.strftime(\"%H:%M:%S\")\nprint(' '.join([\n    \"[#838383]Last Updated[/]:\",\n    f\"[#E599F7]{day}[/]\",\n    \"[#838383]@[/]\",\n    f\"[#00CCFF]{time}[/]\"\n]))\nLast Updated: 2024-08-13 @ 13:27:06"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSo far, for: {Lattice QCD, Quantum Mechanics, Biology (Protein Generation, Drug Discovery), and Climate Modeling / Weather Forecasting}↩︎\nMostly trying to get supercomputers to stop yelling at each other 🫠↩︎\nIf this sounds like something you’d be interested in doing, please feel free to reach out to me!↩︎\nAnd resulted in a patent !!↩︎"
  },
  {
    "objectID": "talks/test/index.html#ai-compute-historical",
    "href": "talks/test/index.html#ai-compute-historical",
    "title": "Test Rendering on Mobile",
    "section": "AI 🤝 Compute [Historical]",
    "text": "AI 🤝 Compute [Historical]\n\n\n\n\n\n\n\nFirst Era:\n\n[1960 – 2012]\n2 year doubling (Moore’s law)\n\n\\simeq 7\\times increase\n\n\n\n \n\nModern Era:\n\n[2012 – present]\n3.4 month doubling\n\n\\simeq \\mathbf{300,000}\\times increase\n\n\n\n\n\n\n\n\nSource."
  },
  {
    "objectID": "talks/test/index.html#ai-compute-historical-1",
    "href": "talks/test/index.html#ai-compute-historical-1",
    "title": "Test Rendering on Mobile",
    "section": "AI 🤝 Compute [Historical]",
    "text": "AI 🤝 Compute [Historical]\n\n\n\n\n\n\n\nFirst Era:\n\n[1960 – 2012]\n2 year doubling (Moore’s law)\n\n\\simeq 7\\times increase\n\n\n\n \n\nModern Era:\n\n[2012 – present]\n3.4 month doubling\n\n\\simeq \\mathbf{300,000}\\times increase\n\n\n\n\n\n\n\n\nSource."
  },
  {
    "objectID": "talks/test/index.html#data-parallel-training",
    "href": "talks/test/index.html#data-parallel-training",
    "title": "Test Rendering on Mobile",
    "section": "Data Parallel Training",
    "text": "Data Parallel Training\n\n\n\nRelatively simple to get up and running (minor modifications to code)\n saforem2/ezpz\nPyTorch – DDP\n DeepSpeed\n\n\n\n\n\n\n\n\nFigure 6: Data Parallelism\n\n\n\n\n\n\nAlso see: 🎬 “Parallel Training Techniques”_"
  },
  {
    "objectID": "talks/test/index.html#deal-with-data",
    "href": "talks/test/index.html#deal-with-data",
    "title": "Test Rendering on Mobile",
    "section": "Deal with Data",
    "text": "Deal with Data\n\nAt each training step, we want to ensure that each worker receives unique data\nThis can be done in one of two ways:\n\nManually partition data (ahead of time) and assign different sections to different workers\n\nEach worker can only see their local portion of the data\n\nFrom each worker, randomly select a mini-batch\n\nEach worker can see the full dataset\n\n\n\n\n\n\n⚠️ Warning\n\n\nDon’t forget your seed!\nWhen randomly selecting, it is important that each worker uses different seeds to ensure they receive unique data"
  },
  {
    "objectID": "talks/test/index.html#broadcast-initial-state",
    "href": "talks/test/index.html#broadcast-initial-state",
    "title": "Test Rendering on Mobile",
    "section": "Broadcast Initial State",
    "text": "Broadcast Initial State\n\nAt the start of training (or when loading from a checkpoint), we want all of our workers to be initialized consistently\n\nBroadcast the model and optimizer states from rank() == 0 worker\n\n\n\n\n\n\n\n  flowchart TD\n    0[\"GPU0\"] --&gt; 1[\"GPU 1\"]\n    0 --&gt; 2[\"GPU 2\"]\n    0 --&gt;|Model + Optimizer State| 3[\"GPU 3\"]\n    0 --&gt; ...\n    0 --&gt; N[\"GPU N\"]"
  },
  {
    "objectID": "talks/test/index.html#best-practices",
    "href": "talks/test/index.html#best-practices",
    "title": "Test Rendering on Mobile",
    "section": "Best Practices",
    "text": "Best Practices\n\n\n\n\n🤝 Keeping things in Sync\n\n\nComputation stalls during communication !!\nKeeping the communication to computation ratio small is important for effective scaling.\n\n\n\n\n\nUse parallel IO whenever possible\n\nFeed each rank from different files\nUse MPI IO to have each rank read its own batch from a file\nUse several ranks to read data, MPI to scatter to remaining ranks\n\nMost practical in big at-scale training\n\n\nTake advantage of data storage\n\nUse striping on lustre\nUse the right optimizations for Aurora, Polaris, etc.\n\nPreload data when possible\n\nOffloading to a GPU frees CPU cycles for loading the next batch of data\n\nminimize IO latency this way"
  },
  {
    "objectID": "talks/test/index.html#why-distributed-training",
    "href": "talks/test/index.html#why-distributed-training",
    "title": "Test Rendering on Mobile",
    "section": "Why Distributed Training?",
    "text": "Why Distributed Training?\n\nSplitting data across workers \\longrightarrow larger batch size1\n\n[micro_batch_size = 1] \\times [N GPUs] \\rightarrow [global_batch_size = N]\n\nSmooth loss landscape\nImproved gradient estimators\nLess iterations needed for same number of epochs\n\nMay need to train for more epochs if another change is not made\ne.g. scaling learning rate\n\nSee Large Batch Training of Convolutional Networks\n\nmicro_batch_size = batch_size per GPU"
  },
  {
    "objectID": "talks/test/index.html#recent-progress",
    "href": "talks/test/index.html#recent-progress",
    "title": "Test Rendering on Mobile",
    "section": "Recent Progress",
    "text": "Recent Progress\n\n\n\nTable 1: Batch-Size-Scaling\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear\nAuthor\nBatch Size\nProcessor\n# Processors\nTime\nAccuracy\n\n\n\n\n2016\nHe\n256\nP100\n8\n29 Hour\n75.30%\n\n\n2019\nYamazaki\n81,920\nV100\n2048\n1.2 Min\n75.08%"
  },
  {
    "objectID": "talks/test/index.html#model-parallel-training",
    "href": "talks/test/index.html#model-parallel-training",
    "title": "Test Rendering on Mobile",
    "section": "Model Parallel Training",
    "text": "Model Parallel Training\n\n\n\n\n\n\n\nSplit up network over multiple workers\n\nEach receives disjoint subset\nAll communication associated with subsets are distributed\n\nCommunication whenever dataflow between two subsets\nTypically more complicated to implement than data parallel training\nSuitable when the model is too large to fit onto a single device (CPU / GPU)\n argonne-lcf/Megatron-DeepSpeed\n🤗 huggingface/nanotron\n\n\n\n\n\n\n\n\n\nFigure 7"
  },
  {
    "objectID": "talks/test/index.html#tensor-model-parallelismefficient-large-scale",
    "href": "talks/test/index.html#tensor-model-parallelismefficient-large-scale",
    "title": "Test Rendering on Mobile",
    "section": "Tensor (Model) Parallelism1",
    "text": "Tensor (Model) Parallelism1\n\nIn Tensor Paralleism each GPU processes only a slice of a tensor and only aggregates the full tensor for operations that require the whole thing.\n\nThe main building block of any transformer is a fully connected nn.Linear followed by a nonlinear activation GeLU.\n\nY = GeLU(XA), where X and Y are the input and output vectors, and A is the weight matrix.\n\nIf we look at the computation in matrix form, it’s easy to see how the matrix multiplication can be split between multiple GPUs:\n\n\nEfficient Large-Scale Language Model Training on GPU Clusters"
  },
  {
    "objectID": "talks/test/index.html#tensor-parallelism-.center-0.9em-background-colorfff",
    "href": "talks/test/index.html#tensor-parallelism-.center-0.9em-background-colorfff",
    "title": "Test Rendering on Mobile",
    "section": "Tensor Parallelism {.center 0.9em;” background-color=“#FFF”}",
    "text": "Tensor Parallelism {.center 0.9em;” background-color=“#FFF”}\n\n\n\n\n\n\nFigure 8: Tensor Parallel GEMM. This information is based on (the much more in-depth) TP Overview by @anton-l"
  },
  {
    "objectID": "talks/test/index.html#d-parallelism",
    "href": "talks/test/index.html#d-parallelism",
    "title": "Test Rendering on Mobile",
    "section": "3D Parallelism",
    "text": "3D Parallelism\n\nDP + TP + PP (3D) Parallelism\n\n\n\n\n\n\n\nFigure 9: Figure taken from 3D parallelism: Scaling to trillion-parameter models"
  },
  {
    "objectID": "talks/test/index.html#collective-operations",
    "href": "talks/test/index.html#collective-operations",
    "title": "Test Rendering on Mobile",
    "section": "Collective Operations",
    "text": "Collective Operations\n\n\n\n\n⌛ Timeouts\n\n\n\nCollective operations have to be called for each rank to form a complete collective operation.\n\nFailure to do so will result in other ranks waiting indefinitely"
  },
  {
    "objectID": "talks/test/index.html#emergent-abilities",
    "href": "talks/test/index.html#emergent-abilities",
    "title": "Test Rendering on Mobile",
    "section": "Emergent Abilities",
    "text": "Emergent Abilities\n\n\nEmergent abilities of Large Language Models Yao et al. (2023)"
  },
  {
    "objectID": "talks/test/index.html#training-llms",
    "href": "talks/test/index.html#training-llms",
    "title": "Test Rendering on Mobile",
    "section": "Training LLMs",
    "text": "Training LLMs\n\nModern parallelism techniques1 enable the training of large language models\n\n\n\n\n\n\n\n\nFigure 16: It’s hungry! Wei et al. (2022)\n\n\n\n\n\n\n\n\n\n\nFigure 17: Visualization from Yang et al. (2023)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSee my slides on Parallel Training Techniques for additional details"
  },
  {
    "objectID": "talks/test/index.html#life-cycle-of-the-llm",
    "href": "talks/test/index.html#life-cycle-of-the-llm",
    "title": "Test Rendering on Mobile",
    "section": "Life-Cycle of the LLM",
    "text": "Life-Cycle of the LLM\n\n\n\n\n\n\n\nData collection + preprocessing\nPre-training\n\nArchitecture decisions:\n{model_size, hyperparameters,\nparallelism, lr_schedule, ...}\n\nSupervised Fine-Tuning\n\nInstruction Tuning\nAlignment\n\nDeploy (+ monitor, re-evaluate, etc.)\n\n\n\n\n\n\n\n\n\nFigure 18: Pre-training: Virtually all of the compute used during pretraining phase1.\n\n\n\n\n\n\nFigure from The Illustrated Transformer"
  },
  {
    "objectID": "talks/test/index.html#forward-pass",
    "href": "talks/test/index.html#forward-pass",
    "title": "Test Rendering on Mobile",
    "section": "Forward Pass",
    "text": "Forward Pass\n\n\n\n\n\n\n\nFigure 19: Language Model trained for causal language modeling. Video from: 🤗 Generation with LLMs"
  },
  {
    "objectID": "talks/test/index.html#generating-text",
    "href": "talks/test/index.html#generating-text",
    "title": "Test Rendering on Mobile",
    "section": "Generating Text",
    "text": "Generating Text\n\n\n\n\n\n\n\nFigure 20: Language Model trained for causal language modeling. Video from: 🤗 Generation with LLMs"
  },
  {
    "objectID": "talks/test/index.html#life-cycle-of-the-llm-pre-training",
    "href": "talks/test/index.html#life-cycle-of-the-llm-pre-training",
    "title": "Test Rendering on Mobile",
    "section": "Life-Cycle of the LLM: Pre-training",
    "text": "Life-Cycle of the LLM: Pre-training\n\n\n\n\n\n\nFigure 21: Pre-training: Virtually all of the compute used during pretraining phase"
  },
  {
    "objectID": "talks/test/index.html#life-cycle-of-the-llm-fine-tuning",
    "href": "talks/test/index.html#life-cycle-of-the-llm-fine-tuning",
    "title": "Test Rendering on Mobile",
    "section": "Life-Cycle of the LLM: Fine-Tuning",
    "text": "Life-Cycle of the LLM: Fine-Tuning\n\n\n\n\n\n\nFigure 22: Fine-tuning1: Fine-tuning actually updates the model’s weights to make the model better at a certain task.\n\n\n\nFigure from The Illustrated Transformer"
  },
  {
    "objectID": "talks/test/index.html#assistant-models",
    "href": "talks/test/index.html#assistant-models",
    "title": "Test Rendering on Mobile",
    "section": "Assistant Models",
    "text": "Assistant Models\n\n\n\n\n\n\nFigure 23"
  },
  {
    "objectID": "talks/test/index.html#clone-repos",
    "href": "talks/test/index.html#clone-repos",
    "title": "Test Rendering on Mobile",
    "section": "Clone Repo(s)",
    "text": "Clone Repo(s)\n\n#[⭐][07:33:08 AM][foremans@x3101c0s13b0n0][~/tmp]\n$ mkdir ~/tmp/polaris-talk\n\n#[⭐][07:33:21 AM][foremans@x3101c0s13b0n0][~/tmp]\n$ cd ~/tmp/polaris-talk\n\n#[⭐][07:33:25 AM][foremans@x3101c0s13b0n0][~/tmp/polaris-talk]\n$ NOW=$(tstamp) && mkdir \"${NOW}\" && cd \"${NOW}\" # && mkdir \"core-dumps-${NOW}\" && mv -v **core\\.** \"core-dumps-${NOW}\" && mv \"core-dumps-${NOW}\" core-dumps\n\n#[⭐][07:33:27 AM][foremans@x3101c0s13b0n0][~/tmp/polaris-talk/2024-07-17-073327]\n$ pwd\n/home/foremans/tmp/polaris-talk/2024-07-17-073327\n\n#[⭐][07:33:31 AM][foremans@x3101c0s13b0n0][~/tmp/polaris-talk/2024-07-17-073327]\n$ git clone https://github.com/saforem2/ezpz ezpz && git clone https://github.com/saforem2/wordplay wordplay\nCloning into 'ezpz'...\nremote: Enumerating objects: 2134, done.`\nremote: Counting objects: 100% (363/363), done.\nremote: Compressing objects: 100% (169/169), done.\nremote: Total 2134 (delta 197), reused 265 (delta 141), pack-reused 1771\nReceiving objects: 100% (2134/2134), 4.27 MiB | 25.01 MiB/s, done.\nResolving deltas: 100% (1117/1117), done.\nCloning into 'wordplay'...\nremote: Enumerating objects: 869, done.\nremote: Counting objects: 100% (72/72), done.\nremote: Compressing objects: 100% (37/37), done.\nremote: Total 869 (delta 29), reused 56 (delta 23), pack-reused 797\nReceiving objects: 100% (869/869), 14.36 MiB | 46.54 MiB/s, done.\nResolving deltas: 100% (395/395), done."
  },
  {
    "objectID": "talks/test/index.html#setup-python-.scrollable",
    "href": "talks/test/index.html#setup-python-.scrollable",
    "title": "Test Rendering on Mobile",
    "section": "Setup Python {.scrollable}",
    "text": "Setup Python {.scrollable}\n\n#[⭐][07:33:53 AM][foremans@x3101c0s13b0n0][~/tmp/polaris-talk/2024-07-17-073327]\n$ source ezpz/src/ezpz/bin/utils.sh && ezpz_setup_python && ezpz_setup_alcf\nUnable to detect PBS or SLURM working directory info...\nUsing /home/foremans/tmp/polaris-talk/2024-07-17-073327 as working directory...\nUsing WORKING_DIR: /home/foremans/tmp/polaris-talk/2024-07-17-073327\nNo conda_prefix OR virtual_env found in environment...\nSetting up conda...\nLmod is automatically replacing \"nvhpc/23.9\" with \"gcc-native/12.3\".\nLmod is automatically replacing \"PrgEnv-nvhpc/8.5.0\" with \"PrgEnv-gnu/8.5.0\".\nDue to MODULEPATH changes, the following have been reloaded:\n  1) cray-mpich/8.1.28\nFound conda at: /soft/applications/conda/2024-04-29/mconda3\nNo VIRTUAL_ENV found in environment!\n    - Trying to setup from /soft/applications/conda/2024-04-29/mconda3\n    - Using VENV_DIR=/home/foremans/tmp/polaris-talk/2024-07-17-073327/venvs/2024-04-29\n    - Creating a new virtual env on top of 2024-04-29 in /home/foremans/tmp/polaris-talk/2024-07-17-073327/venvs/2024-04-29\n[python] Using /home/foremans/tmp/polaris-talk/2024-07-17-073327/venvs/2024-04-29/bin/python3\n\n[ezpz/bin/utils.sh]\n\n[2024-07-17-073407]\n    • USER=foremans\n    • MACHINE=polaris\n    • HOST=x3101c0s13b0n0\n\n[ezpz_setup_host]\n    • Using hostfile: /var/spool/pbs/aux/2024084.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov\n    • Found in environment:\n        • HOSTFILE: /var/spool/pbs/aux/2024084.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov\n        • Writing PBS vars to: /home/foremans/.pbsenv\n\n[ezpz_save_pbs_env]\n    • Setting:\n        • HOSTFILE: /var/spool/pbs/aux/2024084.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov\n        • JOBENV_FILE: /home/foremans/.pbsenv\n\n[HOSTS]\n    • [host:0] - x3101c0s13b0n0.hsn.cm.polaris.alcf.anl.gov\n\n[DIST INFO]\n    • HOSTFILE=/var/spool/pbs/aux/2024084.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov\n    • NHOSTS=1\n    • NGPU_PER_HOST=4\n    • NGPUS=4\n    • DIST_LAUNCH=mpiexec --verbose --envall -n 4 -ppn 4 --hostfile /var/spool/pbs/aux/2024084.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov --cpu-bind depth -d 16\n\n[LAUNCH]:\n    • To launch across all available GPUs, use: launch\n      launch = mpiexec --verbose --envall -n 4 -ppn 4 --hostfile /var/spool/pbs/aux/2024084.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov --cpu-bind depth -d 16"
  },
  {
    "objectID": "talks/test/index.html#install-ezpz-wordplay",
    "href": "talks/test/index.html#install-ezpz-wordplay",
    "title": "Test Rendering on Mobile",
    "section": "Install {ezpz, wordplay}",
    "text": "Install {ezpz, wordplay}\n\n#[⭐][07:34:13 AM][foremans@x3101c0s13b0n0][~/tmp/polaris-talk/2024-07-17-073327]\n$ python3 -m pip install -e ezpz wordplay --require-virtualenv\nLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\nObtaining file:///home/foremans/tmp/polaris-talk/2024-07-17-073327/ezpz\n  Installing build dependencies ... done\n  Checking if build backend supports build_editable ... done\n  Getting requirements to build editable ... done\n  Installing backend dependencies ... done\n  Preparing editable metadata (pyproject.toml) ... done\n\n# ...[clipped]...\n\nSuccessfully built ezpz\nInstalling collected packages: enum34, wordplay, pyinstrument, ezpz\n  Attempting uninstall: ezpz\n    Found existing installation: ezpz 0.1\n    Not uninstalling ezpz at /home/foremans/.local/polaris/conda/2024-04-29/lib/python3.11/site-packages, outside environment /home/foremans/tmp/polaris-talk/2024-07-17-073327/venvs/2024-04-29\n    Cant uninstall 'ezpz'. No files were found to uninstall.\nSuccessfully installed enum34-1.1.10 ezpz pyinstrument-4.6.2 wordplay-1.0.0a4\n[notice] A new release of pip is available: 24.0 -&gt; 24.1.2\n[notice] To update, run: pip install --upgrade pip\n9.62s user 1.11s system 61% cpu 17.505s total\n\n#[⭐][07:34:53 AM][foremans@x3101c0s13b0n0][~/tmp/polaris-talk/2024-07-17-073327]\n$ python3 -m pip install --upgrade wandb\nLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\nRequirement already satisfied: wandb in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages (0.16.6)\nCollecting wandb\n  Downloading wandb-0.17.4-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\nDownloading wandb-0.17.4-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.9 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.9/6.9 MB 2.1 MB/s eta 0:00:00\nInstalling collected packages: wandb\n  Attempting uninstall: wandb\n    Found existing installation: wandb 0.16.6\n    Not uninstalling wandb at /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages, outside environment /home/foremans/tmp/polaris-talk/2024-07-17-073327/venvs/2024-04-29\n    Cant uninstall 'wandb'. No files were found to uninstall.\nSuccessfully installed wandb-0.17.4\n[notice] A new release of pip is available: 24.0 -&gt; 24.1.2\n[notice] To update, run: pip install --upgrade pip"
  },
  {
    "objectID": "talks/test/index.html#launch-ezpz.test_dist",
    "href": "talks/test/index.html#launch-ezpz.test_dist",
    "title": "Test Rendering on Mobile",
    "section": "Launch ezpz.test_dist",
    "text": "Launch ezpz.test_dist\n\n#(👻 2024-04-29)\n#[⭐][07:34:07 AM][foremans@x3101c0s13b0n0][~/tmp/polaris-talk/2024-07-17-073327][⏱ 7s]\n$ which launch\nlaunch: aliased to mpiexec --verbose --envall -n 4 -ppn 4 --hostfile /var/spool/pbs/aux/2024084.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov --cpu-bind depth -d 16\n\n#(👻 2024-04-29)\n#[⭐][07:34:11 AM][foremans@x3101c0s13b0n0][~/tmp/polaris-talk/2024-07-17-073327]\n$ which python3\n/home/foremans/tmp/polaris-talk/2024-07-17-073327/venvs/2024-04-29/bin/python3\n\n#(👻 2024-04-29)\n#[⭐][07:35:21 AM][foremans@x3101c0s13b0n0][~/tmp/polaris-talk/2024-07-17-073327][⏱ 14s]\n$ launch python3 -m ezpz.test_dist | tee ezpz-test-dist-DDP.log\nConnected to tcp://x3101c0s13b0n0.hsn.cm.polaris.alcf.anl.gov:7919\nFound executable /home/foremans/tmp/polaris-talk/2024-07-17-073327/venvs/2024-04-29/bin/python3\nLaunching application cff755ee-557e-4df2-a987-db85a8b7dbe7\n[2024-07-17 07:35:30.304306][INFO][__init__:156] - Setting logging level to 'INFO' on 'RANK == 0'\n[2024-07-17 07:35:30.307036][INFO][__init__:157] - Setting logging level to 'CRITICAL' on all others 'RANK != 0'\n[2024-07-17 07:35:30.307494][INFO][__init__:160] - To disable this behavior, and log from ALL ranks (not recommended), set: 'export LOG_FROM_ALL_RANKS=1'  in your environment, and re-run.\n[2024-07-17 07:35:32.116037][INFO][dist:358] - [device='cuda'][rank=2/3][local_rank=2/3][node=0/0]\n[2024-07-17 07:35:32.116089][INFO][dist:358] - [device='cuda'][rank=3/3][local_rank=3/3][node=0/0]\n[2024-07-17 07:35:32.116940][INFO][dist:358] - [device='cuda'][rank=1/3][local_rank=1/3][node=0/0]\n[2024-07-17 07:35:32.122726][INFO][dist:95] -\n[dist_info]:\n  • DEVICE=cuda\n  • DEVICE_ID=cuda:0\n  • DISTRIBUTED_BACKEND=nccl\n  • GPUS_PER_NODE=4\n  • HOSTS=['x3101c0s13b0n0.hsn.cm.polaris.alcf.anl.gov']\n  • HOSTFILE=/var/spool/pbs/aux/2024084.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov\n  • HOSTNAME=x3101c0s13b0n0.hsn.cm.polaris.alcf.anl.gov\n  • LOCAL_RANK=0\n  • MACHINE=Polaris\n  • NUM_NODES=1\n  • NGPUS=4\n  • NGPUS_AVAILABLE=4\n  • NODE_ID=0\n  • RANK=0\n  • SCHEDULER=PBS\n  • WORLD_SIZE_TOTAL=4\n  • WORLD_SIZE_IN_USE=4\n  • LAUNCH_CMD=mpiexec --verbose --envall -n 4 -ppn 4 --hostfile /var/spool/pbs/aux/2024084.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov --cpu-bind depth -d 16\n[2024-07-17 07:35:32.124800][INFO][dist:725] - [0/4] Using device='cuda' with backend='DDP' + 'nccl' for distributed training.\n[2024-07-17 07:35:32.129169][INFO][dist:358] - [device='cuda'][rank=0/3][local_rank=0/3][node=0/0]\n[2024-07-17 07:35:32.129674][WARNING][dist:364] - Using [4 / 4] available \"cuda\" devices !!\n[2024-07-17 07:35:32.130219][INFO][dist:874] - Setting up wandb from rank: 0\n[2024-07-17 07:35:32.130638][INFO][dist:875] - Using: WB PROJECT: ezpz.test_dist\nwandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\nwandb: Currently logged in as: foremans (aurora_gpt). Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.17.4\nwandb: Run data is saved locally in /home/foremans/tmp/polaris-talk/2024-07-17-073327/wandb/run-20240717_073532-p49rzxtv\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run vibrant-river-284\nwandb: ⭐️ View project at https://wandb.ai/aurora_gpt/ezpz.test_dist\nwandb: 🚀 View run at https://wandb.ai/aurora_gpt/ezpz.test_dist/runs/p49rzxtv\n[2024-07-17 07:35:33.171085][INFO][dist:905] - W&B RUN: [vibrant-river-284](https://wandb.ai/aurora_gpt/ezpz.test_dist/runs/p49rzxtv)\n[2024-07-17 07:35:33.182307][INFO][dist:312] - Updating wandb.run: vibrant-river-284 config with \"DIST_INFO\"\n[2024-07-17 07:35:33.186499][INFO][dist:938] - Running on machine='Polaris'\n[2024-07-17 07:35:33.187790][INFO][dist:95] -\n[timers_import]:\n  • os=1.082196831703186e-06\n  • logging=4.507601261138916e-07\n  • typing=2.9457733035087585e-06\n  • pathlib=1.3122335076332092e-06\n  • ezpz=6.109476089477539e-07\n  • torch=2.9457733035087585e-06\n  • torch_ddp=2.314336597919464e-06\n  • wandb=1.842435449361801e-05\n  • total=3.0086375772953033e-05\n\n[2024-07-17 07:35:33.188979][INFO][dist:95] -\n\n[CONFIG]:\n  • warmup=0\n  • log_freq=1\n  • batch_size=64\n  • input_size=128\n  • output_size=128\n  • dtype=torch.float32\n  • device=cuda\n  • world_size=4\n  • train_iters=100\n\n[2024-07-17 07:35:34.761945][INFO][test_dist:183] - model=Network(\n  (layers): Sequential(\n    (0): Linear(in_features=128, out_features=1024, bias=True)\n    (1): Linear(in_features=1024, out_features=512, bias=True)\n    (2): Linear(in_features=512, out_features=256, bias=True)\n    (3): Linear(in_features=256, out_features=128, bias=True)\n    (4): Linear(in_features=128, out_features=128, bias=True)\n  )\n)\n[2024-07-17 07:35:36.943300][INFO][test_dist:274] - iter=1, loss=2152.41, sps=1.697e+04, dt=0.00377066, dtf=0.001003, dtb=0.002768\n[2024-07-17 07:35:36.948048][INFO][test_dist:274] - iter=2, loss=1577.24, sps=3.611e+04, dt=0.00177221, dtf=0.0005256, dtb=0.001247\n[2024-07-17 07:35:36.952085][INFO][test_dist:274] - iter=3, loss=1201.25, sps=3.59e+04, dt=0.00178271, dtf=0.0004875, dtb=0.001295\n[2024-07-17 07:35:36.956071][INFO][test_dist:274] - iter=4, loss=1034.03, sps=3.704e+04, dt=0.0017279, dtf=0.0005082, dtb=0.00122\n[2024-07-17 07:35:36.959944][INFO][test_dist:274] - iter=5, loss=875.796, sps=3.825e+04, dt=0.00167313, dtf=0.0005121, dtb=0.001161\n[2024-07-17 07:35:36.963806][INFO][test_dist:274] - iter=6, loss=817.544, sps=3.804e+04, dt=0.00168248, dtf=0.0004651, dtb=0.001217\n[2024-07-17 07:35:36.967806][INFO][test_dist:274] - iter=7, loss=734.838, sps=3.536e+04, dt=0.0018099, dtf=0.0004969, dtb=0.001313\n[2024-07-17 07:35:36.971741][INFO][test_dist:274] - iter=8, loss=741.583, sps=3.682e+04, dt=0.00173809, dtf=0.0004537, dtb=0.001284\n[2024-07-17 07:35:36.975672][INFO][test_dist:274] - iter=9, loss=738.157, sps=3.717e+04, dt=0.0017217, dtf=0.0004635, dtb=0.001258\n[2024-07-17 07:35:36.979537][INFO][test_dist:274] - iter=10, loss=727.255, sps=3.857e+04, dt=0.00165911, dtf=0.0004897, dtb=0.001169\n[2024-07-17 07:35:36.983367][INFO][test_dist:274] - iter=11, loss=715.534, sps=3.979e+04, dt=0.00160845, dtf=0.0004246, dtb=0.001184\n[2024-07-17 07:35:36.987262][INFO][test_dist:274] - iter=12, loss=693.96, sps=3.791e+04, dt=0.00168827, dtf=0.0004543, dtb=0.001234\n[2024-07-17 07:35:36.991156][INFO][test_dist:274] - iter=13, loss=693.518, sps=3.815e+04, dt=0.00167748, dtf=0.0004182, dtb=0.001259\n[2024-07-17 07:35:36.994942][INFO][test_dist:274] - iter=14, loss=675.289, sps=4.003e+04, dt=0.00159879, dtf=0.0004048, dtb=0.001194\n[2024-07-17 07:35:36.999681][INFO][test_dist:274] - iter=15, loss=677.706, sps=4.062e+04, dt=0.0015755, dtf=0.0004248, dtb=0.001151\n[2024-07-17 07:35:37.003599][INFO][test_dist:274] - iter=16, loss=671.639, sps=3.754e+04, dt=0.00170499, dtf=0.000416, dtb=0.001289\n[2024-07-17 07:35:37.007565][INFO][test_dist:274] - iter=17, loss=652.219, sps=3.704e+04, dt=0.00172777, dtf=0.0004208, dtb=0.001307\n[2024-07-17 07:35:37.011753][INFO][test_dist:274] - iter=18, loss=633.308, sps=3.191e+04, dt=0.00200554, dtf=0.0004193, dtb=0.001586\n[2024-07-17 07:35:37.015595][INFO][test_dist:274] - iter=19, loss=635.459, sps=3.845e+04, dt=0.0016645, dtf=0.0004236, dtb=0.001241\n[2024-07-17 07:35:37.019356][INFO][test_dist:274] - iter=20, loss=626.979, sps=4.033e+04, dt=0.00158685, dtf=0.0004225, dtb=0.001164\n[2024-07-17 07:35:37.023081][INFO][test_dist:274] - iter=21, loss=612.352, sps=4.105e+04, dt=0.00155914, dtf=0.0004169, dtb=0.001142\n[2024-07-17 07:35:37.026861][INFO][test_dist:274] - iter=22, loss=609.89, sps=4.004e+04, dt=0.00159827, dtf=0.0004155, dtb=0.001183\n[2024-07-17 07:35:37.030555][INFO][test_dist:274] - iter=23, loss=602.673, sps=4.258e+04, dt=0.00150295, dtf=0.0004166, dtb=0.001086\n[2024-07-17 07:35:37.034382][INFO][test_dist:274] - iter=24, loss=613.106, sps=3.918e+04, dt=0.00163367, dtf=0.0004164, dtb=0.001217\n[2024-07-17 07:35:37.038129][INFO][test_dist:274] - iter=25, loss=644.755, sps=4.173e+04, dt=0.00153368, dtf=0.0004175, dtb=0.001116\n[2024-07-17 07:35:37.041943][INFO][test_dist:274] - iter=26, loss=789.106, sps=4.049e+04, dt=0.00158053, dtf=0.0004397, dtb=0.001141\n[2024-07-17 07:35:37.045705][INFO][test_dist:274] - iter=27, loss=691.36, sps=4.166e+04, dt=0.00153641, dtf=0.0004157, dtb=0.001121\n[2024-07-17 07:35:37.049496][INFO][test_dist:274] - iter=28, loss=657.228, sps=4.018e+04, dt=0.00159288, dtf=0.0004209, dtb=0.001172\n[2024-07-17 07:35:37.053229][INFO][test_dist:274] - iter=29, loss=633.212, sps=4.19e+04, dt=0.0015274, dtf=0.0004288, dtb=0.001099\n[2024-07-17 07:35:37.057013][INFO][test_dist:274] - iter=30, loss=640.29, sps=4.012e+04, dt=0.00159538, dtf=0.0004144, dtb=0.001181\n[2024-07-17 07:35:37.060722][INFO][test_dist:274] - iter=31, loss=604.287, sps=4.21e+04, dt=0.00152018, dtf=0.000398, dtb=0.001122\n[2024-07-17 07:35:37.064489][INFO][test_dist:274] - iter=32, loss=640.15, sps=4.079e+04, dt=0.00156912, dtf=0.0004007, dtb=0.001168\n[2024-07-17 07:35:37.068206][INFO][test_dist:274] - iter=33, loss=585.789, sps=4.238e+04, dt=0.00151007, dtf=0.0004199, dtb=0.00109\n[2024-07-17 07:35:37.071974][INFO][test_dist:274] - iter=34, loss=591.99, sps=4.053e+04, dt=0.00157917, dtf=0.000434, dtb=0.001145\n[2024-07-17 07:35:37.075702][INFO][test_dist:274] - iter=35, loss=618.223, sps=4.168e+04, dt=0.00153538, dtf=0.0004152, dtb=0.00112\n[2024-07-17 07:35:37.079496][INFO][test_dist:274] - iter=36, loss=572.365, sps=3.998e+04, dt=0.0016008, dtf=0.0004108, dtb=0.00119\n[2024-07-17 07:35:37.083250][INFO][test_dist:274] - iter=37, loss=573.749, sps=4.276e+04, dt=0.00149675, dtf=0.0004123, dtb=0.001084\n[2024-07-17 07:35:37.086969][INFO][test_dist:274] - iter=38, loss=580.662, sps=4.136e+04, dt=0.00154751, dtf=0.0004129, dtb=0.001135\n[2024-07-17 07:35:37.090636][INFO][test_dist:274] - iter=39, loss=568.836, sps=4.311e+04, dt=0.0014847, dtf=0.000409, dtb=0.001076\n[2024-07-17 07:35:37.094396][INFO][test_dist:274] - iter=40, loss=551.294, sps=4.145e+04, dt=0.00154388, dtf=0.0004118, dtb=0.001132\n[2024-07-17 07:35:37.098103][INFO][test_dist:274] - iter=41, loss=573.647, sps=4.352e+04, dt=0.00147048, dtf=0.0003977, dtb=0.001073\n[2024-07-17 07:35:37.101867][INFO][test_dist:274] - iter=42, loss=545.584, sps=4.257e+04, dt=0.00150354, dtf=0.000433, dtb=0.001071\n[2024-07-17 07:35:37.105639][INFO][test_dist:274] - iter=43, loss=544.877, sps=4.322e+04, dt=0.00148085, dtf=0.0004117, dtb=0.001069\n[2024-07-17 07:35:37.109471][INFO][test_dist:274] - iter=44, loss=559.886, sps=4.028e+04, dt=0.00158879, dtf=0.0004254, dtb=0.001163\n[2024-07-17 07:35:37.113186][INFO][test_dist:274] - iter=45, loss=534.895, sps=4.311e+04, dt=0.00148444, dtf=0.0004153, dtb=0.001069\n[2024-07-17 07:35:37.116972][INFO][test_dist:274] - iter=46, loss=536.457, sps=4.099e+04, dt=0.00156151, dtf=0.0004113, dtb=0.00115\n[2024-07-17 07:35:37.120710][INFO][test_dist:274] - iter=47, loss=548.508, sps=4.183e+04, dt=0.00152993, dtf=0.0004151, dtb=0.001115\n[2024-07-17 07:35:37.124552][INFO][test_dist:274] - iter=48, loss=532.186, sps=4.051e+04, dt=0.0015798, dtf=0.0004379, dtb=0.001142\n[2024-07-17 07:35:37.128266][INFO][test_dist:274] - iter=49, loss=519.254, sps=4.272e+04, dt=0.0014981, dtf=0.0004164, dtb=0.001082\n[2024-07-17 07:35:37.131975][INFO][test_dist:274] - iter=50, loss=535.535, sps=4.16e+04, dt=0.00153862, dtf=0.0004304, dtb=0.001108\n[2024-07-17 07:35:37.135717][INFO][test_dist:274] - iter=51, loss=520.722, sps=4.136e+04, dt=0.00154757, dtf=0.0004158, dtb=0.001132\n[2024-07-17 07:35:37.139451][INFO][test_dist:274] - iter=52, loss=513.063, sps=4.147e+04, dt=0.00154317, dtf=0.0004138, dtb=0.001129\n[2024-07-17 07:35:37.143231][INFO][test_dist:274] - iter=53, loss=514.546, sps=4.038e+04, dt=0.0015848, dtf=0.0004149, dtb=0.00117\n[2024-07-17 07:35:37.146971][INFO][test_dist:274] - iter=54, loss=506.488, sps=4.137e+04, dt=0.00154701, dtf=0.0004132, dtb=0.001134\n[2024-07-17 07:35:37.150659][INFO][test_dist:274] - iter=55, loss=503.01, sps=4.319e+04, dt=0.0014817, dtf=0.000415, dtb=0.001067\n[2024-07-17 07:35:37.154441][INFO][test_dist:274] - iter=56, loss=506.116, sps=4.06e+04, dt=0.00157637, dtf=0.0004211, dtb=0.001155\n[2024-07-17 07:35:37.158180][INFO][test_dist:274] - iter=57, loss=485.523, sps=4.287e+04, dt=0.00149301, dtf=0.000414, dtb=0.001079\n[2024-07-17 07:35:37.161931][INFO][test_dist:274] - iter=58, loss=489.076, sps=4.185e+04, dt=0.00152915, dtf=0.0004162, dtb=0.001113\n[2024-07-17 07:35:37.165759][INFO][test_dist:274] - iter=59, loss=484.844, sps=4.134e+04, dt=0.00154802, dtf=0.0004119, dtb=0.001136\n[2024-07-17 07:35:37.169483][INFO][test_dist:274] - iter=60, loss=496.104, sps=4.209e+04, dt=0.00152069, dtf=0.0003993, dtb=0.001121\n[2024-07-17 07:35:37.173190][INFO][test_dist:274] - iter=61, loss=467.599, sps=4.221e+04, dt=0.00151621, dtf=0.0004142, dtb=0.001102\n[2024-07-17 07:35:37.176950][INFO][test_dist:274] - iter=62, loss=480.055, sps=4.187e+04, dt=0.00152868, dtf=0.0004138, dtb=0.001115\n[2024-07-17 07:35:37.181194][INFO][test_dist:274] - iter=63, loss=483.146, sps=3.656e+04, dt=0.00175062, dtf=0.0006253, dtb=0.001125\n[2024-07-17 07:35:37.185018][INFO][test_dist:274] - iter=64, loss=479.273, sps=4.099e+04, dt=0.00156151, dtf=0.0004447, dtb=0.001117\n[2024-07-17 07:35:37.188752][INFO][test_dist:274] - iter=65, loss=464.753, sps=4.189e+04, dt=0.00152781, dtf=0.0004161, dtb=0.001112\n[2024-07-17 07:35:37.192464][INFO][test_dist:274] - iter=66, loss=462.583, sps=4.188e+04, dt=0.00152824, dtf=0.0004138, dtb=0.001114\n[2024-07-17 07:35:37.196126][INFO][test_dist:274] - iter=67, loss=461.665, sps=4.272e+04, dt=0.00149801, dtf=0.0004293, dtb=0.001069\n[2024-07-17 07:35:37.199838][INFO][test_dist:274] - iter=68, loss=465.25, sps=4.118e+04, dt=0.00155412, dtf=0.0004298, dtb=0.001124\n[2024-07-17 07:35:37.203602][INFO][test_dist:274] - iter=69, loss=460.897, sps=4.01e+04, dt=0.00159593, dtf=0.0004131, dtb=0.001183\n[2024-07-17 07:35:37.207372][INFO][test_dist:274] - iter=70, loss=456.136, sps=4.106e+04, dt=0.00155887, dtf=0.00041, dtb=0.001149\n[2024-07-17 07:35:37.211089][INFO][test_dist:274] - iter=71, loss=447.565, sps=4.158e+04, dt=0.00153923, dtf=0.0004113, dtb=0.001128\n[2024-07-17 07:35:37.214861][INFO][test_dist:274] - iter=72, loss=444.733, sps=4.05e+04, dt=0.00158026, dtf=0.0004127, dtb=0.001168\n[2024-07-17 07:35:37.218601][INFO][test_dist:274] - iter=73, loss=459.152, sps=4.123e+04, dt=0.00155234, dtf=0.0004201, dtb=0.001132\n[2024-07-17 07:35:37.222334][INFO][test_dist:274] - iter=74, loss=444.6, sps=4.226e+04, dt=0.00151444, dtf=0.0004371, dtb=0.001077\n[2024-07-17 07:35:37.226042][INFO][test_dist:274] - iter=75, loss=439.884, sps=4.29e+04, dt=0.001492, dtf=0.0004154, dtb=0.001077\n[2024-07-17 07:35:37.229838][INFO][test_dist:274] - iter=76, loss=438.578, sps=4.086e+04, dt=0.00156632, dtf=0.0004418, dtb=0.001125\n[2024-07-17 07:35:37.233560][INFO][test_dist:274] - iter=77, loss=431.993, sps=4.327e+04, dt=0.00147909, dtf=0.0004096, dtb=0.00107\n[2024-07-17 07:35:37.237367][INFO][test_dist:274] - iter=78, loss=422.338, sps=4.057e+04, dt=0.00157754, dtf=0.0004468, dtb=0.001131\n[2024-07-17 07:35:37.241117][INFO][test_dist:274] - iter=79, loss=427.973, sps=4.288e+04, dt=0.00149254, dtf=0.000415, dtb=0.001077\n[2024-07-17 07:35:37.244895][INFO][test_dist:274] - iter=80, loss=418.703, sps=4.06e+04, dt=0.00157617, dtf=0.0004137, dtb=0.001162\n[2024-07-17 07:35:37.248740][INFO][test_dist:274] - iter=81, loss=427.645, sps=4.031e+04, dt=0.00158766, dtf=0.000415, dtb=0.001173\n[2024-07-17 07:35:37.252447][INFO][test_dist:274] - iter=82, loss=417.629, sps=4.227e+04, dt=0.00151406, dtf=0.0004149, dtb=0.001099\n[2024-07-17 07:35:37.256190][INFO][test_dist:274] - iter=83, loss=411.667, sps=4.189e+04, dt=0.00152778, dtf=0.0004357, dtb=0.001092\n[2024-07-17 07:35:37.259935][INFO][test_dist:274] - iter=84, loss=409.366, sps=4.144e+04, dt=0.0015445, dtf=0.0004575, dtb=0.001087\n[2024-07-17 07:35:37.263677][INFO][test_dist:274] - iter=85, loss=409.511, sps=4.232e+04, dt=0.00151228, dtf=0.0004035, dtb=0.001109\n[2024-07-17 07:35:37.267463][INFO][test_dist:274] - iter=86, loss=409.593, sps=4.101e+04, dt=0.00156049, dtf=0.0004028, dtb=0.001158\n[2024-07-17 07:35:37.271174][INFO][test_dist:274] - iter=87, loss=408.794, sps=4.3e+04, dt=0.00148828, dtf=0.0004006, dtb=0.001088\n[2024-07-17 07:35:37.274926][INFO][test_dist:274] - iter=88, loss=403.151, sps=4.091e+04, dt=0.00156441, dtf=0.000415, dtb=0.001149\n[2024-07-17 07:35:37.278633][INFO][test_dist:274] - iter=89, loss=402.182, sps=4.26e+04, dt=0.00150243, dtf=0.0004147, dtb=0.001088\n[2024-07-17 07:35:37.282372][INFO][test_dist:274] - iter=90, loss=387.829, sps=4.216e+04, dt=0.00151793, dtf=0.0004411, dtb=0.001077\n[2024-07-17 07:35:37.286102][INFO][test_dist:274] - iter=91, loss=393.108, sps=4.308e+04, dt=0.00148558, dtf=0.0004167, dtb=0.001069\n[2024-07-17 07:35:37.289904][INFO][test_dist:274] - iter=92, loss=389.039, sps=4.103e+04, dt=0.00155996, dtf=0.0004359, dtb=0.001124\n[2024-07-17 07:35:37.293618][INFO][test_dist:274] - iter=93, loss=383.54, sps=4.322e+04, dt=0.00148092, dtf=0.0004147, dtb=0.001066\n[2024-07-17 07:35:37.297401][INFO][test_dist:274] - iter=94, loss=384.459, sps=4.1e+04, dt=0.00156106, dtf=0.0004164, dtb=0.001145\n[2024-07-17 07:35:37.301172][INFO][test_dist:274] - iter=95, loss=376.397, sps=4.191e+04, dt=0.0015272, dtf=0.0004129, dtb=0.001114\n[2024-07-17 07:35:37.304924][INFO][test_dist:274] - iter=96, loss=389.544, sps=4.091e+04, dt=0.00156433, dtf=0.0004139, dtb=0.00115\n[2024-07-17 07:35:37.308641][INFO][test_dist:274] - iter=97, loss=365.041, sps=4.343e+04, dt=0.00147362, dtf=0.0004165, dtb=0.001057\n[2024-07-17 07:35:37.312398][INFO][test_dist:274] - iter=98, loss=358.427, sps=4.134e+04, dt=0.00154796, dtf=0.0004143, dtb=0.001134\n[2024-07-17 07:35:37.561881][INFO][test_dist:274] - iter=99, loss=375.596, sps=258.9, dt=0.247161, dtf=0.1969, dtb=0.05026\n\n                            train/dt [2024-07-17-073537]\n     ┌─────────────────────────────────────────────────────────────────────────┐\n0.247┤                                                                        ▝│\n     │                                                                         │\n     │                                                                         │\n0.206┤                                                                         │\n     │                                                                         │\n     │                                                                         │\n0.165┤                                                                         │\n     │                                                                         │\n0.124┤                                                                         │\n     │                                                                         │\n     │                                                                         │\n0.083┤                                                                         │\n     │                                                                         │\n     │                                                                         │\n0.042┤                                                                         │\n     │                                                                         │\n     │                                                                         │\n0.001┤▄▗▖▄▗▖▄▗▖▄▗▖▄▗▖▄▗▖▄▖▄▗▖▄▗▖▄▗▖▄▗▖▄▗▖▄▗▄▗▖▄▗▖▄▗▖▄▗▖▄▗▖▄▗▄▗▖▄▗▖▄▗▖▄▗▖▄▗▖▄▗▖▖│\n     └┬─────────────────┬─────────────────┬─────────────────┬─────────────────┬┘\n     1.0              25.5              50.0              74.5             99.0\ntrain/dt                                iter\n[2024-07-17 07:35:37.589287][INFO][plot:156] - Appending plot to: /home/foremans/tmp/polaris-talk/2024-07-17-073327/test-dist-plots/train/dt.txt\ntext saved in /home/foremans/tmp/polaris-talk/2024-07-17-073327/test-dist-plots/train/dt.txt\n                            train/dtf [2024-07-17-073537]\n     ┌─────────────────────────────────────────────────────────────────────────┐\n0.197┤                                                                        ▝│\n     │                                                                         │\n     │                                                                         │\n0.164┤                                                                         │\n     │                                                                         │\n     │                                                                         │\n0.131┤                                                                         │\n     │                                                                         │\n0.099┤                                                                         │\n     │                                                                         │\n     │                                                                         │\n0.066┤                                                                         │\n     │                                                                         │\n     │                                                                         │\n0.033┤                                                                         │\n     │                                                                         │\n     │                                                                         │\n0.000┤▄▗▖▄▗▖▄▗▖▄▗▖▄▗▖▄▗▖▄▖▄▗▖▄▗▖▄▗▖▄▗▖▄▗▖▄▗▄▗▖▄▗▖▄▗▖▄▗▖▄▗▖▄▗▄▗▖▄▗▖▄▗▖▄▗▖▄▗▖▄▗▖▖│\n     └┬─────────────────┬─────────────────┬─────────────────┬─────────────────┬┘\n     1.0              25.5              50.0              74.5             99.0\ntrain/dtf                               iter\n[2024-07-17 07:35:37.603242][INFO][plot:156] - Appending plot to: /home/foremans/tmp/polaris-talk/2024-07-17-073327/test-dist-plots/train/dtf.txt\ntext saved in /home/foremans/tmp/polaris-talk/2024-07-17-073327/test-dist-plots/train/dtf.txt\n                             train/dtb [2024-07-17-073537]\n      ┌────────────────────────────────────────────────────────────────────────┐\n0.0503┤                                                                       ▝│\n      │                                                                        │\n      │                                                                        │\n0.0421┤                                                                        │\n      │                                                                        │\n      │                                                                        │\n0.0339┤                                                                        │\n      │                                                                        │\n0.0257┤                                                                        │\n      │                                                                        │\n      │                                                                        │\n0.0175┤                                                                        │\n      │                                                                        │\n      │                                                                        │\n0.0093┤                                                                        │\n      │                                                                        │\n      │                                                                        │\n0.0011┤▚▗▖▄▗▖▄▗▖▄▖▄▗▖▄▗▖▄▖▄▗▖▄▗▖▄▗▄▗▖▄▗▖▄▗▖▄▖▄▗▖▄▗▖▄▖▄▗▖▄▗▖▄▗▄▗▖▄▗▖▄▗▄▗▖▄▗▖▄▗▖▖│\n      └┬─────────────────┬─────────────────┬────────────────┬─────────────────┬┘\n      1.0              25.5              50.0             74.5             99.0\ntrain/dtb                                iter\n[2024-07-17 07:35:37.615896][INFO][plot:156] - Appending plot to: /home/foremans/tmp/polaris-talk/2024-07-17-073327/test-dist-plots/train/dtb.txt\ntext saved in /home/foremans/tmp/polaris-talk/2024-07-17-073327/test-dist-plots/train/dtb.txt\n                            train/loss [2024-07-17-073537]\n      ┌────────────────────────────────────────────────────────────────────────┐\n2152.4┤▘                                                                       │\n      │                                                                        │\n      │                                                                        │\n1853.4┤                                                                        │\n      │                                                                        │\n      │▗                                                                       │\n1554.4┤                                                                        │\n      │                                                                        │\n1255.4┤                                                                        │\n      │ ▗                                                                      │\n      │                                                                        │\n 956.4┤  ▘                                                                     │\n      │   ▖                                                                    │\n      │   ▝              ▖                                                     │\n 657.4┤    ▝▘▀▝▘▚▖▄     ▗ ▄                                                    │\n      │            ▝▘▀▝▘▘  ▝▘▀▗▘▚▗▄▗▖▄▗ ▗                                      │\n      │                                ▘▘▝▘▀▘▀▝▘▞▗▘▄▖▄▗▖▄▗▖▄▗▄                 │\n 358.4┤                                                       ▝▘▀▝▘▀▝▀▝▘▀▝▖▚▝▖▄│\n      └┬─────────────────┬─────────────────┬────────────────┬─────────────────┬┘\n      1.0              25.5              50.0             74.5             99.0\ntrain/loss                               iter\n[2024-07-17 07:35:37.655339][INFO][plot:156] - Appending plot to: /home/foremans/tmp/polaris-talk/2024-07-17-073327/test-dist-plots/train/loss.txt\ntext saved in /home/foremans/tmp/polaris-talk/2024-07-17-073327/test-dist-plots/train/loss.txt\n                           train/iter [2024-07-17-073537]\n    ┌──────────────────────────────────────────────────────────────────────────┐\n99.0┤                                                                      ▗▗▖▀│\n    │                                                                   ▄▝▘▘   │\n    │                                                              ▗▖▞▝▘       │\n82.7┤                                                          ▄▗▘▀            │\n    │                                                      ▖▄▝▘                │\n    │                                                 ▗▗▖▀▝                    │\n66.3┤                                              ▄▝▘▘                        │\n    │                                         ▗▖▞▝▘                            │\n50.0┤                                     ▄▗▘▀                                 │\n    │                                 ▖▄▝▘                                     │\n    │                            ▗▗▖▀▝                                         │\n33.7┤                         ▄▝▘▘                                             │\n    │                    ▗▖▞▝▘                                                 │\n    │                ▄▗▘▀                                                      │\n17.3┤            ▖▄▝▘                                                          │\n    │       ▗▗▖▀▝                                                              │\n    │    ▄▝▘▘                                                                  │\n 1.0┤▖▞▝▘                                                                      │\n    └┬─────────────────┬──────────────────┬─────────────────┬─────────────────┬┘\n    1.0              25.5               50.0              74.5             99.0\ntrain/iter                              iter\n[2024-07-17 07:35:37.669214][INFO][plot:156] - Appending plot to: /home/foremans/tmp/polaris-talk/2024-07-17-073327/test-dist-plots/train/iter.txt\ntext saved in /home/foremans/tmp/polaris-talk/2024-07-17-073327/test-dist-plots/train/iter.txt\n                             train/sps [2024-07-17-073537]\n       ┌───────────────────────────────────────────────────────────────────────┐\n43523.3┤                ▖▗  ▖▗ ▖▗ ▖▝ ▚▘▝ ▖▗    ▘▗▖▗▖▖ ▖▄    ▗▖▝ ▖ ▗▖▗ ▘▗▞ ▘▗ ▘ │\n       │       ▖ ▗▘  ▗▝▖  ▀▗ ▖▝▝ ▖▝ ▘  ▖▝ ▘▝▀▗▘▝ ▝   ▝  ▘▞▝▘▘ ▘▝ ▚ ▝ ▘▝  ▝ ▘▝ ▘│\n       │  ▖▀ ▖▞ ▞  ▄ ▘  ▝                                                      │\n36312.5┤▝▝  ▗                                       ▝                          │\n       │            ▖                                                          │\n       │                                                                       │\n29101.8┤                                                                       │\n       │                                                                       │\n21891.1┤                                                                       │\n       │                                                                       │\n       │▖                                                                      │\n14680.4┤                                                                       │\n       │                                                                       │\n       │                                                                       │\n 7469.7┤                                                                       │\n       │                                                                       │\n       │                                                                       │\n  258.9┤                                                                      ▗│\n       └┬─────────────────┬────────────────┬─────────────────┬────────────────┬┘\n       1.0              25.5             50.0              74.5            99.0\ntrain/sps                                iter\n[2024-07-17 07:35:37.681268][INFO][plot:156] - Appending plot to: /home/foremans/tmp/polaris-talk/2024-07-17-073327/test-dist-plots/train/sps.txt\ntext saved in /home/foremans/tmp/polaris-talk/2024-07-17-073327/test-dist-plots/train/sps.txt"
  },
  {
    "objectID": "talks/test/index.html#pyinstrument-profile",
    "href": "talks/test/index.html#pyinstrument-profile",
    "title": "Test Rendering on Mobile",
    "section": "PyInstrument Profile",
    "text": "PyInstrument Profile\n\nRecorded: 07:35:34  Samples:  2227\nDuration: 2.948     CPU time: 5.441\nPyInstrument: v4.6.2\nProgram: /home/foremans/tmp/polaris-talk/2024-07-17-073327/ezpz/src/ezpz/test_dist.py\n2.948 &lt;module&gt;  ezpz/test_dist.py:1\n└─ 2.946 main  ezpz/test_dist.py:217\n   ├─ 2.043 build_model_and_optimizer  ezpz/test_dist.py:171\n   │  └─ 2.011 Adam.__init__  torch/optim/adam.py:15\n   │        [129 frames hidden]  torch, wandb, transformers, jax, func...\n   ├─ 0.326 _forward_step  ezpz/test_dist.py:231\n   │  ├─ 0.279 DistributedDataParallel._wrapped_call_impl  torch/nn/modules/module.py:1528\n   │  │     [13 frames hidden]  torch, wandb, &lt;built-in&gt;\n   │  │        0.273 Network._call_impl  torch/nn/modules/module.py:1534\n   │  │        └─ 0.076 Network.forward  ezpz/test_dist.py:164\n   │  │           └─ 0.076 Sequential._wrapped_call_impl  torch/nn/modules/module.py:1528\n   │  │                 [7 frames hidden]  torch, &lt;built-in&gt;\n   │  └─ 0.046 calc_loss  ezpz/test_dist.py:168\n   ├─ 0.254 _backward_step  ezpz/test_dist.py:236\n   │  ├─ 0.177 Tensor.backward  torch/_tensor.py:466\n   │  │     [4 frames hidden]  torch, &lt;built-in&gt;\n   │  └─ 0.077 wrapper  torch/optim/optimizer.py:374\n   │        [5 frames hidden]  torch\n   ├─ 0.119 tplot_dict  ezpz/plot.py:136\n   │  └─ 0.069 show  plotext/_core.py:292\n   │        [5 frames hidden]  plotext\n   ├─ 0.102 Logger.info  logging/__init__.py:1479\n   │     [6 frames hidden]  logging, rich\n   │        0.102 RichHandler.emit  rich/logging.py:126\n   │        └─ 0.100 Console.print  ezpz/log/console.py:79\n   │           └─ 0.100 Console.print  rich/console.py:1624\n   │                 [5 frames hidden]  rich\n   └─ 0.099 Run.wrapper  wandb/sdk/wandb_run.py:418\n         [13 frames hidden]  wandb, json\n[2024-07-17 07:35:37.876629][INFO][profile:115] - Saving pyinstrument profile output to: /home/foremans/tmp/polaris-talk/2024-07-17-073327/ezpz_pyinstrument_profiles\n[2024-07-17 07:35:37.877255][INFO][profile:123] - PyInstrument profile saved (as html) to:  /home/foremans/tmp/polaris-talk/2024-07-17-073327/ezpz_pyinstrument_profiles/pyinstrument-profile-2024-07-17-073537.html\n[2024-07-17 07:35:37.877936][INFO][profile:131] - PyInstrument profile saved (as text) to:  /home/foremans/tmp/polaris-talk/2024-07-17-073327/ezpz_pyinstrument_profiles/pyinstrument-profile-2024-07-17-073537.txt\n[2024-07-17 07:35:38.391628][INFO][profile:143] - Finished with pyinstrument profiler. Took: 2.94768s\n[2024-07-17 07:35:38.392519][INFO][test_dist:318] - [0] runtime=8.075730s\nwandb: 🚀 View run vibrant-river-284 at: https://wandb.ai/aurora_gpt/ezpz.test_dist/runs/p49rzxtv\nwandb: Find logs at: wandb/run-20240717_073532-p49rzxtv/logs\nApplication cff755ee resources: utime=25s stime=23s maxrss=1434396KB inblock=32 oublock=4320 minflt=670179 majflt=864 nvcsw=195893 nivcsw=1331214"
  },
  {
    "objectID": "talks/test/index.html#example-ezpz",
    "href": "talks/test/index.html#example-ezpz",
    "title": "Test Rendering on Mobile",
    "section": "Example: ezpz 🍋",
    "text": "Example: ezpz 🍋\n\nLink[^ez-video] to video\n\n\nExample: using 🍋 ezpz.test_dist to train a small model using DDP"
  },
  {
    "objectID": "talks/test/index.html#prepare-data",
    "href": "talks/test/index.html#prepare-data",
    "title": "Test Rendering on Mobile",
    "section": "Prepare Data",
    "text": "Prepare Data\n\n#[⭐][07:41:20 AM][foremans@x3101c0s13b0n0][~/tmp/polaris-talk/2024-07-17-073327][⏱ 29s]\n$ python3 wordplay/data/shakespeare_char/prepare.py\nUsing HF_DATASETS_CACHE=/home/foremans/tmp/polaris-talk/2024-07-17-073327/wordplay/data/shakespeare_char/.cache/huggingface\nlength of dataset in characters: 1,115,394\nall the unique characters:\n !$&\\',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\nvocab size: 65\ntrain has 1,003,854 tokens\nval has 111,540 tokens"
  },
  {
    "objectID": "talks/test/index.html#launch-training-ddp",
    "href": "talks/test/index.html#launch-training-ddp",
    "title": "Test Rendering on Mobile",
    "section": "Launch Training (DDP)",
    "text": "Launch Training (DDP)\n\n#(👻 2024-04-29)\n#[⭐][07:42:02 AM][foremans@x3101c0s13b0n0][~/tmp/polaris-talk/2024-07-17-073327]\n$ launch python3 -m wordplay train.backend=DDP train.eval_interval=100 data=shakespeare train.dtype=bf16 model.batch_size=64 model.block_size=1024 train.max_iters=1000 train.log_interval=10 train.compile=false | tee wordplay-gpt2-DDP.log\n[2024-07-17 07:42:11.746540][INFO][__init__:156] - Setting logging level to 'INFO' on 'RANK == 0'\n[2024-07-17 07:42:11.748763][INFO][__init__:157] - Setting logging level to 'CRITICAL' on all others 'RANK != 0'\n[2024-07-17 07:42:11.749453][INFO][__init__:160] - To disable this behavior, and log from ALL ranks (not recommended), set: 'export LOG_FROM_ALL_RANKS=1'  in your environment, and re-run.\n[2024-07-17 07:42:11.772718][INFO][configs:81] - Setting HF_DATASETS_CACHE to /home/foremans/tmp/polaris-talk/2024-07-17-073327/wordplay/.cache/huggingface/datasets\n[2024-07-17 07:42:15.341532][INFO][dist:358] - [device='cuda'][rank=2/3][local_rank=2/3][node=0/0]\n[2024-07-17 07:42:15.342381][INFO][dist:358] - [device='cuda'][rank=1/3][local_rank=1/3][node=0/0]\n[2024-07-17 07:42:15.342430][INFO][dist:358] - [device='cuda'][rank=3/3][local_rank=3/3][node=0/0]\n[2024-07-17 07:42:15.348657][INFO][dist:95] -\n\n[dist_info]:\n  • DEVICE=cuda\n  • DEVICE_ID=cuda:0\n  • DISTRIBUTED_BACKEND=nccl\n  • GPUS_PER_NODE=4\n  • HOSTS=['x3101c0s13b0n0.hsn.cm.polaris.alcf.anl.gov']\n  • HOSTFILE=/var/spool/pbs/aux/2024084.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov\n  • HOSTNAME=x3101c0s13b0n0.hsn.cm.polaris.alcf.anl.gov\n  • LOCAL_RANK=0\n  • MACHINE=Polaris\n  • NUM_NODES=1\n  • NGPUS=4\n  • NGPUS_AVAILABLE=4\n  • NODE_ID=0\n  • RANK=0\n  • SCHEDULER=PBS\n  • WORLD_SIZE_TOTAL=4\n  • WORLD_SIZE_IN_USE=4\n  • LAUNCH_CMD=mpiexec --verbose --envall -n 4 -ppn 4 --hostfile /var/spool/pbs/aux/2024084.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov --cpu-bind depth -d 16\n\n\n[2024-07-17 07:42:15.351446][INFO][dist:725] - [0/4] Using device='cuda' with backend='DDP' + 'nccl' for distributed training.\n[2024-07-17 07:42:15.356169][INFO][dist:358] - [device='cuda'][rank=0/3][local_rank=0/3][node=0/0]\n[2024-07-17 07:42:15.356692][WARNING][dist:364] - Using [4 / 4] available \"cuda\" devices !!\n[2024-07-17 07:42:15.359571][INFO][configs:317] - Loading val from /home/foremans/tmp/polaris-talk/2024-07-17-073327/wordplay/data/shakespeare_char/val.bin\n[2024-07-17 07:42:15.360138][INFO][configs:317] - Loading train from /home/foremans/tmp/polaris-talk/2024-07-17-073327/wordplay/data/shakespeare_char/train.bin\n[2024-07-17 07:42:15.361154][INFO][configs:442] - Tokens per iteration: 262,144\n[2024-07-17 07:42:15.361574][INFO][configs:465] - Using self.ptdtype=torch.float16 on self.device_type='cuda'\n[2024-07-17 07:42:15.362002][INFO][configs:471] - Initializing a new model from scratch\n[2024-07-17 07:42:15.362529][INFO][dist:874] - Setting up wandb from rank: 0\n[2024-07-17 07:42:15.362896][INFO][dist:875] - Using: WB PROJECT: WordPlay\n[2024-07-17 07:42:16.451786][INFO][dist:905] - W&B RUN: [still-frog-17](https://wandb.ai/aurora_gpt/WordPlay/runs/6by9vpcj)\n[2024-07-17 07:42:16.464106][INFO][dist:312] - Updating wandb.run: still-frog-17 config with \"DIST_INFO\"\n[2024-07-17 07:42:16.469424][INFO][dist:938] - Running on machine='Polaris'\n[2024-07-17 07:42:16.471151][WARNING][__main__:89] - {\n    \"train\": {\n        \"framework\": \"pytorch\",\n        \"backend\": \"DDP\",\n        \"device\": null,\n        \"seed\": null,\n        \"port\": null,\n        \"ds_config_path\": null,\n        \"precision\": null,\n        \"ngpus\": null,\n        \"use_wandb\": true,\n        \"eval_interval\": 100,\n        \"log_interval\": 10,\n        \"eval_iters\": 200,\n        \"eval_only\": false,\n        \"always_save_checkpoint\": false,\n        \"init_from\": \"scratch\",\n        \"wandb_project\": \"WordPlay\",\n        \"max_iters\": 1000,\n        \"warmup_iters\": 100,\n        \"dtype\": \"bf16\",\n        \"compile\": false\n    },\n    \"model\": {\n        \"n_layer\": 12,\n        \"n_head\": 12,\n        \"n_embd\": 768,\n        \"batch_size\": 64,\n        \"block_size\": 1024,\n        \"activation\": \"gelu\",\n        \"dropout\": 0.0,\n        \"bias\": false,\n        \"vocab_size\": 65\n    },\n    \"data\": {\n        \"dataset\": \"shakespeare_char\",\n        \"out_dir\": \"out-shakespeare-char\",\n        \"root_path\": null\n    },\n    \"optimizer\": {\n        \"gas\": 1,\n        \"name\": \"AdamW\",\n        \"learning_rate\": 0.0006,\n        \"weight_decay\": 0.1,\n        \"beta1\": 0.9,\n        \"beta2\": 0.95,\n        \"grad_clip\": 1.0,\n        \"decay_lr\": true,\n        \"lr_decay_iters\": 600000,\n        \"min_lr\": 6e-05\n    }\n}\n[2024-07-17 07:42:16.474305][WARNING][__main__:90] - Output dir: /home/foremans/tmp/polaris-talk/outputs/runs/pytorch/DDP/2024-07-17/07-42-13\n[2024-07-17 07:42:16.474922][INFO][trainer:246] - Initializing a new model from scratch\n[2024-07-17 07:42:17.258904][INFO][model:255] - number of parameters: 85.00M\n[2024-07-17 07:42:17.290004][INFO][trainer:264] - Model size: num_params=85003776\n[2024-07-17 07:42:17.292626][INFO][model:445] - num decayed parameter tensors: 50, with 85,771,008 parameters\n[2024-07-17 07:42:17.293296][INFO][model:449] - num non-decayed parameter tensors: 25, with 19,200 parameters\n[2024-07-17 07:42:17.515324][CRITICAL][trainer:316] - \"devid='cuda:1'\"\n[2024-07-17 07:42:17.515340][CRITICAL][trainer:316] - \"devid='cuda:2'\"\n[2024-07-17 07:42:17.515465][CRITICAL][trainer:316] - \"devid='cuda:3'\"\n[2024-07-17 07:42:18.431814][INFO][model:465] - using fused AdamW: True\n[2024-07-17 07:42:18.432620][CRITICAL][trainer:316] - \"devid='cuda:0'\"\n[2024-07-17 07:42:19.951020][INFO][trainer:356] - • self.model=GPT(\n  (transformer): ModuleDict(\n    (wte): Embedding(65, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.0, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x Block(\n        (ln_1): LayerNorm()\n        (attn): CausalSelfAttention(\n          (c_attn): Linear(in_features=768, out_features=2304, bias=False)\n          (c_proj): Linear(in_features=768, out_features=768, bias=False)\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (ln_2): LayerNorm()\n        (mlp): MLP(\n          (c_fc): Linear(in_features=768, out_features=3072, bias=False)\n          (act_fn): GELU(approximate='none')\n          (c_proj): Linear(in_features=3072, out_features=768, bias=False)\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm()\n  )\n  (lm_head): Linear(in_features=768, out_features=65, bias=False)\n)\n[2024-07-17 07:42:19.955340][INFO][trainer:357] - • self.grad_scaler=&lt;torch.cuda.amp.grad_scaler.GradScaler object at 0x145a38f0f090&gt;\n[2024-07-17 07:42:19.956897][INFO][trainer:358] - • self.model_engine=DistributedDataParallel(\n  (module): GPT(\n    (transformer): ModuleDict(\n      (wte): Embedding(65, 768)\n      (wpe): Embedding(1024, 768)\n      (drop): Dropout(p=0.0, inplace=False)\n      (h): ModuleList(\n        (0-11): 12 x Block(\n          (ln_1): LayerNorm()\n          (attn): CausalSelfAttention(\n            (c_attn): Linear(in_features=768, out_features=2304, bias=False)\n            (c_proj): Linear(in_features=768, out_features=768, bias=False)\n            (attn_dropout): Dropout(p=0.0, inplace=False)\n            (resid_dropout): Dropout(p=0.0, inplace=False)\n          )\n          (ln_2): LayerNorm()\n          (mlp): MLP(\n            (c_fc): Linear(in_features=768, out_features=3072, bias=False)\n            (act_fn): GELU(approximate='none')\n            (c_proj): Linear(in_features=3072, out_features=768, bias=False)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n      )\n      (ln_f): LayerNorm()\n    )\n    (lm_head): Linear(in_features=768, out_features=65, bias=False)\n  )\n)\n[2024-07-17 07:42:19.961066][INFO][trainer:359] - • self.optimizer=AdamW (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.95)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: True\n    lr: 0.0006\n    maximize: False\n    weight_decay: 0.1\n\nParameter Group 1\n    amsgrad: False\n    betas: (0.9, 0.95)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: True\n    lr: 0.0006\n    maximize: False\n    weight_decay: 0.0\n)\n[2024-07-17 07:42:19.988827][INFO][trainer:802] - Startup time: 6.7125\n                Training Legend\n┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃    abbr     ┃ desc                           ┃\n┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│    step     │ Current training iteration     │\n│    loss     │ Loss value                     │\n│     dt      │ Elapsed time per training step │\n│     dtf     │ Elapsed time per forward step  │\n│     dtb     │ Elapsed time per backward step │\n│     sps     │ Samples per second             │\n│ sps_per_gpu │ Samples per second (per GPU)   │\n│     tps     │ Tokens per second              │\n│ tps_per_gpu │ Tokens per second (per GPU)    │\n│     mfu     │ Model flops utilization        │\n│ train_loss  │ Training loss value            │\n│  val_loss   │ Validation loss value          │\n└─────────────┴────────────────────────────────┘\n[2024-07-17 07:42:21.451865][INFO][trainer:820] - ['prompt']: 'What is an LLM?'\n[2024-07-17 07:42:21.452667][INFO][trainer:824] - ['response']:\nWhat is an LLM?eelEl\\'$nltPwBSWal,;PWw bbu\\'HiyP\\'FWwF &AhW:ygrn kk-\\'\\'KFlMwnlEfflkc,elpWaWtgml$Pgglhllw lglhFllzczPAFHpeAAPPSltgkrWPPhlEMgcrN ggPWt-WPSSzHSkkrzzk.FFrtSSkgMll&gFXr,hghaueaVPW-pHFF-gg,,,FF,,kbApgg gg\\'aWWzzkk\\'a\\'CggHl$bGeA,FFk,,SF;UF,,aZ ;gglee$,k.US&kg:S,,zVzzc\n[2024-07-17 07:43:01.573073][INFO][trainer:885] - step=10 loss=3.154310 dt=0.282833 dtf=0.005247 dtb=0.011417 sps=14.142633 sps_per_gpu=3.535658 tps=926851.609409 tps_per_gpu=231712.902352 mfu=46.288281 train_loss=4.125778 val_loss=4.128809\n[2024-07-17 07:43:04.402750][INFO][trainer:885] - step=20 loss=2.660851 dt=0.306263 dtf=0.005233 dtb=0.011419 sps=13.060678 sps_per_gpu=3.265170 tps=855944.613638 tps_per_gpu=213986.153409 mfu=45.934162 train_loss=4.125778 val_loss=4.128809\n[2024-07-17 07:43:07.237507][INFO][trainer:885] - step=30 loss=2.543283 dt=0.283021 dtf=0.005238 dtb=0.011245 sps=14.133211 sps_per_gpu=3.533303 tps=926234.088226 tps_per_gpu=231558.522057 mfu=45.966490 train_loss=4.125778 val_loss=4.128809\n[2024-07-17 07:43:10.077248][INFO][trainer:885] - step=40 loss=2.503963 dt=0.285001 dtf=0.005213 dtb=0.011471 sps=14.035061 sps_per_gpu=3.508765 tps=919801.749941 tps_per_gpu=229950.437485 mfu=45.963461 train_loss=4.125778 val_loss=4.128809\n[2024-07-17 07:43:12.917039][INFO][trainer:885] - step=50 loss=2.477469 dt=0.283532 dtf=0.005166 dtb=0.011294 sps=14.107763 sps_per_gpu=3.526941 tps=924566.380009 tps_per_gpu=231141.595002 mfu=45.984530 train_loss=4.125778 val_loss=4.128809\n[2024-07-17 07:43:15.760749][INFO][trainer:885] - step=60 loss=2.471083 dt=0.284630 dtf=0.005140 dtb=0.011224 sps=14.053326 sps_per_gpu=3.513332 tps=920998.786204 tps_per_gpu=230249.696551 mfu=45.985675 train_loss=4.125778 val_loss=4.128809\n[2024-07-17 07:43:18.602785][INFO][trainer:885] - step=70 loss=2.458894 dt=0.283926 dtf=0.005219 dtb=0.010383 sps=14.088155 sps_per_gpu=3.522039 tps=923281.352698 tps_per_gpu=230820.338174 mfu=45.998106 train_loss=4.125778 val_loss=4.128809\n[2024-07-17 07:43:21.451433][INFO][trainer:885] - step=80 loss=2.489088 dt=0.285537 dtf=0.005183 dtb=0.011373 sps=14.008683 sps_per_gpu=3.502171 tps=918073.060430 tps_per_gpu=229518.265108 mfu=45.983282 train_loss=4.125778 val_loss=4.128809\n[2024-07-17 07:43:24.302241][INFO][trainer:885] - step=90 loss=2.471990 dt=0.300767 dtf=0.005445 dtb=0.010290 sps=13.299337 sps_per_gpu=3.324834 tps=871585.359388 tps_per_gpu=217896.339847 mfu=45.737774 train_loss=4.125778 val_loss=4.128809\n[2024-07-17 07:43:27.153275][INFO][trainer:885] - step=100 loss=2.445556 dt=0.285869 dtf=0.005182 dtb=0.011251 sps=13.992403 sps_per_gpu=3.498101 tps=917006.151328 tps_per_gpu=229251.537832 mfu=45.743655 train_loss=4.125778 val_loss=4.128809\n[2024-07-17 07:43:28.182553][INFO][trainer:820] - ['prompt']: 'What is an LLM?'\n[2024-07-17 07:43:28.183179][INFO][trainer:824] - ['response']:\n\nWhat is an LLM?\n\nGoupay my winghimithell bls ger t bon sinthard ht omind be,\nAnd lereind h py balithand frd oforondof wimon me hageas thinero mand,\nThacanes,\nAn frift ghik med d herthecke ntore thack couthen ale, t thit ang d m t h chy me fache ag, wit my hathan glat ng\n[2024-07-17 07:44:06.025837][INFO][trainer:760] - Saving checkpoint to: /home/foremans/tmp/polaris-talk/outputs/runs/pytorch/DDP/2024-07-17/07-42-13\n[2024-07-17 07:44:06.026607][INFO][trainer:761] - Saving model to: /home/foremans/tmp/polaris-talk/outputs/runs/pytorch/DDP/2024-07-17/07-42-13/model.pth\n[2024-07-17 07:44:07.682968][INFO][configs:141] - Appending /home/foremans/tmp/polaris-talk/outputs/runs/pytorch/DDP/2024-07-17/07-42-13 to /home/foremans/tmp/polaris-talk/2024-07-17-073327/wordplay/src/ckpts/checkpoints.log\n[2024-07-17 07:44:10.519506][INFO][trainer:885] - step=110 loss=2.433923 dt=0.285038 dtf=0.005757 dtb=0.011762 sps=14.033209 sps_per_gpu=3.508302 tps=919680.367894 tps_per_gpu=229920.091974 mfu=45.762304 train_loss=2.439494 val_loss=2.478951\n[2024-07-17 07:44:13.362148][INFO][trainer:885] - step=120 loss=2.429014 dt=0.284445 dtf=0.005222 dtb=0.011486 sps=14.062460 sps_per_gpu=3.515615 tps=921597.361532 tps_per_gpu=230399.340383 mfu=45.788661 train_loss=2.439494 val_loss=2.478951\n[2024-07-17 07:44:16.210694][INFO][trainer:885] - step=130 loss=2.402059 dt=0.285559 dtf=0.005199 dtb=0.011765 sps=14.007633 sps_per_gpu=3.501908 tps=918004.211586 tps_per_gpu=229501.052897 mfu=45.794438 train_loss=2.439494 val_loss=2.478951\n[2024-07-17 07:44:19.061546][INFO][trainer:885] - step=140 loss=2.374062 dt=0.285476 dtf=0.005239 dtb=0.011453 sps=14.011662 sps_per_gpu=3.502916 tps=918268.297093 tps_per_gpu=229567.074273 mfu=45.800956 train_loss=2.439494 val_loss=2.478951\n[2024-07-17 07:44:21.917283][INFO][trainer:885] - step=150 loss=2.365385 dt=0.285846 dtf=0.005125 dtb=0.011320 sps=13.993568 sps_per_gpu=3.498392 tps=917082.475791 tps_per_gpu=229270.618948 mfu=45.800900 train_loss=2.439494 val_loss=2.478951\n[2024-07-17 07:44:24.771924][INFO][trainer:885] - step=160 loss=2.317337 dt=0.280788 dtf=0.005173 dtb=0.011249 sps=14.245602 sps_per_gpu=3.561401 tps=933599.792506 tps_per_gpu=233399.948127 mfu=45.883340 train_loss=2.439494 val_loss=2.478951\n[2024-07-17 07:44:27.626812][INFO][trainer:885] - step=170 loss=2.256231 dt=0.284973 dtf=0.005141 dtb=0.011299 sps=14.036416 sps_per_gpu=3.509104 tps=919890.544506 tps_per_gpu=229972.636126 mfu=45.889069 train_loss=2.439494 val_loss=2.478951\n[2024-07-17 07:44:30.480952][INFO][trainer:885] - step=180 loss=2.216419 dt=0.286555 dtf=0.005180 dtb=0.011402 sps=13.958906 sps_per_gpu=3.489726 tps=914810.852170 tps_per_gpu=228702.713043 mfu=45.868857 train_loss=2.439494 val_loss=2.478951\n[2024-07-17 07:44:33.337342][INFO][trainer:885] - step=190 loss=2.145123 dt=0.291456 dtf=0.005409 dtb=0.019347 sps=13.724205 sps_per_gpu=3.431051 tps=899429.467247 tps_per_gpu=224857.366812 mfu=45.773849 train_loss=2.439494 val_loss=2.478951\n[2024-07-17 07:44:36.194584][INFO][trainer:885] - step=200 loss=2.068149 dt=0.285703 dtf=0.005153 dtb=0.011286 sps=14.000555 sps_per_gpu=3.500139 tps=917540.393411 tps_per_gpu=229385.098353 mfu=45.778791 train_loss=2.439494 val_loss=2.478951\n[2024-07-17 07:44:37.224149][INFO][trainer:820] - ['prompt']: 'What is an LLM?'\n[2024-07-17 07:44:37.224745][INFO][trainer:824] - ['response']:\n\nWhat is an LLM?\n\nLORTESS LA:\nNo, sighappat selace? don downd sourciceans note cancen up sof liond\nThis and my man, werame, of re thee\nThise not will I on land brond sul me a fingore?\n\nFLER:\nTisint your not nare lame o igen,-to brorst.\n\nSamERS:\nSin:\nI\\'l hell she lor hen w\n[2024-07-17 07:45:14.409129][INFO][trainer:760] - Saving checkpoint to: /home/foremans/tmp/polaris-talk/outputs/runs/pytorch/DDP/2024-07-17/07-42-13\n[2024-07-17 07:45:14.409820][INFO][trainer:761] - Saving model to: /home/foremans/tmp/polaris-talk/outputs/runs/pytorch/DDP/2024-07-17/07-42-13/model.pth\n[2024-07-17 07:45:16.366935][INFO][configs:141] - Appending /home/foremans/tmp/polaris-talk/outputs/runs/pytorch/DDP/2024-07-17/07-42-13 to /home/foremans/tmp/polaris-talk/2024-07-17-073327/wordplay/src/ckpts/checkpoints.log\n[2024-07-17 07:45:19.245061][INFO][trainer:885] - step=210 loss=1.982169 dt=0.283305 dtf=0.005223 dtb=0.011284 sps=14.119042 sps_per_gpu=3.529760 tps=925305.515083 tps_per_gpu=231326.378771 mfu=45.822019 train_loss=2.045786 val_loss=2.148510\n[2024-07-17 07:45:22.092430][INFO][trainer:885] - step=220 loss=1.897731 dt=0.284759 dtf=0.005217 dtb=0.011187 sps=14.046945 sps_per_gpu=3.511736 tps=920580.608106 tps_per_gpu=230145.152026 mfu=45.837327 train_loss=2.045786 val_loss=2.148510\n[2024-07-17 07:45:24.942639][INFO][trainer:885] - step=230 loss=1.817213 dt=0.285266 dtf=0.005208 dtb=0.011446 sps=14.022003 sps_per_gpu=3.505501 tps=918945.985503 tps_per_gpu=229736.496376 mfu=45.842940 train_loss=2.045786 val_loss=2.148510\n[2024-07-17 07:45:27.797910][INFO][trainer:885] - step=240 loss=1.779287 dt=0.285465 dtf=0.005189 dtb=0.011220 sps=14.012250 sps_per_gpu=3.503062 tps=918306.793546 tps_per_gpu=229576.698387 mfu=45.844800 train_loss=2.045786 val_loss=2.148510\n[2024-07-17 07:45:30.653597][INFO][trainer:885] - step=250 loss=1.704220 dt=0.289284 dtf=0.005471 dtb=0.010346 sps=13.827253 sps_per_gpu=3.456813 tps=906182.836379 tps_per_gpu=226545.709095 mfu=45.785926 train_loss=2.045786 val_loss=2.148510\n[2024-07-17 07:45:33.512769][INFO][trainer:885] - step=260 loss=1.671318 dt=0.287679 dtf=0.005125 dtb=0.011250 sps=13.904380 sps_per_gpu=3.476095 tps=911237.442617 tps_per_gpu=227809.360654 mfu=45.758182 train_loss=2.045786 val_loss=2.148510\n[2024-07-17 07:45:36.373461][INFO][trainer:885] - step=270 loss=1.650952 dt=0.298661 dtf=0.005118 dtb=0.011520 sps=13.393107 sps_per_gpu=3.348277 tps=877730.651421 tps_per_gpu=219432.662855 mfu=45.565875 train_loss=2.045786 val_loss=2.148510\n[2024-07-17 07:45:39.236930][INFO][trainer:885] - step=280 loss=1.573242 dt=0.285970 dtf=0.005171 dtb=0.011290 sps=13.987477 sps_per_gpu=3.496869 tps=916683.279847 tps_per_gpu=229170.819962 mfu=45.587333 train_loss=2.045786 val_loss=2.148510\n[2024-07-17 07:45:42.100605][INFO][trainer:885] - step=290 loss=1.533265 dt=0.286487 dtf=0.005432 dtb=0.011288 sps=13.962259 sps_per_gpu=3.490565 tps=915030.617828 tps_per_gpu=228757.654457 mfu=45.598392 train_loss=2.045786 val_loss=2.148510\n[2024-07-17 07:45:44.964424][INFO][trainer:885] - step=300 loss=1.492064 dt=0.288480 dtf=0.005355 dtb=0.011480 sps=13.865774 sps_per_gpu=3.466443 tps=908707.340870 tps_per_gpu=227176.835218 mfu=45.576766 train_loss=2.045786 val_loss=2.148510\n[2024-07-17 07:45:45.995833][INFO][trainer:820] - ['prompt']: 'What is an LLM?'\n[2024-07-17 07:45:45.996497][INFO][trainer:824] - ['response']:\n\nWhat is an LLM?\n\nRICHMORD:\nChar stire? how in those are name the range hone.\n\nGLOUCESTER:\nNay, in lond's time the palt are worder more\nThat wilt in the purpose be a pey\nAnd thou thine onter hands, and the which broth.\n\nELBOWINCA:\nAt lie my lord with the me an arms be a s\n[2024-07-17 07:46:23.549987][INFO][trainer:760] - Saving checkpoint to: /home/foremans/tmp/polaris-talk/outputs/runs/pytorch/DDP/2024-07-17/07-42-13\n[2024-07-17 07:46:23.550696][INFO][trainer:761] - Saving model to: /home/foremans/tmp/polaris-talk/outputs/runs/pytorch/DDP/2024-07-17/07-42-13/model.pth\n[2024-07-17 07:46:25.496559][INFO][configs:141] - Appending /home/foremans/tmp/polaris-talk/outputs/runs/pytorch/DDP/2024-07-17/07-42-13 to /home/foremans/tmp/polaris-talk/2024-07-17-073327/wordplay/src/ckpts/checkpoints.log\n[2024-07-17 07:46:28.374854][INFO][trainer:885] - step=310 loss=1.444200 dt=0.299907 dtf=0.005333 dtb=0.010637 sps=13.337481 sps_per_gpu=3.334370 tps=874085.133345 tps_per_gpu=218521.283336 mfu=45.384395 train_loss=1.495372 val_loss=1.713714\n[2024-07-17 07:46:31.223079][INFO][trainer:885] - step=320 loss=1.429350 dt=0.285238 dtf=0.005245 dtb=0.011485 sps=14.023353 sps_per_gpu=3.505838 tps=919034.479880 tps_per_gpu=229758.619970 mfu=45.435743 train_loss=1.495372 val_loss=1.713714\n[2024-07-17 07:46:34.074957][INFO][trainer:885] - step=330 loss=1.362220 dt=0.285027 dtf=0.005165 dtb=0.011407 sps=14.033736 sps_per_gpu=3.508434 tps=919714.904826 tps_per_gpu=229928.726207 mfu=45.485355 train_loss=1.495372 val_loss=1.713714\n[2024-07-17 07:46:36.929464][INFO][trainer:885] - step=340 loss=1.350888 dt=0.284436 dtf=0.005199 dtb=0.011287 sps=14.062893 sps_per_gpu=3.515723 tps=921625.744709 tps_per_gpu=230406.436177 mfu=45.539549 train_loss=1.495372 val_loss=1.713714"
  },
  {
    "objectID": "talks/test/index.html#wordplay",
    "href": "talks/test/index.html#wordplay",
    "title": "Test Rendering on Mobile",
    "section": "wordplay 🎮💬",
    "text": "wordplay 🎮💬\n\nLink1 to video\n\n\nExample: Training a LLM to talk like Shakespeare using saforem2/wordplay 🎮💬\nidk why it doesn’t render correctly in the slide (seems like refreshing helps?)"
  },
  {
    "objectID": "talks/test/index.html#bibliography",
    "href": "talks/test/index.html#bibliography",
    "title": "Test Rendering on Mobile",
    "section": "Bibliography",
    "text": "Bibliography\n\n\nWei, Jason, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, et al. 2022. “Emergent Abilities of Large Language Models.” https://arxiv.org/abs/2206.07682.\n\n\nYang, Jingfeng, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Bing Yin, and Xia Hu. 2023. “Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond.” https://arxiv.org/abs/2304.13712.\n\n\nYao, Shunyu, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. “Tree of Thoughts: Deliberate Problem Solving with Large Language Models.” https://arxiv.org/abs/2305.10601."
  }
]