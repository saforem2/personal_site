<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.13">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Sam Foreman">
<meta name="dcterms.date" content="2024-11-05">

<title>Parallel Training Methods â€“ Sam Foreman</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../assets/favicon.svg" rel="icon" type="image/svg+xml">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-3a2195740c1ea8e805a7c8f60f392a8e.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-71e06a8d8feb4f05c6b2c6ff59bc66e3.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/quarto-contrib/fontawesome6-0.1.0/all.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/fontawesome6-0.1.0/latex-fontsize.css" rel="stylesheet">
<script src="../../site_libs/quarto-contrib/iconify-2.1.0/iconify-icon.min.js"></script>
<script src="../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "?",
    "H"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-XVM2Y822Y1"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-XVM2Y822Y1', { 'anonymize_ip': true});
</script>
<meta name="mermaid-theme" content="neutral">
<script src="../../site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="../../site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="../../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">
<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-TC329HJ');</script>
<!-- End Google Tag Manager -->

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../css/colors-oklch.min.css">
<link rel="stylesheet" href="../../css/custom.css">
<link rel="stylesheet" href="../../css/svgbob.css">
<link rel="stylesheet" href="../../static/fonts/IosevkaTerm/IosevkaTerm.css">
<link rel="stylesheet" href="../../static/fonts/IosevkaQP/IosevkaQP.css">
<meta property="og:title" content="Parallel Training Methods for AI">
<meta property="og:description" content="Talks, Posts, Projects and More">
<meta property="og:image" content="https://samforeman.me/talks/ai-for-science-2024/assets/thumbnail.png">
<meta property="og:site_name" content="Sam Foreman">
<meta property="og:image:height" content="2572">
<meta property="og:image:width" content="4112">
<meta name="twitter:title" content="Parallel Training Methods for AI">
<meta name="twitter:description" content="Talks, Posts, Projects and More">
<meta name="twitter:image" content="https://samforeman.me/talks/ai-for-science-2024/assets/thumbnail.png">
<meta name="twitter:creator" content="saforem2">
<meta name="twitter:site" content="saforem2">
<meta name="twitter:card" content="summary">
<meta name="twitter:image-height" content="2572">
<meta name="twitter:image-width" content="4112">
<meta name="citation_title" content="Parallel Training Methods">
<meta name="citation_author" content="Sam Foreman">
<meta name="citation_publication_date" content="2024-11-05">
<meta name="citation_cover_date" content="2024-11-05">
<meta name="citation_year" content="2024">
<meta name="citation_online_date" content="2024-11-05">
<meta name="citation_fulltext_html_url" content="https://samforeman.me/talks/ai-for-science-2024/slides">
<meta name="citation_language" content="en">
<meta name="citation_reference" content="citation_title=Superconductivity of in and sn samples;,citation_author=George Deamont;,citation_author=Sam Foreman;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;">
<meta name="citation_reference" content="citation_title=RG-inspired machine learning for lattice field theory;,citation_author=Sam Foreman;,citation_author=Joel Giedt;,citation_author=Yannick Meurice;,citation_author=Judah Unmuth-Yockey;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_volume=175;,citation_conference_title=EPJ web of conferences;,citation_conference=EDP Sciences;">
<meta name="citation_reference" content="citation_title=Large energy density in three-plate nanocapacitors due to coulomb blockade;,citation_author=A Hubler;,citation_author=S Foreman;,citation_author=J Liu;,citation_author=L Wortsmann;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_issue=10;,citation_volume=123;,citation_journal_title=Journal of Applied Physics;,citation_publisher=AIP Publishing;">
<meta name="citation_reference" content="citation_title=Examples of renormalization group transformations for image sets;,citation_author=Samuel Foreman;,citation_author=Joel Giedt;,citation_author=Yannick Meurice;,citation_author=Judah Unmuth-Yockey;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_issue=5;,citation_volume=98;,citation_journal_title=Physical Review E;,citation_publisher=American Physical Society;">
<meta name="citation_reference" content="citation_title=Machine learning inspired analysis of the Ising model transition;,citation_author=Samuel Foreman;,citation_author=Joel Giedt;,citation_author=Yannick Meurice;,citation_author=Judah Unmuth-Yockey;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_doi=10.22323/1.334.0245;,citation_volume=LATTICE2018;,citation_journal_title=PoS;">
<meta name="citation_reference" content="citation_title=Machine learning inspired analysis of the ising model transition;,citation_author=Samuel Foreman;,citation_author=Joel Giedt;,citation_author=Yannick Meurice;,citation_author=Judah Unmuth-Yockey;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_conference_title=Lattice 2018;">
<meta name="citation_reference" content="citation_title=Learning better physics: A machine learning approach to lattice gauge theory;,citation_author=Samuel Alfred Foreman;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_dissertation_institution=University of Iowa;">
<meta name="citation_reference" content="citation_title=Machine learning and neural networks for field theory;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C Osborn;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;">
<meta name="citation_reference" content="citation_title=HMC with normalizing flows;,citation_author=Sam Foreman;,citation_author=Taku Izubuchi;,citation_author=Luchang Jin;,citation_author=Xiao-Yong Jin;,citation_author=James C Osborn;,citation_author=Akio Tomiya;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_journal_title=arXiv preprint arXiv:2112.01586;">
<meta name="citation_reference" content="citation_title=LeapfrogLayers: A trainable framework for effective topological sampling;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C Osborn;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_journal_title=arXiv preprint arXiv:2112.01582;">
<meta name="citation_reference" content="citation_title=Energy storage in quantum resonators;,citation_author=Jiaqi Liu;,citation_author=Alfred W Hubler;,citation_author=Samuel Alfred Foreman;,citation_author=Katharina Ott;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;">
<meta name="citation_reference" content="citation_title=Applications of machine learning to lattice quantum field theory;,citation_author=Denis Boyda;,citation_author=Salvatore CalÄ±Ì€;,citation_author=Sam Foreman;,citation_author=Lena Funcke;,citation_author=Daniel C Hackett;,citation_author=Yin Lin;,citation_author=Gert Aarts;,citation_author=Andrei Alexandru;,citation_author=Xiao-Yong Jin;,citation_author=Biagio Lucini;,citation_author=others;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_journal_title=arXiv preprint arXiv:2202.05838;">
<meta name="citation_reference" content="citation_title=Lattice QCD and particle physics;,citation_author=Andreas S Kronfeld;,citation_author=Tanmoy Bhattacharya;,citation_author=Thomas Blum;,citation_author=Norman H Christ;,citation_author=Carleton DeTar;,citation_author=William Detmold;,citation_author=Robert Edwards;,citation_author=Anna Hasenfratz;,citation_author=Huey-Wen Lin;,citation_author=Swagato Mukherjee;,citation_author=others;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_journal_title=arXiv preprint arXiv:2207.07641;">
<meta name="citation_reference" content="citation_title=GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics;,citation_author=Maxim Zvyagin;,citation_author=Alexander Brace;,citation_author=Kyle Hippe;,citation_author=Yuntian Deng;,citation_author=Bin Zhang;,citation_author=Cindy Orozco Bohorquez;,citation_author=Austin Clyde;,citation_author=Bharat Kale;,citation_author=Danilo Perez-Rivera;,citation_author=Heng Ma;,citation_author=others;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_issue=6;,citation_volume=37;,citation_journal_title=The International Journal of High Performance Computing Applications;,citation_publisher=SAGE Publications Sage UK: London, England;">
<meta name="citation_reference" content="citation_title=MLMC: Machine learning monte carlo;,citation_author=Sam Foreman;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_conference_title=The international symposium on lattice field theory;">
<meta name="citation_reference" content="citation_title=Superconductivity of in and sn samples;,citation_author=George Deamont;,citation_author=Sam Foreman;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;">
<meta name="citation_reference" content="citation_title=A comprehensive performance study of large language models on novel AI accelerators;,citation_author=Murali Emani;,citation_author=Sam Foreman;,citation_author=Varuni Sastry;,citation_author=Zhen Xie;,citation_author=Siddhisanket Raskar;,citation_author=William Arnold;,citation_author=Rajeev Thakur;,citation_author=Venkatram Vishwanath;,citation_author=Michael E Papka;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_journal_title=arXiv preprint arXiv:2310.04607;">
<meta name="citation_reference" content="citation_title=DeepSpeed4Science initiative: Enabling large-scale scientific discovery through sophisticated AI system technologies;,citation_author=Shuaiwen Leon Song;,citation_author=Bonnie Kruft;,citation_author=Minjia Zhang;,citation_author=Conglong Li;,citation_author=Shiyang Chen;,citation_author=Chengming Zhang;,citation_author=Masahiro Tanaka;,citation_author=Xiaoxia Wu;,citation_author=Jeff Rasley;,citation_author=Ammar Ahmad Awan;,citation_author=others;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_journal_title=arXiv preprint arXiv:2310.04610;">
<meta name="citation_reference" content="citation_title=Protein generation via genome-scale language models with bio-physical scoring;,citation_author=Gautham Dharuman;,citation_author=Logan Ward;,citation_author=Heng Ma;,citation_author=Priyanka V Setty;,citation_author=Ozan Gokdemir;,citation_author=Sam Foreman;,citation_author=Murali Emani;,citation_author=Kyle Hippe;,citation_author=Alexander Brace;,citation_author=Kristopher Keipert;,citation_author=others;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_conference_title=Proceedings of the SCâ€™23 workshops of the international conference on high performance computing, network, storage, and analysis;">
<meta name="citation_reference" content="citation_title=MLMC: Machine learning monte carlo for lattice gauge theory;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C Osborn;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_journal_title=arXiv preprint arXiv:2312.08936;">
<meta name="citation_reference" content="citation_title=Snowmass 2021 computational frontier CompF03 topical group report: Machine learning;,citation_author=Phiala Shanahan;,citation_author=Kazuhiro Terao;,citation_author=Daniel Whiteson;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_journal_title=arXiv preprint arXiv:2209.07559;">
<meta name="citation_reference" content="citation_title=Thorough characterization and analysis of large transformer model training at-scale;,citation_author=Scott Cheng;,citation_author=Jun-Liang Lin;,citation_author=Murali Emani;,citation_author=Siddhisanket Raskar;,citation_author=Sam Foreman;,citation_author=Zhen Xie;,citation_author=Venkatram Vishwanath;,citation_author=Mahmut Taylan Kandemir;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_issue=1;,citation_volume=8;,citation_journal_title=Proceedings of the ACM on Measurement and Analysis of Computing Systems;,citation_publisher=ACM New York, NY, USA;">
<meta name="citation_reference" content="citation_title=Communities through energy justice projects;,citation_author=Mary Ann Leung;,citation_author=Katharine Cahill;,citation_author=Rebecca Hartman-Baker;,citation_author=Paige Kinsley;,citation_author=Lois Curfman McInnes;,citation_author=Suzanne Parete-Koon;,citation_author=Subil Abraham;,citation_author=Lacy Beach Barrier;,citation_author=Gladys Chen;,citation_author=Lizanne DeStefano;,citation_author=others;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_issue=1;,citation_volume=15;,citation_journal_title=Journal of Computational Science;">
<meta name="citation_reference" content="citation_title=Applications of a foundation model approach for weather and climate;,citation_author=Troy Arcomano;,citation_author=Alexander Wikner;,citation_author=Romit Maulik;,citation_author=Veerabhadra Rao Kotamarthi;,citation_author=Sam Foreman;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_volume=2023;,citation_conference_title=AGU fall meeting abstracts;">
<meta name="citation_reference" content="citation_title=Toward a holistic performance evaluation of large language models across diverse ai accelerators;,citation_author=Murali Emani;,citation_author=Sam Foreman;,citation_author=Varuni Sastry;,citation_author=Zhen Xie;,citation_author=Siddhisanket Raskar;,citation_author=William Arnold;,citation_author=Rajeev Thakur;,citation_author=Venkatram Vishwanath;,citation_author=Michael E Papka;,citation_author=Sanjif Shanmugavelu;,citation_author=others;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_conference_title=2024 IEEE international parallel and distributed processing symposium workshops (IPDPSW);,citation_conference=IEEE;">
<meta name="citation_reference" content="citation_title=Intro to HPC bootcamp: Engaging new communities through energy justice projects;,citation_author=Suzanne Parete-Koon;,citation_author=Michael Sandoval;,citation_author=Kellen Leland;,citation_author=Subil Abraham;,citation_author=Mary Ann Leung;,citation_author=Rebecca Hartman-Baker;,citation_author=Paige Kinsley;,citation_author=Lois McInnes;,citation_author=Sreeranjani Ramprakash;,citation_author=Lacy Beach Barrier;,citation_author=others;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_issue=1;,citation_volume=15;,citation_journal_title=Journal of Computational Science Education;,citation_publisher=Oak Ridge National Laboratory (ORNL), Oak Ridge, TN (United States);">
<meta name="citation_reference" content="citation_title=MProt-DPO: Breaking the ExaFLOPS barrier for multimodal protein design workflows with direct preference optimization;,citation_author=Gautham Dharuman;,citation_author=Kyle Hippe;,citation_author=Alexander Brace;,citation_author=Sam Foreman;,citation_author=VÃ¤inÃ¤ HatanpÃ¤Ã¤;,citation_author=Varuni K Sastry;,citation_author=Huihuo Zheng;,citation_author=Logan Ward;,citation_author=Servesh Muralidharan;,citation_author=Archit Vasan;,citation_author=others;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_conference_title=2024 SC24: International conference for high performance computing, networking, storage and analysis SC;,citation_conference=IEEE Computer Society;">
<meta name="citation_reference" content="citation_title=Emergent abilities of large language models;,citation_author=Jason Wei;,citation_author=Yi Tay;,citation_author=Rishi Bommasani;,citation_author=Colin Raffel;,citation_author=Barret Zoph;,citation_author=Sebastian Borgeaud;,citation_author=Dani Yogatama;,citation_author=Maarten Bosma;,citation_author=Denny Zhou;,citation_author=Donald Metzler;,citation_author=Ed H. Chi;,citation_author=Tatsunori Hashimoto;,citation_author=Oriol Vinyals;,citation_author=Percy Liang;,citation_author=Jeff Dean;,citation_author=William Fedus;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2206.07682;">
<meta name="citation_reference" content="citation_title=DeepSpeed4Science initiative: Enabling large-scale scientific discovery through sophisticated AI system technologies;,citation_author=Shuaiwen Leon Song;,citation_author=Bonnie Kruft;,citation_author=Minjia Zhang;,citation_author=Conglong Li;,citation_author=Shiyang Chen;,citation_author=Chengming Zhang;,citation_author=Masahiro Tanaka;,citation_author=Xiaoxia Wu;,citation_author=Jeff Rasley;,citation_author=Ammar Ahmad Awan;,citation_author=Connor Holmes;,citation_author=Martin Cai;,citation_author=Adam Ghanem;,citation_author=Zhongzhu Zhou;,citation_author=Yuxiong He;,citation_author=Pete Luferenko;,citation_author=Divya Kumar;,citation_author=Jonathan Weyn;,citation_author=Ruixiong Zhang;,citation_author=Sylwester Klocek;,citation_author=Volodymyr Vragov;,citation_author=Mohammed AlQuraishi;,citation_author=Gustaf Ahdritz;,citation_author=Christina Floristean;,citation_author=Cristina Negri;,citation_author=Rao Kotamarthi;,citation_author=Venkatram Vishwanath;,citation_author=Arvind Ramanathan;,citation_author=Sam Foreman;,citation_author=Kyle Hippe;,citation_author=Troy Arcomano;,citation_author=Romit Maulik;,citation_author=Maxim Zvyagin;,citation_author=Alexander Brace;,citation_author=Bin Zhang;,citation_author=Cindy Orozco Bohorquez;,citation_author=Austin Clyde;,citation_author=Bharat Kale;,citation_author=Danilo Perez-Rivera;,citation_author=Heng Ma;,citation_author=Carla M. Mann;,citation_author=Michael Irvin;,citation_author=J. Gregory Pauloski;,citation_author=Logan Ward;,citation_author=Valerie Hayot;,citation_author=Murali Emani;,citation_author=Zhen Xie;,citation_author=Diangen Lin;,citation_author=Maulik Shukla;,citation_author=Ian Foster;,citation_author=James J. Davis;,citation_author=Michael E. Papka;,citation_author=Thomas Brettin;,citation_author=Prasanna Balaprakash;,citation_author=Gina Tourassi;,citation_author=John Gounley;,citation_author=Heidi Hanson;,citation_author=Thomas E Potok;,citation_author=Massimiliano Lupo Pasini;,citation_author=Kate Evans;,citation_author=Dan Lu;,citation_author=Dalton Lunga;,citation_author=Junqi Yin;,citation_author=Sajal Dash;,citation_author=Feiyi Wang;,citation_author=Mallikarjun Shankar;,citation_author=Isaac Lyngaas;,citation_author=Xiao Wang;,citation_author=Guojing Cong;,citation_author=Pei Zhang;,citation_author=Ming Fan;,citation_author=Siyan Liu;,citation_author=Adolfy Hoisie;,citation_author=Shinjae Yoo;,citation_author=Yihui Ren;,citation_author=William Tang;,citation_author=Kyle Felker;,citation_author=Alexey Svyatkovskiy;,citation_author=Hang Liu;,citation_author=Ashwin Aji;,citation_author=Angela Dalton;,citation_author=Michael Schulte;,citation_author=Karl Schulz;,citation_author=Yuntian Deng;,citation_author=Weili Nie;,citation_author=Josh Romero;,citation_author=Christian Dallago;,citation_author=Arash Vahdat;,citation_author=Chaowei Xiao;,citation_author=Thomas Gibbs;,citation_author=Anima Anandkumar;,citation_author=Rick Stevens;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2310.04610;">
<meta name="citation_reference" content="citation_title=Emergent abilities of large language models;,citation_author=Jason Wei;,citation_author=Yi Tay;,citation_author=Rishi Bommasani;,citation_author=Colin Raffel;,citation_author=Barret Zoph;,citation_author=Sebastian Borgeaud;,citation_author=Dani Yogatama;,citation_author=Maarten Bosma;,citation_author=Denny Zhou;,citation_author=Donald Metzler;,citation_author=Ed H. Chi;,citation_author=Tatsunori Hashimoto;,citation_author=Oriol Vinyals;,citation_author=Percy Liang;,citation_author=Jeff Dean;,citation_author=William Fedus;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2206.07682;">
<meta name="citation_reference" content="citation_title=The climate risk &amp;amp;amp; resilience portal (ClimRR) metadata and data dictionary;,citation_author=C. Burdi;,citation_author=Wall. T Branham;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://dub.sh/ClimRR-Metadata;">
<meta name="citation_reference" content="citation_title=Progress on $(g-2)_\mu$ from lattice QCD;,citation_author=Hartmut Wittig;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2306.04165;">
<meta name="citation_reference" content="citation_title=Hybrid Monte Carlo;,citation_author=S. Duane;,citation_author=A. D. Kennedy;,citation_author=B. J. Pendleton;,citation_author=D. Roweth;,citation_publication_date=1987;,citation_cover_date=1987;,citation_year=1987;,citation_doi=10.1016/0370-2693(87)91197-X;,citation_volume=195;,citation_journal_title=Phys. Lett. B;">
<meta name="citation_reference" content="citation_title=Snowmass 2021 Computational Frontier CompF03 Topical Group Report: Machine Learning;,citation_author=Phiala Shanahan;,citation_author=others;,citation_publication_date=2022-09;,citation_cover_date=2022-09;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2209.07559;">
<meta name="citation_reference" content="citation_title=Applications of Machine Learning to Lattice Quantum Field Theory;,citation_author=Denis Boyda;,citation_author=others;,citation_publication_date=2022-02;,citation_cover_date=2022-02;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2202.05838;,citation_conference_title=Snowmass 2021;">
<meta name="citation_reference" content="citation_title=HMC with Normalizing Flows;,citation_author=Sam Foreman;,citation_author=Taku Izubuchi;,citation_author=Luchang Jin;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_author=Akio Tomiya;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01586;,citation_doi=10.22323/1.396.0073;,citation_volume=LATTICE2021;,citation_journal_title=PoS;">
<meta name="citation_reference" content="citation_title=LeapfrogLayers: A Trainable Framework for Effective Topological Sampling;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01582;,citation_doi=10.22323/1.396.0508;,citation_volume=LATTICE2021;,citation_journal_title=PoS;">
<meta name="citation_reference" content="citation_title=Deep Learning Hamiltonian Monte Carlo;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_publication_date=2021-05;,citation_cover_date=2021-05;,citation_year=2021;,citation_fulltext_html_url=https://arxiv.org/abs/2105.03418;,citation_conference_title=9th International Conference on Learning Representations;">
<meta name="citation_reference" content="citation_title=Deep learning hamiltonian monte carlo;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://arxiv.org/abs/2105.03418;">
<meta name="citation_reference" content="citation_title=Deep Learning Hamiltonian Monte Carlo;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_publication_date=2021-05;,citation_cover_date=2021-05;,citation_year=2021;,citation_fulltext_html_url=https://arxiv.org/abs/2105.03418;,citation_conference_title=9th International Conference on Learning Representations;">
<meta name="citation_reference" content="citation_title=Deep learning hamiltonian monte carlo;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C Osborn;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_journal_title=arXiv preprint arXiv:2105.03418;">
<meta name="citation_reference" content="citation_title=LeapfrogLayers: A trainable framework for effective topological sampling;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C Osborn;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_journal_title=arXiv preprint arXiv:2112.01582;">
<meta name="citation_reference" content="citation_title=Energy Justice Analysis of Climate Data with ClimRR;,citation_author=Sam Foreman;,citation_publication_date=2023-08-07;,citation_cover_date=2023-08-07;,citation_year=2023;,citation_fulltext_html_url=https://saforem2.github.io/climate-analysis;,citation_language=en;">
<meta name="citation_reference" content="citation_author=Sam Foreman;,citation_publication_date=2023-08-19;,citation_cover_date=2023-08-19;,citation_year=2023;,citation_fulltext_html_url=https://saforem2.github.io/l2hmc-qcd;,citation_language=en;">
<meta name="citation_reference" content="citation_title=MLMC: Machine learning monte carlo for lattice gauge theory;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James Osborn;,citation_publication_date=00;,citation_cover_date=00;,citation_year=0;,citation_conference_title=40th international symposium on lattice field theory (lattice 2023) (batavia, IL, united states, 07/31/2023 - 08/04/2023);">
<meta name="citation_reference" content="citation_title=Progress on $(g-2)_\mu$ from lattice QCD;,citation_author=Hartmut Wittig;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2306.04165;">
<meta name="citation_reference" content="citation_title=Hybrid Monte Carlo;,citation_author=S. Duane;,citation_author=A. D. Kennedy;,citation_author=B. J. Pendleton;,citation_author=D. Roweth;,citation_publication_date=1987;,citation_cover_date=1987;,citation_year=1987;,citation_doi=10.1016/0370-2693(87)91197-X;,citation_volume=195;,citation_journal_title=Phys. Lett. B;">
<meta name="citation_reference" content="citation_title=Snowmass 2021 Computational Frontier CompF03 Topical Group Report: Machine Learning;,citation_author=Phiala Shanahan;,citation_author=others;,citation_publication_date=2022-09;,citation_cover_date=2022-09;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2209.07559;">
<meta name="citation_reference" content="citation_title=Applications of Machine Learning to Lattice Quantum Field Theory;,citation_author=Denis Boyda;,citation_author=others;,citation_publication_date=2022-02;,citation_cover_date=2022-02;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2202.05838;,citation_conference_title=Snowmass 2021;">
<meta name="citation_reference" content="citation_title=LeapfrogLayers: A Trainable Framework for Effective Topological Sampling;,citation_author=S. Foreman;,citation_author=X. Jin;,citation_author=J. Osborn;,citation_publication_date=2022-07;,citation_cover_date=2022-07;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01582;,citation_doi=10.22323/1.396.0508;,citation_conference_title=The 38th international symposium on lattice field theory;">
<meta name="citation_reference" content="citation_title=HMC with Normalizing Flows;,citation_author=Sam Foreman;,citation_author=Taku Izubuchi;,citation_author=Luchang Jin;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_author=Akio Tomiya;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01586;,citation_doi=10.22323/1.396.0073;,citation_volume=LATTICE2021;,citation_journal_title=PoS;">
<meta name="citation_reference" content="citation_title=Mastering language models;,citation_author=Samuel Montgomery;,citation_publication_date=2023-10;,citation_cover_date=2023-10;,citation_year=2023;,citation_fulltext_html_url=https://towardsdatascience.com/mastering-language-models-32e1d891511a
           ;,citation_journal_title=Medium;,citation_publisher=Towards Data Science;">
<meta name="citation_reference" content="citation_title=Harnessing the power of LLMs in practice: A survey on ChatGPT and beyond;,citation_author=Jingfeng Yang;,citation_author=Hongye Jin;,citation_author=Ruixiang Tang;,citation_author=Xiaotian Han;,citation_author=Qizhang Feng;,citation_author=Haoming Jiang;,citation_author=Bing Yin;,citation_author=Xia Hu;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2304.13712;">
<meta name="citation_reference" content="citation_title=Training tips for the transformer model;,citation_author=Martin Popel;,citation_author=OndÅ™ej Bojar;,citation_publication_date=2018-04;,citation_cover_date=2018-04;,citation_year=2018;,citation_fulltext_html_url=https://doi.org/10.2478%2Fpralin-2018-0002;,citation_issue=1;,citation_doi=10.2478/pralin-2018-0002;,citation_volume=110;,citation_journal_title=The Prague Bulletin of Mathematical Linguistics;,citation_publisher=Charles University in Prague, Karolinum Press;">
<meta name="citation_reference" content="citation_title=Attention is all you need;,citation_author=Ashish Vaswani;,citation_author=Noam Shazeer;,citation_author=Niki Parmar;,citation_author=Jakob Uszkoreit;,citation_author=Llion Jones;,citation_author=Aidan N. Gomez;,citation_author=Lukasz Kaiser;,citation_author=Illia Polosukhin;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_fulltext_html_url=https://arxiv.org/abs/1706.03762;">
<meta name="citation_reference" content="citation_title=Tree of thoughts: Deliberate problem solving with large language models;,citation_author=Shunyu Yao;,citation_author=Dian Yu;,citation_author=Jeffrey Zhao;,citation_author=Izhak Shafran;,citation_author=Thomas L. Griffiths;,citation_author=Yuan Cao;,citation_author=Karthik Narasimhan;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2305.10601;">
<meta name="citation_reference" content="citation_title=GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics;,citation_abstract=We seek to transform how new and emergent variants of pandemiccausing viruses, specifically SARS-CoV-2, are identified and classified. By adapting large language models (LLMs) for genomic data, we build genome-scale language models (GenSLMs) which can learn the evolutionary landscape of SARS-CoV-2 genomes. By pretraining on over 110 million prokaryotic gene sequences and finetuning a SARS-CoV-2-specific model on 1.5 million genomes, we show that GenSLMs can accurately and rapidly identify variants of concern. Thus, to our knowledge, GenSLMs represents one of the first whole genome scale foundation models which can generalize to other prediction tasks. We demonstrate scaling of GenSLMs on GPU-based supercomputers and AI-hardware accelerators utilizing 1.63 Zettaflops in training runs with a sustained performance of 121 PFLOPS in mixed precision and peak of 850 PFLOPS. We present initial scientific insights from examining GenSLMs in tracking evolutionary dynamics of SARS-CoV-2, paving the path to realizing this on large biological data.Competing Interest StatementThe authors have declared no competing interest.;,citation_author=Maxim Zvyagin;,citation_author=Alexander Brace;,citation_author=Kyle Hippe;,citation_author=Yuntian Deng;,citation_author=Bin Zhang;,citation_author=Cindy Orozco Bohorquez;,citation_author=Austin Clyde;,citation_author=Bharat Kale;,citation_author=Danilo Perez-Rivera;,citation_author=Heng Ma;,citation_author=Carla M. Mann;,citation_author=Michael Irvin;,citation_author=J. Gregory Pauloski;,citation_author=Logan Ward;,citation_author=Valerie Hayot-Sasson;,citation_author=Murali Emani;,citation_author=Sam Foreman;,citation_author=Zhen Xie;,citation_author=Diangen Lin;,citation_author=Maulik Shukla;,citation_author=Weili Nie;,citation_author=Josh Romero;,citation_author=Christian Dallago;,citation_author=Arash Vahdat;,citation_author=Chaowei Xiao;,citation_author=Thomas Gibbs;,citation_author=Ian Foster;,citation_author=James J. Davis;,citation_author=Michael E. Papka;,citation_author=Thomas Brettin;,citation_author=Rick Stevens;,citation_author=Anima Anandkumar;,citation_author=Venkatram Vishwanath;,citation_author=Arvind Ramanathan;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://www.biorxiv.org/content/early/2022/11/23/2022.10.10.511571;,citation_doi=10.1101/2022.10.10.511571;,citation_journal_title=bioRxiv;,citation_publisher=Cold Spring Harbor Laboratory;">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../assets/signature-navbar-orig.svg" alt="Sam Foreman" class="navbar-logo">
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../talks/index.html"> 
<span class="menu-text">talks</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts/index.html"> 
<span class="menu-text">posts</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-projects" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">projects</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-projects">    
        <li>
    <a class="dropdown-item" href="../../projects/index.qmd">
 <span class="dropdown-text">ğŸ“š All Projects</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://saforem2.github.io/ezpz">
 <span class="dropdown-text">ğŸ‹ <code>ezpz</code></span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://saforem2.github.io/l2hmc-qcd">
 <span class="dropdown-text">ğŸŸ¥ <code>l2hmc-qcd</code></span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/argonne-lcf/Megatron-DeepSpeed)">
 <span class="dropdown-text">ğŸ¤– <code>Megatron-DeepSpeed</code></span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://saforem2.github.io/wordplay">
 <span class="dropdown-text">ğŸ’¬ <code>wordplay</code> ğŸ®</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://www.alcf.anl.gov/alcf-ai-science-training-series?">
 <span class="dropdown-text">ğŸ“ <code>ai-science-training</code></span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://saforem2.github.io/enrich">
 <span class="dropdown-text">ğŸ’¸ <code>enrich</code></span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://saforem2.github.io/ambivalent">
 <span class="dropdown-text">ğŸ¤·ğŸ»â€â™‚ï¸<code>ambivalent</code></span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://saforem2.github.io/climate-analysis">
 <span class="dropdown-text">ğŸŒ <code>climate-analysis</code></span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/saforem2/glitz">
 <span class="dropdown-text">ğŸ¨ <code>glitz</code></span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/saforem2/personal_site">
 <span class="dropdown-text">ğŸ™‹ğŸ»<code>personal_site</code></span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/saforem2/notes-demo">
 <span class="dropdown-text">ğŸ—’ï¸ <code>Notes-Demo</code></span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/saforem2/personal_site"> 
<span class="menu-text"><span class="icon dim-text" style="font-size: 1.25rem;"><iconify-icon role="img" inline="" icon="ph:github-logo" aria-label="Icon github-logo from ph Iconify.design set." title="Icon github-logo from ph Iconify.design set."></iconify-icon></span></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../index.xml"> 
<span class="menu-text"><span class="icon dim-text" style="font-size: 1.25rem;"><iconify-icon role="img" inline="" icon="ph:rss" aria-label="Icon rss from ph Iconify.design set." title="Icon rss from ph Iconify.design set."></iconify-icon></span></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview">ğŸ‘€ Overview</a></li>
  <li><a href="#outline" id="toc-outline" class="nav-link" data-scroll-target="#outline">ğŸ“‘ Outline</a></li>
  <li><a href="#scaling-overview" id="toc-scaling-overview" class="nav-link" data-scroll-target="#scaling-overview">ğŸš€ Scaling: Overview</a></li>
  <li><a href="#training-on-a-single-device" id="toc-training-on-a-single-device" class="nav-link" data-scroll-target="#training-on-a-single-device">ğŸ¢ Training on a Single Device</a></li>
  <li><a href="#training-on-multiple-gpus-data-parallelism" id="toc-training-on-multiple-gpus-data-parallelism" class="nav-link" data-scroll-target="#training-on-multiple-gpus-data-parallelism">ğŸï¸ Training on Multiple GPUs: Data Parallelism</a>
  <ul class="collapse">
  <li><a href="#data-parallel-forward-pass" id="toc-data-parallel-forward-pass" class="nav-link" data-scroll-target="#data-parallel-forward-pass">Data Parallel: Forward Pass</a></li>
  <li><a href="#data-parallel-backward-pass" id="toc-data-parallel-backward-pass" class="nav-link" data-scroll-target="#data-parallel-backward-pass">Data Parallel: Backward Pass</a></li>
  <li><a href="#data-parallel-full-setup" id="toc-data-parallel-full-setup" class="nav-link" data-scroll-target="#data-parallel-full-setup">Data Parallel: Full Setup</a></li>
  <li><a href="#data-parallel-training" id="toc-data-parallel-training" class="nav-link" data-scroll-target="#data-parallel-training">Data Parallel: Training</a></li>
  </ul></li>
  <li><a href="#communication" id="toc-communication" class="nav-link" data-scroll-target="#communication">ğŸ—£ï¸ Communication</a>
  <ul class="collapse">
  <li><a href="#allreduce" id="toc-allreduce" class="nav-link" data-scroll-target="#allreduce">AllReduce</a></li>
  <li><a href="#reduce" id="toc-reduce" class="nav-link" data-scroll-target="#reduce">Reduce</a></li>
  <li><a href="#broadcast" id="toc-broadcast" class="nav-link" data-scroll-target="#broadcast">Broadcast</a></li>
  <li><a href="#allgather" id="toc-allgather" class="nav-link" data-scroll-target="#allgather">AllGather</a></li>
  <li><a href="#scatter" id="toc-scatter" class="nav-link" data-scroll-target="#scatter">Scatter</a></li>
  </ul></li>
  <li><a href="#why-distributed-training" id="toc-why-distributed-training" class="nav-link" data-scroll-target="#why-distributed-training">âš¡ Why Distributed Training?</a>
  <ul class="collapse">
  <li><a href="#why-distributed-training-speedup" id="toc-why-distributed-training-speedup" class="nav-link" data-scroll-target="#why-distributed-training-speedup">Why Distributed Training? Speedup!</a></li>
  <li><a href="#dealing-with-data" id="toc-dealing-with-data" class="nav-link" data-scroll-target="#dealing-with-data">Dealing with Data</a></li>
  <li><a href="#broadcast-initial-state" id="toc-broadcast-initial-state" class="nav-link" data-scroll-target="#broadcast-initial-state">Broadcast Initial State</a></li>
  <li><a href="#best-practices" id="toc-best-practices" class="nav-link" data-scroll-target="#best-practices">Best Practices</a></li>
  <li><a href="#going-beyond-data-parallelism" id="toc-going-beyond-data-parallelism" class="nav-link" data-scroll-target="#going-beyond-data-parallelism">Going Beyond Data Parallelism</a></li>
  <li><a href="#going-beyond-data-parallelism-deepspeed-zero" id="toc-going-beyond-data-parallelism-deepspeed-zero" class="nav-link" data-scroll-target="#going-beyond-data-parallelism-deepspeed-zero">Going beyond Data Parallelism: <iconify-icon role="img" inline="" icon="logos:microsoft-icon" aria-label="Icon microsoft-icon from logos Iconify.design set." title="Icon microsoft-icon from logos Iconify.design set."></iconify-icon> DeepSpeed + <code>ZeRO</code></a></li>
  <li><a href="#fully-sharded-data-parallel-pytorch-fsdp" id="toc-fully-sharded-data-parallel-pytorch-fsdp" class="nav-link" data-scroll-target="#fully-sharded-data-parallel-pytorch-fsdp">Fully Sharded Data Parallel: ğŸ”¥ PyTorch + <code>FSDP</code></a></li>
  </ul></li>
  <li><a href="#additional-parallelism-strategies" id="toc-additional-parallelism-strategies" class="nav-link" data-scroll-target="#additional-parallelism-strategies">ğŸ•¸ï¸ Additional Parallelism Strategies</a>
  <ul class="collapse">
  <li><a href="#pipeline-parallelism-pp" id="toc-pipeline-parallelism-pp" class="nav-link" data-scroll-target="#pipeline-parallelism-pp">Pipeline Parallelism (PP)</a></li>
  <li><a href="#tensor-parallel-tp" id="toc-tensor-parallel-tp" class="nav-link" data-scroll-target="#tensor-parallel-tp">Tensor Parallel (TP)</a></li>
  <li><a href="#tensor-parallel-tp-1" id="toc-tensor-parallel-tp-1" class="nav-link" data-scroll-target="#tensor-parallel-tp-1">Tensor Parallel (TP)</a></li>
  </ul></li>
  <li><a href="#tensor-model-parallel-training-example" id="toc-tensor-model-parallel-training-example" class="nav-link" data-scroll-target="#tensor-model-parallel-training-example">Tensor (/ Model) Parallel Training: Example</a>
  <ul class="collapse">
  <li><a href="#tensor-model-parallelismefficient-large-scale" id="toc-tensor-model-parallelismefficient-large-scale" class="nav-link" data-scroll-target="#tensor-model-parallelismefficient-large-scale">Tensor (Model) Parallelism</a></li>
  <li><a href="#tensor-parallelism" id="toc-tensor-parallelism" class="nav-link" data-scroll-target="#tensor-parallelism">Tensor Parallelism</a></li>
  <li><a href="#d-parallelism" id="toc-d-parallelism" class="nav-link" data-scroll-target="#d-parallelism">3D Parallelism</a></li>
  <li><a href="#deciding-on-a-parallelism-strategy" id="toc-deciding-on-a-parallelism-strategy" class="nav-link" data-scroll-target="#deciding-on-a-parallelism-strategy">Deciding on a Parallelism Strategy</a></li>
  </ul></li>
  <li><a href="#large-language-models" id="toc-large-language-models" class="nav-link" data-scroll-target="#large-language-models">ğŸ¦™ Large Language Models</a>
  <ul class="collapse">
  <li><a href="#emergent-abilities" id="toc-emergent-abilities" class="nav-link" data-scroll-target="#emergent-abilities">ğŸ”® Emergent Abilities</a></li>
  <li><a href="#training-llms" id="toc-training-llms" class="nav-link" data-scroll-target="#training-llms">ğŸš‚ Training LLMs</a></li>
  <li><a href="#life-cycle-of-the-llm" id="toc-life-cycle-of-the-llm" class="nav-link" data-scroll-target="#life-cycle-of-the-llm">â™»ï¸ Life-Cycle of the LLM</a></li>
  <li><a href="#life-cycle-of-the-llm-1" id="toc-life-cycle-of-the-llm-1" class="nav-link" data-scroll-target="#life-cycle-of-the-llm-1">ğŸ€ Life-Cycle of the LLM</a></li>
  <li><a href="#forward-pass" id="toc-forward-pass" class="nav-link" data-scroll-target="#forward-pass">â© Forward Pass</a></li>
  <li><a href="#generating-text" id="toc-generating-text" class="nav-link" data-scroll-target="#generating-text">ğŸ’¬ Generating Text</a></li>
  </ul></li>
  <li><a href="#hands-on" id="toc-hands-on" class="nav-link" data-scroll-target="#hands-on">ğŸ‘‹ Hands On</a>
  <ul class="collapse">
  <li><a href="#hands-on-getting-started" id="toc-hands-on-getting-started" class="nav-link" data-scroll-target="#hands-on-getting-started">ğŸ§‘â€ğŸ’» Hands On: Getting Started</a></li>
  <li><a href="#install-ezpz-wordplay" id="toc-install-ezpz-wordplay" class="nav-link" data-scroll-target="#install-ezpz-wordplay">ğŸ“¦ Install {<code>ezpz</code>, <code>wordplay</code>}</a></li>
  <li><a href="#ezpz-example-video" id="toc-ezpz-example-video" class="nav-link" data-scroll-target="#ezpz-example-video"><i class="fa-brands fa-github" aria-label="github"></i> <code>ezpz</code>: Example [video]</a></li>
  <li><a href="#install-wordplay" id="toc-install-wordplay" class="nav-link" data-scroll-target="#install-wordplay">Install <code>wordplay</code> ğŸ®ğŸ’¬</a></li>
  <li><a href="#prepare-data" id="toc-prepare-data" class="nav-link" data-scroll-target="#prepare-data">Prepare Data</a></li>
  <li><a href="#launch-training-ddp" id="toc-launch-training-ddp" class="nav-link" data-scroll-target="#launch-training-ddp">Launch Training (DDP)</a></li>
  <li><a href="#training-example-output" id="toc-training-example-output" class="nav-link" data-scroll-target="#training-example-output">Training: Example Output</a></li>
  <li><a href="#wordplay-example-video" id="toc-wordplay-example-video" class="nav-link" data-scroll-target="#wordplay-example-video"><i class="fa-brands fa-github" aria-label="github"></i> <code>wordplay</code>: Example [video]</a></li>
  </ul></li>
  <li><a href="#thank-you" id="toc-thank-you" class="nav-link" data-scroll-target="#thank-you">â¤ï¸ Thank you!</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">ğŸ““ References</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/saforem2/personal_site/blob/main/talks/ai-for-science-2024/index.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/saforem2/personal_site/edit/main/talks/ai-for-science-2024/index.qmd" class="toc-action"><i class="bi empty"></i>Edit this page</a></li><li><a href="https://github.com/saforem2/personal_site/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div><div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="ai-for-science-2024.md"><i class="bi bi-file-code"></i>Github (GFM)</a></li><li><a href="slides.html"><i class="bi bi-file-slides"></i>RevealJS</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-TC329HJ" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- 100% privacy-first analytics -->
<script data-collect-dnt="true" async="" src="https://scripts.simpleanalyticscdn.com/latest.js"></script>
<noscript><img src="https://queue.simpleanalyticscdn.com/noscript.gif?collect-dnt=true" alt="" referrerpolicy="no-referrer-when-downgrade"></noscript>

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Parallel Training Methods</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author"><a href="https://samforeman.me">Sam Foreman</a> <a href="mailto:foremans@anl.gov" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            <a href="https://alcf.anl.gov/about/people/sam-foreman">
            </a><a href="https://alcf.anl.gov/about/people/sam-foreman">ALCF</a>
            
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">November 5, 2024</p>
    </div>
  </div>
  
    <div>
    <div class="quarto-title-meta-heading">Modified</div>
    <div class="quarto-title-meta-contents">
      <p class="date-modified">February 3, 2025</p>
    </div>
  </div>
    
  </div>
  


</header>


<section id="overview" class="level2" data-background-color="white">
<h2 data-background-color="white" class="anchored" data-anchor-id="overview">ğŸ‘€ Overview</h2>
<ul>
<li>ğŸ“Š Slides @ <a href="https://samforeman.me/talks/ai-for-science-2024/slides">samforeman.me/talks/ai-for-science-2024/slides</a>
<ul>
<li>ğŸ“„ HTML version: <a href="https://samforeman.me/talks/ai-for-science-2024">samforeman.me/talks/ai-for-science-2024</a></li>
</ul></li>
<li><i class="fa-brands fa-github" aria-label="github"></i> <a href="https://github.com/argonne-lcf/ai-science-training-series">argonne-lcf/<code>ai-science-training-series</code></a>
<ul>
<li><a href="https://www.alcf.anl.gov/alcf-ai-science-training-series">Series Page</a></li>
</ul></li>
</ul>
</section>
<section id="outline" class="level2" data-background-color="white">
<h2 data-background-color="white" class="anchored" data-anchor-id="outline">ğŸ“‘ Outline</h2>
<ol type="1">
<li><a href="#scaling-overview">Scaling: Overview</a></li>
<li><a href="#data-parallel-training">Data Parallel Training</a>
<ol type="1">
<li><a href="#communication">Communication</a></li>
<li><a href="#why-distributed-training">Why Distributed Training?</a></li>
</ol></li>
<li><a href="#going-beyond-data-parallelism">Beyond Data Parallelism</a>
<ol type="1">
<li><a href="#additional-parallelism-strategies">Additional Parallelism Strategies</a></li>
</ol></li>
<li><a href="#large-language-models">Large Language Models</a></li>
<li><a href="#hands-on">Hands On</a></li>
</ol>
</section>
<section id="scaling-overview" class="level2" data-background-color="white">
<h2 data-background-color="white" class="anchored" data-anchor-id="scaling-overview">ğŸš€ Scaling: Overview</h2>
<ul>
<li>âœ… <strong>Goal</strong>:
<ul>
<li>Minimize: <span class="highlight-red">Cost</span> (i.e.&nbsp;amount of time spent training)</li>
<li>Maximize: <span class="highlight-blue">Performance</span></li>
</ul>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="ğŸ“‘ Note">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
ğŸ“‘ Note
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>See <a href="https://huggingface.co/docs/transformers/v4.46.0/performance">ğŸ¤— Performance and Scalability</a> for more details</p>
</div>
</div>
</div></li>
</ul>
</section>
<section id="training-on-a-single-device" class="level2" data-background-color="white">
<h2 data-background-color="white" class="anchored" data-anchor-id="training-on-a-single-device">ğŸ¢ Training on a Single Device</h2>
<ul>
<li>See <a href="https://huggingface.co/docs/transformers/v4.46.0/perf_train_gpu_one">ğŸ¤— Methods and tools for efficient training on a single GPU</a></li>
</ul>
<div id="fig-single-gpu-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-single-gpu-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR
    subgraph G0["`GPU0`"]
        subgraph N0["`Network`"]
        end
        L0("`Loss`")
    end
    subgraph D["`Data`"]
        x("`x0`")
        x1("`x1`")
        x2("`x2`")
    end
    x --&gt; N0
    N0 --&gt; L0
    L0 --&gt; N0
classDef block fill:#CCCCCC02,stroke:#838383,stroke-width:1px,color:#838383
classDef red fill:#ff8181,stroke:#333,stroke-width:1px,color:#000
classDef green fill:#98E6A5,stroke:#333,stroke-width:1px,color:#000
classDef blue fill:#7DCAFF,stroke:#333,stroke-width:1px,color:#000
classDef grey fill:#cccccc,stroke:#333,stroke-width:1px,color:#000
class x,L0 red
class x1, green
class x2, blue
class x3, grey
class N0,D,G0,n0 block
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-single-gpu-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: <strong>SLOW</strong> !! model size limited by GPU memory
</figcaption>
</figure>
</div>
</section>
<section id="training-on-multiple-gpus-data-parallelism" class="level2" data-background-color="white">
<h2 data-background-color="white" class="anchored" data-anchor-id="training-on-multiple-gpus-data-parallelism">ğŸï¸ Training on Multiple GPUs: Data Parallelism</h2>
<div id="fig-ddp-training-mermaid" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ddp-training-mermaid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR
    subgraph D["`Data`"]
        direction TB
        x("`xâ‚€`")
        x1("`xâ‚`")
        x2("`xâ‚‚`")
    end
    direction LR
    subgraph G0["`GPU0`"]
        direction LR
        subgraph N0["`NN`"]
        end
        %%y0("`yâ‚€`")
        L0["`Loss`"]
    end
    subgraph G1["`GPU1`"]
        direction LR
        subgraph N1["`NN`"]
        end
        L1["`Loss`"]
    end
    subgraph G2["`GPU2`"]
        direction LR
        subgraph N2["`NN`"]
        end
        L2["`Loss`"]
    end
    x --&gt; G0
    x1 --&gt; G1
    x2 --&gt; G2
    N0 --&gt; L0
    N1 --&gt; L1
    N2 --&gt; L2
classDef block fill:#CCCCCC02,stroke:#838383,stroke-width:1px,color:#838383
classDef text fill:#CCCCCC02,stroke:#838383,stroke-width:0px,color:#838383
classDef grey fill:#cccccc,stroke:#333,stroke-width:1px,color:#000
classDef red fill:#ff8181,stroke:#333,stroke-width:1px,color:#000
classDef orange fill:#FFC47F,stroke:#333,stroke-width:1px,color:#000
classDef yellow fill:#FFFF7F,stroke:#333,stroke-width:1px,color:#000
classDef green fill:#98E6A5,stroke:#333,stroke-width:1px,color:#000
classDef blue fill:#7DCAFF,stroke:#333,stroke-width:1px,color:#000
classDef purple fill:#FFCBE6,stroke:#333,stroke-width:1px,color:#000
class x,y0,L0 red
class x1,L1 green
class x2,L2 blue
class x3,ar grey
class D,N0,N1,N2,G0,G1,G2,GU block
class AR block
class bc text
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ddp-training-mermaid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Each GPU receives <strong>unique</strong> data at each step
</figcaption>
</figure>
</div>
<section id="data-parallel-forward-pass" class="level3" data-background-color="white">
<h3 data-background-color="white" class="anchored" data-anchor-id="data-parallel-forward-pass">Data Parallel: Forward Pass</h3>
<div id="fig-ddp-training-mermaid-allreduce" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ddp-training-mermaid-allreduce-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR
    subgraph D["`Data`"]
        direction TB
        %%xp("`xâ‚™â‚Šâ‚`")
        x("`xâ‚€`")
        x1("`xâ‚`")
        x2("`xâ‚‚`")
    end
    direction LR
    subgraph G0["`GPU0`"]
        direction LR
        subgraph N0["`NN`"]
        end
        %%y0("`yâ‚€`")
        L0["`Loss`"]
    end
    subgraph G1["`GPU1`"]
        direction LR
        subgraph N1["`NN`"]
        end
        L1["`Loss`"]
    end
    subgraph G2["`GPU2`"]
        direction LR
        subgraph N2["`NN`"]
        end
        L2["`Loss`"]
    end
    subgraph AR["`Average Grads`"]
        direction TB
        ar("`(1/n) âˆ‘ gâ‚™`")
    end
    x --&gt; G0
    x1 --&gt; G1
    x2 --&gt; G2
    N0 --&gt; L0
    N1 --&gt; L1
    N2 --&gt; L2
    G0 -.-&gt; AR
    G1 -.-&gt; AR
    G2 -.-&gt; AR
classDef block fill:#CCCCCC02,stroke:#838383,stroke-width:1px,color:#838383
classDef grey fill:#cccccc,stroke:#333,stroke-width:1px,color:#000
classDef red fill:#ff8181,stroke:#333,stroke-width:1px,color:#000
classDef orange fill:#FFC47F,stroke:#333,stroke-width:1px,color:#000
classDef yellow fill:#FFFF7F,stroke:#333,stroke-width:1px,color:#000
classDef green fill:#98E6A5,stroke:#333,stroke-width:1px,color:#000
classDef blue fill:#7DCAFF,stroke:#333,stroke-width:1px,color:#000
classDef purple fill:#FFCBE6,stroke:#333,stroke-width:1px,color:#000
classDef text fill:#CCCCCC02,stroke:#838383,stroke-width:0px,color:#838383
class x,y0,L0 red
class x1,L1 green
class x2,L2 blue
class x3,ar grey
class D,N0,N1,N2,G0,G1,G2,GU block
class AR block
class bc text
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ddp-training-mermaid-allreduce-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Average gradients across all GPUs
</figcaption>
</figure>
</div>
</section>
<section id="data-parallel-backward-pass" class="level3" data-background-color="white">
<h3 data-background-color="white" class="anchored" data-anchor-id="data-parallel-backward-pass">Data Parallel: Backward Pass</h3>
<div id="fig-ddp-backward-mermaid" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ddp-backward-mermaid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart RL
    subgraph D["`Data`"]
        direction TB
        x("`xâ‚€`")
        x1("`xâ‚`")
        x2("`xâ‚‚`")
    end
    subgraph G0["`GPU0`"]
        direction RL
        subgraph N0["`NN`"]
        end
        L0["`Loss`"]
    end
    subgraph G1["`GPU1`"]
        direction RL
        subgraph N1["`NN`"]
        end
        L1["`Loss`"]
    end
    subgraph G2["`GPU2`"]
        direction RL
        subgraph N2["`NN`"]
        end
        L2["`Loss`"]
    end
    subgraph BC["`Send Updates`"]
        direction TB
    end
    BC -.-&gt; G0
    BC -.-&gt; G1
    BC -.-&gt; G2
    L0 ~~~ N0
    L1 ~~~ N1
    L2 ~~~ N2
    G0 ~~~ x
    G1 ~~~ x1
    G2 ~~~ x2
classDef grey fill:#cccccc,stroke:#333,stroke-width:1px,color:#000
classDef block fill:#CCCCCC02,stroke:#838383,stroke-width:1px,color:#838383
classDef red fill:#ff8181,stroke:#333,stroke-width:1px,color:#000
classDef orange fill:#FFC47F,stroke:#333,stroke-width:1px,color:#000
classDef yellow fill:#FFFF7F,stroke:#333,stroke-width:1px,color:#000
classDef green fill:#98E6A5,stroke:#333,stroke-width:1px,color:#000
classDef blue fill:#7DCAFF,stroke:#333,stroke-width:1px,color:#000
classDef purple fill:#FFCBE6,stroke:#333,stroke-width:1px,color:#000
classDef text fill:#CCCCCC02,stroke:#838383,stroke-width:0px,color:#838383
class x,y0,L0 red
class x1,L1 green
class x2,L2 blue
class x3,ar grey
class D,N0,N1,N2,G0,G1,G2,GU block
class BC block
class bc text
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ddp-backward-mermaid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Send global updates back to each GPU
</figcaption>
</figure>
</div>
</section>
<section id="data-parallel-full-setup" class="level3" data-background-color="white">
<h3 data-background-color="white" class="anchored" data-anchor-id="data-parallel-full-setup">Data Parallel: Full Setup</h3>
<div id="fig-ddp-training" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ddp-training-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR
    subgraph D["`Data`"]
        direction TB
        x("`xâ‚€`")
        x1("`xâ‚`")
        x2("`xâ‚‚`")
    end
    direction LR
    subgraph G0["`GPU0`"]
        direction LR
        subgraph N0["`NN`"]
        end
        L0["`L0`"]
    end
    subgraph G1["`GPU1`"]
        direction LR
        subgraph N1["`NN`"]
        end
        L1["`L1`"]
    end
    subgraph G2["`GPU2`"]
        direction LR
        subgraph N2["`NN`"]
        end
        L2["`L2`"]
    end
    subgraph AR["`Average Grads`"]
        direction TB
        ar("`(1/n) âˆ‘ gâ‚™`")
        bc("`Update Weights`")
        ar --&gt; bc
    end
    x --&gt; G0
    x1 --&gt; G1
    x2 --&gt; G2
    N0 --&gt; L0
    N1 --&gt; L1
    N2 --&gt; L2
    G0 &lt;-.-&gt; AR
    G1 &lt;-.-&gt; AR
    G2 &lt;-.-&gt; AR
classDef block fill:#CCCCCC02,stroke:#838383,stroke-width:1px,color:#838383
classDef grey fill:#cccccc,stroke:#333,stroke-width:1px,color:#000
classDef red fill:#ff8181,stroke:#333,stroke-width:1px,color:#000
classDef orange fill:#FFC47F,stroke:#333,stroke-width:1px,color:#000
classDef yellow fill:#FFFF7F,stroke:#333,stroke-width:1px,color:#000
classDef green fill:#98E6A5,stroke:#333,stroke-width:1px,color:#000
classDef blue fill:#7DCAFF,stroke:#333,stroke-width:1px,color:#000
classDef purple fill:#FFCBE6,stroke:#333,stroke-width:1px,color:#000
classDef text fill:#CCCCCC02,stroke:#838383,stroke-width:0px,color:#838383
class x,y0,L0 red
class x1,L1 green
class x2,L2 blue
class x3,ar grey
class D,N0,N1,N2,G0,G1,G2,GU block
class AR block
class bc text
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ddp-training-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: See: <a href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html">PyTorch / Distributed Data Parallel</a>
</figcaption>
</figure>
</div>
</section>
<section id="data-parallel-training" class="level3" data-background-color="white">
<h3 data-background-color="white" class="anchored" data-anchor-id="data-parallel-training">Data Parallel: Training</h3>
<ul>
<li>Each GPU:
<ul>
<li>has <strong>identical copy</strong> of model</li>
<li>works on a <strong>unique</strong> subset of data</li>
</ul></li>
<li>Easy to get started (minor modifications to code):
<ul>
<li><i class="fa-brands fa-github" aria-label="github"></i> <a href="https://github.com/saforem2/ezpz">saforem2/<code>ezpz</code></a></li>
<li>ğŸ”¥ <a href="https://pytorch.org/docs/stable/notes/ddp.html">PyTorch / <code>DDP</code></a></li>
<li>ğŸ¤— <a href="https://huggingface.co/docs/transformers/accelerate">HF / <code>Accelerate</code></a></li>
<li><iconify-icon role="img" inline="" icon="logos:microsoft-icon" aria-label="Icon microsoft-icon from logos Iconify.design set." title="Icon microsoft-icon from logos Iconify.design set."></iconify-icon> <a href="https://www.deepspeed.ai/">Microsoft / <code>DeepSpeed</code></a></li>
</ul></li>
<li>Requires <strong>global</strong> communication
<ul>
<li>every rank <em>must participate</em> (collective communication) !!</li>
</ul></li>
</ul>
</section>
</section>
<section id="communication" class="level2" data-background-color="white">
<h2 data-background-color="white" class="anchored" data-anchor-id="communication">ğŸ—£ï¸ Communication</h2>
<ul>
<li>Need mechanism(s) for communicating across GPUs:
<ul>
<li><a href="https://mpi4py.readthedocs.io/en/stable/tutorial.html"><code>mpi4py</code></a></li>
<li><a href="https://pytorch.org/docs/stable/distributed.html"><code>torch.distributed</code></a></li>
</ul></li>
<li>Collective Communication:
<ul>
<li><a href="https://developer.nvidia.com/nccl">Nvidia Collective Communications Library (NCCL)</a></li>
<li><a href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/oneccl.html#gs.gouznn">Intel oneAPI Collective Communications Library (oneCCL)</a></li>
</ul>
<div class="callout callout-style-simple callout-warning no-icon callout-titled" title="âŒ› Timeouts">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
âŒ› Timeouts
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<ul>
<li>Collective operations have to be called for each <code>rank</code> to form a complete collective operation.
<ul>
<li>Failure to do so will result in other ranks waiting <strong>indefinitely</strong></li>
</ul></li>
</ul>
</div>
</div>
</div></li>
</ul>
<section id="allreduce" class="level3" data-background-color="white">
<h3 data-background-color="white" class="anchored" data-anchor-id="allreduce">AllReduce</h3>
<p>Perform <em>reductions</em> on data (e.g.&nbsp;<code>sum</code>, <code>min</code>, <code>max</code>) across ranks, send result back to everyone.</p>
<div id="fig-all-reduce-mermaid" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-all-reduce-mermaid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart TD
  subgraph R0["`Rank 0`"]
    x0("`x0`")
  end
  subgraph R1["`Rank 1`"]
    x1("`x1`")
  end
  subgraph R2["`Rank 2`"]
    x2("`x2`")
  end
  subgraph R3["`Rank 3`"]
    x3("`x3`")
  end
  subgraph AR["`Allreduce`"]
    xp["`x' = âˆ‘ xâ‚™ `"]
  end
  subgraph AR3["`Rank 3`"]
    xp3("`x'`")
  end
  subgraph AR2["`Rank 2`"]
    xp2("`x'`")
  end
  subgraph AR1["`Rank 1`"]
    xp1("`x'`")
  end
  subgraph AR0["`Rank 0`"]
    xp0("`x'`")
  end
  x0 --&gt; AR
  x1 --&gt; AR
  x2 --&gt; AR
  x3 --&gt; AR
  AR --&gt; xp0
  AR --&gt; xp1
  AR --&gt; xp2
  AR --&gt; xp3
classDef block fill:#CCCCCC02,stroke:#838383,stroke-width:1px,color:#838383
classDef red fill:#ff8181,stroke:#333,stroke-width:1px,color:#000
classDef orange fill:#FFC47F,stroke:#333,stroke-width:1px,color:#000
classDef yellow fill:#FFFF7F,stroke:#333,stroke-width:1px,color:#000
classDef green fill:#98E6A5,stroke:#333,stroke-width:1px,color:#000
classDef pink fill:#E599F7,stroke:#333,stroke-width:1px,color:#000
classDef blue fill:#7DCAFF,stroke:#333,stroke-width:1px,color:#000
classDef purple fill:#FFCBE6,stroke:#333,stroke-width:1px,color:#000
class R0,R1,R2,R3,AR,AR0,AR1,AR2,AR3 block
class xp,xp0,xp1,xp2,xp3, purple
class x0, red
class x1, green
class x2, blue
class x3, yellow
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-all-reduce-mermaid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: All-Reduce operation: each rank receives the reduction of input values across ranks.
</figcaption>
</figure>
</div>
</section>
<section id="reduce" class="level3" data-background-color="white">
<h3 data-background-color="white" class="anchored" data-anchor-id="reduce">Reduce</h3>
<ul>
<li>Perform a <em>reduction</em> on data across ranks, send to individual</li>
</ul>
<div id="fig-reduce-mermaid" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-reduce-mermaid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart TD
  subgraph R0["`Rank 0`"]
    x0("`x0`")
  end
  subgraph R1["`Rank 1`"]
    x1("`x1`")
  end
  subgraph R2["`Rank 2`"]
    x2("`x2`")
  end
  subgraph R3["`Rank 3`"]
    x3("`x3`")
  end
  subgraph AR["`Reduce`"]
    xp["`x'=reduce(x, 2, SUM)`"]
  end
  subgraph AR3["`Rank 3`"]
  end
  subgraph AR2["`Rank 2`"]
    xp2("`x'`")
  end
  subgraph AR1["`Rank 1`"]
  end
  subgraph AR0["`Rank 0`"]
  end
  x0 --&gt; AR
  x1 --&gt; AR
  x2 --&gt; AR
  x3 --&gt; AR
  AR --&gt; AR3
  AR --&gt; xp2
  AR --&gt; AR1
  AR --&gt; AR0
classDef block fill:#CCCCCC02,stroke:#838383,stroke-width:1px,color:#838383
classDef red fill:#ff8181,stroke:#333,stroke-width:1px,color:#000
classDef orange fill:#FFC47F,stroke:#333,stroke-width:1px,color:#000
classDef green fill:#98E6A5,stroke:#333,stroke-width:1px,color:#000
classDef yellow fill:#FFFF7F,stroke:#333,stroke-width:1px,color:#000
classDef blue fill:#7DCAFF,stroke:#333,stroke-width:1px,color:#000
classDef purple fill:#FFCBE6,stroke:#333,stroke-width:1px,color:#000
classDef pink fill:#E599F7,stroke:#333,stroke-width:1px,color:#000
class R0,R1,R2,R3,AR,AR0,AR1,AR2,AR3, block
class xp,xp2 purple
class x0, red
class x1, green
class x2, blue
class x3, yellow
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-reduce-mermaid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Reduce operation: one rank receives the reduction of input values across ranks
</figcaption>
</figure>
</div>
</section>
<section id="broadcast" class="level3" data-background-color="white">
<h3 data-background-color="white" class="anchored" data-anchor-id="broadcast">Broadcast</h3>
<div id="fig-broadcast-mermaid" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-broadcast-mermaid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart TD
  subgraph R3["`Rank 3`"]
  end
  subgraph R2["`Rank 2`"]
    x2("`x2`")
  end
  subgraph R1["`Rank 1`"]
  end
  subgraph R0["`Rank 0`"]
  end
  subgraph AR["` `"]
    xp["`broadcast(x2, 2)`"]
  end
  subgraph AR0["`Rank 0`"]
    xp0("`x2`")
  end
  subgraph AR1["`Rank 1`"]
    xp1("`x2`")
  end
  subgraph AR2["`Rank 2`"]
    xp2("`x2`")
  end
  subgraph AR3["`Rank 3`"]
    xp3("`x2`")
  end
  x2 --&gt; AR
  AR --&gt; AR0
  AR --&gt; AR1
  AR --&gt; AR2
  AR --&gt; AR3
classDef text fill:#CCCCCC02,stroke:#838383,stroke-width:0px,color:#838383,font-weight:500
classDef block fill:#CCCCCC02,stroke:#838383,stroke-width:1px,font-weight:500,color:#838383
classDef blue fill:#7DCAFF,stroke:#333,stroke-width:1px,color:#000
classDef yellow fill:#FFFF7F,stroke:#333,stroke-width:1px,color:#000
class R0,R1,R2,R3,AR0,AR1,AR2,AR3,AR, block
class x2,xp0,xp1,xp2,xp3 blue
class xp, text
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-broadcast-mermaid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: <code>broadcast</code> (send) a tensor <code><span class="math inline">x</span></code> from one rank to all ranks
</figcaption>
</figure>
</div>
</section>
<section id="allgather" class="level3" data-background-color="white">
<h3 data-background-color="white" class="anchored" data-anchor-id="allgather">AllGather</h3>
<div id="fig-allgather-mermaid" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-allgather-mermaid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR
  subgraph R0["`Rank 0`"]
    x0("`x0`")
  end
  subgraph R1["`Rank 1`"]
    x1("`x1`")
  end
  subgraph R2["`Rank 2`"]
    x2("`x2`")
  end
  subgraph AG["`Allgather`"]
    %%xp0["`z=[empty_like(x) for _ in range(4)]`"]
    %%xp1["`dist.all_gather(z, x)`"]
  end
  subgraph AG2["`Rank 2`"]
    direction TB
    xp02("`x0`")
    xp12("`x1`")
    xp22("`x2`")
  end
  subgraph AG1["`Rank 1`"]
    direction TB
    xp01("`x0`")
    xp11("`x1`")
    xp21("`x2`")
  end
  subgraph AG0["`Rank 0`"]
    direction TB
    xp00("`x0`")
    xp10("`x1`")
    xp20("`x2`")
  end
  x0 --&gt; AG
  x1 --&gt; AG
  x2 --&gt; AG
  AG --&gt; AG0
  AG --&gt; AG1
  AG --&gt; AG2
classDef red fill:#ff8181,stroke:#333,stroke-width:1px,color:#000
classDef orange fill:#FFC47F,stroke:#333,stroke-width:1px,color:#000
classDef yellow fill:#FFFF7F,stroke:#333,stroke-width:1px,color:#000
classDef green fill:#98E6A5,stroke:#333,stroke-width:1px,color:#000
classDef blue fill:#7DCAFF,stroke:#333,stroke-width:1px,color:#000
classDef purple fill:#FFCBE6,stroke:#333,stroke-width:1px,color:#000
classDef block fill:#CCCCCC02,stroke:#838383,stroke-width:1px,font-weight:500,color:#838383
classDef text fill:#CCCCCC02,stroke:#838383,stroke-width:0px,color:#838383
class xp0,xp1, text
class AG0,AG1,AG2,AG3,AG,R0,R1,R2,R3, block
class xp00,xp01,xp02,xp03, red
class xp10,xp11,xp12,xp13, green
class xp20,xp21,xp22,xp23, blue
class xp30,xp31,xp32,xp33, yellow
class x0, red
class x1, green
class x2, blue
class x3, yellow
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-allgather-mermaid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: Gathers tensors from the whole group in a list.
</figcaption>
</figure>
</div>
</section>
<section id="scatter" class="level3" data-background-color="white">
<h3 data-background-color="white" class="anchored" data-anchor-id="scatter">Scatter</h3>
<div id="fig-scatter-mermaid" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-scatter-mermaid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart TD
  subgraph R3["`Rank 3`"]
  end
  subgraph R2["`Rank 2`"]
  end
  subgraph R1["`Rank 1`"]
    direction TB
    xp0("`x0`")
    xp1("`x1`")
    xp2("`x2`")
    xp3("`x3`")
  end
  subgraph R0["`Rank 0`"]
  end
  subgraph S["`Scatter`"]
  end
  subgraph S3["`Rank 3`"]
    x3("`x3`")
  end
  subgraph S2["`Rank 2`"]
    x2("`x2`")
  end
  subgraph S1["`Rank 1`"]
    x1("`x1`")
  end
  subgraph S0["`Rank 0`"]
    x0("`x0`")
  end
  R1 --&gt; S
  S --&gt; S0
  S --&gt; S1
  S --&gt; S2
  S --&gt; S3
classDef red fill:#ff8181,stroke:#333,stroke-width:1px,color:#000
classDef orange fill:#FFC47F,stroke:#333,stroke-width:1px,color:#000
classDef yellow fill:#FFFF7F,stroke:#333,stroke-width:1px,color:#000
classDef green fill:#98E6A5,stroke:#333,stroke-width:1px,color:#000
classDef blue fill:#7DCAFF,stroke:#333,stroke-width:1px,color:#000
classDef purple fill:#FFCBE6,stroke:#333,stroke-width:1px,color:#000
classDef block fill:#CCCCCC02,stroke:#838383,stroke-width:1px,font-weight:500,color:#838383
class AG0,AG1,AG2,AG3,S,R0,R1,R2,R3,S0,S1,S2,S3, block
class x0,xp0, red
class x1,xp1, green
class x2,xp2, blue
class x3,xp3, yellow
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-scatter-mermaid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: Scatters a list of tensors to the whole group
</figcaption>
</figure>
</div>
</section>
</section>
<section id="why-distributed-training" class="level2" data-background-color="white">
<h2 data-background-color="white" class="anchored" data-anchor-id="why-distributed-training">âš¡ Why Distributed Training?</h2>
<ul>
<li><code>N</code> workers each processing unique batch<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> of data:
<ul>
<li>[<code>micro_batch_size = 1</code>] <span class="math inline">\times</span> [<code>N</code> GPUs] <span class="math inline">\rightarrow</span> [<b><code>global_batch_size = N</code></b>]</li>
</ul></li>
<li>Improved gradient estimators
<ul>
<li>Smooth loss landscape</li>
<li>Less iterations needed for same number of epochs
<ul>
<li>common to scale learning rate <code>lr *= sqrt(N)</code></li>
</ul></li>
</ul></li>
<li>See: <a href="https://arxiv.org/abs/1708.03888">Large Batch Training of Convolutional Networks</a></li>
</ul>
<section id="why-distributed-training-speedup" class="level3" data-background-color="white">
<h3 data-background-color="white" class="anchored" data-anchor-id="why-distributed-training-speedup">Why Distributed Training? Speedup!</h3>
<div id="tbl-recent-progress" class="responsive striped hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-recent-progress-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Recent progress
</figcaption>
<div aria-describedby="tbl-recent-progress-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="table-responsive">
<table class="table-striped table-hover caption-top table">
<colgroup>
<col style="width: 11%">
<col style="width: 14%">
<col style="width: 8%">
<col style="width: 17%">
<col style="width: 14%">
<col style="width: 13%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Year</th>
<th style="text-align: center;">Author</th>
<th style="text-align: center;">GPU</th>
<th style="text-align: center;">Batch Size</th>
<th style="text-align: center;"># GPU</th>
<th style="text-align: center;">TIME (s)</th>
<th style="text-align: center;">ACC</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">2016</td>
<td style="text-align: center;">He</td>
<td style="text-align: center;">P100</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;"><span class="red-bg">8</span></td>
<td style="text-align: center;"><span class="red-bg">104,400</span></td>
<td style="text-align: center;">75.30%</td>
</tr>
<tr class="even">
<td style="text-align: center;">2019</td>
<td style="text-align: center;">Yamazaki</td>
<td style="text-align: center;">V100</td>
<td style="text-align: center;">81,920</td>
<td style="text-align: center;"><span class="blue-bg">2048</span></td>
<td style="text-align: center;"><span class="blue-bg">72</span></td>
<td style="text-align: center;">75.08%</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</section>
<section id="dealing-with-data" class="level3" data-background-color="white">
<h3 data-background-color="white" class="anchored" data-anchor-id="dealing-with-data">Dealing with Data</h3>
<ul>
<li>At each training step, we want to ensure that <strong>each worker receives unique data</strong></li>
<li>This can be done in one of two ways:
<ol type="1">
<li>Manually partition data (ahead of time)
<ul>
<li>Assign <strong>unique subsets</strong> to each worker</li>
<li>Each worker can only see their local portion of the data</li>
<li>Most common approach</li>
</ul></li>
<li>From each worker, randomly select a mini-batch
<ul>
<li>Each worker can see the full dataset</li>
<li>âš ï¸ When randomly selecting, it is important that each worker uses different seeds to ensure they receive unique data</li>
</ul></li>
</ol></li>
</ul>
</section>
<section id="broadcast-initial-state" class="level3" data-background-color="white">
<h3 data-background-color="white" class="anchored" data-anchor-id="broadcast-initial-state">Broadcast Initial State</h3>
<ul>
<li>At the start of training (or when loading from a checkpoint), we want all of our workers to be initialized consistently
<ul>
<li><strong>Broadcast</strong> the model and optimizer states from <code>rank() == 0</code> worker</li>
</ul></li>
</ul>
<div id="fig-broadcast" class="r-stretch quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-broadcast-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart TD
  0["GPU0"] --&gt; 1["GPU 1"]
  CKPT --&gt; 0
  0 --&gt; 2["GPU 2"]
  0 --Model + Optim. State--&gt;3["GPU 3"]
  0 --&gt; X["`...`"]
  0 --&gt; N["GPU N"]
classDef text fill:#CCCCCC02,stroke:#838383,stroke-width:0px,color:#838383,font-weight:500
classDef block fill:#CCCCCC02,stroke:#838383,stroke-width:1px,font-weight:500,color:#838383
class 0,1,2,3,N,X,CKPT block
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-broadcast-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11: To ensure all workers have the same copies, we load on <code>RANK==0</code> and <code>broadcast</code>
</figcaption>
</figure>
</div>
</section>
<section id="best-practices" class="level3" data-background-color="white">
<h3 data-background-color="white" class="anchored" data-anchor-id="best-practices">Best Practices</h3>
<div class="callout callout-style-simple callout-important no-icon callout-titled" title="â° Keeping things in Sync">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
â° Keeping things in Sync
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p><strong>Computation stalls during communication !!</strong></p>
<p>Keeping the communication to computation ratio small is important for effective scaling.</p>
</div>
</div>
</div>
<div class="flex-container">
<div class="column" style="width:50%;">
<ul>
<li>Use parallel IO whenever possible
<ul>
<li>Feed each rank from different files</li>
<li>Use MPI IO to have each rank read its own batch from a file</li>
<li>Use several ranks to read data, MPI to scatter to remaining ranks
<ul>
<li>Most practical in big <em>at-scale</em> training</li>
</ul></li>
</ul></li>
</ul>
</div>
<div class="column" style="width:50%;">
<ul>
<li>Take advantage of data storage
<ul>
<li>Use <a href="https://wiki.lustre.org/Configuring_Lustre_File_Striping">striping on lustre</a></li>
</ul></li>
<li>Use the right optimizations for Aurora, Polaris, etc.</li>
<li>Preload data when possible
<ul>
<li>Offloading to a GPU frees CPU cycles for loading the next batch of data
<ul>
<li><strong>minimize IO latency this way</strong></li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
</section>
<section id="going-beyond-data-parallelism" class="level3" data-background-color="white">
<h3 data-background-color="white" class="anchored" data-anchor-id="going-beyond-data-parallelism">Going Beyond Data Parallelism</h3>
<ul>
<li>âœ… Useful when model fits on single GPU:
<ul>
<li>ultimately <strong>limited by GPU memory</strong></li>
<li>model performance limited by size</li>
</ul></li>
<li>âš ï¸ When model does not fit on a single GPU:
<ul>
<li>Offloading (can only get you so farâ€¦):
<ul>
<li><iconify-icon role="img" inline="" icon="logos:microsoft-icon" aria-label="Icon microsoft-icon from logos Iconify.design set." title="Icon microsoft-icon from logos Iconify.design set."></iconify-icon> <a href="https://www.deepspeed.ai/tutorials/zero/">DeepSpeed + <code>ZeRO</code></a></li>
<li>ğŸ”¥ <a href="https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/">PyTorch + <code>FSDP</code></a></li>
</ul></li>
<li>Otherwise, resort to <a href="#additional-parallelism-strategies">model parallelism strategies</a></li>
</ul></li>
</ul>
</section>
<section id="going-beyond-data-parallelism-deepspeed-zero" class="level3" data-background-color="white">
<h3 data-background-color="white" class="anchored" data-anchor-id="going-beyond-data-parallelism-deepspeed-zero">Going beyond Data Parallelism: <iconify-icon role="img" inline="" icon="logos:microsoft-icon" aria-label="Icon microsoft-icon from logos Iconify.design set." title="Icon microsoft-icon from logos Iconify.design set."></iconify-icon> DeepSpeed + <code>ZeRO</code></h3>
<ul>
<li>Depending on the <code>ZeRO</code> stage (1, 2, 3), we can offload:
<ol type="1">
<li><strong>Stage 1</strong>: optimizer states <span class="math inline">\left(P_{\mathrm{os}}\right)</span></li>
<li><strong>Stage 2</strong>: gradients + opt. states <span class="math inline">\left(P_{\mathrm{os}+\mathrm{g}}\right)</span></li>
<li><strong>Stage 3</strong>: model params + grads + opt. states <span class="math inline">\left(P_{\mathrm{os}+\mathrm{g}+\mathrm{p}}\right)</span></li>
</ol></li>
</ul>
<div id="fig-zero" class="r-stretch quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-zero-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./assets/zero.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;12:  DeepSpeed + ZeRO"><img src="./assets/zero.png" class="r-stretch img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-zero-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12: <iconify-icon role="img" inline="" icon="logos:microsoft-icon" aria-label="Icon microsoft-icon from logos Iconify.design set." title="Icon microsoft-icon from logos Iconify.design set."></iconify-icon> <a href="deepspeed.ai">DeepSpeed</a> + <a href="https://www.deepspeed.ai/tutorials/zero-offload/"><code>ZeRO</code></a>
</figcaption>
</figure>
</div>
</section>
<section id="fully-sharded-data-parallel-pytorch-fsdp" class="level3 smaller" data-background-color="white">
<h3 class="smaller anchored" data-background-color="white" data-anchor-id="fully-sharded-data-parallel-pytorch-fsdp">Fully Sharded Data Parallel: ğŸ”¥ PyTorch + <code>FSDP</code></h3>
<ul>
<li>Instead of maintaining per-GPU copy of <code>{params, grads, opt_states}</code>, FSDP shards (distributes) these across data-parallel workers
<ul>
<li>can optionally offload the sharded model params to CPU</li>
</ul></li>
<li><a href="https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/">Introducing PyTorch Fully Sharded Data Parallel (FSDP) API | PyTorch</a></li>
</ul>
<div id="fig-fsdp" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-fsdp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="assets/fsdp.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;13: FSDP Workflow. Source"><img src="assets/fsdp.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-fsdp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13: FSDP Workflow. <a href="https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/">Source</a>
</figcaption>
</figure>
</div>
</section>
</section>
<section id="additional-parallelism-strategies" class="level2" data-background-color="white">
<h2 data-background-color="white" class="anchored" data-anchor-id="additional-parallelism-strategies">ğŸ•¸ï¸ Additional Parallelism Strategies</h2>
<ul>
<li><strong>Tensor (/ Model) Parallelism</strong> (<code>TP</code>):
<ul>
<li>ğŸ¤— <a href="https://huggingface.co/docs/text-generation-inference/en/conceptual/tensor_parallelism">Tensor Parallelism</a></li>
<li>ğŸ”¥ <a href="https://pytorch.org/tutorials/intermediate/TP_tutorial.html">Large Scale Transformer model training with Tensor Parallel (TP)</a></li>
</ul></li>
<li><strong>Pipeline Parallelism</strong> (<code>PP</code>):
<ul>
<li>ğŸ”¥ <a href="https://pytorch.org/docs/main/distributed.pipelining.html">PyTorch</a>, <iconify-icon role="img" inline="" icon="logos:microsoft-icon" aria-label="Icon microsoft-icon from logos Iconify.design set." title="Icon microsoft-icon from logos Iconify.design set."></iconify-icon> <a href="https://deepspeed.readthedocs.io/en/latest/pipeline.html">DeepSpeed</a></li>
</ul></li>
<li><strong>Sequence Parallelism</strong> (<code>SP</code>):
<ul>
<li><iconify-icon role="img" inline="" icon="logos:microsoft-icon" aria-label="Icon microsoft-icon from logos Iconify.design set." title="Icon microsoft-icon from logos Iconify.design set."></iconify-icon> <a href="https://github.com/microsoft/DeepSpeed/blob/master/blogs/deepspeed-ulysses/README.md">DeepSpeed Ulysses</a></li>
<li><a href="https://docs.nvidia.com/megatron-core/developer-guide/latest/api-guide/context_parallel.html">Megatron / Context Parallelism</a></li>
<li><a href="https://arxiv.org/abs/2405.07719v3">Unified Sequence Parallel (USP)</a>
<ul>
<li><i class="fa-brands fa-github" aria-label="github"></i> <a href="https://github.com/feifeibear/long-context-attention">feifeibear/<code>long-context-attention</code></a></li>
</ul></li>
</ul></li>
<li><label><input type="checkbox" checked=""><i class="fa-brands fa-github" aria-label="github"></i> <a href="https://github.com/argonne-lcf/Megatron-DeepSpeed">argonne-lcf/<code>Megatron-DeepSpeed</code></a></label>
<ul>
<li>Supports 4D Parallelism (<code>DP</code> + <code>TP</code> + <code>PP</code> + <code>SP</code>)</li>
</ul></li>
</ul>
<section id="pipeline-parallelism-pp" class="level3" data-background-color="white">
<h3 data-background-color="white" class="anchored" data-anchor-id="pipeline-parallelism-pp">Pipeline Parallelism (PP)</h3>
<div class="flex-container" style="place-content: end space-evenly;">
<div class="column" style="width:60%;">
<ul>
<li>Model is split up <strong>vertically</strong> (layer-level) across multiple GPUs</li>
<li>Each GPU:
<ul>
<li>has a portion of the full model</li>
<li>processes <em>in parallel</em> different stages of the pipeline (on a small chunk of the batch)</li>
</ul></li>
<li>See:
<ul>
<li>ğŸ”¥ <a href="https://pytorch.org/docs/main/distributed.pipelining.html">PyTorch / Pipeline Parallelism</a></li>
<li><iconify-icon role="img" inline="" icon="logos:microsoft-icon" aria-label="Icon microsoft-icon from logos Iconify.design set." title="Icon microsoft-icon from logos Iconify.design set."></iconify-icon> <a href="https://deepspeed.readthedocs.io/en/latest/pipeline.html">DeepSpeed / Pipeline Parallelism</a></li>
</ul></li>
</ul>
</div>
<div class="column" style="width:40%;">
<div id="fig-pipeline-parallelism" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pipeline-parallelism-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart TB
    subgraph G0["`GPU 0`"]
        direction LR
        a0("`Layer 0`")
        b0("`Layer 1`")
    end
    subgraph G1["`GPU 1`"]
        direction LR
        a1("`Layer 2`")
        b1("`Layer 3`")
    end
    a0 -.-&gt; b0
    b0 --&gt; a1
    a1 -.-&gt; b1
classDef block fill:#CCCCCC02,stroke:#838383,stroke-width:1px,color:#838383
classDef red fill:#ff8181,stroke:#333,stroke-width:1px,color:#000
classDef orange fill:#FFC47F,stroke:#333,stroke-width:1px,color:#000
classDef yellow fill:#FFFF7F,stroke:#333,stroke-width:1px,color:#000
classDef green fill:#98E6A5,stroke:#333,stroke-width:1px,color:#000
classDef blue fill:#7DCAFF,stroke:#333,stroke-width:1px,color:#000
classDef purple fill:#FFCBE6,stroke:#333,stroke-width:1px,color:#000
class G0,G1, block
class a0, red
class b0, green
class a1, blue
class b1, yellow
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pipeline-parallelism-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14: Pipeline Parallelism
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="tensor-parallel-tp" class="level3" data-background-color="white">
<h3 data-background-color="white" class="anchored" data-anchor-id="tensor-parallel-tp">Tensor Parallel (TP)</h3>
<div>

</div>
<div class="quarto-layout-panel" data-layout="[50,50]">
<div class="quarto-layout-row">
<div class="column quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<ul>
<li>Each tensor is split up into multiple chunks</li>
<li>Each shard of the tensor resides on its designated GPU</li>
<li>During processing each shard gets processed separately (and in parallel) on different GPUs
<ul>
<li>synced at the end of the step</li>
</ul></li>
<li>See: <a href="https://huggingface.co/docs/transformers/v4.15.0/parallelism">ğŸ¤— Model Parallelism</a> for additional details</li>
</ul>
</div>
<div class="column quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-model-parallel-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-model-parallel-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR
   subgraph G0["`GPU0`"]
    direction TB
    a0("`Layer 0`")
    b0("`Layer 1`")
    c0("`Layer 2`")
    d0("`Layer 3`")
   end
   subgraph G1["`GPU1`"]
    direction TB
    a1("`Layer 0`")
    b1("`Layer 1`")
    c1("`Layer 2`")
    d1("`Layer 3`")
   end
   a0 &lt;-.-&gt; a1
   b0 &lt;-.-&gt; b1
   c0 &lt;-.-&gt; c1
   d0 &lt;-.-&gt; d1
classDef red fill:#ff8181,stroke:#333,stroke-width:1px,color:#000
classDef orange fill:#FFC47F,stroke:#333,stroke-width:1px,color:#000
classDef yellow fill:#FFFF7F,stroke:#333,stroke-width:1px,color:#000
classDef green fill:#98E6A5,stroke:#333,stroke-width:1px,color:#000
classDef blue fill:#7DCAFF,stroke:#333,stroke-width:1px,color:#000
classDef purple fill:#FFCBE6,stroke:#333,stroke-width:1px,color:#000
classDef block fill:#CCCCCC02,stroke:#838383,stroke-width:1px,color:#838383
class G0,G1, block
class a0,a1 red
class b0,b1 green
class c0,c1 blue
class d0,d1 yellow
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-model-parallel-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15: Tensor Parallel Training
</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="tensor-parallel-tp-1" class="level3" data-background-color="white">
<h3 data-background-color="white" class="anchored" data-anchor-id="tensor-parallel-tp-1">Tensor Parallel (TP)</h3>
<div>

</div>
<div class="quarto-layout-panel" data-layout="[50,50]">
<div class="quarto-layout-row">
<div class="column quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<ul>
<li>Suitable when the model is too large to fit onto a single device (CPU / GPU)</li>
<li>Typically <strong>more complicated</strong> to implement than data parallel training
<ul>
<li>This is what one may call <em>horizontal parallelism</em></li>
<li>Communication whenever dataflow between two subsets</li>
</ul></li>
<li><i class="fa-brands fa-github" aria-label="github"></i> <a href="https://github.com/argonne-lcf/Megatron-DeepSpeed"><code>argonne-lcf/Megatron-DeepSpeed</code></a></li>
<li>ğŸ¤— <a href="https://github.com/huggingface/nanotron"><code>huggingface/nanotron</code></a></li>
</ul>
</div>
<div class="column quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-model-parallel-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-model-parallel-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR
   subgraph G0["`GPU0`"]
    direction TB
    a0("`Layer 0`")
    b0("`Layer 1`")
    c0("`Layer 2`")
    d0("`Layer 3`")
   end
   subgraph G1["`GPU1`"]
    direction TB
    a1("`Layer 0`")
    b1("`Layer 1`")
    c1("`Layer 2`")
    d1("`Layer 3`")
   end
   a0 &lt;-.-&gt; a1
   b0 &lt;-.-&gt; b1
   c0 &lt;-.-&gt; c1
   d0 &lt;-.-&gt; d1
classDef red fill:#ff8181,stroke:#333,stroke-width:1px,color:#000
classDef orange fill:#FFC47F,stroke:#333,stroke-width:1px,color:#000
classDef yellow fill:#FFFF7F,stroke:#333,stroke-width:1px,color:#000
classDef green fill:#98E6A5,stroke:#333,stroke-width:1px,color:#000
classDef blue fill:#7DCAFF,stroke:#333,stroke-width:1px,color:#000
classDef purple fill:#FFCBE6,stroke:#333,stroke-width:1px,color:#000
classDef block fill:#CCCCCC02,stroke:#838383,stroke-width:1px,color:#838383
class G0,G1, block
class a0,a1 red
class b0,b1 green
class c0,c1 blue
class d0,d1 yellow
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-model-parallel-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16: Tensor Parallel Training
</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="tensor-model-parallel-training-example" class="level2" data-background-color="white">
<h2 data-background-color="white" class="anchored" data-anchor-id="tensor-model-parallel-training-example">Tensor (/ Model) Parallel Training: Example</h2>
<p>Want to compute: <span class="math inline">y = \sum_{i} x_{i} W_{i} = x_0 * W_0 + x_1 * W_1 + x_2 * W_2</span><br>
where each GPU only has only its portion of the full weights as shown below</p>
<ol type="1">
<li>Compute: <span class="math inline">y_{0} = x_{0} * W_{0}\rightarrow</span> <code>GPU1</code></li>
<li>Compute: <span class="math inline">y_{1} = y_{0} + x_{1} * W_{1}\rightarrow</span> <code>GPU2</code></li>
<li>Compute: <span class="math inline">y = y_{1} + x_{2} * W_{2} = \sum_{i} x_{i} W_{i}</span> âœ…</li>
</ol>
<div id="fig-tensor-parallel-example" class="quarto-float quarto-figure quarto-figure-center anchored" style="width:75%; margin-left: auto; margin-right: auto; text-align:center;">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tensor-parallel-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR
    subgraph X0["`GPU0`"]
        direction LR
        a("`W0`")
    end
    subgraph X1["`GPU1`"]
        direction LR
        b("`W1`")
    end
    subgraph X2["`GPU2`"]
        direction LR
        c("`W2`")
    end
  t0("`xâ‚€`")--&gt;X0
  X0 --&gt;|"`xâ‚€ Wâ‚€`"|X1
  X1 --&gt;|"`xâ‚€ Wâ‚€ &lt;br&gt;+ xâ‚ Wâ‚`"|X2
  t1("`xâ‚`") --&gt; X1
  t2("`xâ‚‚`") --&gt; X2
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-tensor-parallel-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17
</figcaption>
</figure>
</div>
<section id="tensor-model-parallelismefficient-large-scale" class="level3" data-background-color="white">
<h3 data-background-color="white" class="anchored" data-anchor-id="tensor-model-parallelismefficient-large-scale">Tensor (Model) Parallelism<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></h3>
<ul>
<li>In <strong>Tensor Paralleism</strong> each GPU processes only a slice of a tensor and only aggregates the full tensor for operations that require the whole thing.
<ul>
<li>The main building block of any transformer is a fully connected <code>nn.Linear</code> followed by a nonlinear activation GeLU.
<ul>
<li><code>Y = GeLU(XA)</code>, where X and Y are the input and output vectors, and A is the weight matrix.</li>
</ul></li>
<li>If we look at the computation in matrix form, itâ€™s easy to see how the matrix multiplication can be split between multiple GPUs:</li>
</ul></li>
</ul>
</section>
<section id="tensor-parallelism" class="level3" data-background-color="white">
<h3 data-background-color="white" class="anchored" data-anchor-id="tensor-parallelism">Tensor Parallelism</h3>
<div id="fig-parallel-gemm" class="quarto-float quarto-figure quarto-figure-center anchored" style="max-width: 80%; margin-left: auto; margin-right: auto;">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-parallel-gemm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="assets/parallelism-tp-parallel_gemm.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure&nbsp;18: Tensor Parallel GEMM. This information is based on (the much more in-depth) TP Overview by @anton-l"><img src="assets/parallelism-tp-parallel_gemm.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-parallel-gemm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;18: Tensor Parallel GEMM. This information is based on (the much more in-depth) <a href="https://github.com/huggingface/transformers/issues/10321#issuecomment-783543530">TP Overview</a> by <a href="https://github.com/anton-l">@anton-l</a>
</figcaption>
</figure>
</div>
</section>
<section id="d-parallelism" class="level3" data-background-color="white">
<h3 data-background-color="white" class="anchored" data-anchor-id="d-parallelism">3D Parallelism</h3>
<ul>
<li><code>DP</code> + <code>TP</code> + <code>PP</code> (3D) Parallelism</li>
</ul>
<div id="fig-3dparallel" class="quarto-float quarto-figure quarto-figure-center anchored" style="text-align:center!important;">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-3dparallel-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="assets/parallelism-deepspeed-3d.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Figure&nbsp;19: Figure taken from 3D parallelism: Scaling to trillion-parameter models"><img src="assets/parallelism-deepspeed-3d.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-3dparallel-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;19: Figure taken from <a href="https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/">3D parallelism: Scaling to trillion-parameter models</a>
</figcaption>
</figure>
</div>
</section>
<section id="deciding-on-a-parallelism-strategy" class="level3" data-background-color="white">
<h3 data-background-color="white" class="anchored" data-anchor-id="deciding-on-a-parallelism-strategy">Deciding on a Parallelism Strategy</h3>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" role="tab" aria-controls="tabset-1-1" aria-selected="true">Single GPU</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" role="tab" aria-controls="tabset-1-2" aria-selected="false">Single Node / Multi-GPU</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-3" role="tab" aria-controls="tabset-1-3" aria-selected="false">Multi-Node / Multi-GPU</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1-1-tab">
<ul>
<li>Model fits onto a single GPU:
<ul>
<li>Normal use</li>
</ul></li>
<li>Model <strong>DOES NOT</strong> fit on a single GPU:
<ul>
<li><code>ZeRO</code> + Offload CPU (or, optionally, <code>NVMe</code>)</li>
</ul></li>
<li>Largest layer <strong>DOES NOT</strong> fit on a single GPU:
<ul>
<li><code>ZeRO</code> + Enable <a href="https://deepspeed.readthedocs.io/en/latest/zero3.html#memory-centric-tiling">Memory Centric Tiling (MCT)</a>
<ul>
<li>MCT Allows running of arbitrarily large layers by automatically splitting them and executing them sequentially.</li>
</ul></li>
</ul></li>
</ul>
</div>
<div id="tabset-1-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-2-tab">
<div class="flex-container">
<div class="column">
<ul>
<li>Model fits onto a single GPU
<ul>
<li><a href="https://pytorch.org/docs/stable/notes/ddp.html"><code>DDP</code></a></li>
<li><a href="https://deepspeed.readthedocs.io/en/latest/zero3.html"><code>ZeRO</code></a></li>
</ul></li>
</ul>
</div>
<div class="column">
<ul>
<li>Model <strong>DOES NOT</strong> fit onto a single GPU
<ol type="1">
<li><a href="https://www.deepspeed.ai/tutorials/pipeline/">Pipeline Parallelism (<code>PP</code>)</a></li>
<li><a href="https://deepspeed.readthedocs.io/en/latest/zero3.html"><code>ZeRO</code></a></li>
<li><a href="https://pytorch.org/docs/stable/distributed.tensor.parallel.html">Tensor Parallelism (<code>TP</code>)</a></li>
</ol></li>
</ul>
</div>
</div>
<ul>
<li><p>With sufficiently fast connectivity between nodes, these three strategies should be comparable.</p>
<ul>
<li>Otherwise, <code>PP</code> <span class="math inline">&gt;</span> <code>ZeRO</code> <span class="math inline">\simeq</span> <code>TP</code>.</li>
</ul></li>
</ul>
</div>
<div id="tabset-1-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-3-tab">
<ul>
<li><p>When you have fast inter-node connectivity:</p>
<ul>
<li><code>ZeRO</code> (virtually <strong>NO</strong> modifications)</li>
<li><code>PP</code> + <code>ZeRO</code> + <code>TP</code> + <code>DP</code> (less communication, at the cost of <strong>MAJOR</strong> modifications)
<ul>
<li><p>when you have slow inter-node connectivity and still low on GPU memory:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode numberSource bash number-lines code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1"></a><span class="ex">DP</span> + PP + TP + ZeRO-1</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ul></li>
<li><strong>NOTE</strong>: <code>TP</code> is almost <em>always</em> used within a single node, e.g.<br>
<code>TP &lt;= GPUS_PER_NODE</code></li>
</ul></li>
</ul>
</div>
</div>
</div>
</section>
</section>
<section id="large-language-models" class="level2" data-background-color="white">
<h2 data-background-color="white" class="anchored" data-anchor-id="large-language-models">ğŸ¦™ Large Language Models</h2>
<div id="fig-llms" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-llms-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./assets/llms.gif" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Figure&nbsp;20: Large Language Models have (LLM)s have taken the NLP community world by storm."><img src="./assets/llms.gif" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-llms-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;20: Large Language Models have (LLM)s have taken the <del>NLP community</del> <strong>world</strong> by storm<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.
</figcaption>
</figure>
</div>
<section id="emergent-abilities" class="level3" data-background-color="#FBFBFD">
<h3 data-background-color="#FBFBFD" class="anchored" data-anchor-id="emergent-abilities">ğŸ”® Emergent Abilities</h3>
<div id="fig-emergent-abilities" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-emergent-abilities-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./assets/emergent-abilities.gif" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Figure&nbsp;21: See @wei2022emergentabilitieslargelanguage, @yao2023tree"><img src="./assets/emergent-abilities.gif" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-emergent-abilities-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;21: See <span class="citation" data-cites="wei2022emergentabilitieslargelanguage">Wei et al. (<a href="#ref-wei2022emergentabilitieslargelanguage" role="doc-biblioref">2022</a>)</span>, <span class="citation" data-cites="yao2023tree">Yao et al. (<a href="#ref-yao2023tree" role="doc-biblioref">2023</a>)</span>
</figcaption>
</figure>
</div>
</section>
<section id="training-llms" class="level3 smaller" data-background-color="white">
<h3 class="smaller anchored" data-background-color="white" data-anchor-id="training-llms">ğŸš‚ Training LLMs</h3>
<div>

</div>
<div class="quarto-layout-panel" data-layout="[15,-2,10]">
<div class="quarto-layout-row quarto-layout-valign-bottom">
<div class="quarto-layout-cell" style="flex-basis: 55.6%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./assets/evolution.gif" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="Visualization from Hannibal046/Awesome-LLM"><img src="./assets/evolution.gif" class="img-fluid figure-img" alt="Visualization from Hannibal046/Awesome-LLM"></a></p>
<figcaption>Visualization from <a href="https://github.com/Hannibal046/Awesome-LLM">Hannibal046/Awesome-LLM</a></figcaption>
</figure>
</div>
</div>
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 7.4%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 37.0%;justify-content: center;">
<p><a href="./assets/it_hungers.jpeg" class="lightbox" data-gallery="quarto-lightbox-gallery-8"><img src="./assets/it_hungers.jpeg" class="img-fluid"></a></p>
</div>
</div>
</div>
</section>
<section id="life-cycle-of-the-llm" class="level3" data-auto-animate="true" data-background-color="white">
<h3 data-auto-animate="true" data-background-color="white" class="anchored" data-anchor-id="life-cycle-of-the-llm">â™»ï¸ Life-Cycle of the LLM</h3>
<div class="flex-container">
<div class="column" style="width: 40%;">
<ol type="1">
<li>Data collection + preprocessing</li>
<li><strong>Pre-training</strong>
<ul>
<li>Architecture decisions, model size, etc.</li>
</ul></li>
<li>Supervised Fine-Tuning
<ul>
<li>Instruction Tuning</li>
<li>Alignment</li>
</ul></li>
<li>Deploy (+ monitor, re-evaluate, etc.)</li>
</ol>
</div>
<div class="column" style="width:50%;">
<div id="fig-pretrain-two" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pretrain-two-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./assets/gpt3-training-step-back-prop.gif" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Figure&nbsp;22: Pre-training: Virtually all of the compute used during pre-training."><img src="./assets/gpt3-training-step-back-prop.gif" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pretrain-two-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;22: <strong>Pre-training</strong>: Virtually <em>all of the compute</em> used during pre-training<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="life-cycle-of-the-llm-1" class="level3" data-auto-animate="true" data-background-color="white">
<h3 data-auto-animate="true" data-background-color="white" class="anchored" data-anchor-id="life-cycle-of-the-llm-1">ğŸ€ Life-Cycle of the LLM</h3>
<div class="flex-container">
<div class="column" style="width: 50%;">
<ol type="1">
<li>Data collection + preprocessing</li>
<li>Pre-training
<ul>
<li>Architecture decisions, model size, etc.</li>
</ul></li>
<li><strong>Supervised Fine-Tuning</strong>
<ul>
<li>Instruction Tuning</li>
<li>Alignment</li>
</ul></li>
<li>Deploy (+ monitor, re-evaluate, etc.)</li>
</ol>
</div>
<div class="column" style="width:50%;">
<div id="fig-finetune-lifecycle" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-finetune-lifecycle-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./assets/gpt3-fine-tuning.gif" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="Figure&nbsp;23: Fine-tuning: Fine-tuning actually updates the modelâ€™s weights to make the model better at a certain task."><img src="./assets/gpt3-fine-tuning.gif" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-finetune-lifecycle-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;23: <strong>Fine-tuning</strong>: Fine-tuning actually updates the modelâ€™s weights to make the model better at a certain task<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="forward-pass" class="level3" data-background-color="white">
<h3 data-background-color="white" class="anchored" data-anchor-id="forward-pass">â© Forward Pass</h3>
<div id="fig-hf-assisted-generation" class="quarto-float quarto-figure quarto-figure-center anchored" style="width:100%;">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-hf-assisted-generation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./assets/hf_assisted_generation.mov" class="lightbox" data-gallery="quarto-lightbox-gallery-11" title="Figure&nbsp;24: Language Model trained for causal language modeling."><video src="./assets/hf_assisted_generation.mov" class="img-fluid" controls=""></video></a><a href="./assets/hf_assisted_generation.mov">Video</a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hf-assisted-generation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;24: Language Model trained for causal language modeling<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>.
</figcaption>
</figure>
</div>
</section>
<section id="generating-text" class="level3" data-background-color="white">
<h3 data-background-color="white" class="anchored" data-anchor-id="generating-text">ğŸ’¬ Generating Text</h3>
<div id="fig-generating-text" class="quarto-float quarto-figure quarto-figure-center anchored" style="width: 100%;">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-generating-text-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./assets/hf_assisted_generation2.mov" class="lightbox" data-gallery="quarto-lightbox-gallery-12" title="Figure&nbsp;25: Language Model trained for causal language modeling."><video src="./assets/hf_assisted_generation2.mov" class="img-fluid" controls=""></video></a><a href="./assets/hf_assisted_generation2.mov">Video</a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-generating-text-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;25: Language Model trained for causal language modeling<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>.
</figcaption>
</figure>
</div>
</section>
</section>
<section id="hands-on" class="level2" data-background-color="white">
<h2 data-background-color="white" class="anchored" data-anchor-id="hands-on">ğŸ‘‹ Hands On</h2>
<p><i class="fa-brands fa-github" aria-label="github"></i> <a href="https://github.com/argonne-lcf/ai-science-training-series/tree/main/06_parallel_training#hands-on">ai-science-training-series / 06_parallel_training</a></p>
<section id="hands-on-getting-started" class="level3" data-background-color="white">
<h3 data-background-color="white" class="anchored" data-anchor-id="hands-on-getting-started">ğŸ§‘â€ğŸ’» Hands On: Getting Started</h3>
<ol type="1">
<li><p>ğŸŒ± Clone Repo(s):</p>
<ul>
<li><p><i class="fa-brands fa-github" aria-label="github"></i> <a href="https://github.com/saforem2/wordplay">saforem2/<code>wordplay</code></a></p>
<div class="sourceCode" id="cb2"><pre class="sourceCode numberSource bash number-lines code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1"></a><span class="fu">git</span> clone https://github.com/saforem2/wordplay</span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="bu">cd</span> wordplay</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p><i class="fa-brands fa-github" aria-label="github"></i> <a href="https://github.com/saforem2/ezpz">saforem2/<code>ezpz</code></a></p>
<div class="sourceCode" id="cb3"><pre class="sourceCode numberSource bash number-lines code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1"></a><span class="fu">git</span> clone https://github.com/saforem2/ezpz deps/ezpz</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ul></li>
<li><p>ğŸ Setup Python:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode numberSource bash number-lines code-with-copy"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1"></a><span class="bu">export</span> <span class="va">PBS_O_WORKDIR</span><span class="op">=</span><span class="va">$(</span><span class="bu">pwd</span><span class="va">)</span> <span class="kw">&amp;&amp;</span> <span class="bu">source</span> deps/ezpz/src/ezpz/bin/utils.sh</span>
<span id="cb4-2"><a href="#cb4-2"></a><span class="ex">ezpz_setup_python</span></span>
<span id="cb4-3"><a href="#cb4-3"></a><span class="ex">ezpz_setup_job</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ol>
</section>
<section id="install-ezpz-wordplay" class="level3" data-background-color="white">
<h3 data-background-color="white" class="anchored" data-anchor-id="install-ezpz-wordplay">ğŸ“¦ Install {<code>ezpz</code>, <code>wordplay</code>}</h3>
<ol type="1">
<li><p>Install Python packages:</p>
<ol type="1">
<li><p><i class="fa-brands fa-github" aria-label="github"></i> <a href="https://github.com/saforem2/ezpz">saforem2/<code>ezpz</code></a>:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode numberSource bash number-lines code-with-copy"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1"></a><span class="ex">python3</span> <span class="at">-m</span> pip install <span class="at">-e</span> <span class="st">"./deps/ezpz"</span> <span class="at">--require-virtualenv</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p><i class="fa-brands fa-github" aria-label="github"></i> <a href="https://github.com/saforem2/ezpz">saforem2/<code>wordplay</code></a>:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode numberSource bash number-lines code-with-copy"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1"></a><span class="co"># from inside `wordplay/`</span></span>
<span id="cb6-2"><a href="#cb6-2"></a><span class="ex">python3</span> <span class="at">-m</span> pip install <span class="at">-e</span> . <span class="at">--require-virtualenv</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ol></li>
<li><p>Test distributed setup:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode numberSource bash number-lines code-with-copy"><code class="sourceCode bash"><span id="cb7-1"><a href="#cb7-1"></a><span class="ex">mpirun</span> <span class="at">-n</span> <span class="st">"</span><span class="va">${NGPUS}</span><span class="st">"</span> python3 <span class="at">-m</span> ezpz.test_dist</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>See: ğŸ‹ <a href="https://github.com/saforem2/ezpz/blob/main/src/ezpz/test_dist.py"><code>ezpz/test_dist.py</code></a></p></li>
</ol>
</section>
<section id="ezpz-example-video" class="level3" data-background-color="#121314">
<h3 data-background-color="#121314" class="anchored" data-anchor-id="ezpz-example-video"><i class="fa-brands fa-github" aria-label="github"></i> <a href="https://github.com/saforem2/ezpz"><code>ezpz</code></a>: Example [<a href="https://asciinema.org/a/668460">video</a>]</h3>
<div id="fig-ezpz-asciinema" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ezpz-asciinema-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<script src="https://asciinema.org/a/668460.js" id="asciicast-668460" async="true"></script>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ezpz-asciinema-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;26: Example: using <a href="https://github.com/saforem2/ezpz/blob/main/src/ezpz/test_dist.py">ğŸ‹ <code>ezpz.test_dist</code></a> to train a small model using DDP
</figcaption>
</figure>
</div>
</section>
<section id="install-wordplay" class="level3" data-background-color="white">
<h3 data-background-color="white" class="anchored" data-anchor-id="install-wordplay">Install <a href="https://github.com/saforem2/wordplay"><code>wordplay</code> ğŸ®ğŸ’¬</a></h3>
<div id="fig-nanoGPT" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nanoGPT-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./assets/nanogpt.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-13" title="Figure&nbsp;27: The simplest, fastest repository for training / finetuning GPT based models. Figure from karpathy/nanoGPT"><img src="./assets/nanogpt.jpg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nanoGPT-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;27: The simplest, fastest repository for training / finetuning GPT based models. Figure from <a href="https://github.com/karpathy/nanoGPT">karpathy/<code>nanoGPT</code></a>
</figcaption>
</figure>
</div>
</section>
<section id="prepare-data" class="level3" data-background-color="white">
<h3 data-background-color="white" class="anchored" data-anchor-id="prepare-data">Prepare Data</h3>
<div class="sourceCode" id="cb8"><pre class="sourceCode numberSource bash number-lines code-with-copy"><code class="sourceCode bash"><span id="cb8-1"><a href="#cb8-1"></a><span class="ex">$</span> python3 wordplay/data/shakespeare_char/prepare.py</span>
<span id="cb8-2"><a href="#cb8-2"></a><span class="ex">Using</span> HF_DATASETS_CACHE=/home/foremans/tmp/polaris-talk/2024-07-17-073327/wordplay/data/shakespeare_char/.cache/huggingface</span>
<span id="cb8-3"><a href="#cb8-3"></a><span class="ex">length</span> of dataset in characters: 1,115,394</span>
<span id="cb8-4"><a href="#cb8-4"></a><span class="ex">all</span> the unique characters:</span>
<span id="cb8-5"><a href="#cb8-5"></a> <span class="ex">!$</span><span class="kw">&amp;</span><span class="ex">\',-.3:</span><span class="kw">;</span><span class="ex">?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz</span></span>
<span id="cb8-6"><a href="#cb8-6"></a><span class="ex">vocab</span> size: 65</span>
<span id="cb8-7"><a href="#cb8-7"></a><span class="ex">train</span> has 1,003,854 tokens</span>
<span id="cb8-8"><a href="#cb8-8"></a><span class="ex">val</span> has 111,540 tokens</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="launch-training-ddp" class="level3" data-background-color="white">
<h3 data-background-color="white" class="anchored" data-anchor-id="launch-training-ddp">Launch Training (DDP)</h3>
<div class="sourceCode" id="cb9"><pre class="sourceCode numberSource bash number-lines code-with-copy"><code class="sourceCode bash"><span id="cb9-1"><a href="#cb9-1"></a><span class="ex">launch</span> python3 <span class="at">-m</span> wordplay <span class="dt">\</span></span>
<span id="cb9-2"><a href="#cb9-2"></a>    train.backend=DDP <span class="dt">\</span></span>
<span id="cb9-3"><a href="#cb9-3"></a>    train.eval_interval=100 <span class="dt">\</span></span>
<span id="cb9-4"><a href="#cb9-4"></a>    data=shakespeare <span class="dt">\</span></span>
<span id="cb9-5"><a href="#cb9-5"></a>    train.dtype=bf16 <span class="dt">\</span></span>
<span id="cb9-6"><a href="#cb9-6"></a>    model.batch_size=64 <span class="dt">\</span></span>
<span id="cb9-7"><a href="#cb9-7"></a>    model.block_size=1024 <span class="dt">\</span></span>
<span id="cb9-8"><a href="#cb9-8"></a>    train.max_iters=1000 <span class="dt">\</span></span>
<span id="cb9-9"><a href="#cb9-9"></a>    train.log_interval=10 <span class="dt">\</span></span>
<span id="cb9-10"><a href="#cb9-10"></a>    train.compile=false <span class="dt">\</span></span>
<span id="cb9-11"><a href="#cb9-11"></a>    <span class="kw">|</span> <span class="fu">tee</span> wordplay-gpt2-DDP.log</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="training-example-output" class="level3" data-background-color="white">
<h3 data-background-color="white" class="anchored" data-anchor-id="training-example-output">Training: Example Output</h3>
<div class="sourceCode" id="cb10"><pre class="sourceCode numberSource bash number-lines code-with-copy"><code class="sourceCode bash"><span id="cb10-1"><a href="#cb10-1"></a><span class="ex">$</span> launch python3 <span class="at">-m</span> wordplay <span class="dt">\</span></span>
<span id="cb10-2"><a href="#cb10-2"></a>    train.backend=DDP <span class="dt">\</span></span>
<span id="cb10-3"><a href="#cb10-3"></a>    train.eval_interval=100 <span class="dt">\</span></span>
<span id="cb10-4"><a href="#cb10-4"></a>    data=shakespeare <span class="dt">\</span></span>
<span id="cb10-5"><a href="#cb10-5"></a>    train.dtype=bf16 <span class="dt">\</span></span>
<span id="cb10-6"><a href="#cb10-6"></a>    model.batch_size=64 <span class="dt">\</span></span>
<span id="cb10-7"><a href="#cb10-7"></a>    model.block_size=1024 <span class="dt">\</span></span>
<span id="cb10-8"><a href="#cb10-8"></a>    train.max_iters=1000 <span class="dt">\</span></span>
<span id="cb10-9"><a href="#cb10-9"></a>    train.log_interval=10 <span class="dt">\</span></span>
<span id="cb10-10"><a href="#cb10-10"></a>    train.compile=false <span class="dt">\</span></span>
<span id="cb10-11"><a href="#cb10-11"></a>    <span class="kw">|</span> <span class="fu">tee</span> wordplay-gpt2-DDP.log</span>
<span id="cb10-12"><a href="#cb10-12"></a><span class="ex">[2024-07-17</span> 07:42:11.746540]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">__init__:156</span><span class="pp">]</span> <span class="at">-</span> Setting logging level to <span class="st">'INFO'</span> on <span class="st">'RANK == 0'</span></span>
<span id="cb10-13"><a href="#cb10-13"></a><span class="ex">[2024-07-17</span> 07:42:11.748763]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">__init__:157</span><span class="pp">]</span> <span class="at">-</span> Setting logging level to <span class="st">'CRITICAL'</span> on all others <span class="st">'RANK != 0'</span></span>
<span id="cb10-14"><a href="#cb10-14"></a><span class="ex">[2024-07-17</span> 07:42:11.749453]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">__init__:160</span><span class="pp">]</span> <span class="at">-</span> To disable this behavior, and log from ALL ranks <span class="er">(</span><span class="ex">not</span> recommended<span class="kw">)</span><span class="ex">,</span> set: <span class="st">'export LOG_FROM_ALL_RANKS=1'</span>  in your environment, and re-run.</span>
<span id="cb10-15"><a href="#cb10-15"></a><span class="ex">[2024-07-17</span> 07:42:11.772718]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">configs:81</span><span class="pp">]</span> <span class="at">-</span> Setting HF_DATASETS_CACHE to /home/foremans/tmp/polaris-talk/2024-07-17-073327/wordplay/.cache/huggingface/datasets</span>
<span id="cb10-16"><a href="#cb10-16"></a><span class="ex">[2024-07-17</span> 07:42:15.341532]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist:358</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'cuda'</span><span class="pp">][</span><span class="ss">rank=2/3</span><span class="pp">][</span><span class="ss">local_rank=2/3</span><span class="pp">][</span><span class="ss">node=0/0</span><span class="pp">]</span></span>
<span id="cb10-17"><a href="#cb10-17"></a><span class="ex">[2024-07-17</span> 07:42:15.342381]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist:358</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'cuda'</span><span class="pp">][</span><span class="ss">rank=1/3</span><span class="pp">][</span><span class="ss">local_rank=1/3</span><span class="pp">][</span><span class="ss">node=0/0</span><span class="pp">]</span></span>
<span id="cb10-18"><a href="#cb10-18"></a><span class="ex">[2024-07-17</span> 07:42:15.342430]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist:358</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'cuda'</span><span class="pp">][</span><span class="ss">rank=3/3</span><span class="pp">][</span><span class="ss">local_rank=3/3</span><span class="pp">][</span><span class="ss">node=0/0</span><span class="pp">]</span></span>
<span id="cb10-19"><a href="#cb10-19"></a><span class="ex">[2024-07-17</span> 07:42:15.348657]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist:95</span><span class="pp">]</span> <span class="at">-</span></span>
<span id="cb10-20"><a href="#cb10-20"></a></span>
<span id="cb10-21"><a href="#cb10-21"></a><span class="ex">[dist_info]:</span></span>
<span id="cb10-22"><a href="#cb10-22"></a>  <span class="ex">â€¢</span> DEVICE=cuda</span>
<span id="cb10-23"><a href="#cb10-23"></a>  <span class="ex">â€¢</span> DEVICE_ID=cuda:0</span>
<span id="cb10-24"><a href="#cb10-24"></a>  <span class="ex">â€¢</span> DISTRIBUTED_BACKEND=nccl</span>
<span id="cb10-25"><a href="#cb10-25"></a>  <span class="ex">â€¢</span> GPUS_PER_NODE=4</span>
<span id="cb10-26"><a href="#cb10-26"></a>  <span class="ex">â€¢</span> HOSTS=<span class="pp">[</span><span class="st">'x3101c0s13b0n0.hsn.cm.polaris.alcf.anl.gov'</span><span class="pp">]</span></span>
<span id="cb10-27"><a href="#cb10-27"></a>  <span class="ex">â€¢</span> HOSTFILE=/var/spool/pbs/aux/2024084.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov</span>
<span id="cb10-28"><a href="#cb10-28"></a>  <span class="ex">â€¢</span> HOSTNAME=x3101c0s13b0n0.hsn.cm.polaris.alcf.anl.gov</span>
<span id="cb10-29"><a href="#cb10-29"></a>  <span class="ex">â€¢</span> LOCAL_RANK=0</span>
<span id="cb10-30"><a href="#cb10-30"></a>  <span class="ex">â€¢</span> MACHINE=Polaris</span>
<span id="cb10-31"><a href="#cb10-31"></a>  <span class="ex">â€¢</span> NUM_NODES=1</span>
<span id="cb10-32"><a href="#cb10-32"></a>  <span class="ex">â€¢</span> NGPUS=4</span>
<span id="cb10-33"><a href="#cb10-33"></a>  <span class="ex">â€¢</span> NGPUS_AVAILABLE=4</span>
<span id="cb10-34"><a href="#cb10-34"></a>  <span class="ex">â€¢</span> NODE_ID=0</span>
<span id="cb10-35"><a href="#cb10-35"></a>  <span class="ex">â€¢</span> RANK=0</span>
<span id="cb10-36"><a href="#cb10-36"></a>  <span class="ex">â€¢</span> SCHEDULER=PBS</span>
<span id="cb10-37"><a href="#cb10-37"></a>  <span class="ex">â€¢</span> WORLD_SIZE_TOTAL=4</span>
<span id="cb10-38"><a href="#cb10-38"></a>  <span class="ex">â€¢</span> WORLD_SIZE_IN_USE=4</span>
<span id="cb10-39"><a href="#cb10-39"></a>  <span class="ex">â€¢</span> LAUNCH_CMD=mpiexec <span class="at">--verbose</span> <span class="at">--envall</span> <span class="at">-n</span> 4 <span class="at">-ppn</span> 4 <span class="at">--hostfile</span> /var/spool/pbs/aux/2024084.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov <span class="at">--cpu-bind</span> depth <span class="at">-d</span> 16</span>
<span id="cb10-40"><a href="#cb10-40"></a></span>
<span id="cb10-41"><a href="#cb10-41"></a><span class="ex">[2024-07-17</span> 07:42:15.351446]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist:725</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">0/4</span><span class="pp">]</span> Using device=<span class="st">'cuda'</span> with backend=<span class="st">'DDP'</span> + <span class="st">'nccl'</span> for distributed training.</span>
<span id="cb10-42"><a href="#cb10-42"></a><span class="ex">[2024-07-17</span> 07:42:15.356169]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist:358</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'cuda'</span><span class="pp">][</span><span class="ss">rank=0/3</span><span class="pp">][</span><span class="ss">local_rank=0/3</span><span class="pp">][</span><span class="ss">node=0/0</span><span class="pp">]</span></span>
<span id="cb10-43"><a href="#cb10-43"></a><span class="ex">[2024-07-17</span> 07:42:15.356692]<span class="pp">[</span><span class="ss">WARNING</span><span class="pp">][</span><span class="ss">dist:364</span><span class="pp">]</span> <span class="at">-</span> Using [4 / 4] available <span class="st">"cuda"</span> devices !!</span>
<span id="cb10-44"><a href="#cb10-44"></a><span class="ex">[2024-07-17</span> 07:42:15.359571]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">configs:317</span><span class="pp">]</span> <span class="at">-</span> Loading val from /home/foremans/tmp/polaris-talk/2024-07-17-073327/wordplay/data/shakespeare_char/val.bin</span>
<span id="cb10-45"><a href="#cb10-45"></a><span class="ex">[2024-07-17</span> 07:42:15.360138]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">configs:317</span><span class="pp">]</span> <span class="at">-</span> Loading train from /home/foremans/tmp/polaris-talk/2024-07-17-073327/wordplay/data/shakespeare_char/train.bin</span>
<span id="cb10-46"><a href="#cb10-46"></a><span class="ex">[2024-07-17</span> 07:42:15.361154]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">configs:442</span><span class="pp">]</span> <span class="at">-</span> Tokens per iteration: 262,144</span>
<span id="cb10-47"><a href="#cb10-47"></a><span class="ex">[2024-07-17</span> 07:42:15.361574]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">configs:465</span><span class="pp">]</span> <span class="at">-</span> Using self.ptdtype=torch.float16 on self.device_type=<span class="st">'cuda'</span></span>
<span id="cb10-48"><a href="#cb10-48"></a><span class="ex">[2024-07-17</span> 07:42:15.362002]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">configs:471</span><span class="pp">]</span> <span class="at">-</span> Initializing a new model from scratch</span>
<span id="cb10-49"><a href="#cb10-49"></a><span class="ex">[2024-07-17</span> 07:42:15.362529]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist:874</span><span class="pp">]</span> <span class="at">-</span> Setting up wandb from rank: 0</span>
<span id="cb10-50"><a href="#cb10-50"></a><span class="ex">[2024-07-17</span> 07:42:15.362896]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist:875</span><span class="pp">]</span> <span class="at">-</span> Using: WB PROJECT: WordPlay</span>
<span id="cb10-51"><a href="#cb10-51"></a><span class="ex">[2024-07-17</span> 07:42:16.451786]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist:905</span><span class="pp">]</span> <span class="at">-</span> W<span class="kw">&amp;</span><span class="ex">B</span> RUN: <span class="pp">[</span><span class="ss">still</span><span class="pp">-</span><span class="ss">frog</span><span class="pp">-</span><span class="ss">17</span><span class="pp">]</span><span class="er">(</span><span class="ex">https://wandb.ai/aurora_gpt/WordPlay/runs/6by9vpcj</span><span class="kw">)</span></span>
<span id="cb10-52"><a href="#cb10-52"></a><span class="ex">[2024-07-17</span> 07:42:16.464106]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist:312</span><span class="pp">]</span> <span class="at">-</span> Updating wandb.run: still-frog-17 config with <span class="st">"DIST_INFO"</span></span>
<span id="cb10-53"><a href="#cb10-53"></a><span class="ex">[2024-07-17</span> 07:42:16.469424]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist:938</span><span class="pp">]</span> <span class="at">-</span> Running on machine=<span class="st">'Polaris'</span></span>
<span id="cb10-54"><a href="#cb10-54"></a><span class="ex">[2024-07-17</span> 07:42:16.471151]<span class="pp">[</span><span class="ss">WARNING</span><span class="pp">][</span><span class="ss">__main__:89</span><span class="pp">]</span> <span class="at">-</span> {</span>
<span id="cb10-55"><a href="#cb10-55"></a>    <span class="st">"train"</span><span class="ex">:</span> {</span>
<span id="cb10-56"><a href="#cb10-56"></a>        <span class="st">"framework"</span><span class="ex">:</span> <span class="st">"pytorch"</span>,</span>
<span id="cb10-57"><a href="#cb10-57"></a>        <span class="st">"backend"</span><span class="ex">:</span> <span class="st">"DDP"</span>,</span>
<span id="cb10-58"><a href="#cb10-58"></a>        <span class="st">"device"</span><span class="ex">:</span> null,</span>
<span id="cb10-59"><a href="#cb10-59"></a>        <span class="st">"seed"</span><span class="ex">:</span> null,</span>
<span id="cb10-60"><a href="#cb10-60"></a>        <span class="st">"port"</span><span class="ex">:</span> null,</span>
<span id="cb10-61"><a href="#cb10-61"></a>        <span class="st">"ds_config_path"</span><span class="ex">:</span> null,</span>
<span id="cb10-62"><a href="#cb10-62"></a>        <span class="st">"precision"</span><span class="ex">:</span> null,</span>
<span id="cb10-63"><a href="#cb10-63"></a>        <span class="st">"ngpus"</span><span class="ex">:</span> null,</span>
<span id="cb10-64"><a href="#cb10-64"></a>        <span class="st">"use_wandb"</span><span class="ex">:</span> true,</span>
<span id="cb10-65"><a href="#cb10-65"></a>        <span class="st">"eval_interval"</span><span class="ex">:</span> 100,</span>
<span id="cb10-66"><a href="#cb10-66"></a>        <span class="st">"log_interval"</span><span class="ex">:</span> 10,</span>
<span id="cb10-67"><a href="#cb10-67"></a>        <span class="st">"eval_iters"</span><span class="ex">:</span> 200,</span>
<span id="cb10-68"><a href="#cb10-68"></a>        <span class="st">"eval_only"</span><span class="ex">:</span> false,</span>
<span id="cb10-69"><a href="#cb10-69"></a>        <span class="st">"always_save_checkpoint"</span><span class="ex">:</span> false,</span>
<span id="cb10-70"><a href="#cb10-70"></a>        <span class="st">"init_from"</span><span class="ex">:</span> <span class="st">"scratch"</span>,</span>
<span id="cb10-71"><a href="#cb10-71"></a>        <span class="st">"wandb_project"</span><span class="ex">:</span> <span class="st">"WordPlay"</span>,</span>
<span id="cb10-72"><a href="#cb10-72"></a>        <span class="st">"max_iters"</span><span class="ex">:</span> 1000,</span>
<span id="cb10-73"><a href="#cb10-73"></a>        <span class="st">"warmup_iters"</span><span class="ex">:</span> 100,</span>
<span id="cb10-74"><a href="#cb10-74"></a>        <span class="st">"dtype"</span><span class="ex">:</span> <span class="st">"bf16"</span>,</span>
<span id="cb10-75"><a href="#cb10-75"></a>        <span class="st">"compile"</span><span class="ex">:</span> false</span>
<span id="cb10-76"><a href="#cb10-76"></a>    <span class="er">}</span><span class="ex">,</span></span>
<span id="cb10-77"><a href="#cb10-77"></a>    <span class="st">"model"</span><span class="ex">:</span> {</span>
<span id="cb10-78"><a href="#cb10-78"></a>        <span class="st">"n_layer"</span><span class="ex">:</span> 12,</span>
<span id="cb10-79"><a href="#cb10-79"></a>        <span class="st">"n_head"</span><span class="ex">:</span> 12,</span>
<span id="cb10-80"><a href="#cb10-80"></a>        <span class="st">"n_embd"</span><span class="ex">:</span> 768,</span>
<span id="cb10-81"><a href="#cb10-81"></a>        <span class="st">"batch_size"</span><span class="ex">:</span> 64,</span>
<span id="cb10-82"><a href="#cb10-82"></a>        <span class="st">"block_size"</span><span class="ex">:</span> 1024,</span>
<span id="cb10-83"><a href="#cb10-83"></a>        <span class="st">"activation"</span><span class="ex">:</span> <span class="st">"gelu"</span>,</span>
<span id="cb10-84"><a href="#cb10-84"></a>        <span class="st">"dropout"</span><span class="ex">:</span> 0.0,</span>
<span id="cb10-85"><a href="#cb10-85"></a>        <span class="st">"bias"</span><span class="ex">:</span> false,</span>
<span id="cb10-86"><a href="#cb10-86"></a>        <span class="st">"vocab_size"</span><span class="ex">:</span> 65</span>
<span id="cb10-87"><a href="#cb10-87"></a>    <span class="er">}</span><span class="ex">,</span></span>
<span id="cb10-88"><a href="#cb10-88"></a>    <span class="st">"data"</span><span class="ex">:</span> {</span>
<span id="cb10-89"><a href="#cb10-89"></a>        <span class="st">"dataset"</span><span class="ex">:</span> <span class="st">"shakespeare_char"</span>,</span>
<span id="cb10-90"><a href="#cb10-90"></a>        <span class="st">"out_dir"</span><span class="ex">:</span> <span class="st">"out-shakespeare-char"</span>,</span>
<span id="cb10-91"><a href="#cb10-91"></a>        <span class="st">"root_path"</span><span class="ex">:</span> null</span>
<span id="cb10-92"><a href="#cb10-92"></a>    <span class="er">}</span><span class="ex">,</span></span>
<span id="cb10-93"><a href="#cb10-93"></a>    <span class="st">"optimizer"</span><span class="ex">:</span> {</span>
<span id="cb10-94"><a href="#cb10-94"></a>        <span class="st">"gas"</span><span class="ex">:</span> 1,</span>
<span id="cb10-95"><a href="#cb10-95"></a>        <span class="st">"name"</span><span class="ex">:</span> <span class="st">"AdamW"</span>,</span>
<span id="cb10-96"><a href="#cb10-96"></a>        <span class="st">"learning_rate"</span><span class="ex">:</span> 0.0006,</span>
<span id="cb10-97"><a href="#cb10-97"></a>        <span class="st">"weight_decay"</span><span class="ex">:</span> 0.1,</span>
<span id="cb10-98"><a href="#cb10-98"></a>        <span class="st">"beta1"</span><span class="ex">:</span> 0.9,</span>
<span id="cb10-99"><a href="#cb10-99"></a>        <span class="st">"beta2"</span><span class="ex">:</span> 0.95,</span>
<span id="cb10-100"><a href="#cb10-100"></a>        <span class="st">"grad_clip"</span><span class="ex">:</span> 1.0,</span>
<span id="cb10-101"><a href="#cb10-101"></a>        <span class="st">"decay_lr"</span><span class="ex">:</span> true,</span>
<span id="cb10-102"><a href="#cb10-102"></a>        <span class="st">"lr_decay_iters"</span><span class="ex">:</span> 600000,</span>
<span id="cb10-103"><a href="#cb10-103"></a>        <span class="st">"min_lr"</span><span class="ex">:</span> 6e-05</span>
<span id="cb10-104"><a href="#cb10-104"></a>    <span class="er">}</span></span>
<span id="cb10-105"><a href="#cb10-105"></a><span class="er">}</span></span>
<span id="cb10-106"><a href="#cb10-106"></a><span class="ex">[2024-07-17</span> 07:42:16.474305]<span class="pp">[</span><span class="ss">WARNING</span><span class="pp">][</span><span class="ss">__main__:90</span><span class="pp">]</span> <span class="at">-</span> Output dir: /home/foremans/tmp/polaris-talk/outputs/runs/pytorch/DDP/2024-07-17/07-42-13</span>
<span id="cb10-107"><a href="#cb10-107"></a><span class="ex">[2024-07-17</span> 07:42:16.474922]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:246</span><span class="pp">]</span> <span class="at">-</span> Initializing a new model from scratch</span>
<span id="cb10-108"><a href="#cb10-108"></a><span class="ex">[2024-07-17</span> 07:42:17.258904]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">model:255</span><span class="pp">]</span> <span class="at">-</span> number of parameters: 85.00M</span>
<span id="cb10-109"><a href="#cb10-109"></a><span class="ex">[2024-07-17</span> 07:42:17.290004]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:264</span><span class="pp">]</span> <span class="at">-</span> Model size: num_params=85003776</span>
<span id="cb10-110"><a href="#cb10-110"></a><span class="ex">[2024-07-17</span> 07:42:17.292626]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">model:445</span><span class="pp">]</span> <span class="at">-</span> num decayed parameter tensors: 50, with 85,771,008 parameters</span>
<span id="cb10-111"><a href="#cb10-111"></a><span class="ex">[2024-07-17</span> 07:42:17.293296]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">model:449</span><span class="pp">]</span> <span class="at">-</span> num non-decayed parameter tensors: 25, with 19,200 parameters</span>
<span id="cb10-112"><a href="#cb10-112"></a><span class="ex">[2024-07-17</span> 07:42:17.515324]<span class="pp">[</span><span class="ss">CRITICAL</span><span class="pp">][</span><span class="ss">trainer:316</span><span class="pp">]</span> <span class="at">-</span> <span class="st">"devid='cuda:1'"</span></span>
<span id="cb10-113"><a href="#cb10-113"></a><span class="ex">[2024-07-17</span> 07:42:17.515340]<span class="pp">[</span><span class="ss">CRITICAL</span><span class="pp">][</span><span class="ss">trainer:316</span><span class="pp">]</span> <span class="at">-</span> <span class="st">"devid='cuda:2'"</span></span>
<span id="cb10-114"><a href="#cb10-114"></a><span class="ex">[2024-07-17</span> 07:42:17.515465]<span class="pp">[</span><span class="ss">CRITICAL</span><span class="pp">][</span><span class="ss">trainer:316</span><span class="pp">]</span> <span class="at">-</span> <span class="st">"devid='cuda:3'"</span></span>
<span id="cb10-115"><a href="#cb10-115"></a><span class="ex">[2024-07-17</span> 07:42:18.431814]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">model:465</span><span class="pp">]</span> <span class="at">-</span> using fused AdamW: True</span>
<span id="cb10-116"><a href="#cb10-116"></a><span class="ex">[2024-07-17</span> 07:42:18.432620]<span class="pp">[</span><span class="ss">CRITICAL</span><span class="pp">][</span><span class="ss">trainer:316</span><span class="pp">]</span> <span class="at">-</span> <span class="st">"devid='cuda:0'"</span></span>
<span id="cb10-117"><a href="#cb10-117"></a><span class="ex">[2024-07-17</span> 07:42:19.951020]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:356</span><span class="pp">]</span> <span class="at">-</span> â€¢ self.model=GPT<span class="er">(</span></span>
<span id="cb10-118"><a href="#cb10-118"></a>  <span class="kw">(</span><span class="ex">transformer</span><span class="kw">)</span><span class="bu">:</span> ModuleDict<span class="er">(</span></span>
<span id="cb10-119"><a href="#cb10-119"></a>    <span class="kw">(</span><span class="ex">wte</span><span class="kw">)</span><span class="bu">:</span> Embedding<span class="er">(</span><span class="ex">65,</span> 768<span class="kw">)</span></span>
<span id="cb10-120"><a href="#cb10-120"></a>    <span class="kw">(</span><span class="ex">wpe</span><span class="kw">)</span><span class="bu">:</span> Embedding<span class="er">(</span><span class="ex">1024,</span> 768<span class="kw">)</span></span>
<span id="cb10-121"><a href="#cb10-121"></a>    <span class="kw">(</span><span class="ex">drop</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb10-122"><a href="#cb10-122"></a>    <span class="kw">(</span><span class="ex">h</span><span class="kw">)</span><span class="bu">:</span> ModuleList<span class="er">(</span></span>
<span id="cb10-123"><a href="#cb10-123"></a>      <span class="kw">(</span><span class="ex">0-11</span><span class="kw">)</span><span class="bu">:</span> 12 x Block<span class="er">(</span></span>
<span id="cb10-124"><a href="#cb10-124"></a>        <span class="kw">(</span><span class="ex">ln_1</span><span class="kw">)</span><span class="bu">:</span> LayerNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb10-125"><a href="#cb10-125"></a>        <span class="kw">(</span><span class="ex">attn</span><span class="kw">)</span><span class="bu">:</span> CausalSelfAttention<span class="er">(</span></span>
<span id="cb10-126"><a href="#cb10-126"></a>          <span class="kw">(</span><span class="ex">c_attn</span><span class="kw">)</span><span class="bu">:</span> Linear<span class="er">(</span><span class="va">in_features</span><span class="op">=</span>768, <span class="va">out_features</span><span class="op">=</span>2304, <span class="va">bias</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb10-127"><a href="#cb10-127"></a>          <span class="kw">(</span><span class="ex">c_proj</span><span class="kw">)</span><span class="bu">:</span> Linear<span class="er">(</span><span class="va">in_features</span><span class="op">=</span>768, <span class="va">out_features</span><span class="op">=</span>768, <span class="va">bias</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb10-128"><a href="#cb10-128"></a>          <span class="kw">(</span><span class="ex">attn_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb10-129"><a href="#cb10-129"></a>          <span class="kw">(</span><span class="ex">resid_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb10-130"><a href="#cb10-130"></a>        <span class="kw">)</span></span>
<span id="cb10-131"><a href="#cb10-131"></a>        <span class="kw">(</span><span class="ex">ln_2</span><span class="kw">)</span><span class="bu">:</span> LayerNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb10-132"><a href="#cb10-132"></a>        <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> MLP<span class="er">(</span></span>
<span id="cb10-133"><a href="#cb10-133"></a>          <span class="kw">(</span><span class="ex">c_fc</span><span class="kw">)</span><span class="bu">:</span> Linear<span class="er">(</span><span class="va">in_features</span><span class="op">=</span>768, <span class="va">out_features</span><span class="op">=</span>3072, <span class="va">bias</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb10-134"><a href="#cb10-134"></a>          <span class="kw">(</span><span class="ex">act_fn</span><span class="kw">)</span><span class="bu">:</span> GELU<span class="er">(</span><span class="va">approximate</span><span class="op">=</span><span class="st">'none'</span><span class="kw">)</span></span>
<span id="cb10-135"><a href="#cb10-135"></a>          <span class="kw">(</span><span class="ex">c_proj</span><span class="kw">)</span><span class="bu">:</span> Linear<span class="er">(</span><span class="va">in_features</span><span class="op">=</span>3072, <span class="va">out_features</span><span class="op">=</span>768, <span class="va">bias</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb10-136"><a href="#cb10-136"></a>          <span class="kw">(</span><span class="ex">dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb10-137"><a href="#cb10-137"></a>        <span class="kw">)</span></span>
<span id="cb10-138"><a href="#cb10-138"></a>      <span class="kw">)</span></span>
<span id="cb10-139"><a href="#cb10-139"></a>    <span class="kw">)</span></span>
<span id="cb10-140"><a href="#cb10-140"></a>    <span class="kw">(</span><span class="ex">ln_f</span><span class="kw">)</span><span class="bu">:</span> LayerNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb10-141"><a href="#cb10-141"></a>  <span class="kw">)</span></span>
<span id="cb10-142"><a href="#cb10-142"></a>  <span class="kw">(</span><span class="ex">lm_head</span><span class="kw">)</span><span class="bu">:</span> Linear<span class="er">(</span><span class="va">in_features</span><span class="op">=</span>768, <span class="va">out_features</span><span class="op">=</span>65, <span class="va">bias</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb10-143"><a href="#cb10-143"></a><span class="kw">)</span></span>
<span id="cb10-144"><a href="#cb10-144"></a><span class="ex">[2024-07-17</span> 07:42:19.955340]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:357</span><span class="pp">]</span> <span class="at">-</span> â€¢ self.grad_scaler=<span class="op">&lt;</span>torch.cuda.amp.grad_scaler.GradScaler object at 0x145a38f0f090<span class="op">&gt;</span></span>
<span id="cb10-145"><a href="#cb10-145"></a><span class="ex">[2024-07-17</span> 07:42:19.956897]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:358</span><span class="pp">]</span> <span class="at">-</span> â€¢ self.model_engine=DistributedDataParallel<span class="er">(</span></span>
<span id="cb10-146"><a href="#cb10-146"></a>  <span class="kw">(</span><span class="ex">module</span><span class="kw">)</span><span class="bu">:</span> GPT<span class="er">(</span></span>
<span id="cb10-147"><a href="#cb10-147"></a>    <span class="kw">(</span><span class="ex">transformer</span><span class="kw">)</span><span class="bu">:</span> ModuleDict<span class="er">(</span></span>
<span id="cb10-148"><a href="#cb10-148"></a>      <span class="kw">(</span><span class="ex">wte</span><span class="kw">)</span><span class="bu">:</span> Embedding<span class="er">(</span><span class="ex">65,</span> 768<span class="kw">)</span></span>
<span id="cb10-149"><a href="#cb10-149"></a>      <span class="kw">(</span><span class="ex">wpe</span><span class="kw">)</span><span class="bu">:</span> Embedding<span class="er">(</span><span class="ex">1024,</span> 768<span class="kw">)</span></span>
<span id="cb10-150"><a href="#cb10-150"></a>      <span class="kw">(</span><span class="ex">drop</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb10-151"><a href="#cb10-151"></a>      <span class="kw">(</span><span class="ex">h</span><span class="kw">)</span><span class="bu">:</span> ModuleList<span class="er">(</span></span>
<span id="cb10-152"><a href="#cb10-152"></a>        <span class="kw">(</span><span class="ex">0-11</span><span class="kw">)</span><span class="bu">:</span> 12 x Block<span class="er">(</span></span>
<span id="cb10-153"><a href="#cb10-153"></a>          <span class="kw">(</span><span class="ex">ln_1</span><span class="kw">)</span><span class="bu">:</span> LayerNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb10-154"><a href="#cb10-154"></a>          <span class="kw">(</span><span class="ex">attn</span><span class="kw">)</span><span class="bu">:</span> CausalSelfAttention<span class="er">(</span></span>
<span id="cb10-155"><a href="#cb10-155"></a>            <span class="kw">(</span><span class="ex">c_attn</span><span class="kw">)</span><span class="bu">:</span> Linear<span class="er">(</span><span class="va">in_features</span><span class="op">=</span>768, <span class="va">out_features</span><span class="op">=</span>2304, <span class="va">bias</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb10-156"><a href="#cb10-156"></a>            <span class="kw">(</span><span class="ex">c_proj</span><span class="kw">)</span><span class="bu">:</span> Linear<span class="er">(</span><span class="va">in_features</span><span class="op">=</span>768, <span class="va">out_features</span><span class="op">=</span>768, <span class="va">bias</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb10-157"><a href="#cb10-157"></a>            <span class="kw">(</span><span class="ex">attn_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb10-158"><a href="#cb10-158"></a>            <span class="kw">(</span><span class="ex">resid_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb10-159"><a href="#cb10-159"></a>          <span class="kw">)</span></span>
<span id="cb10-160"><a href="#cb10-160"></a>          <span class="kw">(</span><span class="ex">ln_2</span><span class="kw">)</span><span class="bu">:</span> LayerNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb10-161"><a href="#cb10-161"></a>          <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> MLP<span class="er">(</span></span>
<span id="cb10-162"><a href="#cb10-162"></a>            <span class="kw">(</span><span class="ex">c_fc</span><span class="kw">)</span><span class="bu">:</span> Linear<span class="er">(</span><span class="va">in_features</span><span class="op">=</span>768, <span class="va">out_features</span><span class="op">=</span>3072, <span class="va">bias</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb10-163"><a href="#cb10-163"></a>            <span class="kw">(</span><span class="ex">act_fn</span><span class="kw">)</span><span class="bu">:</span> GELU<span class="er">(</span><span class="va">approximate</span><span class="op">=</span><span class="st">'none'</span><span class="kw">)</span></span>
<span id="cb10-164"><a href="#cb10-164"></a>            <span class="kw">(</span><span class="ex">c_proj</span><span class="kw">)</span><span class="bu">:</span> Linear<span class="er">(</span><span class="va">in_features</span><span class="op">=</span>3072, <span class="va">out_features</span><span class="op">=</span>768, <span class="va">bias</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb10-165"><a href="#cb10-165"></a>            <span class="kw">(</span><span class="ex">dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb10-166"><a href="#cb10-166"></a>          <span class="kw">)</span></span>
<span id="cb10-167"><a href="#cb10-167"></a>        <span class="kw">)</span></span>
<span id="cb10-168"><a href="#cb10-168"></a>      <span class="kw">)</span></span>
<span id="cb10-169"><a href="#cb10-169"></a>      <span class="kw">(</span><span class="ex">ln_f</span><span class="kw">)</span><span class="bu">:</span> LayerNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb10-170"><a href="#cb10-170"></a>    <span class="kw">)</span></span>
<span id="cb10-171"><a href="#cb10-171"></a>    <span class="kw">(</span><span class="ex">lm_head</span><span class="kw">)</span><span class="bu">:</span> Linear<span class="er">(</span><span class="va">in_features</span><span class="op">=</span>768, <span class="va">out_features</span><span class="op">=</span>65, <span class="va">bias</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb10-172"><a href="#cb10-172"></a>  <span class="kw">)</span></span>
<span id="cb10-173"><a href="#cb10-173"></a><span class="kw">)</span></span>
<span id="cb10-174"><a href="#cb10-174"></a><span class="ex">[2024-07-17</span> 07:42:19.961066]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:359</span><span class="pp">]</span> <span class="at">-</span> â€¢ self.optimizer=AdamW <span class="er">(</span></span>
<span id="cb10-175"><a href="#cb10-175"></a><span class="ex">Parameter</span> Group 0</span>
<span id="cb10-176"><a href="#cb10-176"></a>    <span class="ex">amsgrad:</span> False</span>
<span id="cb10-177"><a href="#cb10-177"></a>    <span class="ex">betas:</span> <span class="er">(</span><span class="ex">0.9,</span> 0.95<span class="kw">)</span></span>
<span id="cb10-178"><a href="#cb10-178"></a>    <span class="ex">capturable:</span> False</span>
<span id="cb10-179"><a href="#cb10-179"></a>    <span class="ex">differentiable:</span> False</span>
<span id="cb10-180"><a href="#cb10-180"></a>    <span class="ex">eps:</span> 1e-08</span>
<span id="cb10-181"><a href="#cb10-181"></a>    <span class="ex">foreach:</span> None</span>
<span id="cb10-182"><a href="#cb10-182"></a>    <span class="ex">fused:</span> True</span>
<span id="cb10-183"><a href="#cb10-183"></a>    <span class="ex">lr:</span> 0.0006</span>
<span id="cb10-184"><a href="#cb10-184"></a>    <span class="ex">maximize:</span> False</span>
<span id="cb10-185"><a href="#cb10-185"></a>    <span class="ex">weight_decay:</span> 0.1</span>
<span id="cb10-186"><a href="#cb10-186"></a></span>
<span id="cb10-187"><a href="#cb10-187"></a><span class="ex">Parameter</span> Group 1</span>
<span id="cb10-188"><a href="#cb10-188"></a>    <span class="ex">amsgrad:</span> False</span>
<span id="cb10-189"><a href="#cb10-189"></a>    <span class="ex">betas:</span> <span class="er">(</span><span class="ex">0.9,</span> 0.95<span class="kw">)</span></span>
<span id="cb10-190"><a href="#cb10-190"></a>    <span class="ex">capturable:</span> False</span>
<span id="cb10-191"><a href="#cb10-191"></a>    <span class="ex">differentiable:</span> False</span>
<span id="cb10-192"><a href="#cb10-192"></a>    <span class="ex">eps:</span> 1e-08</span>
<span id="cb10-193"><a href="#cb10-193"></a>    <span class="ex">foreach:</span> None</span>
<span id="cb10-194"><a href="#cb10-194"></a>    <span class="ex">fused:</span> True</span>
<span id="cb10-195"><a href="#cb10-195"></a>    <span class="ex">lr:</span> 0.0006</span>
<span id="cb10-196"><a href="#cb10-196"></a>    <span class="ex">maximize:</span> False</span>
<span id="cb10-197"><a href="#cb10-197"></a>    <span class="ex">weight_decay:</span> 0.0</span>
<span id="cb10-198"><a href="#cb10-198"></a><span class="kw">)</span></span>
<span id="cb10-199"><a href="#cb10-199"></a><span class="ex">[2024-07-17</span> 07:42:19.988827]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:802</span><span class="pp">]</span> <span class="at">-</span> Startup time: 6.7125</span>
<span id="cb10-200"><a href="#cb10-200"></a>                <span class="ex">Training</span> Legend</span>
<span id="cb10-201"><a href="#cb10-201"></a><span class="ex">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“</span></span>
<span id="cb10-202"><a href="#cb10-202"></a><span class="ex">â”ƒ</span>    abbr     â”ƒ desc                           â”ƒ</span>
<span id="cb10-203"><a href="#cb10-203"></a><span class="ex">â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©</span></span>
<span id="cb10-204"><a href="#cb10-204"></a><span class="ex">â”‚</span>    step     â”‚ Current training iteration     â”‚</span>
<span id="cb10-205"><a href="#cb10-205"></a><span class="ex">â”‚</span>    loss     â”‚ Loss value                     â”‚</span>
<span id="cb10-206"><a href="#cb10-206"></a><span class="ex">â”‚</span>     dt      â”‚ Elapsed time per training step â”‚</span>
<span id="cb10-207"><a href="#cb10-207"></a><span class="ex">â”‚</span>     dtf     â”‚ Elapsed time per forward step  â”‚</span>
<span id="cb10-208"><a href="#cb10-208"></a><span class="ex">â”‚</span>     dtb     â”‚ Elapsed time per backward step â”‚</span>
<span id="cb10-209"><a href="#cb10-209"></a><span class="ex">â”‚</span>     sps     â”‚ Samples per second             â”‚</span>
<span id="cb10-210"><a href="#cb10-210"></a><span class="ex">â”‚</span> sps_per_gpu â”‚ Samples per second <span class="er">(</span><span class="ex">per</span> GPU<span class="kw">)</span>   <span class="ex">â”‚</span></span>
<span id="cb10-211"><a href="#cb10-211"></a><span class="ex">â”‚</span>     tps     â”‚ Tokens per second              â”‚</span>
<span id="cb10-212"><a href="#cb10-212"></a><span class="ex">â”‚</span> tps_per_gpu â”‚ Tokens per second <span class="er">(</span><span class="ex">per</span> GPU<span class="kw">)</span>    <span class="ex">â”‚</span></span>
<span id="cb10-213"><a href="#cb10-213"></a><span class="ex">â”‚</span>     mfu     â”‚ Model flops utilization        â”‚</span>
<span id="cb10-214"><a href="#cb10-214"></a><span class="ex">â”‚</span> train_loss  â”‚ Training loss value            â”‚</span>
<span id="cb10-215"><a href="#cb10-215"></a><span class="ex">â”‚</span>  val_loss   â”‚ Validation loss value          â”‚</span>
<span id="cb10-216"><a href="#cb10-216"></a><span class="ex">â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span></span>
<span id="cb10-217"><a href="#cb10-217"></a><span class="ex">[2024-07-17</span> 07:42:21.451865]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:820</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="st">'prompt'</span><span class="pp">]</span>: <span class="st">'What is an LLM?'</span></span>
<span id="cb10-218"><a href="#cb10-218"></a><span class="ex">[2024-07-17</span> 07:42:21.452667]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:824</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="st">'response'</span><span class="pp">]</span>:</span>
<span id="cb10-219"><a href="#cb10-219"></a><span class="ex">What</span> is an LLM<span class="pp">?</span>eelEl\'<span class="va">$nltPwBSWal</span>,<span class="kw">;</span><span class="ex">PWw</span> bbu<span class="dt">\'</span>HiyP<span class="dt">\'</span>FWwF <span class="kw">&amp;</span><span class="ex">AhW:ygrn</span> kk-<span class="dt">\'\'</span>KFlMwnlEfflkc,elpWaWtgml<span class="va">$Pgglhllw</span> lglhFllzczPAFHpeAAPPSltgkrWPPhlEMgcrN ggPWt-WPSSzHSkkrzzk.FFrtSSkgMll<span class="kw">&amp;</span><span class="ex">gFXr,hghaueaVPW-pHFF-gg,,,FF,,kbApgg</span> gg<span class="dt">\'</span>aWWzzkk<span class="dt">\'</span>a<span class="dt">\'</span>CggHl<span class="va">$bGeA</span>,FFk,,SF<span class="kw">;</span><span class="ex">UF,,aZ</span> <span class="kw">;</span><span class="ex">gglee$,k.US</span><span class="kw">&amp;</span><span class="ex">kg:S,,zVzzc</span></span>
<span id="cb10-220"><a href="#cb10-220"></a><span class="ex">[2024-07-17</span> 07:43:01.573073]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=10 loss=3.154310 dt=0.282833 dtf=0.005247 dtb=0.011417 sps=14.142633 sps_per_gpu=3.535658 tps=926851.609409 tps_per_gpu=231712.902352 mfu=46.288281 train_loss=4.125778 val_loss=4.128809</span>
<span id="cb10-221"><a href="#cb10-221"></a><span class="ex">[2024-07-17</span> 07:43:04.402750]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=20 loss=2.660851 dt=0.306263 dtf=0.005233 dtb=0.011419 sps=13.060678 sps_per_gpu=3.265170 tps=855944.613638 tps_per_gpu=213986.153409 mfu=45.934162 train_loss=4.125778 val_loss=4.128809</span>
<span id="cb10-222"><a href="#cb10-222"></a><span class="ex">[2024-07-17</span> 07:43:07.237507]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=30 loss=2.543283 dt=0.283021 dtf=0.005238 dtb=0.011245 sps=14.133211 sps_per_gpu=3.533303 tps=926234.088226 tps_per_gpu=231558.522057 mfu=45.966490 train_loss=4.125778 val_loss=4.128809</span>
<span id="cb10-223"><a href="#cb10-223"></a><span class="ex">[2024-07-17</span> 07:43:10.077248]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=40 loss=2.503963 dt=0.285001 dtf=0.005213 dtb=0.011471 sps=14.035061 sps_per_gpu=3.508765 tps=919801.749941 tps_per_gpu=229950.437485 mfu=45.963461 train_loss=4.125778 val_loss=4.128809</span>
<span id="cb10-224"><a href="#cb10-224"></a><span class="ex">[2024-07-17</span> 07:43:12.917039]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=50 loss=2.477469 dt=0.283532 dtf=0.005166 dtb=0.011294 sps=14.107763 sps_per_gpu=3.526941 tps=924566.380009 tps_per_gpu=231141.595002 mfu=45.984530 train_loss=4.125778 val_loss=4.128809</span>
<span id="cb10-225"><a href="#cb10-225"></a><span class="ex">[2024-07-17</span> 07:43:15.760749]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=60 loss=2.471083 dt=0.284630 dtf=0.005140 dtb=0.011224 sps=14.053326 sps_per_gpu=3.513332 tps=920998.786204 tps_per_gpu=230249.696551 mfu=45.985675 train_loss=4.125778 val_loss=4.128809</span>
<span id="cb10-226"><a href="#cb10-226"></a><span class="ex">[2024-07-17</span> 07:43:18.602785]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=70 loss=2.458894 dt=0.283926 dtf=0.005219 dtb=0.010383 sps=14.088155 sps_per_gpu=3.522039 tps=923281.352698 tps_per_gpu=230820.338174 mfu=45.998106 train_loss=4.125778 val_loss=4.128809</span>
<span id="cb10-227"><a href="#cb10-227"></a><span class="ex">[2024-07-17</span> 07:43:21.451433]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=80 loss=2.489088 dt=0.285537 dtf=0.005183 dtb=0.011373 sps=14.008683 sps_per_gpu=3.502171 tps=918073.060430 tps_per_gpu=229518.265108 mfu=45.983282 train_loss=4.125778 val_loss=4.128809</span>
<span id="cb10-228"><a href="#cb10-228"></a><span class="ex">[2024-07-17</span> 07:43:24.302241]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=90 loss=2.471990 dt=0.300767 dtf=0.005445 dtb=0.010290 sps=13.299337 sps_per_gpu=3.324834 tps=871585.359388 tps_per_gpu=217896.339847 mfu=45.737774 train_loss=4.125778 val_loss=4.128809</span>
<span id="cb10-229"><a href="#cb10-229"></a><span class="ex">[2024-07-17</span> 07:43:27.153275]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=100 loss=2.445556 dt=0.285869 dtf=0.005182 dtb=0.011251 sps=13.992403 sps_per_gpu=3.498101 tps=917006.151328 tps_per_gpu=229251.537832 mfu=45.743655 train_loss=4.125778 val_loss=4.128809</span>
<span id="cb10-230"><a href="#cb10-230"></a><span class="ex">[2024-07-17</span> 07:43:28.182553]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:820</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="st">'prompt'</span><span class="pp">]</span>: <span class="st">'What is an LLM?'</span></span>
<span id="cb10-231"><a href="#cb10-231"></a><span class="ex">[2024-07-17</span> 07:43:28.183179]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:824</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="st">'response'</span><span class="pp">]</span>:</span>
<span id="cb10-232"><a href="#cb10-232"></a></span>
<span id="cb10-233"><a href="#cb10-233"></a><span class="ex">What</span> is an LLM<span class="pp">?</span></span>
<span id="cb10-234"><a href="#cb10-234"></a></span>
<span id="cb10-235"><a href="#cb10-235"></a><span class="ex">Goupay</span> my winghimithell bls ger t bon sinthard ht omind be,</span>
<span id="cb10-236"><a href="#cb10-236"></a><span class="ex">And</span> lereind h py balithand frd oforondof wimon me hageas thinero mand,</span>
<span id="cb10-237"><a href="#cb10-237"></a><span class="ex">Thacanes,</span></span>
<span id="cb10-238"><a href="#cb10-238"></a><span class="ex">An</span> frift ghik med d herthecke ntore thack couthen ale, t thit ang d m t h chy me fache ag, wit my hathan glat ng</span>
<span id="cb10-239"><a href="#cb10-239"></a><span class="ex">[2024-07-17</span> 07:44:06.025837]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:760</span><span class="pp">]</span> <span class="at">-</span> Saving checkpoint to: /home/foremans/tmp/polaris-talk/outputs/runs/pytorch/DDP/2024-07-17/07-42-13</span>
<span id="cb10-240"><a href="#cb10-240"></a><span class="ex">[2024-07-17</span> 07:44:06.026607]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:761</span><span class="pp">]</span> <span class="at">-</span> Saving model to: /home/foremans/tmp/polaris-talk/outputs/runs/pytorch/DDP/2024-07-17/07-42-13/model.pth</span>
<span id="cb10-241"><a href="#cb10-241"></a><span class="ex">[2024-07-17</span> 07:44:07.682968]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">configs:141</span><span class="pp">]</span> <span class="at">-</span> Appending /home/foremans/tmp/polaris-talk/outputs/runs/pytorch/DDP/2024-07-17/07-42-13 to /home/foremans/tmp/polaris-talk/2024-07-17-073327/wordplay/src/ckpts/checkpoints.log</span>
<span id="cb10-242"><a href="#cb10-242"></a><span class="ex">[2024-07-17</span> 07:44:10.519506]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=110 loss=2.433923 dt=0.285038 dtf=0.005757 dtb=0.011762 sps=14.033209 sps_per_gpu=3.508302 tps=919680.367894 tps_per_gpu=229920.091974 mfu=45.762304 train_loss=2.439494 val_loss=2.478951</span>
<span id="cb10-243"><a href="#cb10-243"></a><span class="ex">[2024-07-17</span> 07:44:13.362148]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=120 loss=2.429014 dt=0.284445 dtf=0.005222 dtb=0.011486 sps=14.062460 sps_per_gpu=3.515615 tps=921597.361532 tps_per_gpu=230399.340383 mfu=45.788661 train_loss=2.439494 val_loss=2.478951</span>
<span id="cb10-244"><a href="#cb10-244"></a><span class="ex">[2024-07-17</span> 07:44:16.210694]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=130 loss=2.402059 dt=0.285559 dtf=0.005199 dtb=0.011765 sps=14.007633 sps_per_gpu=3.501908 tps=918004.211586 tps_per_gpu=229501.052897 mfu=45.794438 train_loss=2.439494 val_loss=2.478951</span>
<span id="cb10-245"><a href="#cb10-245"></a><span class="ex">[2024-07-17</span> 07:44:19.061546]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=140 loss=2.374062 dt=0.285476 dtf=0.005239 dtb=0.011453 sps=14.011662 sps_per_gpu=3.502916 tps=918268.297093 tps_per_gpu=229567.074273 mfu=45.800956 train_loss=2.439494 val_loss=2.478951</span>
<span id="cb10-246"><a href="#cb10-246"></a><span class="ex">[2024-07-17</span> 07:44:21.917283]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=150 loss=2.365385 dt=0.285846 dtf=0.005125 dtb=0.011320 sps=13.993568 sps_per_gpu=3.498392 tps=917082.475791 tps_per_gpu=229270.618948 mfu=45.800900 train_loss=2.439494 val_loss=2.478951</span>
<span id="cb10-247"><a href="#cb10-247"></a><span class="ex">[2024-07-17</span> 07:44:24.771924]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=160 loss=2.317337 dt=0.280788 dtf=0.005173 dtb=0.011249 sps=14.245602 sps_per_gpu=3.561401 tps=933599.792506 tps_per_gpu=233399.948127 mfu=45.883340 train_loss=2.439494 val_loss=2.478951</span>
<span id="cb10-248"><a href="#cb10-248"></a><span class="ex">[2024-07-17</span> 07:44:27.626812]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=170 loss=2.256231 dt=0.284973 dtf=0.005141 dtb=0.011299 sps=14.036416 sps_per_gpu=3.509104 tps=919890.544506 tps_per_gpu=229972.636126 mfu=45.889069 train_loss=2.439494 val_loss=2.478951</span>
<span id="cb10-249"><a href="#cb10-249"></a><span class="ex">[2024-07-17</span> 07:44:30.480952]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=180 loss=2.216419 dt=0.286555 dtf=0.005180 dtb=0.011402 sps=13.958906 sps_per_gpu=3.489726 tps=914810.852170 tps_per_gpu=228702.713043 mfu=45.868857 train_loss=2.439494 val_loss=2.478951</span>
<span id="cb10-250"><a href="#cb10-250"></a><span class="ex">[2024-07-17</span> 07:44:33.337342]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=190 loss=2.145123 dt=0.291456 dtf=0.005409 dtb=0.019347 sps=13.724205 sps_per_gpu=3.431051 tps=899429.467247 tps_per_gpu=224857.366812 mfu=45.773849 train_loss=2.439494 val_loss=2.478951</span>
<span id="cb10-251"><a href="#cb10-251"></a><span class="ex">[2024-07-17</span> 07:44:36.194584]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=200 loss=2.068149 dt=0.285703 dtf=0.005153 dtb=0.011286 sps=14.000555 sps_per_gpu=3.500139 tps=917540.393411 tps_per_gpu=229385.098353 mfu=45.778791 train_loss=2.439494 val_loss=2.478951</span>
<span id="cb10-252"><a href="#cb10-252"></a><span class="ex">[2024-07-17</span> 07:44:37.224149]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:820</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="st">'prompt'</span><span class="pp">]</span>: <span class="st">'What is an LLM?'</span></span>
<span id="cb10-253"><a href="#cb10-253"></a><span class="ex">[2024-07-17</span> 07:44:37.224745]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:824</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="st">'response'</span><span class="pp">]</span>:</span>
<span id="cb10-254"><a href="#cb10-254"></a></span>
<span id="cb10-255"><a href="#cb10-255"></a><span class="ex">What</span> is an LLM<span class="pp">?</span></span>
<span id="cb10-256"><a href="#cb10-256"></a></span>
<span id="cb10-257"><a href="#cb10-257"></a><span class="ex">LORTESS</span> LA:</span>
<span id="cb10-258"><a href="#cb10-258"></a><span class="ex">No,</span> sighappat selace<span class="pp">?</span> don downd sourciceans note cancen up sof liond</span>
<span id="cb10-259"><a href="#cb10-259"></a><span class="ex">This</span> and my man, werame, of re thee</span>
<span id="cb10-260"><a href="#cb10-260"></a><span class="ex">Thise</span> not will I on land brond sul me a fingore<span class="pp">?</span></span>
<span id="cb10-261"><a href="#cb10-261"></a></span>
<span id="cb10-262"><a href="#cb10-262"></a><span class="ex">FLER:</span></span>
<span id="cb10-263"><a href="#cb10-263"></a><span class="ex">Tisint</span> your not nare lame o igen,-to brorst.</span>
<span id="cb10-264"><a href="#cb10-264"></a></span>
<span id="cb10-265"><a href="#cb10-265"></a><span class="ex">SamERS:</span></span>
<span id="cb10-266"><a href="#cb10-266"></a><span class="ex">Sin:</span></span>
<span id="cb10-267"><a href="#cb10-267"></a><span class="ex">I\'l</span> hell she lor hen w</span>
<span id="cb10-268"><a href="#cb10-268"></a><span class="ex">[2024-07-17</span> 07:45:14.409129]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:760</span><span class="pp">]</span> <span class="at">-</span> Saving checkpoint to: /home/foremans/tmp/polaris-talk/outputs/runs/pytorch/DDP/2024-07-17/07-42-13</span>
<span id="cb10-269"><a href="#cb10-269"></a><span class="ex">[2024-07-17</span> 07:45:14.409820]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:761</span><span class="pp">]</span> <span class="at">-</span> Saving model to: /home/foremans/tmp/polaris-talk/outputs/runs/pytorch/DDP/2024-07-17/07-42-13/model.pth</span>
<span id="cb10-270"><a href="#cb10-270"></a><span class="ex">[2024-07-17</span> 07:45:16.366935]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">configs:141</span><span class="pp">]</span> <span class="at">-</span> Appending /home/foremans/tmp/polaris-talk/outputs/runs/pytorch/DDP/2024-07-17/07-42-13 to /home/foremans/tmp/polaris-talk/2024-07-17-073327/wordplay/src/ckpts/checkpoints.log</span>
<span id="cb10-271"><a href="#cb10-271"></a><span class="ex">[2024-07-17</span> 07:45:19.245061]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=210 loss=1.982169 dt=0.283305 dtf=0.005223 dtb=0.011284 sps=14.119042 sps_per_gpu=3.529760 tps=925305.515083 tps_per_gpu=231326.378771 mfu=45.822019 train_loss=2.045786 val_loss=2.148510</span>
<span id="cb10-272"><a href="#cb10-272"></a><span class="ex">[2024-07-17</span> 07:45:22.092430]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=220 loss=1.897731 dt=0.284759 dtf=0.005217 dtb=0.011187 sps=14.046945 sps_per_gpu=3.511736 tps=920580.608106 tps_per_gpu=230145.152026 mfu=45.837327 train_loss=2.045786 val_loss=2.148510</span>
<span id="cb10-273"><a href="#cb10-273"></a><span class="ex">[2024-07-17</span> 07:45:24.942639]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=230 loss=1.817213 dt=0.285266 dtf=0.005208 dtb=0.011446 sps=14.022003 sps_per_gpu=3.505501 tps=918945.985503 tps_per_gpu=229736.496376 mfu=45.842940 train_loss=2.045786 val_loss=2.148510</span>
<span id="cb10-274"><a href="#cb10-274"></a><span class="ex">[2024-07-17</span> 07:45:27.797910]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=240 loss=1.779287 dt=0.285465 dtf=0.005189 dtb=0.011220 sps=14.012250 sps_per_gpu=3.503062 tps=918306.793546 tps_per_gpu=229576.698387 mfu=45.844800 train_loss=2.045786 val_loss=2.148510</span>
<span id="cb10-275"><a href="#cb10-275"></a><span class="ex">[2024-07-17</span> 07:45:30.653597]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=250 loss=1.704220 dt=0.289284 dtf=0.005471 dtb=0.010346 sps=13.827253 sps_per_gpu=3.456813 tps=906182.836379 tps_per_gpu=226545.709095 mfu=45.785926 train_loss=2.045786 val_loss=2.148510</span>
<span id="cb10-276"><a href="#cb10-276"></a><span class="ex">[2024-07-17</span> 07:45:33.512769]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=260 loss=1.671318 dt=0.287679 dtf=0.005125 dtb=0.011250 sps=13.904380 sps_per_gpu=3.476095 tps=911237.442617 tps_per_gpu=227809.360654 mfu=45.758182 train_loss=2.045786 val_loss=2.148510</span>
<span id="cb10-277"><a href="#cb10-277"></a><span class="ex">[2024-07-17</span> 07:45:36.373461]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=270 loss=1.650952 dt=0.298661 dtf=0.005118 dtb=0.011520 sps=13.393107 sps_per_gpu=3.348277 tps=877730.651421 tps_per_gpu=219432.662855 mfu=45.565875 train_loss=2.045786 val_loss=2.148510</span>
<span id="cb10-278"><a href="#cb10-278"></a><span class="ex">[2024-07-17</span> 07:45:39.236930]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=280 loss=1.573242 dt=0.285970 dtf=0.005171 dtb=0.011290 sps=13.987477 sps_per_gpu=3.496869 tps=916683.279847 tps_per_gpu=229170.819962 mfu=45.587333 train_loss=2.045786 val_loss=2.148510</span>
<span id="cb10-279"><a href="#cb10-279"></a><span class="ex">[2024-07-17</span> 07:45:42.100605]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=290 loss=1.533265 dt=0.286487 dtf=0.005432 dtb=0.011288 sps=13.962259 sps_per_gpu=3.490565 tps=915030.617828 tps_per_gpu=228757.654457 mfu=45.598392 train_loss=2.045786 val_loss=2.148510</span>
<span id="cb10-280"><a href="#cb10-280"></a><span class="ex">[2024-07-17</span> 07:45:44.964424]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=300 loss=1.492064 dt=0.288480 dtf=0.005355 dtb=0.011480 sps=13.865774 sps_per_gpu=3.466443 tps=908707.340870 tps_per_gpu=227176.835218 mfu=45.576766 train_loss=2.045786 val_loss=2.148510</span>
<span id="cb10-281"><a href="#cb10-281"></a><span class="ex">[2024-07-17</span> 07:45:45.995833]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:820</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="st">'prompt'</span><span class="pp">]</span>: <span class="st">'What is an LLM?'</span></span>
<span id="cb10-282"><a href="#cb10-282"></a><span class="ex">[2024-07-17</span> 07:45:45.996497]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:824</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="st">'response'</span><span class="pp">]</span>:</span>
<span id="cb10-283"><a href="#cb10-283"></a></span>
<span id="cb10-284"><a href="#cb10-284"></a><span class="ex">What</span> is an LLM<span class="pp">?</span></span>
<span id="cb10-285"><a href="#cb10-285"></a></span>
<span id="cb10-286"><a href="#cb10-286"></a><span class="ex">RICHMORD:</span></span>
<span id="cb10-287"><a href="#cb10-287"></a><span class="ex">Char</span> stire<span class="pp">?</span> how in those are name the range hone.</span>
<span id="cb10-288"><a href="#cb10-288"></a></span>
<span id="cb10-289"><a href="#cb10-289"></a><span class="ex">GLOUCESTER:</span></span>
<span id="cb10-290"><a href="#cb10-290"></a><span class="ex">Nay,</span> in lond<span class="st">'s time the palt are worder more</span></span>
<span id="cb10-291"><a href="#cb10-291"></a><span class="st">That wilt in the purpose be a pey</span></span>
<span id="cb10-292"><a href="#cb10-292"></a><span class="st">And thou thine onter hands, and the which broth.</span></span>
<span id="cb10-293"><a href="#cb10-293"></a></span>
<span id="cb10-294"><a href="#cb10-294"></a><span class="st">ELBOWINCA:</span></span>
<span id="cb10-295"><a href="#cb10-295"></a><span class="st">At lie my lord with the me an arms be a s</span></span>
<span id="cb10-296"><a href="#cb10-296"></a><span class="st">[2024-07-17 07:46:23.549987][INFO][trainer:760] - Saving checkpoint to: /home/foremans/tmp/polaris-talk/outputs/runs/pytorch/DDP/2024-07-17/07-42-13</span></span>
<span id="cb10-297"><a href="#cb10-297"></a><span class="st">[2024-07-17 07:46:23.550696][INFO][trainer:761] - Saving model to: /home/foremans/tmp/polaris-talk/outputs/runs/pytorch/DDP/2024-07-17/07-42-13/model.pth</span></span>
<span id="cb10-298"><a href="#cb10-298"></a><span class="st">[2024-07-17 07:46:25.496559][INFO][configs:141] - Appending /home/foremans/tmp/polaris-talk/outputs/runs/pytorch/DDP/2024-07-17/07-42-13 to /home/foremans/tmp/polaris-talk/2024-07-17-073327/wordplay/src/ckpts/checkpoints.log</span></span>
<span id="cb10-299"><a href="#cb10-299"></a><span class="st">[2024-07-17 07:46:28.374854][INFO][trainer:885] - step=310 loss=1.444200 dt=0.299907 dtf=0.005333 dtb=0.010637 sps=13.337481 sps_per_gpu=3.334370 tps=874085.133345 tps_per_gpu=218521.283336 mfu=45.384395 train_loss=1.495372 val_loss=1.713714</span></span>
<span id="cb10-300"><a href="#cb10-300"></a><span class="st">[2024-07-17 07:46:31.223079][INFO][trainer:885] - step=320 loss=1.429350 dt=0.285238 dtf=0.005245 dtb=0.011485 sps=14.023353 sps_per_gpu=3.505838 tps=919034.479880 tps_per_gpu=229758.619970 mfu=45.435743 train_loss=1.495372 val_loss=1.713714</span></span>
<span id="cb10-301"><a href="#cb10-301"></a><span class="st">[2024-07-17 07:46:34.074957][INFO][trainer:885] - step=330 loss=1.362220 dt=0.285027 dtf=0.005165 dtb=0.011407 sps=14.033736 sps_per_gpu=3.508434 tps=919714.904826 tps_per_gpu=229928.726207 mfu=45.485355 train_loss=1.495372 val_loss=1.713714</span></span>
<span id="cb10-302"><a href="#cb10-302"></a><span class="st">[2024-07-17 07:46:36.929464][INFO][trainer:885] - step=340 loss=1.350888 dt=0.284436 dtf=0.005199 dtb=0.011287 sps=14.062893 sps_per_gpu=3.515723 tps=921625.744709 tps_per_gpu=230406.436177 mfu=45.539549 train_loss=1.495372 val_loss=1.713714</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="wordplay-example-video" class="level3" data-background-color="#121314">
<h3 data-background-color="#121314" class="anchored" data-anchor-id="wordplay-example-video"><i class="fa-brands fa-github" aria-label="github"></i> <a href="https://github.com/saforem2/wordplay"><code>wordplay</code></a>: Example [<a href="https://asciinema.org/a/668462">video</a>]</h3>
<div id="fig-wordplay-asciinema" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-wordplay-asciinema-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<script src="https://asciinema.org/a/668462.js" id="asciicast-668462" async="true"></script>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-wordplay-asciinema-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;28: Training a LLM to talk like Shakespeare using <a href="https://github.com/saforem2/wordplay">saforem2/<code>wordplay</code> ğŸ®ğŸ’¬</a>
</figcaption>
</figure>
</div>
</section>
</section>
<section id="thank-you" class="level2" data-background-color="white">
<h2 data-background-color="white" class="anchored" data-anchor-id="thank-you">â¤ï¸ Thank you!</h2>
<ul>
<li><p>Organizers</p></li>
<li><p>Feel free to reach out!</p>
<p><split even=""></split></p>
<p><a href="https://samforeman.me"><i class="fas fa-home"></i></a> <a href="mailto:foremans@anl.gov"><i class="far fa-paper-plane"></i></a> <a href="https://www.twitter.com/saforem2"><i class="fab fa-twitter"></i></a></p>
<p></p></li>
</ul>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="Acknowledgements">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Acknowledgements
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>This research used resources of the Argonne Leadership Computing Facility, which is a DOE Office of Science User Facility supported under Contract DE-AC02-06CH11357</p>
</div>
</div>
</div>
</section>
<section id="references" class="level2" data-background-color="white">
<h2 data-background-color="white" class="anchored" data-anchor-id="references">ğŸ““ References</h2>
<ul>
<li>Title slide (Tetris animation) from: <a href="https://emilhvitfeldt.github.io/quarto-iframe-examples/tetris/index.html" class="uri">https://emilhvitfeldt.github.io/quarto-iframe-examples/tetris/index.html</a></li>
</ul>




</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-wei2022emergentabilitieslargelanguage" class="csl-entry" role="listitem">
Wei, Jason, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, et al. 2022. <span>â€œEmergent Abilities of Large Language Models.â€</span> <a href="https://arxiv.org/abs/2206.07682">https://arxiv.org/abs/2206.07682</a>.
</div>
<div id="ref-yao2023tree" class="csl-entry" role="listitem">
Yao, Shunyu, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. <span>â€œTree of Thoughts: Deliberate Problem Solving with Large Language Models.â€</span> <a href="https://arxiv.org/abs/2305.10601">https://arxiv.org/abs/2305.10601</a>.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p><code>micro_batch_size</code> = batch_size <strong>per</strong> GPU<a href="#fnref1" class="footnote-back" role="doc-backlink">â†©ï¸</a></p></li>
<li id="fn2"><p><a href="https://arxiv.org/abs/2104.04473">Efficient Large-Scale Language Model Training on GPU Clusters</a><a href="#fnref2" class="footnote-back" role="doc-backlink">â†©ï¸</a></p></li>
<li id="fn3"><p>Source: <a href="https://github.com/Hannibal046/Awesome-LLM"><i class="fa-brands fa-github" aria-label="github"></i> <code>Hannibal046/Awesome-LLM</code></a><a href="#fnref3" class="footnote-back" role="doc-backlink">â†©ï¸</a></p></li>
<li id="fn4"><p>Figure from <a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a><a href="#fnref4" class="footnote-back" role="doc-backlink">â†©ï¸</a></p></li>
<li id="fn5"><p>Figure from <a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a><a href="#fnref5" class="footnote-back" role="doc-backlink">â†©ï¸</a></p></li>
<li id="fn6"><p>Video from: <a href="https://huggingface.co/docs/transformers/main/en/llm_tutorial">ğŸ¤— Generation with LLMs</a><a href="#fnref6" class="footnote-back" role="doc-backlink">â†©ï¸</a></p></li>
<li id="fn7"><p>Video from: <a href="https://huggingface.co/docs/transformers/main/en/llm_tutorial">ğŸ¤— Generation with LLMs</a><a href="#fnref7" class="footnote-back" role="doc-backlink">â†©ï¸</a></p></li>
</ol>
</section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@unpublished{foreman2024,
  author = {Foreman, Sam},
  title = {Parallel {Training} {Methods}},
  date = {2024-11-05},
  url = {https://samforeman.me/talks/ai-for-science-2024/slides},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-foreman2024" class="csl-entry quarto-appendix-citeas" role="listitem">
Foreman, Sam. 2024. <span>â€œParallel Training Methods.â€</span> November
5. <a href="https://samforeman.me/talks/ai-for-science-2024/slides">https://samforeman.me/talks/ai-for-science-2024/slides</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = true;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/samforeman\.me");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<input type="hidden" id="giscus-base-theme" value="dark">
<input type="hidden" id="giscus-alt-theme" value="light">
<script>
function loadGiscusWhenReady() {
  // Function to get the theme based on body class
  const getTheme = () => {
    const baseTheme = document.getElementById('giscus-base-theme').value;
    const altTheme = document.getElementById('giscus-alt-theme').value;
    return document.body.classList.contains('quarto-dark') ? altTheme : baseTheme;
  };
  // Create the Giscus script and add it to the desired location
  const loadGiscus = () => {
    const script = document.createElement("script");
    script.src = "https://giscus.app/client.js";
    script.async = true;
    script.dataset.repo = "saforem2/personal_site";
    script.dataset.repoId = "R_kgDOGbjyRw";
    script.dataset.category = "General";
    script.dataset.categoryId = "DIC_kwDOGbjyR84CjWfk";
    script.dataset.mapping = "title";
    script.dataset.reactionsEnabled = "1";
    script.dataset.emitMetadata = "0";
    script.dataset.inputPosition = "top";
    script.dataset.theme = getTheme();
    script.dataset.lang = "en";
    script.crossOrigin = "anonymous";
    // Append the script to the desired div instead of at the end of the body
    document.getElementById("quarto-content").appendChild(script);
  };
  // MutationObserver to detect when the 'quarto-light' or 'quarto-dark' class is added to the body
  const observer = new MutationObserver((mutations) => {
    for (const mutation of mutations) {
      if (mutation.type === "attributes" && mutation.attributeName === "class") {
        if (document.body.classList.contains('quarto-light') || document.body.classList.contains('quarto-dark')) {
          loadGiscus();
          observer.disconnect(); // Stop observing once Giscus is loaded
          break;
        }
      }
    }
  });
  // Start observing the body for class attribute changes
  observer.observe(document.body, {
    attributes: true,
    attributeFilter: ["class"],
  });
}
loadGiscusWhenReady();
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb11" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb11-1"><a href="#cb11-1"></a><span class="co">---</span></span>
<span id="cb11-2"><a href="#cb11-2"></a><span class="an">title:</span><span class="co"> "Parallel Training Methods"</span></span>
<span id="cb11-3"><a href="#cb11-3"></a><span class="an">callout-style:</span><span class="co"> simple</span></span>
<span id="cb11-4"><a href="#cb11-4"></a><span class="an">location:</span><span class="co"> "Intro to AI-driven Science on Supercomputers"</span></span>
<span id="cb11-5"><a href="#cb11-5"></a><span class="an">location-url:</span><span class="co"> "https://www.alcf.anl.gov/alcf-ai-science-training-series"</span></span>
<span id="cb11-6"><a href="#cb11-6"></a><span class="co"># location: "[Intro to AI-driven Science on Supercomputers](https://www.alcf.anl.gov/alcf-ai-science-training-series)"</span></span>
<span id="cb11-7"><a href="#cb11-7"></a><span class="an">date:</span><span class="co"> 2024-11-05</span></span>
<span id="cb11-8"><a href="#cb11-8"></a><span class="an">number-sections:</span><span class="co"> false</span></span>
<span id="cb11-9"><a href="#cb11-9"></a><span class="an">image:</span><span class="co"> ./assets/thumbnail.png</span></span>
<span id="cb11-10"><a href="#cb11-10"></a><span class="an">lightbox:</span><span class="co"> auto</span></span>
<span id="cb11-11"><a href="#cb11-11"></a><span class="an">editor:</span></span>
<span id="cb11-12"><a href="#cb11-12"></a><span class="co">  render-on-save: true</span></span>
<span id="cb11-13"><a href="#cb11-13"></a><span class="co">  freeze: auto</span></span>
<span id="cb11-14"><a href="#cb11-14"></a><span class="an">twitter-card:</span></span>
<span id="cb11-15"><a href="#cb11-15"></a><span class="co">  image: ./assets/thumbnail.png</span></span>
<span id="cb11-16"><a href="#cb11-16"></a><span class="co">  site: "saforem2"</span></span>
<span id="cb11-17"><a href="#cb11-17"></a><span class="co">  creator: "saforem2"</span></span>
<span id="cb11-18"><a href="#cb11-18"></a><span class="co">  title: "Parallel Training Methods for AI"</span></span>
<span id="cb11-19"><a href="#cb11-19"></a><span class="co">  description: ""</span></span>
<span id="cb11-20"><a href="#cb11-20"></a><span class="co">  card-style: summary</span></span>
<span id="cb11-21"><a href="#cb11-21"></a><span class="an">open-graph:</span></span>
<span id="cb11-22"><a href="#cb11-22"></a><span class="co">  title: "Parallel Training Methods for AI"</span></span>
<span id="cb11-23"><a href="#cb11-23"></a><span class="co">  description: ""</span></span>
<span id="cb11-24"><a href="#cb11-24"></a><span class="co">  image: "./assets/thumbnail.png"</span></span>
<span id="cb11-25"><a href="#cb11-25"></a><span class="an">citation:</span></span>
<span id="cb11-26"><a href="#cb11-26"></a><span class="co">   author: Sam Foreman</span></span>
<span id="cb11-27"><a href="#cb11-27"></a><span class="co">   type: speech</span></span>
<span id="cb11-28"><a href="#cb11-28"></a><span class="co">   url: https://samforeman.me/talks/ai-for-science-2024/slides</span></span>
<span id="cb11-29"><a href="#cb11-29"></a><span class="co"># toc-expand: true</span></span>
<span id="cb11-30"><a href="#cb11-30"></a><span class="an">format:</span></span>
<span id="cb11-31"><a href="#cb11-31"></a><span class="co">  html:</span></span>
<span id="cb11-32"><a href="#cb11-32"></a><span class="co">      shift-heading-level-by: 1</span></span>
<span id="cb11-33"><a href="#cb11-33"></a><span class="co">      image: "./assets/thumbnail.png"</span></span>
<span id="cb11-34"><a href="#cb11-34"></a><span class="co">      mermaid:</span></span>
<span id="cb11-35"><a href="#cb11-35"></a><span class="co">        layout: elk</span></span>
<span id="cb11-36"><a href="#cb11-36"></a><span class="co">        useMaxWidth: true</span></span>
<span id="cb11-37"><a href="#cb11-37"></a><span class="co">  gfm:</span></span>
<span id="cb11-38"><a href="#cb11-38"></a><span class="co">    shift-heading-level-by: 1</span></span>
<span id="cb11-39"><a href="#cb11-39"></a><span class="co">    output-file: "ai-for-science-2024.md"</span></span>
<span id="cb11-40"><a href="#cb11-40"></a><span class="co">  revealjs:</span></span>
<span id="cb11-41"><a href="#cb11-41"></a><span class="co">      resources:</span></span>
<span id="cb11-42"><a href="#cb11-42"></a><span class="co">        - ./tetris/</span></span>
<span id="cb11-43"><a href="#cb11-43"></a><span class="co">      image: "./assets/thumbnail.png"</span></span>
<span id="cb11-44"><a href="#cb11-44"></a><span class="co">      pdf-separate-fragments: true</span></span>
<span id="cb11-45"><a href="#cb11-45"></a><span class="co">      center: true</span></span>
<span id="cb11-46"><a href="#cb11-46"></a><span class="co">      callout-style: simple</span></span>
<span id="cb11-47"><a href="#cb11-47"></a><span class="co">      mermaid-format: png</span></span>
<span id="cb11-48"><a href="#cb11-48"></a><span class="co">      mermaid:</span></span>
<span id="cb11-49"><a href="#cb11-49"></a><span class="co">        format: png</span></span>
<span id="cb11-50"><a href="#cb11-50"></a><span class="co">        layout: elk</span></span>
<span id="cb11-51"><a href="#cb11-51"></a><span class="co">        useMaxWidth: true</span></span>
<span id="cb11-52"><a href="#cb11-52"></a><span class="co">      highlight-style: atom-one</span></span>
<span id="cb11-53"><a href="#cb11-53"></a><span class="co">      footer: "[samforeman.me/talks/ai-for-science-2024/slides](https://samforeman.me/talks/ai-for-science-2024/slides)"</span></span>
<span id="cb11-54"><a href="#cb11-54"></a><span class="co">      slide-url: https://samforeman.me/talks/ai-for-science-2024/slides.html</span></span>
<span id="cb11-55"><a href="#cb11-55"></a><span class="co">      template-partials:</span></span>
<span id="cb11-56"><a href="#cb11-56"></a><span class="co">        - ./title-slide.html</span></span>
<span id="cb11-57"><a href="#cb11-57"></a><span class="co">      title-slide-attributes:</span></span>
<span id="cb11-58"><a href="#cb11-58"></a><span class="co">        data-background-size: contain</span></span>
<span id="cb11-59"><a href="#cb11-59"></a><span class="co">        data-background-iframe: "./tetris/index.html"</span></span>
<span id="cb11-60"><a href="#cb11-60"></a><span class="co">---</span></span>
<span id="cb11-61"><a href="#cb11-61"></a></span>
<span id="cb11-62"><a href="#cb11-62"></a><span class="fu"># ğŸ‘€ Overview {background-color="white"}</span></span>
<span id="cb11-63"><a href="#cb11-63"></a></span>
<span id="cb11-64"><a href="#cb11-64"></a><span class="ss">- </span>ğŸ“Š Slides @ <span class="co">[</span><span class="ot">samforeman.me/talks/ai-for-science-2024/slides</span><span class="co">](https://samforeman.me/talks/ai-for-science-2024/slides)</span></span>
<span id="cb11-65"><a href="#cb11-65"></a><span class="ss">  - </span>ğŸ“„ HTML version: <span class="co">[</span><span class="ot">samforeman.me/talks/ai-for-science-2024</span><span class="co">](https://samforeman.me/talks/ai-for-science-2024)</span></span>
<span id="cb11-66"><a href="#cb11-66"></a></span>
<span id="cb11-67"><a href="#cb11-67"></a><span class="ss">- </span>{{&lt; fa brands github &gt;}} <span class="co">[</span><span class="ot">argonne-lcf/`ai-science-training-series`</span><span class="co">](https://github.com/argonne-lcf/ai-science-training-series)</span></span>
<span id="cb11-68"><a href="#cb11-68"></a><span class="ss">    - </span><span class="co">[</span><span class="ot">Series Page</span><span class="co">](https://www.alcf.anl.gov/alcf-ai-science-training-series)</span></span>
<span id="cb11-69"><a href="#cb11-69"></a></span>
<span id="cb11-70"><a href="#cb11-70"></a><span class="fu"># ğŸ“‘ Outline {background-color="white"}</span></span>
<span id="cb11-71"><a href="#cb11-71"></a></span>
<span id="cb11-72"><a href="#cb11-72"></a><span class="ss">1. </span><span class="co">[</span><span class="ot">Scaling: Overview</span><span class="co">](#scaling-overview)</span></span>
<span id="cb11-73"><a href="#cb11-73"></a><span class="ss">1. </span><span class="co">[</span><span class="ot">Data Parallel Training</span><span class="co">](#data-parallel-training)</span></span>
<span id="cb11-74"><a href="#cb11-74"></a><span class="ss">    1. </span><span class="co">[</span><span class="ot">Communication</span><span class="co">](#communication)</span></span>
<span id="cb11-75"><a href="#cb11-75"></a><span class="ss">    1. </span><span class="co">[</span><span class="ot">Why Distributed Training?</span><span class="co">](#why-distributed-training)</span></span>
<span id="cb11-76"><a href="#cb11-76"></a><span class="ss">1. </span><span class="co">[</span><span class="ot">Beyond Data Parallelism</span><span class="co">](#going-beyond-data-parallelism)</span></span>
<span id="cb11-77"><a href="#cb11-77"></a><span class="ss">    1. </span><span class="co">[</span><span class="ot">Additional Parallelism Strategies</span><span class="co">](#additional-parallelism-strategies)</span></span>
<span id="cb11-78"><a href="#cb11-78"></a><span class="ss">1. </span><span class="co">[</span><span class="ot">Large Language Models</span><span class="co">](#large-language-models)</span></span>
<span id="cb11-79"><a href="#cb11-79"></a><span class="ss">1. </span><span class="co">[</span><span class="ot">Hands On</span><span class="co">](#hands-on)</span></span>
<span id="cb11-80"><a href="#cb11-80"></a></span>
<span id="cb11-81"><a href="#cb11-81"></a><span class="fu"># ğŸš€ Scaling: Overview {background-color="white"}</span></span>
<span id="cb11-82"><a href="#cb11-82"></a></span>
<span id="cb11-83"><a href="#cb11-83"></a><span class="ss">- </span>âœ… **Goal**:</span>
<span id="cb11-84"><a href="#cb11-84"></a><span class="ss">  - </span>Minimize: <span class="co">[</span><span class="ot">Cost</span><span class="co">]</span>{.highlight-red} (i.e. amount of time spent training)</span>
<span id="cb11-85"><a href="#cb11-85"></a><span class="ss">  - </span>Maximize: <span class="co">[</span><span class="ot">Performance</span><span class="co">]</span>{.highlight-blue}</span>
<span id="cb11-86"><a href="#cb11-86"></a></span>
<span id="cb11-87"><a href="#cb11-87"></a>  ::: {.callout-note collapse=false icon=false title="ğŸ“‘ Note"}</span>
<span id="cb11-88"><a href="#cb11-88"></a>  See</span>
<span id="cb11-89"><a href="#cb11-89"></a>  <span class="co">[</span><span class="ot">ğŸ¤— Performance and Scalability</span><span class="co">](https://huggingface.co/docs/transformers/v4.46.0/performance)</span></span>
<span id="cb11-90"><a href="#cb11-90"></a>  for more details</span>
<span id="cb11-91"><a href="#cb11-91"></a>  :::</span>
<span id="cb11-92"><a href="#cb11-92"></a></span>
<span id="cb11-93"><a href="#cb11-93"></a></span>
<span id="cb11-94"><a href="#cb11-94"></a><span class="fu"># ğŸ¢ Training on a Single Device {background-color="white"}</span></span>
<span id="cb11-95"><a href="#cb11-95"></a></span>
<span id="cb11-96"><a href="#cb11-96"></a><span class="ss">- </span>See <span class="co">[</span><span class="ot">ğŸ¤— Methods and tools for efficient training on a single GPU</span><span class="co">](https://huggingface.co/docs/transformers/v4.46.0/perf_train_gpu_one)</span></span>
<span id="cb11-97"><a href="#cb11-97"></a></span>
<span id="cb11-98"><a href="#cb11-98"></a>::: {#fig-single-gpu-1}</span>
<span id="cb11-99"><a href="#cb11-99"></a></span>
<span id="cb11-102"><a href="#cb11-102"></a><span class="in">```{mermaid}</span></span>
<span id="cb11-103"><a href="#cb11-103"></a>flowchart LR</span>
<span id="cb11-104"><a href="#cb11-104"></a>    subgraph G0[<span class="ot">"</span><span class="st">`GPU0`</span><span class="ot">"</span>]</span>
<span id="cb11-105"><a href="#cb11-105"></a>        subgraph N0[<span class="ot">"</span><span class="st">`Network`</span><span class="ot">"</span>]</span>
<span id="cb11-106"><a href="#cb11-106"></a>        end</span>
<span id="cb11-107"><a href="#cb11-107"></a>        L0(<span class="ot">"</span><span class="st">`Loss`</span><span class="ot">"</span>)</span>
<span id="cb11-108"><a href="#cb11-108"></a>    end</span>
<span id="cb11-109"><a href="#cb11-109"></a>    subgraph D[<span class="ot">"</span><span class="st">`Data`</span><span class="ot">"</span>]</span>
<span id="cb11-110"><a href="#cb11-110"></a>        x(<span class="ot">"</span><span class="st">`x0`</span><span class="ot">"</span>)</span>
<span id="cb11-111"><a href="#cb11-111"></a>        x1(<span class="ot">"</span><span class="st">`x1`</span><span class="ot">"</span>)</span>
<span id="cb11-112"><a href="#cb11-112"></a>        x2(<span class="ot">"</span><span class="st">`x2`</span><span class="ot">"</span>)</span>
<span id="cb11-113"><a href="#cb11-113"></a>    end</span>
<span id="cb11-114"><a href="#cb11-114"></a>    x --&gt; N0</span>
<span id="cb11-115"><a href="#cb11-115"></a>    N0 --&gt; L0</span>
<span id="cb11-116"><a href="#cb11-116"></a>    L0 --&gt; N0</span>
<span id="cb11-117"><a href="#cb11-117"></a>classDef block fill:<span class="co">#CCCCCC02,stroke:#838383,stroke-width:1px,color:#838383</span></span>
<span id="cb11-118"><a href="#cb11-118"></a>classDef red fill:<span class="co">#ff8181,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-119"><a href="#cb11-119"></a>classDef green fill:<span class="co">#98E6A5,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-120"><a href="#cb11-120"></a>classDef blue fill:<span class="co">#7DCAFF,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-121"><a href="#cb11-121"></a>classDef grey fill:<span class="co">#cccccc,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-122"><a href="#cb11-122"></a>class x,L0 red</span>
<span id="cb11-123"><a href="#cb11-123"></a>class x1, green</span>
<span id="cb11-124"><a href="#cb11-124"></a>class x2, blue</span>
<span id="cb11-125"><a href="#cb11-125"></a>class x3, grey</span>
<span id="cb11-126"><a href="#cb11-126"></a>class N0,D,G0,n0 block</span>
<span id="cb11-127"><a href="#cb11-127"></a><span class="in">```</span></span>
<span id="cb11-128"><a href="#cb11-128"></a></span>
<span id="cb11-129"><a href="#cb11-129"></a>**SLOW** !! model size limited by GPU memory</span>
<span id="cb11-130"><a href="#cb11-130"></a>:::</span>
<span id="cb11-131"><a href="#cb11-131"></a></span>
<span id="cb11-132"><a href="#cb11-132"></a>::: {.content-visible when-format="revealjs" unless-format="html"}</span>
<span id="cb11-133"><a href="#cb11-133"></a></span>
<span id="cb11-134"><a href="#cb11-134"></a><span class="fu"># Single GPU {background-color="white"}</span></span>
<span id="cb11-135"><a href="#cb11-135"></a></span>
<span id="cb11-136"><a href="#cb11-136"></a><span class="ss">- </span>See <span class="co">[</span><span class="ot">ğŸ¤— Methods and tools for efficient training on a single GPU</span><span class="co">](https://huggingface.co/docs/transformers/v4.46.0/perf_train_gpu_one)</span></span>
<span id="cb11-137"><a href="#cb11-137"></a></span>
<span id="cb11-138"><a href="#cb11-138"></a>::: {#fig-single-gpu-2}</span>
<span id="cb11-139"><a href="#cb11-139"></a></span>
<span id="cb11-142"><a href="#cb11-142"></a><span class="in">```{mermaid}</span></span>
<span id="cb11-143"><a href="#cb11-143"></a>flowchart LR</span>
<span id="cb11-144"><a href="#cb11-144"></a>    subgraph G0[<span class="ot">"</span><span class="st">`GPU0`</span><span class="ot">"</span>]</span>
<span id="cb11-145"><a href="#cb11-145"></a>        subgraph N0[<span class="ot">"</span><span class="st">`Network`</span><span class="ot">"</span>]</span>
<span id="cb11-146"><a href="#cb11-146"></a>        end</span>
<span id="cb11-147"><a href="#cb11-147"></a>        L0(<span class="ot">"</span><span class="st">`Loss`</span><span class="ot">"</span>)</span>
<span id="cb11-148"><a href="#cb11-148"></a>    end</span>
<span id="cb11-149"><a href="#cb11-149"></a>    subgraph D[<span class="ot">"</span><span class="st">`Data`</span><span class="ot">"</span>]</span>
<span id="cb11-150"><a href="#cb11-150"></a>        x(<span class="ot">"</span><span class="st">`x0`</span><span class="ot">"</span>)</span>
<span id="cb11-151"><a href="#cb11-151"></a>        x1(<span class="ot">"</span><span class="st">`x1`</span><span class="ot">"</span>)</span>
<span id="cb11-152"><a href="#cb11-152"></a>        x2(<span class="ot">"</span><span class="st">`x2`</span><span class="ot">"</span>)</span>
<span id="cb11-153"><a href="#cb11-153"></a>    end</span>
<span id="cb11-154"><a href="#cb11-154"></a>    x --&gt; N0</span>
<span id="cb11-155"><a href="#cb11-155"></a>    N0 --&gt; L0</span>
<span id="cb11-156"><a href="#cb11-156"></a>    L0 --&gt; N0</span>
<span id="cb11-157"><a href="#cb11-157"></a>classDef block fill:<span class="co">#CCCCCC02,stroke:#838383,stroke-width:1px,color:#838383</span></span>
<span id="cb11-158"><a href="#cb11-158"></a>classDef red fill:<span class="co">#ff8181,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-159"><a href="#cb11-159"></a>classDef grey fill:<span class="co">#cccccc,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-160"><a href="#cb11-160"></a>classDef orange fill:<span class="co">#FFC47F,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-161"><a href="#cb11-161"></a>classDef yellow fill:<span class="co">#FFFF7F,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-162"><a href="#cb11-162"></a>classDef green fill:<span class="co">#98E6A5,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-163"><a href="#cb11-163"></a>classDef blue fill:<span class="co">#7DCAFF,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-164"><a href="#cb11-164"></a>classDef purple fill:<span class="co">#FFCBE6,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-165"><a href="#cb11-165"></a>classDef text fill:<span class="co">#CCCCCC02,stroke:#838383,stroke-width:0px,color:#838383</span></span>
<span id="cb11-166"><a href="#cb11-166"></a>class x,L0 green</span>
<span id="cb11-167"><a href="#cb11-167"></a>class x1, blue</span>
<span id="cb11-168"><a href="#cb11-168"></a>class x2, yellow</span>
<span id="cb11-169"><a href="#cb11-169"></a>class x3, grey</span>
<span id="cb11-170"><a href="#cb11-170"></a>class N0,D,G0,n0 block</span>
<span id="cb11-171"><a href="#cb11-171"></a><span class="in">```</span></span>
<span id="cb11-172"><a href="#cb11-172"></a></span>
<span id="cb11-173"><a href="#cb11-173"></a>**SLOW** !! model size limited by GPU memory</span>
<span id="cb11-174"><a href="#cb11-174"></a>:::</span>
<span id="cb11-175"><a href="#cb11-175"></a></span>
<span id="cb11-176"><a href="#cb11-176"></a><span class="fu"># Single GPU {background-color="white"}</span></span>
<span id="cb11-177"><a href="#cb11-177"></a></span>
<span id="cb11-178"><a href="#cb11-178"></a><span class="ss">- </span>See <span class="co">[</span><span class="ot">ğŸ¤— Methods and tools for efficient training on a single GPU</span><span class="co">](https://huggingface.co/docs/transformers/v4.46.0/perf_train_gpu_one)</span></span>
<span id="cb11-179"><a href="#cb11-179"></a></span>
<span id="cb11-180"><a href="#cb11-180"></a>::: {#fig-single-gpu-3}</span>
<span id="cb11-181"><a href="#cb11-181"></a></span>
<span id="cb11-184"><a href="#cb11-184"></a><span class="in">```{mermaid}</span></span>
<span id="cb11-185"><a href="#cb11-185"></a>flowchart LR</span>
<span id="cb11-186"><a href="#cb11-186"></a>    subgraph G0[<span class="ot">"</span><span class="st">`GPU0`</span><span class="ot">"</span>]</span>
<span id="cb11-187"><a href="#cb11-187"></a>        subgraph N0[<span class="ot">"</span><span class="st">`Network`</span><span class="ot">"</span>]</span>
<span id="cb11-188"><a href="#cb11-188"></a>        end</span>
<span id="cb11-189"><a href="#cb11-189"></a>        L0(<span class="ot">"</span><span class="st">`Loss`</span><span class="ot">"</span>)</span>
<span id="cb11-190"><a href="#cb11-190"></a>    end</span>
<span id="cb11-191"><a href="#cb11-191"></a>    subgraph D[<span class="ot">"</span><span class="st">`Data`</span><span class="ot">"</span>]</span>
<span id="cb11-192"><a href="#cb11-192"></a>        x(<span class="ot">"</span><span class="st">`xâ‚‚`</span><span class="ot">"</span>)</span>
<span id="cb11-193"><a href="#cb11-193"></a>        x1(<span class="ot">"</span><span class="st">`xâ‚ƒ`</span><span class="ot">"</span>)</span>
<span id="cb11-194"><a href="#cb11-194"></a>        x2(<span class="ot">"</span><span class="st">`xâ‚„`</span><span class="ot">"</span>)</span>
<span id="cb11-195"><a href="#cb11-195"></a>    end</span>
<span id="cb11-196"><a href="#cb11-196"></a>    x --&gt; N0</span>
<span id="cb11-197"><a href="#cb11-197"></a>    N0 --&gt; L0</span>
<span id="cb11-198"><a href="#cb11-198"></a>    L0 --&gt; N0</span>
<span id="cb11-199"><a href="#cb11-199"></a>classDef block fill:<span class="co">#CCCCCC02,stroke:#838383,stroke-width:1px,color:#838383</span></span>
<span id="cb11-200"><a href="#cb11-200"></a>classDef red fill:<span class="co">#ff8181,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-201"><a href="#cb11-201"></a>classDef grey fill:<span class="co">#cccccc,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-202"><a href="#cb11-202"></a>classDef orange fill:<span class="co">#FFC47F,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-203"><a href="#cb11-203"></a>classDef yellow fill:<span class="co">#FFFF7F,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-204"><a href="#cb11-204"></a>classDef green fill:<span class="co">#98E6A5,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-205"><a href="#cb11-205"></a>classDef blue fill:<span class="co">#7DCAFF,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-206"><a href="#cb11-206"></a>classDef purple fill:<span class="co">#FFCBE6,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-207"><a href="#cb11-207"></a>classDef text fill:<span class="co">#CCCCCC02,stroke:#838383,stroke-width:0px,color:#838383</span></span>
<span id="cb11-208"><a href="#cb11-208"></a>class x,L0 blue</span>
<span id="cb11-209"><a href="#cb11-209"></a>class x1, yellow</span>
<span id="cb11-210"><a href="#cb11-210"></a>class x2, purple</span>
<span id="cb11-211"><a href="#cb11-211"></a>class x3, grey</span>
<span id="cb11-212"><a href="#cb11-212"></a>class N0,D,G0,n0 block</span>
<span id="cb11-213"><a href="#cb11-213"></a><span class="in">```</span></span>
<span id="cb11-214"><a href="#cb11-214"></a></span>
<span id="cb11-215"><a href="#cb11-215"></a>**SLOW** !! model size limited by GPU memory</span>
<span id="cb11-216"><a href="#cb11-216"></a>:::</span>
<span id="cb11-217"><a href="#cb11-217"></a></span>
<span id="cb11-218"><a href="#cb11-218"></a>:::</span>
<span id="cb11-219"><a href="#cb11-219"></a></span>
<span id="cb11-220"><a href="#cb11-220"></a><span class="fu"># ğŸï¸ Training on Multiple GPUs: Data Parallelism {background-color="white"}</span></span>
<span id="cb11-221"><a href="#cb11-221"></a></span>
<span id="cb11-222"><a href="#cb11-222"></a><span class="co">&lt;!--::: {.panel-tabset}--&gt;</span></span>
<span id="cb11-223"><a href="#cb11-223"></a></span>
<span id="cb11-224"><a href="#cb11-224"></a>::: {#fig-ddp-training-mermaid}</span>
<span id="cb11-225"><a href="#cb11-225"></a></span>
<span id="cb11-228"><a href="#cb11-228"></a><span class="in">```{mermaid}</span></span>
<span id="cb11-229"><a href="#cb11-229"></a>flowchart LR</span>
<span id="cb11-230"><a href="#cb11-230"></a>    subgraph D[<span class="ot">"</span><span class="st">`Data`</span><span class="ot">"</span>]</span>
<span id="cb11-231"><a href="#cb11-231"></a>        direction TB</span>
<span id="cb11-232"><a href="#cb11-232"></a>        x(<span class="ot">"</span><span class="st">`xâ‚€`</span><span class="ot">"</span>)</span>
<span id="cb11-233"><a href="#cb11-233"></a>        x1(<span class="ot">"</span><span class="st">`xâ‚`</span><span class="ot">"</span>)</span>
<span id="cb11-234"><a href="#cb11-234"></a>        x2(<span class="ot">"</span><span class="st">`xâ‚‚`</span><span class="ot">"</span>)</span>
<span id="cb11-235"><a href="#cb11-235"></a>    end</span>
<span id="cb11-236"><a href="#cb11-236"></a>    direction LR</span>
<span id="cb11-237"><a href="#cb11-237"></a>    subgraph G0[<span class="ot">"</span><span class="st">`GPU0`</span><span class="ot">"</span>]</span>
<span id="cb11-238"><a href="#cb11-238"></a>        direction LR</span>
<span id="cb11-239"><a href="#cb11-239"></a>        subgraph N0[<span class="ot">"</span><span class="st">`NN`</span><span class="ot">"</span>]</span>
<span id="cb11-240"><a href="#cb11-240"></a>        end</span>
<span id="cb11-241"><a href="#cb11-241"></a>        %<span class="dt">%y0</span>(<span class="ot">"</span><span class="st">`yâ‚€`</span><span class="ot">"</span>)</span>
<span id="cb11-242"><a href="#cb11-242"></a>        L0[<span class="ot">"</span><span class="st">`Loss`</span><span class="ot">"</span>]</span>
<span id="cb11-243"><a href="#cb11-243"></a>    end</span>
<span id="cb11-244"><a href="#cb11-244"></a>    subgraph G1[<span class="ot">"</span><span class="st">`GPU1`</span><span class="ot">"</span>]</span>
<span id="cb11-245"><a href="#cb11-245"></a>        direction LR</span>
<span id="cb11-246"><a href="#cb11-246"></a>        subgraph N1[<span class="ot">"</span><span class="st">`NN`</span><span class="ot">"</span>]</span>
<span id="cb11-247"><a href="#cb11-247"></a>        end</span>
<span id="cb11-248"><a href="#cb11-248"></a>        L1[<span class="ot">"</span><span class="st">`Loss`</span><span class="ot">"</span>]</span>
<span id="cb11-249"><a href="#cb11-249"></a>    end</span>
<span id="cb11-250"><a href="#cb11-250"></a>    subgraph G2[<span class="ot">"</span><span class="st">`GPU2`</span><span class="ot">"</span>]</span>
<span id="cb11-251"><a href="#cb11-251"></a>        direction LR</span>
<span id="cb11-252"><a href="#cb11-252"></a>        subgraph N2[<span class="ot">"</span><span class="st">`NN`</span><span class="ot">"</span>]</span>
<span id="cb11-253"><a href="#cb11-253"></a>        end</span>
<span id="cb11-254"><a href="#cb11-254"></a>        L2[<span class="ot">"</span><span class="st">`Loss`</span><span class="ot">"</span>]</span>
<span id="cb11-255"><a href="#cb11-255"></a>    end</span>
<span id="cb11-256"><a href="#cb11-256"></a>    x --&gt; G0</span>
<span id="cb11-257"><a href="#cb11-257"></a>    x1 --&gt; G1</span>
<span id="cb11-258"><a href="#cb11-258"></a>    x2 --&gt; G2</span>
<span id="cb11-259"><a href="#cb11-259"></a>    N0 --&gt; L0</span>
<span id="cb11-260"><a href="#cb11-260"></a>    N1 --&gt; L1</span>
<span id="cb11-261"><a href="#cb11-261"></a>    N2 --&gt; L2</span>
<span id="cb11-262"><a href="#cb11-262"></a>classDef block fill:<span class="co">#CCCCCC02,stroke:#838383,stroke-width:1px,color:#838383</span></span>
<span id="cb11-263"><a href="#cb11-263"></a>classDef text fill:<span class="co">#CCCCCC02,stroke:#838383,stroke-width:0px,color:#838383</span></span>
<span id="cb11-264"><a href="#cb11-264"></a>classDef grey fill:<span class="co">#cccccc,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-265"><a href="#cb11-265"></a>classDef red fill:<span class="co">#ff8181,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-266"><a href="#cb11-266"></a>classDef orange fill:<span class="co">#FFC47F,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-267"><a href="#cb11-267"></a>classDef yellow fill:<span class="co">#FFFF7F,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-268"><a href="#cb11-268"></a>classDef green fill:<span class="co">#98E6A5,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-269"><a href="#cb11-269"></a>classDef blue fill:<span class="co">#7DCAFF,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-270"><a href="#cb11-270"></a>classDef purple fill:<span class="co">#FFCBE6,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-271"><a href="#cb11-271"></a>class x,y0,L0 red</span>
<span id="cb11-272"><a href="#cb11-272"></a>class x1,L1 green</span>
<span id="cb11-273"><a href="#cb11-273"></a>class x2,L2 blue</span>
<span id="cb11-274"><a href="#cb11-274"></a>class x3,ar grey</span>
<span id="cb11-275"><a href="#cb11-275"></a>class D,N0,N1,N2,G0,G1,G2,GU block</span>
<span id="cb11-276"><a href="#cb11-276"></a>class AR block</span>
<span id="cb11-277"><a href="#cb11-277"></a>class bc text</span>
<span id="cb11-278"><a href="#cb11-278"></a><span class="in">```</span></span>
<span id="cb11-279"><a href="#cb11-279"></a></span>
<span id="cb11-280"><a href="#cb11-280"></a>Each GPU receives **unique** data at each step</span>
<span id="cb11-281"><a href="#cb11-281"></a>:::</span>
<span id="cb11-282"><a href="#cb11-282"></a></span>
<span id="cb11-283"><a href="#cb11-283"></a><span class="fu">## Data Parallel: Forward Pass {background-color="white"}</span></span>
<span id="cb11-284"><a href="#cb11-284"></a></span>
<span id="cb11-285"><a href="#cb11-285"></a>::: {#fig-ddp-training-mermaid-allreduce}</span>
<span id="cb11-286"><a href="#cb11-286"></a></span>
<span id="cb11-289"><a href="#cb11-289"></a><span class="in">```{mermaid}</span></span>
<span id="cb11-290"><a href="#cb11-290"></a>flowchart LR</span>
<span id="cb11-291"><a href="#cb11-291"></a>    subgraph D[<span class="ot">"</span><span class="st">`Data`</span><span class="ot">"</span>]</span>
<span id="cb11-292"><a href="#cb11-292"></a>        direction TB</span>
<span id="cb11-293"><a href="#cb11-293"></a>        %<span class="dt">%xp</span>(<span class="ot">"</span><span class="st">`xâ‚™â‚Šâ‚`</span><span class="ot">"</span>)</span>
<span id="cb11-294"><a href="#cb11-294"></a>        x(<span class="ot">"</span><span class="st">`xâ‚€`</span><span class="ot">"</span>)</span>
<span id="cb11-295"><a href="#cb11-295"></a>        x1(<span class="ot">"</span><span class="st">`xâ‚`</span><span class="ot">"</span>)</span>
<span id="cb11-296"><a href="#cb11-296"></a>        x2(<span class="ot">"</span><span class="st">`xâ‚‚`</span><span class="ot">"</span>)</span>
<span id="cb11-297"><a href="#cb11-297"></a>    end</span>
<span id="cb11-298"><a href="#cb11-298"></a>    direction LR</span>
<span id="cb11-299"><a href="#cb11-299"></a>    subgraph G0[<span class="ot">"</span><span class="st">`GPU0`</span><span class="ot">"</span>]</span>
<span id="cb11-300"><a href="#cb11-300"></a>        direction LR</span>
<span id="cb11-301"><a href="#cb11-301"></a>        subgraph N0[<span class="ot">"</span><span class="st">`NN`</span><span class="ot">"</span>]</span>
<span id="cb11-302"><a href="#cb11-302"></a>        end</span>
<span id="cb11-303"><a href="#cb11-303"></a>        %<span class="dt">%y0</span>(<span class="ot">"</span><span class="st">`yâ‚€`</span><span class="ot">"</span>)</span>
<span id="cb11-304"><a href="#cb11-304"></a>        L0[<span class="ot">"</span><span class="st">`Loss`</span><span class="ot">"</span>]</span>
<span id="cb11-305"><a href="#cb11-305"></a>    end</span>
<span id="cb11-306"><a href="#cb11-306"></a>    subgraph G1[<span class="ot">"</span><span class="st">`GPU1`</span><span class="ot">"</span>]</span>
<span id="cb11-307"><a href="#cb11-307"></a>        direction LR</span>
<span id="cb11-308"><a href="#cb11-308"></a>        subgraph N1[<span class="ot">"</span><span class="st">`NN`</span><span class="ot">"</span>]</span>
<span id="cb11-309"><a href="#cb11-309"></a>        end</span>
<span id="cb11-310"><a href="#cb11-310"></a>        L1[<span class="ot">"</span><span class="st">`Loss`</span><span class="ot">"</span>]</span>
<span id="cb11-311"><a href="#cb11-311"></a>    end</span>
<span id="cb11-312"><a href="#cb11-312"></a>    subgraph G2[<span class="ot">"</span><span class="st">`GPU2`</span><span class="ot">"</span>]</span>
<span id="cb11-313"><a href="#cb11-313"></a>        direction LR</span>
<span id="cb11-314"><a href="#cb11-314"></a>        subgraph N2[<span class="ot">"</span><span class="st">`NN`</span><span class="ot">"</span>]</span>
<span id="cb11-315"><a href="#cb11-315"></a>        end</span>
<span id="cb11-316"><a href="#cb11-316"></a>        L2[<span class="ot">"</span><span class="st">`Loss`</span><span class="ot">"</span>]</span>
<span id="cb11-317"><a href="#cb11-317"></a>    end</span>
<span id="cb11-318"><a href="#cb11-318"></a>    subgraph AR[<span class="ot">"</span><span class="st">`Average Grads`</span><span class="ot">"</span>]</span>
<span id="cb11-319"><a href="#cb11-319"></a>        direction TB</span>
<span id="cb11-320"><a href="#cb11-320"></a>        ar(<span class="ot">"</span><span class="st">`(1/n) âˆ‘ gâ‚™`</span><span class="ot">"</span>)</span>
<span id="cb11-321"><a href="#cb11-321"></a>    end</span>
<span id="cb11-322"><a href="#cb11-322"></a>    x --&gt; G0</span>
<span id="cb11-323"><a href="#cb11-323"></a>    x1 --&gt; G1</span>
<span id="cb11-324"><a href="#cb11-324"></a>    x2 --&gt; G2</span>
<span id="cb11-325"><a href="#cb11-325"></a>    N0 --&gt; L0</span>
<span id="cb11-326"><a href="#cb11-326"></a>    N1 --&gt; L1</span>
<span id="cb11-327"><a href="#cb11-327"></a>    N2 --&gt; L2</span>
<span id="cb11-328"><a href="#cb11-328"></a>    G0 -.-&gt; AR</span>
<span id="cb11-329"><a href="#cb11-329"></a>    G1 -.-&gt; AR</span>
<span id="cb11-330"><a href="#cb11-330"></a>    G2 -.-&gt; AR</span>
<span id="cb11-331"><a href="#cb11-331"></a>classDef block fill:<span class="co">#CCCCCC02,stroke:#838383,stroke-width:1px,color:#838383</span></span>
<span id="cb11-332"><a href="#cb11-332"></a>classDef grey fill:<span class="co">#cccccc,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-333"><a href="#cb11-333"></a>classDef red fill:<span class="co">#ff8181,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-334"><a href="#cb11-334"></a>classDef orange fill:<span class="co">#FFC47F,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-335"><a href="#cb11-335"></a>classDef yellow fill:<span class="co">#FFFF7F,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-336"><a href="#cb11-336"></a>classDef green fill:<span class="co">#98E6A5,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-337"><a href="#cb11-337"></a>classDef blue fill:<span class="co">#7DCAFF,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-338"><a href="#cb11-338"></a>classDef purple fill:<span class="co">#FFCBE6,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-339"><a href="#cb11-339"></a>classDef text fill:<span class="co">#CCCCCC02,stroke:#838383,stroke-width:0px,color:#838383</span></span>
<span id="cb11-340"><a href="#cb11-340"></a>class x,y0,L0 red</span>
<span id="cb11-341"><a href="#cb11-341"></a>class x1,L1 green</span>
<span id="cb11-342"><a href="#cb11-342"></a>class x2,L2 blue</span>
<span id="cb11-343"><a href="#cb11-343"></a>class x3,ar grey</span>
<span id="cb11-344"><a href="#cb11-344"></a>class D,N0,N1,N2,G0,G1,G2,GU block</span>
<span id="cb11-345"><a href="#cb11-345"></a>class AR block</span>
<span id="cb11-346"><a href="#cb11-346"></a>class bc text</span>
<span id="cb11-347"><a href="#cb11-347"></a><span class="in">```</span></span>
<span id="cb11-348"><a href="#cb11-348"></a></span>
<span id="cb11-349"><a href="#cb11-349"></a>Average gradients across all GPUs</span>
<span id="cb11-350"><a href="#cb11-350"></a>:::</span>
<span id="cb11-351"><a href="#cb11-351"></a></span>
<span id="cb11-352"><a href="#cb11-352"></a><span class="fu">## Data Parallel: Backward Pass {background-color="white"}</span></span>
<span id="cb11-353"><a href="#cb11-353"></a></span>
<span id="cb11-354"><a href="#cb11-354"></a>::: {#fig-ddp-backward-mermaid}</span>
<span id="cb11-355"><a href="#cb11-355"></a></span>
<span id="cb11-358"><a href="#cb11-358"></a><span class="in">```{mermaid}</span></span>
<span id="cb11-359"><a href="#cb11-359"></a>flowchart RL</span>
<span id="cb11-360"><a href="#cb11-360"></a>    subgraph D[<span class="ot">"</span><span class="st">`Data`</span><span class="ot">"</span>]</span>
<span id="cb11-361"><a href="#cb11-361"></a>        direction TB</span>
<span id="cb11-362"><a href="#cb11-362"></a>        x(<span class="ot">"</span><span class="st">`xâ‚€`</span><span class="ot">"</span>)</span>
<span id="cb11-363"><a href="#cb11-363"></a>        x1(<span class="ot">"</span><span class="st">`xâ‚`</span><span class="ot">"</span>)</span>
<span id="cb11-364"><a href="#cb11-364"></a>        x2(<span class="ot">"</span><span class="st">`xâ‚‚`</span><span class="ot">"</span>)</span>
<span id="cb11-365"><a href="#cb11-365"></a>    end</span>
<span id="cb11-366"><a href="#cb11-366"></a>    subgraph G0[<span class="ot">"</span><span class="st">`GPU0`</span><span class="ot">"</span>]</span>
<span id="cb11-367"><a href="#cb11-367"></a>        direction RL</span>
<span id="cb11-368"><a href="#cb11-368"></a>        subgraph N0[<span class="ot">"</span><span class="st">`NN`</span><span class="ot">"</span>]</span>
<span id="cb11-369"><a href="#cb11-369"></a>        end</span>
<span id="cb11-370"><a href="#cb11-370"></a>        L0[<span class="ot">"</span><span class="st">`Loss`</span><span class="ot">"</span>]</span>
<span id="cb11-371"><a href="#cb11-371"></a>    end</span>
<span id="cb11-372"><a href="#cb11-372"></a>    subgraph G1[<span class="ot">"</span><span class="st">`GPU1`</span><span class="ot">"</span>]</span>
<span id="cb11-373"><a href="#cb11-373"></a>        direction RL</span>
<span id="cb11-374"><a href="#cb11-374"></a>        subgraph N1[<span class="ot">"</span><span class="st">`NN`</span><span class="ot">"</span>]</span>
<span id="cb11-375"><a href="#cb11-375"></a>        end</span>
<span id="cb11-376"><a href="#cb11-376"></a>        L1[<span class="ot">"</span><span class="st">`Loss`</span><span class="ot">"</span>]</span>
<span id="cb11-377"><a href="#cb11-377"></a>    end</span>
<span id="cb11-378"><a href="#cb11-378"></a>    subgraph G2[<span class="ot">"</span><span class="st">`GPU2`</span><span class="ot">"</span>]</span>
<span id="cb11-379"><a href="#cb11-379"></a>        direction RL</span>
<span id="cb11-380"><a href="#cb11-380"></a>        subgraph N2[<span class="ot">"</span><span class="st">`NN`</span><span class="ot">"</span>]</span>
<span id="cb11-381"><a href="#cb11-381"></a>        end</span>
<span id="cb11-382"><a href="#cb11-382"></a>        L2[<span class="ot">"</span><span class="st">`Loss`</span><span class="ot">"</span>]</span>
<span id="cb11-383"><a href="#cb11-383"></a>    end</span>
<span id="cb11-384"><a href="#cb11-384"></a>    subgraph BC[<span class="ot">"</span><span class="st">`Send Updates`</span><span class="ot">"</span>]</span>
<span id="cb11-385"><a href="#cb11-385"></a>        direction TB</span>
<span id="cb11-386"><a href="#cb11-386"></a>    end</span>
<span id="cb11-387"><a href="#cb11-387"></a>    BC -.-&gt; G0</span>
<span id="cb11-388"><a href="#cb11-388"></a>    BC -.-&gt; G1</span>
<span id="cb11-389"><a href="#cb11-389"></a>    BC -.-&gt; G2</span>
<span id="cb11-390"><a href="#cb11-390"></a>    L0 ~~~ N0</span>
<span id="cb11-391"><a href="#cb11-391"></a>    L1 ~~~ N1</span>
<span id="cb11-392"><a href="#cb11-392"></a>    L2 ~~~ N2</span>
<span id="cb11-393"><a href="#cb11-393"></a>    G0 ~~~ x</span>
<span id="cb11-394"><a href="#cb11-394"></a>    G1 ~~~ x1</span>
<span id="cb11-395"><a href="#cb11-395"></a>    G2 ~~~ x2</span>
<span id="cb11-396"><a href="#cb11-396"></a>classDef grey fill:<span class="co">#cccccc,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-397"><a href="#cb11-397"></a>classDef block fill:<span class="co">#CCCCCC02,stroke:#838383,stroke-width:1px,color:#838383</span></span>
<span id="cb11-398"><a href="#cb11-398"></a>classDef red fill:<span class="co">#ff8181,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-399"><a href="#cb11-399"></a>classDef orange fill:<span class="co">#FFC47F,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-400"><a href="#cb11-400"></a>classDef yellow fill:<span class="co">#FFFF7F,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-401"><a href="#cb11-401"></a>classDef green fill:<span class="co">#98E6A5,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-402"><a href="#cb11-402"></a>classDef blue fill:<span class="co">#7DCAFF,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-403"><a href="#cb11-403"></a>classDef purple fill:<span class="co">#FFCBE6,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-404"><a href="#cb11-404"></a>classDef text fill:<span class="co">#CCCCCC02,stroke:#838383,stroke-width:0px,color:#838383</span></span>
<span id="cb11-405"><a href="#cb11-405"></a>class x,y0,L0 red</span>
<span id="cb11-406"><a href="#cb11-406"></a>class x1,L1 green</span>
<span id="cb11-407"><a href="#cb11-407"></a>class x2,L2 blue</span>
<span id="cb11-408"><a href="#cb11-408"></a>class x3,ar grey</span>
<span id="cb11-409"><a href="#cb11-409"></a>class D,N0,N1,N2,G0,G1,G2,GU block</span>
<span id="cb11-410"><a href="#cb11-410"></a>class BC block</span>
<span id="cb11-411"><a href="#cb11-411"></a>class bc text</span>
<span id="cb11-412"><a href="#cb11-412"></a><span class="in">```</span></span>
<span id="cb11-413"><a href="#cb11-413"></a></span>
<span id="cb11-414"><a href="#cb11-414"></a>Send global updates back to each GPU</span>
<span id="cb11-415"><a href="#cb11-415"></a>:::</span>
<span id="cb11-416"><a href="#cb11-416"></a></span>
<span id="cb11-417"><a href="#cb11-417"></a></span>
<span id="cb11-418"><a href="#cb11-418"></a><span class="fu">## Data Parallel: Full Setup {background-color="white"}</span></span>
<span id="cb11-419"><a href="#cb11-419"></a></span>
<span id="cb11-420"><a href="#cb11-420"></a>::: {#fig-ddp-training}</span>
<span id="cb11-421"><a href="#cb11-421"></a></span>
<span id="cb11-424"><a href="#cb11-424"></a><span class="in">```{mermaid}</span></span>
<span id="cb11-425"><a href="#cb11-425"></a>flowchart LR</span>
<span id="cb11-426"><a href="#cb11-426"></a>    subgraph D[<span class="ot">"</span><span class="st">`Data`</span><span class="ot">"</span>]</span>
<span id="cb11-427"><a href="#cb11-427"></a>        direction TB</span>
<span id="cb11-428"><a href="#cb11-428"></a>        x(<span class="ot">"</span><span class="st">`xâ‚€`</span><span class="ot">"</span>)</span>
<span id="cb11-429"><a href="#cb11-429"></a>        x1(<span class="ot">"</span><span class="st">`xâ‚`</span><span class="ot">"</span>)</span>
<span id="cb11-430"><a href="#cb11-430"></a>        x2(<span class="ot">"</span><span class="st">`xâ‚‚`</span><span class="ot">"</span>)</span>
<span id="cb11-431"><a href="#cb11-431"></a>    end</span>
<span id="cb11-432"><a href="#cb11-432"></a>    direction LR</span>
<span id="cb11-433"><a href="#cb11-433"></a>    subgraph G0[<span class="ot">"</span><span class="st">`GPU0`</span><span class="ot">"</span>]</span>
<span id="cb11-434"><a href="#cb11-434"></a>        direction LR</span>
<span id="cb11-435"><a href="#cb11-435"></a>        subgraph N0[<span class="ot">"</span><span class="st">`NN`</span><span class="ot">"</span>]</span>
<span id="cb11-436"><a href="#cb11-436"></a>        end</span>
<span id="cb11-437"><a href="#cb11-437"></a>        L0[<span class="ot">"</span><span class="st">`L0`</span><span class="ot">"</span>]</span>
<span id="cb11-438"><a href="#cb11-438"></a>    end</span>
<span id="cb11-439"><a href="#cb11-439"></a>    subgraph G1[<span class="ot">"</span><span class="st">`GPU1`</span><span class="ot">"</span>]</span>
<span id="cb11-440"><a href="#cb11-440"></a>        direction LR</span>
<span id="cb11-441"><a href="#cb11-441"></a>        subgraph N1[<span class="ot">"</span><span class="st">`NN`</span><span class="ot">"</span>]</span>
<span id="cb11-442"><a href="#cb11-442"></a>        end</span>
<span id="cb11-443"><a href="#cb11-443"></a>        L1[<span class="ot">"</span><span class="st">`L1`</span><span class="ot">"</span>]</span>
<span id="cb11-444"><a href="#cb11-444"></a>    end</span>
<span id="cb11-445"><a href="#cb11-445"></a>    subgraph G2[<span class="ot">"</span><span class="st">`GPU2`</span><span class="ot">"</span>]</span>
<span id="cb11-446"><a href="#cb11-446"></a>        direction LR</span>
<span id="cb11-447"><a href="#cb11-447"></a>        subgraph N2[<span class="ot">"</span><span class="st">`NN`</span><span class="ot">"</span>]</span>
<span id="cb11-448"><a href="#cb11-448"></a>        end</span>
<span id="cb11-449"><a href="#cb11-449"></a>        L2[<span class="ot">"</span><span class="st">`L2`</span><span class="ot">"</span>]</span>
<span id="cb11-450"><a href="#cb11-450"></a>    end</span>
<span id="cb11-451"><a href="#cb11-451"></a>    subgraph AR[<span class="ot">"</span><span class="st">`Average Grads`</span><span class="ot">"</span>]</span>
<span id="cb11-452"><a href="#cb11-452"></a>        direction TB</span>
<span id="cb11-453"><a href="#cb11-453"></a>        ar(<span class="ot">"</span><span class="st">`(1/n) âˆ‘ gâ‚™`</span><span class="ot">"</span>)</span>
<span id="cb11-454"><a href="#cb11-454"></a>        bc(<span class="ot">"</span><span class="st">`Update Weights`</span><span class="ot">"</span>)</span>
<span id="cb11-455"><a href="#cb11-455"></a>        ar --&gt; bc</span>
<span id="cb11-456"><a href="#cb11-456"></a>    end</span>
<span id="cb11-457"><a href="#cb11-457"></a>    x --&gt; G0</span>
<span id="cb11-458"><a href="#cb11-458"></a>    x1 --&gt; G1</span>
<span id="cb11-459"><a href="#cb11-459"></a>    x2 --&gt; G2</span>
<span id="cb11-460"><a href="#cb11-460"></a>    N0 --&gt; L0</span>
<span id="cb11-461"><a href="#cb11-461"></a>    N1 --&gt; L1</span>
<span id="cb11-462"><a href="#cb11-462"></a>    N2 --&gt; L2</span>
<span id="cb11-463"><a href="#cb11-463"></a>    G0 &lt;-.-&gt; AR</span>
<span id="cb11-464"><a href="#cb11-464"></a>    G1 &lt;-.-&gt; AR</span>
<span id="cb11-465"><a href="#cb11-465"></a>    G2 &lt;-.-&gt; AR</span>
<span id="cb11-466"><a href="#cb11-466"></a>classDef block fill:<span class="co">#CCCCCC02,stroke:#838383,stroke-width:1px,color:#838383</span></span>
<span id="cb11-467"><a href="#cb11-467"></a>classDef grey fill:<span class="co">#cccccc,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-468"><a href="#cb11-468"></a>classDef red fill:<span class="co">#ff8181,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-469"><a href="#cb11-469"></a>classDef orange fill:<span class="co">#FFC47F,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-470"><a href="#cb11-470"></a>classDef yellow fill:<span class="co">#FFFF7F,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-471"><a href="#cb11-471"></a>classDef green fill:<span class="co">#98E6A5,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-472"><a href="#cb11-472"></a>classDef blue fill:<span class="co">#7DCAFF,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-473"><a href="#cb11-473"></a>classDef purple fill:<span class="co">#FFCBE6,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-474"><a href="#cb11-474"></a>classDef text fill:<span class="co">#CCCCCC02,stroke:#838383,stroke-width:0px,color:#838383</span></span>
<span id="cb11-475"><a href="#cb11-475"></a>class x,y0,L0 red</span>
<span id="cb11-476"><a href="#cb11-476"></a>class x1,L1 green</span>
<span id="cb11-477"><a href="#cb11-477"></a>class x2,L2 blue</span>
<span id="cb11-478"><a href="#cb11-478"></a>class x3,ar grey</span>
<span id="cb11-479"><a href="#cb11-479"></a>class D,N0,N1,N2,G0,G1,G2,GU block</span>
<span id="cb11-480"><a href="#cb11-480"></a>class AR block</span>
<span id="cb11-481"><a href="#cb11-481"></a>class bc text</span>
<span id="cb11-482"><a href="#cb11-482"></a><span class="in">```</span></span>
<span id="cb11-483"><a href="#cb11-483"></a></span>
<span id="cb11-484"><a href="#cb11-484"></a>See: <span class="co">[</span><span class="ot">PyTorch / Distributed Data Parallel</span><span class="co">](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)</span></span>
<span id="cb11-485"><a href="#cb11-485"></a>:::</span>
<span id="cb11-486"><a href="#cb11-486"></a></span>
<span id="cb11-487"><a href="#cb11-487"></a><span class="fu">## Data Parallel: Training {background-color="white"}</span></span>
<span id="cb11-488"><a href="#cb11-488"></a></span>
<span id="cb11-489"><a href="#cb11-489"></a><span class="ss">- </span>Each GPU:</span>
<span id="cb11-490"><a href="#cb11-490"></a><span class="ss">  - </span>has **identical copy** of model</span>
<span id="cb11-491"><a href="#cb11-491"></a><span class="ss">  - </span>works on a **unique** subset of data</span>
<span id="cb11-492"><a href="#cb11-492"></a></span>
<span id="cb11-493"><a href="#cb11-493"></a><span class="ss">- </span>Easy to get started (minor modifications to code):</span>
<span id="cb11-494"><a href="#cb11-494"></a><span class="ss">    - </span>{{&lt; fa brands github &gt;}} <span class="co">[</span><span class="ot">saforem2/`ezpz`</span><span class="co">](https://github.com/saforem2/ezpz)</span></span>
<span id="cb11-495"><a href="#cb11-495"></a><span class="ss">    - </span>ğŸ”¥ <span class="co">[</span><span class="ot">PyTorch / `DDP`</span><span class="co">](https://pytorch.org/docs/stable/notes/ddp.html)</span></span>
<span id="cb11-496"><a href="#cb11-496"></a><span class="ss">    - </span>ğŸ¤— <span class="co">[</span><span class="ot">HF / `Accelerate`</span><span class="co">](https://huggingface.co/docs/transformers/accelerate)</span></span>
<span id="cb11-497"><a href="#cb11-497"></a><span class="ss">    - </span>{{<span class="dt">&lt;</span><span class="kw">iconify</span><span class="ot"> logos microsoft-icon </span><span class="dt">&gt;</span>}} <span class="co">[</span><span class="ot">Microsoft / `DeepSpeed`</span><span class="co">](https://www.deepspeed.ai/)</span></span>
<span id="cb11-498"><a href="#cb11-498"></a></span>
<span id="cb11-499"><a href="#cb11-499"></a><span class="ss">- </span>Requires **global** communication</span>
<span id="cb11-500"><a href="#cb11-500"></a><span class="ss">  - </span>every rank _must participate_ (collective communication) !!</span>
<span id="cb11-501"><a href="#cb11-501"></a></span>
<span id="cb11-502"><a href="#cb11-502"></a><span class="fu"># ğŸ—£ï¸ Communication {background-color="white"}</span></span>
<span id="cb11-503"><a href="#cb11-503"></a></span>
<span id="cb11-504"><a href="#cb11-504"></a><span class="ss">- </span>Need mechanism(s) for communicating across GPUs:</span>
<span id="cb11-505"><a href="#cb11-505"></a><span class="ss">  - </span><span class="co">[</span><span class="ot">`mpi4py`</span><span class="co">](https://mpi4py.readthedocs.io/en/stable/tutorial.html)</span></span>
<span id="cb11-506"><a href="#cb11-506"></a><span class="ss">  - </span><span class="co">[</span><span class="ot">`torch.distributed`</span><span class="co">](https://pytorch.org/docs/stable/distributed.html)</span></span>
<span id="cb11-507"><a href="#cb11-507"></a><span class="ss">- </span>Collective Communication:</span>
<span id="cb11-508"><a href="#cb11-508"></a><span class="ss">  - </span><span class="co">[</span><span class="ot">Nvidia Collective Communications Library (NCCL)</span><span class="co">](https://developer.nvidia.com/nccl)</span></span>
<span id="cb11-509"><a href="#cb11-509"></a><span class="ss">  - </span><span class="co">[</span><span class="ot">Intel oneAPI Collective Communications Library (oneCCL)</span><span class="co">](https://www.intel.com/content/www/us/en/developer/tools/oneapi/oneccl.html#gs.gouznn)</span></span>
<span id="cb11-510"><a href="#cb11-510"></a></span>
<span id="cb11-511"><a href="#cb11-511"></a>  ::: {.callout-warning collapse=false icon=false title="âŒ› Timeouts"}</span>
<span id="cb11-512"><a href="#cb11-512"></a><span class="ss">  - </span>Collective operations have to be called for each <span class="in">`rank`</span> to form a complete collective operation.</span>
<span id="cb11-513"><a href="#cb11-513"></a><span class="ss">    - </span>Failure to do so will result in other ranks waiting **indefinitely**</span>
<span id="cb11-514"><a href="#cb11-514"></a>  :::</span>
<span id="cb11-515"><a href="#cb11-515"></a></span>
<span id="cb11-516"><a href="#cb11-516"></a><span class="fu">## AllReduce {background-color="white"}</span></span>
<span id="cb11-517"><a href="#cb11-517"></a></span>
<span id="cb11-518"><a href="#cb11-518"></a>Perform _reductions_ on data (e.g. <span class="in">`sum`</span>, <span class="in">`min`</span>, <span class="in">`max`</span>) across ranks, send result back to everyone.</span>
<span id="cb11-519"><a href="#cb11-519"></a></span>
<span id="cb11-520"><a href="#cb11-520"></a>::: {#fig-all-reduce-mermaid}</span>
<span id="cb11-521"><a href="#cb11-521"></a></span>
<span id="cb11-524"><a href="#cb11-524"></a><span class="in">```{mermaid}</span></span>
<span id="cb11-525"><a href="#cb11-525"></a>flowchart TD</span>
<span id="cb11-526"><a href="#cb11-526"></a>  subgraph R0[<span class="ot">"</span><span class="st">`Rank 0`</span><span class="ot">"</span>]</span>
<span id="cb11-527"><a href="#cb11-527"></a>    x0(<span class="ot">"</span><span class="st">`x0`</span><span class="ot">"</span>)</span>
<span id="cb11-528"><a href="#cb11-528"></a>  end</span>
<span id="cb11-529"><a href="#cb11-529"></a>  subgraph R1[<span class="ot">"</span><span class="st">`Rank 1`</span><span class="ot">"</span>]</span>
<span id="cb11-530"><a href="#cb11-530"></a>    x1(<span class="ot">"</span><span class="st">`x1`</span><span class="ot">"</span>)</span>
<span id="cb11-531"><a href="#cb11-531"></a>  end</span>
<span id="cb11-532"><a href="#cb11-532"></a>  subgraph R2[<span class="ot">"</span><span class="st">`Rank 2`</span><span class="ot">"</span>]</span>
<span id="cb11-533"><a href="#cb11-533"></a>    x2(<span class="ot">"</span><span class="st">`x2`</span><span class="ot">"</span>)</span>
<span id="cb11-534"><a href="#cb11-534"></a>  end</span>
<span id="cb11-535"><a href="#cb11-535"></a>  subgraph R3[<span class="ot">"</span><span class="st">`Rank 3`</span><span class="ot">"</span>]</span>
<span id="cb11-536"><a href="#cb11-536"></a>    x3(<span class="ot">"</span><span class="st">`x3`</span><span class="ot">"</span>)</span>
<span id="cb11-537"><a href="#cb11-537"></a>  end</span>
<span id="cb11-538"><a href="#cb11-538"></a>  subgraph AR[<span class="ot">"</span><span class="st">`Allreduce`</span><span class="ot">"</span>]</span>
<span id="cb11-539"><a href="#cb11-539"></a>    xp[<span class="ot">"</span><span class="st">`x' = âˆ‘ xâ‚™ `</span><span class="ot">"</span>]</span>
<span id="cb11-540"><a href="#cb11-540"></a>  end</span>
<span id="cb11-541"><a href="#cb11-541"></a>  subgraph AR3[<span class="ot">"</span><span class="st">`Rank 3`</span><span class="ot">"</span>]</span>
<span id="cb11-542"><a href="#cb11-542"></a>    xp3(<span class="ot">"</span><span class="st">`x'`</span><span class="ot">"</span>)</span>
<span id="cb11-543"><a href="#cb11-543"></a>  end</span>
<span id="cb11-544"><a href="#cb11-544"></a>  subgraph AR2[<span class="ot">"</span><span class="st">`Rank 2`</span><span class="ot">"</span>]</span>
<span id="cb11-545"><a href="#cb11-545"></a>    xp2(<span class="ot">"</span><span class="st">`x'`</span><span class="ot">"</span>)</span>
<span id="cb11-546"><a href="#cb11-546"></a>  end</span>
<span id="cb11-547"><a href="#cb11-547"></a>  subgraph AR1[<span class="ot">"</span><span class="st">`Rank 1`</span><span class="ot">"</span>]</span>
<span id="cb11-548"><a href="#cb11-548"></a>    xp1(<span class="ot">"</span><span class="st">`x'`</span><span class="ot">"</span>)</span>
<span id="cb11-549"><a href="#cb11-549"></a>  end</span>
<span id="cb11-550"><a href="#cb11-550"></a>  subgraph AR0[<span class="ot">"</span><span class="st">`Rank 0`</span><span class="ot">"</span>]</span>
<span id="cb11-551"><a href="#cb11-551"></a>    xp0(<span class="ot">"</span><span class="st">`x'`</span><span class="ot">"</span>)</span>
<span id="cb11-552"><a href="#cb11-552"></a>  end</span>
<span id="cb11-553"><a href="#cb11-553"></a>  x0 --&gt; AR</span>
<span id="cb11-554"><a href="#cb11-554"></a>  x1 --&gt; AR</span>
<span id="cb11-555"><a href="#cb11-555"></a>  x2 --&gt; AR</span>
<span id="cb11-556"><a href="#cb11-556"></a>  x3 --&gt; AR</span>
<span id="cb11-557"><a href="#cb11-557"></a>  AR --&gt; xp0</span>
<span id="cb11-558"><a href="#cb11-558"></a>  AR --&gt; xp1</span>
<span id="cb11-559"><a href="#cb11-559"></a>  AR --&gt; xp2</span>
<span id="cb11-560"><a href="#cb11-560"></a>  AR --&gt; xp3</span>
<span id="cb11-561"><a href="#cb11-561"></a>classDef block fill:<span class="co">#CCCCCC02,stroke:#838383,stroke-width:1px,color:#838383</span></span>
<span id="cb11-562"><a href="#cb11-562"></a>classDef red fill:<span class="co">#ff8181,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-563"><a href="#cb11-563"></a>classDef orange fill:<span class="co">#FFC47F,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-564"><a href="#cb11-564"></a>classDef yellow fill:<span class="co">#FFFF7F,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-565"><a href="#cb11-565"></a>classDef green fill:<span class="co">#98E6A5,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-566"><a href="#cb11-566"></a>classDef pink fill:<span class="co">#E599F7,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-567"><a href="#cb11-567"></a>classDef blue fill:<span class="co">#7DCAFF,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-568"><a href="#cb11-568"></a>classDef purple fill:<span class="co">#FFCBE6,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-569"><a href="#cb11-569"></a>class R0,R1,R2,R3,AR,AR0,AR1,AR2,AR3 block</span>
<span id="cb11-570"><a href="#cb11-570"></a>class xp,xp0,xp1,xp2,xp3, purple</span>
<span id="cb11-571"><a href="#cb11-571"></a>class x0, red</span>
<span id="cb11-572"><a href="#cb11-572"></a>class x1, green</span>
<span id="cb11-573"><a href="#cb11-573"></a>class x2, blue</span>
<span id="cb11-574"><a href="#cb11-574"></a>class x3, yellow</span>
<span id="cb11-575"><a href="#cb11-575"></a><span class="in">```</span></span>
<span id="cb11-576"><a href="#cb11-576"></a></span>
<span id="cb11-577"><a href="#cb11-577"></a>All-Reduce operation: each rank receives the reduction of input values across ranks.</span>
<span id="cb11-578"><a href="#cb11-578"></a>:::</span>
<span id="cb11-579"><a href="#cb11-579"></a></span>
<span id="cb11-580"><a href="#cb11-580"></a><span class="fu">## Reduce {background-color="white"}</span></span>
<span id="cb11-581"><a href="#cb11-581"></a></span>
<span id="cb11-582"><a href="#cb11-582"></a><span class="ss">- </span>Perform a _reduction_ on data across ranks, send to individual</span>
<span id="cb11-583"><a href="#cb11-583"></a></span>
<span id="cb11-584"><a href="#cb11-584"></a>::: {#fig-reduce-mermaid}</span>
<span id="cb11-585"><a href="#cb11-585"></a></span>
<span id="cb11-588"><a href="#cb11-588"></a><span class="in">```{mermaid}</span></span>
<span id="cb11-589"><a href="#cb11-589"></a>flowchart TD</span>
<span id="cb11-590"><a href="#cb11-590"></a>  subgraph R0[<span class="ot">"</span><span class="st">`Rank 0`</span><span class="ot">"</span>]</span>
<span id="cb11-591"><a href="#cb11-591"></a>    x0(<span class="ot">"</span><span class="st">`x0`</span><span class="ot">"</span>)</span>
<span id="cb11-592"><a href="#cb11-592"></a>  end</span>
<span id="cb11-593"><a href="#cb11-593"></a>  subgraph R1[<span class="ot">"</span><span class="st">`Rank 1`</span><span class="ot">"</span>]</span>
<span id="cb11-594"><a href="#cb11-594"></a>    x1(<span class="ot">"</span><span class="st">`x1`</span><span class="ot">"</span>)</span>
<span id="cb11-595"><a href="#cb11-595"></a>  end</span>
<span id="cb11-596"><a href="#cb11-596"></a>  subgraph R2[<span class="ot">"</span><span class="st">`Rank 2`</span><span class="ot">"</span>]</span>
<span id="cb11-597"><a href="#cb11-597"></a>    x2(<span class="ot">"</span><span class="st">`x2`</span><span class="ot">"</span>)</span>
<span id="cb11-598"><a href="#cb11-598"></a>  end</span>
<span id="cb11-599"><a href="#cb11-599"></a>  subgraph R3[<span class="ot">"</span><span class="st">`Rank 3`</span><span class="ot">"</span>]</span>
<span id="cb11-600"><a href="#cb11-600"></a>    x3(<span class="ot">"</span><span class="st">`x3`</span><span class="ot">"</span>)</span>
<span id="cb11-601"><a href="#cb11-601"></a>  end</span>
<span id="cb11-602"><a href="#cb11-602"></a>  subgraph AR[<span class="ot">"</span><span class="st">`Reduce`</span><span class="ot">"</span>]</span>
<span id="cb11-603"><a href="#cb11-603"></a>    xp[<span class="ot">"</span><span class="st">`x'=reduce(x, 2, SUM)`</span><span class="ot">"</span>]</span>
<span id="cb11-604"><a href="#cb11-604"></a>  end</span>
<span id="cb11-605"><a href="#cb11-605"></a>  subgraph AR3[<span class="ot">"</span><span class="st">`Rank 3`</span><span class="ot">"</span>]</span>
<span id="cb11-606"><a href="#cb11-606"></a>  end</span>
<span id="cb11-607"><a href="#cb11-607"></a>  subgraph AR2[<span class="ot">"</span><span class="st">`Rank 2`</span><span class="ot">"</span>]</span>
<span id="cb11-608"><a href="#cb11-608"></a>    xp2(<span class="ot">"</span><span class="st">`x'`</span><span class="ot">"</span>)</span>
<span id="cb11-609"><a href="#cb11-609"></a>  end</span>
<span id="cb11-610"><a href="#cb11-610"></a>  subgraph AR1[<span class="ot">"</span><span class="st">`Rank 1`</span><span class="ot">"</span>]</span>
<span id="cb11-611"><a href="#cb11-611"></a>  end</span>
<span id="cb11-612"><a href="#cb11-612"></a>  subgraph AR0[<span class="ot">"</span><span class="st">`Rank 0`</span><span class="ot">"</span>]</span>
<span id="cb11-613"><a href="#cb11-613"></a>  end</span>
<span id="cb11-614"><a href="#cb11-614"></a>  x0 --&gt; AR</span>
<span id="cb11-615"><a href="#cb11-615"></a>  x1 --&gt; AR</span>
<span id="cb11-616"><a href="#cb11-616"></a>  x2 --&gt; AR</span>
<span id="cb11-617"><a href="#cb11-617"></a>  x3 --&gt; AR</span>
<span id="cb11-618"><a href="#cb11-618"></a>  AR --&gt; AR3</span>
<span id="cb11-619"><a href="#cb11-619"></a>  AR --&gt; xp2</span>
<span id="cb11-620"><a href="#cb11-620"></a>  AR --&gt; AR1</span>
<span id="cb11-621"><a href="#cb11-621"></a>  AR --&gt; AR0</span>
<span id="cb11-622"><a href="#cb11-622"></a>classDef block fill:<span class="co">#CCCCCC02,stroke:#838383,stroke-width:1px,color:#838383</span></span>
<span id="cb11-623"><a href="#cb11-623"></a>classDef red fill:<span class="co">#ff8181,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-624"><a href="#cb11-624"></a>classDef orange fill:<span class="co">#FFC47F,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-625"><a href="#cb11-625"></a>classDef green fill:<span class="co">#98E6A5,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-626"><a href="#cb11-626"></a>classDef yellow fill:<span class="co">#FFFF7F,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-627"><a href="#cb11-627"></a>classDef blue fill:<span class="co">#7DCAFF,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-628"><a href="#cb11-628"></a>classDef purple fill:<span class="co">#FFCBE6,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-629"><a href="#cb11-629"></a>classDef pink fill:<span class="co">#E599F7,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-630"><a href="#cb11-630"></a>class R0,R1,R2,R3,AR,AR0,AR1,AR2,AR3, block</span>
<span id="cb11-631"><a href="#cb11-631"></a>class xp,xp2 purple</span>
<span id="cb11-632"><a href="#cb11-632"></a>class x0, red</span>
<span id="cb11-633"><a href="#cb11-633"></a>class x1, green</span>
<span id="cb11-634"><a href="#cb11-634"></a>class x2, blue</span>
<span id="cb11-635"><a href="#cb11-635"></a>class x3, yellow</span>
<span id="cb11-636"><a href="#cb11-636"></a><span class="in">```</span></span>
<span id="cb11-637"><a href="#cb11-637"></a></span>
<span id="cb11-638"><a href="#cb11-638"></a>Reduce operation: one rank receives the reduction of input values across ranks</span>
<span id="cb11-639"><a href="#cb11-639"></a>:::</span>
<span id="cb11-640"><a href="#cb11-640"></a></span>
<span id="cb11-641"><a href="#cb11-641"></a><span class="fu">## Broadcast {background-color="white"}</span></span>
<span id="cb11-642"><a href="#cb11-642"></a></span>
<span id="cb11-643"><a href="#cb11-643"></a>::: {#fig-broadcast-mermaid}</span>
<span id="cb11-644"><a href="#cb11-644"></a></span>
<span id="cb11-647"><a href="#cb11-647"></a><span class="in">```{mermaid}</span></span>
<span id="cb11-648"><a href="#cb11-648"></a>flowchart TD</span>
<span id="cb11-649"><a href="#cb11-649"></a>  subgraph R3[<span class="ot">"</span><span class="st">`Rank 3`</span><span class="ot">"</span>]</span>
<span id="cb11-650"><a href="#cb11-650"></a>  end</span>
<span id="cb11-651"><a href="#cb11-651"></a>  subgraph R2[<span class="ot">"</span><span class="st">`Rank 2`</span><span class="ot">"</span>]</span>
<span id="cb11-652"><a href="#cb11-652"></a>    x2(<span class="ot">"</span><span class="st">`x2`</span><span class="ot">"</span>)</span>
<span id="cb11-653"><a href="#cb11-653"></a>  end</span>
<span id="cb11-654"><a href="#cb11-654"></a>  subgraph R1[<span class="ot">"</span><span class="st">`Rank 1`</span><span class="ot">"</span>]</span>
<span id="cb11-655"><a href="#cb11-655"></a>  end</span>
<span id="cb11-656"><a href="#cb11-656"></a>  subgraph R0[<span class="ot">"</span><span class="st">`Rank 0`</span><span class="ot">"</span>]</span>
<span id="cb11-657"><a href="#cb11-657"></a>  end</span>
<span id="cb11-658"><a href="#cb11-658"></a>  subgraph AR[<span class="ot">"</span><span class="st">` `</span><span class="ot">"</span>]</span>
<span id="cb11-659"><a href="#cb11-659"></a>    xp[<span class="ot">"</span><span class="st">`broadcast(x2, 2)`</span><span class="ot">"</span>]</span>
<span id="cb11-660"><a href="#cb11-660"></a>  end</span>
<span id="cb11-661"><a href="#cb11-661"></a>  subgraph AR0[<span class="ot">"</span><span class="st">`Rank 0`</span><span class="ot">"</span>]</span>
<span id="cb11-662"><a href="#cb11-662"></a>    xp0(<span class="ot">"</span><span class="st">`x2`</span><span class="ot">"</span>)</span>
<span id="cb11-663"><a href="#cb11-663"></a>  end</span>
<span id="cb11-664"><a href="#cb11-664"></a>  subgraph AR1[<span class="ot">"</span><span class="st">`Rank 1`</span><span class="ot">"</span>]</span>
<span id="cb11-665"><a href="#cb11-665"></a>    xp1(<span class="ot">"</span><span class="st">`x2`</span><span class="ot">"</span>)</span>
<span id="cb11-666"><a href="#cb11-666"></a>  end</span>
<span id="cb11-667"><a href="#cb11-667"></a>  subgraph AR2[<span class="ot">"</span><span class="st">`Rank 2`</span><span class="ot">"</span>]</span>
<span id="cb11-668"><a href="#cb11-668"></a>    xp2(<span class="ot">"</span><span class="st">`x2`</span><span class="ot">"</span>)</span>
<span id="cb11-669"><a href="#cb11-669"></a>  end</span>
<span id="cb11-670"><a href="#cb11-670"></a>  subgraph AR3[<span class="ot">"</span><span class="st">`Rank 3`</span><span class="ot">"</span>]</span>
<span id="cb11-671"><a href="#cb11-671"></a>    xp3(<span class="ot">"</span><span class="st">`x2`</span><span class="ot">"</span>)</span>
<span id="cb11-672"><a href="#cb11-672"></a>  end</span>
<span id="cb11-673"><a href="#cb11-673"></a>  x2 --&gt; AR</span>
<span id="cb11-674"><a href="#cb11-674"></a>  AR --&gt; AR0</span>
<span id="cb11-675"><a href="#cb11-675"></a>  AR --&gt; AR1</span>
<span id="cb11-676"><a href="#cb11-676"></a>  AR --&gt; AR2</span>
<span id="cb11-677"><a href="#cb11-677"></a>  AR --&gt; AR3</span>
<span id="cb11-678"><a href="#cb11-678"></a>classDef text fill:<span class="co">#CCCCCC02,stroke:#838383,stroke-width:0px,color:#838383,font-weight:500</span></span>
<span id="cb11-679"><a href="#cb11-679"></a>classDef block fill:<span class="co">#CCCCCC02,stroke:#838383,stroke-width:1px,font-weight:500,color:#838383</span></span>
<span id="cb11-680"><a href="#cb11-680"></a>classDef blue fill:<span class="co">#7DCAFF,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-681"><a href="#cb11-681"></a>classDef yellow fill:<span class="co">#FFFF7F,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-682"><a href="#cb11-682"></a>class R0,R1,R2,R3,AR0,AR1,AR2,AR3,AR, block</span>
<span id="cb11-683"><a href="#cb11-683"></a>class x2,xp0,xp1,xp2,xp3 blue</span>
<span id="cb11-684"><a href="#cb11-684"></a>class xp, text</span>
<span id="cb11-685"><a href="#cb11-685"></a><span class="in">```</span></span>
<span id="cb11-686"><a href="#cb11-686"></a><span class="in">`broadcast`</span> (send) a tensor <span class="dt">&lt;</span><span class="kw">code</span><span class="dt">&gt;</span>$x$<span class="dt">&lt;/</span><span class="kw">code</span><span class="dt">&gt;</span> from one rank to all ranks</span>
<span id="cb11-687"><a href="#cb11-687"></a>:::</span>
<span id="cb11-688"><a href="#cb11-688"></a></span>
<span id="cb11-689"><a href="#cb11-689"></a><span class="fu">## AllGather {background-color="white"}</span></span>
<span id="cb11-690"><a href="#cb11-690"></a></span>
<span id="cb11-691"><a href="#cb11-691"></a>::: {#fig-allgather-mermaid}</span>
<span id="cb11-692"><a href="#cb11-692"></a></span>
<span id="cb11-695"><a href="#cb11-695"></a><span class="in">```{mermaid}</span></span>
<span id="cb11-696"><a href="#cb11-696"></a>flowchart LR</span>
<span id="cb11-697"><a href="#cb11-697"></a>  subgraph R0[<span class="ot">"</span><span class="st">`Rank 0`</span><span class="ot">"</span>]</span>
<span id="cb11-698"><a href="#cb11-698"></a>    x0(<span class="ot">"</span><span class="st">`x0`</span><span class="ot">"</span>)</span>
<span id="cb11-699"><a href="#cb11-699"></a>  end</span>
<span id="cb11-700"><a href="#cb11-700"></a>  subgraph R1[<span class="ot">"</span><span class="st">`Rank 1`</span><span class="ot">"</span>]</span>
<span id="cb11-701"><a href="#cb11-701"></a>    x1(<span class="ot">"</span><span class="st">`x1`</span><span class="ot">"</span>)</span>
<span id="cb11-702"><a href="#cb11-702"></a>  end</span>
<span id="cb11-703"><a href="#cb11-703"></a>  subgraph R2[<span class="ot">"</span><span class="st">`Rank 2`</span><span class="ot">"</span>]</span>
<span id="cb11-704"><a href="#cb11-704"></a>    x2(<span class="ot">"</span><span class="st">`x2`</span><span class="ot">"</span>)</span>
<span id="cb11-705"><a href="#cb11-705"></a>  end</span>
<span id="cb11-706"><a href="#cb11-706"></a>  subgraph AG[<span class="ot">"</span><span class="st">`Allgather`</span><span class="ot">"</span>]</span>
<span id="cb11-707"><a href="#cb11-707"></a>    %<span class="dt">%xp0</span>[<span class="ot">"</span><span class="st">`z=[empty_like(x) for _ in range(4)]`</span><span class="ot">"</span>]</span>
<span id="cb11-708"><a href="#cb11-708"></a>    %<span class="dt">%xp1</span>[<span class="ot">"</span><span class="st">`dist.all_gather(z, x)`</span><span class="ot">"</span>]</span>
<span id="cb11-709"><a href="#cb11-709"></a>  end</span>
<span id="cb11-710"><a href="#cb11-710"></a>  subgraph AG2[<span class="ot">"</span><span class="st">`Rank 2`</span><span class="ot">"</span>]</span>
<span id="cb11-711"><a href="#cb11-711"></a>    direction TB</span>
<span id="cb11-712"><a href="#cb11-712"></a>    xp02(<span class="ot">"</span><span class="st">`x0`</span><span class="ot">"</span>)</span>
<span id="cb11-713"><a href="#cb11-713"></a>    xp12(<span class="ot">"</span><span class="st">`x1`</span><span class="ot">"</span>)</span>
<span id="cb11-714"><a href="#cb11-714"></a>    xp22(<span class="ot">"</span><span class="st">`x2`</span><span class="ot">"</span>)</span>
<span id="cb11-715"><a href="#cb11-715"></a>  end</span>
<span id="cb11-716"><a href="#cb11-716"></a>  subgraph AG1[<span class="ot">"</span><span class="st">`Rank 1`</span><span class="ot">"</span>]</span>
<span id="cb11-717"><a href="#cb11-717"></a>    direction TB</span>
<span id="cb11-718"><a href="#cb11-718"></a>    xp01(<span class="ot">"</span><span class="st">`x0`</span><span class="ot">"</span>)</span>
<span id="cb11-719"><a href="#cb11-719"></a>    xp11(<span class="ot">"</span><span class="st">`x1`</span><span class="ot">"</span>)</span>
<span id="cb11-720"><a href="#cb11-720"></a>    xp21(<span class="ot">"</span><span class="st">`x2`</span><span class="ot">"</span>)</span>
<span id="cb11-721"><a href="#cb11-721"></a>  end</span>
<span id="cb11-722"><a href="#cb11-722"></a>  subgraph AG0[<span class="ot">"</span><span class="st">`Rank 0`</span><span class="ot">"</span>]</span>
<span id="cb11-723"><a href="#cb11-723"></a>    direction TB</span>
<span id="cb11-724"><a href="#cb11-724"></a>    xp00(<span class="ot">"</span><span class="st">`x0`</span><span class="ot">"</span>)</span>
<span id="cb11-725"><a href="#cb11-725"></a>    xp10(<span class="ot">"</span><span class="st">`x1`</span><span class="ot">"</span>)</span>
<span id="cb11-726"><a href="#cb11-726"></a>    xp20(<span class="ot">"</span><span class="st">`x2`</span><span class="ot">"</span>)</span>
<span id="cb11-727"><a href="#cb11-727"></a>  end</span>
<span id="cb11-728"><a href="#cb11-728"></a>  x0 --&gt; AG</span>
<span id="cb11-729"><a href="#cb11-729"></a>  x1 --&gt; AG</span>
<span id="cb11-730"><a href="#cb11-730"></a>  x2 --&gt; AG</span>
<span id="cb11-731"><a href="#cb11-731"></a>  AG --&gt; AG0</span>
<span id="cb11-732"><a href="#cb11-732"></a>  AG --&gt; AG1</span>
<span id="cb11-733"><a href="#cb11-733"></a>  AG --&gt; AG2</span>
<span id="cb11-734"><a href="#cb11-734"></a>classDef red fill:<span class="co">#ff8181,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-735"><a href="#cb11-735"></a>classDef orange fill:<span class="co">#FFC47F,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-736"><a href="#cb11-736"></a>classDef yellow fill:<span class="co">#FFFF7F,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-737"><a href="#cb11-737"></a>classDef green fill:<span class="co">#98E6A5,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-738"><a href="#cb11-738"></a>classDef blue fill:<span class="co">#7DCAFF,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-739"><a href="#cb11-739"></a>classDef purple fill:<span class="co">#FFCBE6,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-740"><a href="#cb11-740"></a>classDef block fill:<span class="co">#CCCCCC02,stroke:#838383,stroke-width:1px,font-weight:500,color:#838383</span></span>
<span id="cb11-741"><a href="#cb11-741"></a>classDef text fill:<span class="co">#CCCCCC02,stroke:#838383,stroke-width:0px,color:#838383</span></span>
<span id="cb11-742"><a href="#cb11-742"></a>class xp0,xp1, text</span>
<span id="cb11-743"><a href="#cb11-743"></a>class AG0,AG1,AG2,AG3,AG,R0,R1,R2,R3, block</span>
<span id="cb11-744"><a href="#cb11-744"></a>class xp00,xp01,xp02,xp03, red</span>
<span id="cb11-745"><a href="#cb11-745"></a>class xp10,xp11,xp12,xp13, green</span>
<span id="cb11-746"><a href="#cb11-746"></a>class xp20,xp21,xp22,xp23, blue</span>
<span id="cb11-747"><a href="#cb11-747"></a>class xp30,xp31,xp32,xp33, yellow</span>
<span id="cb11-748"><a href="#cb11-748"></a>class x0, red</span>
<span id="cb11-749"><a href="#cb11-749"></a>class x1, green</span>
<span id="cb11-750"><a href="#cb11-750"></a>class x2, blue</span>
<span id="cb11-751"><a href="#cb11-751"></a>class x3, yellow</span>
<span id="cb11-752"><a href="#cb11-752"></a><span class="in">```</span></span>
<span id="cb11-753"><a href="#cb11-753"></a></span>
<span id="cb11-754"><a href="#cb11-754"></a>Gathers tensors from the whole group in a list.</span>
<span id="cb11-755"><a href="#cb11-755"></a>:::</span>
<span id="cb11-756"><a href="#cb11-756"></a></span>
<span id="cb11-757"><a href="#cb11-757"></a><span class="fu">## Scatter {background-color="white"}</span></span>
<span id="cb11-758"><a href="#cb11-758"></a></span>
<span id="cb11-759"><a href="#cb11-759"></a>::: {#fig-scatter-mermaid}</span>
<span id="cb11-760"><a href="#cb11-760"></a></span>
<span id="cb11-763"><a href="#cb11-763"></a><span class="in">```{mermaid}</span></span>
<span id="cb11-764"><a href="#cb11-764"></a>flowchart TD</span>
<span id="cb11-765"><a href="#cb11-765"></a>  subgraph R3[<span class="ot">"</span><span class="st">`Rank 3`</span><span class="ot">"</span>]</span>
<span id="cb11-766"><a href="#cb11-766"></a>  end</span>
<span id="cb11-767"><a href="#cb11-767"></a>  subgraph R2[<span class="ot">"</span><span class="st">`Rank 2`</span><span class="ot">"</span>]</span>
<span id="cb11-768"><a href="#cb11-768"></a>  end</span>
<span id="cb11-769"><a href="#cb11-769"></a>  subgraph R1[<span class="ot">"</span><span class="st">`Rank 1`</span><span class="ot">"</span>]</span>
<span id="cb11-770"><a href="#cb11-770"></a>    direction TB</span>
<span id="cb11-771"><a href="#cb11-771"></a>    xp0(<span class="ot">"</span><span class="st">`x0`</span><span class="ot">"</span>)</span>
<span id="cb11-772"><a href="#cb11-772"></a>    xp1(<span class="ot">"</span><span class="st">`x1`</span><span class="ot">"</span>)</span>
<span id="cb11-773"><a href="#cb11-773"></a>    xp2(<span class="ot">"</span><span class="st">`x2`</span><span class="ot">"</span>)</span>
<span id="cb11-774"><a href="#cb11-774"></a>    xp3(<span class="ot">"</span><span class="st">`x3`</span><span class="ot">"</span>)</span>
<span id="cb11-775"><a href="#cb11-775"></a>  end</span>
<span id="cb11-776"><a href="#cb11-776"></a>  subgraph R0[<span class="ot">"</span><span class="st">`Rank 0`</span><span class="ot">"</span>]</span>
<span id="cb11-777"><a href="#cb11-777"></a>  end</span>
<span id="cb11-778"><a href="#cb11-778"></a>  subgraph S[<span class="ot">"</span><span class="st">`Scatter`</span><span class="ot">"</span>]</span>
<span id="cb11-779"><a href="#cb11-779"></a>  end</span>
<span id="cb11-780"><a href="#cb11-780"></a>  subgraph S3[<span class="ot">"</span><span class="st">`Rank 3`</span><span class="ot">"</span>]</span>
<span id="cb11-781"><a href="#cb11-781"></a>    x3(<span class="ot">"</span><span class="st">`x3`</span><span class="ot">"</span>)</span>
<span id="cb11-782"><a href="#cb11-782"></a>  end</span>
<span id="cb11-783"><a href="#cb11-783"></a>  subgraph S2[<span class="ot">"</span><span class="st">`Rank 2`</span><span class="ot">"</span>]</span>
<span id="cb11-784"><a href="#cb11-784"></a>    x2(<span class="ot">"</span><span class="st">`x2`</span><span class="ot">"</span>)</span>
<span id="cb11-785"><a href="#cb11-785"></a>  end</span>
<span id="cb11-786"><a href="#cb11-786"></a>  subgraph S1[<span class="ot">"</span><span class="st">`Rank 1`</span><span class="ot">"</span>]</span>
<span id="cb11-787"><a href="#cb11-787"></a>    x1(<span class="ot">"</span><span class="st">`x1`</span><span class="ot">"</span>)</span>
<span id="cb11-788"><a href="#cb11-788"></a>  end</span>
<span id="cb11-789"><a href="#cb11-789"></a>  subgraph S0[<span class="ot">"</span><span class="st">`Rank 0`</span><span class="ot">"</span>]</span>
<span id="cb11-790"><a href="#cb11-790"></a>    x0(<span class="ot">"</span><span class="st">`x0`</span><span class="ot">"</span>)</span>
<span id="cb11-791"><a href="#cb11-791"></a>  end</span>
<span id="cb11-792"><a href="#cb11-792"></a>  R1 --&gt; S</span>
<span id="cb11-793"><a href="#cb11-793"></a>  S --&gt; S0</span>
<span id="cb11-794"><a href="#cb11-794"></a>  S --&gt; S1</span>
<span id="cb11-795"><a href="#cb11-795"></a>  S --&gt; S2</span>
<span id="cb11-796"><a href="#cb11-796"></a>  S --&gt; S3</span>
<span id="cb11-797"><a href="#cb11-797"></a>classDef red fill:<span class="co">#ff8181,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-798"><a href="#cb11-798"></a>classDef orange fill:<span class="co">#FFC47F,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-799"><a href="#cb11-799"></a>classDef yellow fill:<span class="co">#FFFF7F,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-800"><a href="#cb11-800"></a>classDef green fill:<span class="co">#98E6A5,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-801"><a href="#cb11-801"></a>classDef blue fill:<span class="co">#7DCAFF,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-802"><a href="#cb11-802"></a>classDef purple fill:<span class="co">#FFCBE6,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-803"><a href="#cb11-803"></a>classDef block fill:<span class="co">#CCCCCC02,stroke:#838383,stroke-width:1px,font-weight:500,color:#838383</span></span>
<span id="cb11-804"><a href="#cb11-804"></a>class AG0,AG1,AG2,AG3,S,R0,R1,R2,R3,S0,S1,S2,S3, block</span>
<span id="cb11-805"><a href="#cb11-805"></a>class x0,xp0, red</span>
<span id="cb11-806"><a href="#cb11-806"></a>class x1,xp1, green</span>
<span id="cb11-807"><a href="#cb11-807"></a>class x2,xp2, blue</span>
<span id="cb11-808"><a href="#cb11-808"></a>class x3,xp3, yellow</span>
<span id="cb11-809"><a href="#cb11-809"></a><span class="in">```</span></span>
<span id="cb11-810"><a href="#cb11-810"></a></span>
<span id="cb11-811"><a href="#cb11-811"></a>Scatters a list of tensors to the whole group</span>
<span id="cb11-812"><a href="#cb11-812"></a>:::</span>
<span id="cb11-813"><a href="#cb11-813"></a></span>
<span id="cb11-814"><a href="#cb11-814"></a><span class="co">&lt;!--</span></span>
<span id="cb11-815"><a href="#cb11-815"></a><span class="co">## AllReduce {background-color="white"}</span></span>
<span id="cb11-816"><a href="#cb11-816"></a></span>
<span id="cb11-817"><a href="#cb11-817"></a><span class="co">Perform _reductions_ on data (e.g. `sum`, `min`, `max`) across ranks, send result back to everyone.</span></span>
<span id="cb11-818"><a href="#cb11-818"></a></span>
<span id="cb11-819"><a href="#cb11-819"></a><span class="co">![All-Reduce operation: each rank receives the reduction of input values across ranks.](./assets/collective-allreduce-sum.drawio.svg){#fig-all-reduce .r-stretch}</span></span>
<span id="cb11-820"><a href="#cb11-820"></a></span>
<span id="cb11-821"><a href="#cb11-821"></a><span class="co">## Reduce {background-color="white"}</span></span>
<span id="cb11-822"><a href="#cb11-822"></a></span>
<span id="cb11-823"><a href="#cb11-823"></a><span class="co">- Perform a _reduction_ on data across ranks, send to individual</span></span>
<span id="cb11-824"><a href="#cb11-824"></a></span>
<span id="cb11-825"><a href="#cb11-825"></a><span class="co">![Reduce operation: one rank receives the reduction of input values across ranks](./assets/collective-reduce-sum.drawio.svg){#fig-reduce .r-stretch}</span></span>
<span id="cb11-826"><a href="#cb11-826"></a></span>
<span id="cb11-827"><a href="#cb11-827"></a><span class="co">## Broadcast {background-color="white"}</span></span>
<span id="cb11-828"><a href="#cb11-828"></a></span>
<span id="cb11-829"><a href="#cb11-829"></a><span class="co">![`broadcast` (_send_) a tensor &lt;code&gt;$x$&lt;/code&gt; from one rank to all ranks](./assets/collective-broadcast.drawio.svg){#fig-broadcast .r-stretch}</span></span>
<span id="cb11-830"><a href="#cb11-830"></a></span>
<span id="cb11-831"><a href="#cb11-831"></a><span class="co">## AllGather {background-color="white"}</span></span>
<span id="cb11-832"><a href="#cb11-832"></a></span>
<span id="cb11-833"><a href="#cb11-833"></a><span class="co">![Gathers tensors from the whole group in a list.](./assets/collective-allgather.drawio.svg){#fig-allgather .r-stretch}</span></span>
<span id="cb11-834"><a href="#cb11-834"></a></span>
<span id="cb11-835"><a href="#cb11-835"></a><span class="co">## Scatter {background-color="white"}</span></span>
<span id="cb11-836"><a href="#cb11-836"></a></span>
<span id="cb11-837"><a href="#cb11-837"></a><span class="co">::: {#fig-scatter}</span></span>
<span id="cb11-838"><a href="#cb11-838"></a><span class="co">![](./assets/collective-scatter.drawio.svg){.r-stretch}</span></span>
<span id="cb11-839"><a href="#cb11-839"></a></span>
<span id="cb11-840"><a href="#cb11-840"></a><span class="co">Scatters a list of tensors to the whole group</span></span>
<span id="cb11-841"><a href="#cb11-841"></a><span class="co">:::</span></span>
<span id="cb11-842"><a href="#cb11-842"></a><span class="co">--&gt;</span></span>
<span id="cb11-843"><a href="#cb11-843"></a></span>
<span id="cb11-844"><a href="#cb11-844"></a><span class="fu"># âš¡ Why Distributed Training? {background-color="white"}</span></span>
<span id="cb11-845"><a href="#cb11-845"></a></span>
<span id="cb11-846"><a href="#cb11-846"></a><span class="ss">- </span><span class="in">`N`</span> workers each processing unique batch<span class="ot">[^mbs]</span> of data:</span>
<span id="cb11-847"><a href="#cb11-847"></a><span class="ss">  - </span><span class="sc">\[</span><span class="in">`micro_batch_size = 1`</span><span class="sc">\]</span> $\times$ <span class="sc">\[</span><span class="in">`N`</span> GPUs<span class="sc">\]</span> $\rightarrow$ <span class="co">[</span><span class="ot">&lt;b&gt;&lt;code&gt;global_batch_size = N&lt;/code&gt;&lt;/b&gt;</span><span class="co">]</span></span>
<span id="cb11-848"><a href="#cb11-848"></a><span class="ss">- </span>Improved gradient estimators</span>
<span id="cb11-849"><a href="#cb11-849"></a><span class="ss">    - </span>Smooth loss landscape</span>
<span id="cb11-850"><a href="#cb11-850"></a><span class="ss">    - </span>Less iterations needed for same number of epochs</span>
<span id="cb11-851"><a href="#cb11-851"></a><span class="ss">        - </span>common to scale learning rate <span class="in">`lr *= sqrt(N)`</span></span>
<span id="cb11-852"><a href="#cb11-852"></a><span class="ss">- </span>See: <span class="co">[</span><span class="ot">Large Batch Training of Convolutional Networks</span><span class="co">](https://arxiv.org/abs/1708.03888)</span></span>
<span id="cb11-853"><a href="#cb11-853"></a></span>
<span id="cb11-854"><a href="#cb11-854"></a><span class="ot">[^mbs]: </span><span class="in">`micro_batch_size`</span> = batch_size **per** GPU</span>
<span id="cb11-855"><a href="#cb11-855"></a></span>
<span id="cb11-856"><a href="#cb11-856"></a><span class="fu">## Why Distributed Training? Speedup! {background-color="white"}</span></span>
<span id="cb11-857"><a href="#cb11-857"></a></span>
<span id="cb11-858"><a href="#cb11-858"></a>::: {#tbl-recent-progress}</span>
<span id="cb11-859"><a href="#cb11-859"></a></span>
<span id="cb11-860"><a href="#cb11-860"></a><span class="pp">|</span> Year     <span class="pp">|</span> Author     <span class="pp">|</span> GPU    <span class="pp">|</span> Batch Size   <span class="pp">|</span> <span class="sc">\#</span> GPU           <span class="pp">|</span> TIME (s)           <span class="pp">|</span> ACC            <span class="pp">|</span></span>
<span id="cb11-861"><a href="#cb11-861"></a><span class="pp">| :------:</span> <span class="pp">| :--------:</span> <span class="pp">| :----:</span> <span class="pp">| :----------:</span> <span class="pp">| :--------:</span>       <span class="pp">| :-------:</span>          <span class="pp">| :------------:</span> <span class="pp">|</span></span>
<span id="cb11-862"><a href="#cb11-862"></a><span class="pp">|</span> 2016     <span class="pp">|</span> He         <span class="pp">|</span> P100   <span class="pp">|</span> 256          <span class="pp">|</span> <span class="co">[</span><span class="ot">8</span><span class="co">]</span>{.red-bg}     <span class="pp">|</span> <span class="co">[</span><span class="ot">104,400</span><span class="co">]</span>{.red-bg} <span class="pp">|</span> 75.30%         <span class="pp">|</span></span>
<span id="cb11-863"><a href="#cb11-863"></a><span class="pp">|</span> 2019     <span class="pp">|</span> Yamazaki   <span class="pp">|</span> V100   <span class="pp">|</span> 81,920       <span class="pp">|</span> <span class="co">[</span><span class="ot">2048</span><span class="co">]</span>{.blue-bg} <span class="pp">|</span> <span class="co">[</span><span class="ot">72</span><span class="co">]</span>{.blue-bg}     <span class="pp">|</span> 75.08%         <span class="pp">|</span></span>
<span id="cb11-864"><a href="#cb11-864"></a></span>
<span id="cb11-865"><a href="#cb11-865"></a>Recent progress {.responsive .striped .hover}</span>
<span id="cb11-866"><a href="#cb11-866"></a></span>
<span id="cb11-867"><a href="#cb11-867"></a>:::</span>
<span id="cb11-868"><a href="#cb11-868"></a></span>
<span id="cb11-869"><a href="#cb11-869"></a><span class="fu">## Dealing with Data {background-color="white"}</span></span>
<span id="cb11-870"><a href="#cb11-870"></a></span>
<span id="cb11-871"><a href="#cb11-871"></a><span class="ss">- </span>At each training step, we want to ensure that **each worker receives unique</span>
<span id="cb11-872"><a href="#cb11-872"></a>data**</span>
<span id="cb11-873"><a href="#cb11-873"></a><span class="ss">- </span>This can be done in one of two ways:</span>
<span id="cb11-874"><a href="#cb11-874"></a><span class="ss">    1. </span>Manually partition data (ahead of time)</span>
<span id="cb11-875"><a href="#cb11-875"></a><span class="ss">        - </span>Assign **unique subsets** to each worker</span>
<span id="cb11-876"><a href="#cb11-876"></a><span class="ss">        - </span>Each worker can only see their local portion of the data</span>
<span id="cb11-877"><a href="#cb11-877"></a><span class="ss">        - </span>Most common approach</span>
<span id="cb11-878"><a href="#cb11-878"></a><span class="ss">    2. </span>From each worker, randomly select a mini-batch</span>
<span id="cb11-879"><a href="#cb11-879"></a><span class="ss">        - </span>Each worker can see the full dataset</span>
<span id="cb11-880"><a href="#cb11-880"></a><span class="ss">        - </span>âš ï¸ When randomly selecting, it is important that each worker uses</span>
<span id="cb11-881"><a href="#cb11-881"></a>        different seeds to ensure they receive unique data</span>
<span id="cb11-882"><a href="#cb11-882"></a></span>
<span id="cb11-883"><a href="#cb11-883"></a><span class="fu">## Broadcast Initial State {background-color="white"}</span></span>
<span id="cb11-884"><a href="#cb11-884"></a></span>
<span id="cb11-885"><a href="#cb11-885"></a><span class="ss">- </span>At the start of training (or when loading from a checkpoint), we want all of our workers to be</span>
<span id="cb11-886"><a href="#cb11-886"></a>  initialized consistently</span>
<span id="cb11-887"><a href="#cb11-887"></a><span class="ss">  - </span>**Broadcast** the model and optimizer states from <span class="in">`rank() == 0`</span> worker</span>
<span id="cb11-888"><a href="#cb11-888"></a></span>
<span id="cb11-889"><a href="#cb11-889"></a>::: {#fig-broadcast .r-stretch}</span>
<span id="cb11-890"><a href="#cb11-890"></a></span>
<span id="cb11-893"><a href="#cb11-893"></a><span class="in">```{mermaid}</span></span>
<span id="cb11-894"><a href="#cb11-894"></a>flowchart TD</span>
<span id="cb11-895"><a href="#cb11-895"></a>  <span class="dv">0</span>[<span class="ot">"</span><span class="st">GPU0</span><span class="ot">"</span>] --&gt; <span class="dv">1</span>[<span class="ot">"</span><span class="st">GPU 1</span><span class="ot">"</span>]</span>
<span id="cb11-896"><a href="#cb11-896"></a>  CKPT --&gt; <span class="dv">0</span></span>
<span id="cb11-897"><a href="#cb11-897"></a>  <span class="dv">0</span> --&gt; <span class="dv">2</span>[<span class="ot">"</span><span class="st">GPU 2</span><span class="ot">"</span>]</span>
<span id="cb11-898"><a href="#cb11-898"></a>  <span class="dv">0</span> --Model + Optim. State--&gt;<span class="dv">3</span>[<span class="ot">"</span><span class="st">GPU 3</span><span class="ot">"</span>]</span>
<span id="cb11-899"><a href="#cb11-899"></a>  <span class="dv">0</span> --&gt; X[<span class="ot">"</span><span class="st">`...`</span><span class="ot">"</span>]</span>
<span id="cb11-900"><a href="#cb11-900"></a>  <span class="dv">0</span> --&gt; N[<span class="ot">"</span><span class="st">GPU N</span><span class="ot">"</span>]</span>
<span id="cb11-901"><a href="#cb11-901"></a>classDef text fill:<span class="co">#CCCCCC02,stroke:#838383,stroke-width:0px,color:#838383,font-weight:500</span></span>
<span id="cb11-902"><a href="#cb11-902"></a>classDef block fill:<span class="co">#CCCCCC02,stroke:#838383,stroke-width:1px,font-weight:500,color:#838383</span></span>
<span id="cb11-903"><a href="#cb11-903"></a>class <span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,N,X,CKPT block</span>
<span id="cb11-904"><a href="#cb11-904"></a><span class="in">```</span></span>
<span id="cb11-905"><a href="#cb11-905"></a></span>
<span id="cb11-906"><a href="#cb11-906"></a>To ensure all workers have the same copies, we load on <span class="in">`RANK==0`</span> and</span>
<span id="cb11-907"><a href="#cb11-907"></a><span class="in">`broadcast`</span></span>
<span id="cb11-908"><a href="#cb11-908"></a>:::</span>
<span id="cb11-909"><a href="#cb11-909"></a></span>
<span id="cb11-910"><a href="#cb11-910"></a><span class="fu">## Best Practices {background-color="white"}</span></span>
<span id="cb11-911"><a href="#cb11-911"></a></span>
<span id="cb11-912"><a href="#cb11-912"></a>::: {.callout-important collapse=false icon=false title="â° Keeping things in Sync"}</span>
<span id="cb11-913"><a href="#cb11-913"></a>**Computation stalls during communication !!**</span>
<span id="cb11-914"><a href="#cb11-914"></a></span>
<span id="cb11-915"><a href="#cb11-915"></a>Keeping the communication to computation ratio small is important for effective scaling.</span>
<span id="cb11-916"><a href="#cb11-916"></a>:::</span>
<span id="cb11-917"><a href="#cb11-917"></a></span>
<span id="cb11-918"><a href="#cb11-918"></a>::: {.flex-container}</span>
<span id="cb11-919"><a href="#cb11-919"></a></span>
<span id="cb11-920"><a href="#cb11-920"></a>::: {.column style="width:50%;"}</span>
<span id="cb11-921"><a href="#cb11-921"></a><span class="ss">- </span>Use parallel IO whenever possible</span>
<span id="cb11-922"><a href="#cb11-922"></a><span class="ss">  - </span>Feed each rank from different files</span>
<span id="cb11-923"><a href="#cb11-923"></a><span class="ss">  - </span>Use MPI IO to have each rank read its own batch from a file</span>
<span id="cb11-924"><a href="#cb11-924"></a><span class="ss">  - </span>Use several ranks to read data, MPI to scatter to remaining ranks</span>
<span id="cb11-925"><a href="#cb11-925"></a><span class="ss">    - </span>Most practical in big _at-scale_ training</span>
<span id="cb11-926"><a href="#cb11-926"></a>:::</span>
<span id="cb11-927"><a href="#cb11-927"></a></span>
<span id="cb11-928"><a href="#cb11-928"></a>::: {.column style="width:50%;"}</span>
<span id="cb11-929"><a href="#cb11-929"></a><span class="ss">- </span>Take advantage of data storage</span>
<span id="cb11-930"><a href="#cb11-930"></a><span class="ss">  - </span>Use <span class="co">[</span><span class="ot">striping on lustre</span><span class="co">](https://wiki.lustre.org/Configuring_Lustre_File_Striping)</span></span>
<span id="cb11-931"><a href="#cb11-931"></a><span class="ss">- </span>Use the right optimizations for Aurora, Polaris, etc.</span>
<span id="cb11-932"><a href="#cb11-932"></a><span class="ss">- </span>Preload data when possible</span>
<span id="cb11-933"><a href="#cb11-933"></a><span class="ss">  - </span>Offloading to a GPU frees CPU cycles for loading the next batch of data</span>
<span id="cb11-934"><a href="#cb11-934"></a><span class="ss">    - </span>**minimize IO latency this way**</span>
<span id="cb11-935"><a href="#cb11-935"></a>:::</span>
<span id="cb11-936"><a href="#cb11-936"></a></span>
<span id="cb11-937"><a href="#cb11-937"></a>:::</span>
<span id="cb11-938"><a href="#cb11-938"></a></span>
<span id="cb11-939"><a href="#cb11-939"></a><span class="fu">## Going Beyond Data Parallelism {background-color="white"}</span></span>
<span id="cb11-940"><a href="#cb11-940"></a></span>
<span id="cb11-941"><a href="#cb11-941"></a><span class="ss">- </span>âœ… Useful when model fits on single GPU:</span>
<span id="cb11-942"><a href="#cb11-942"></a><span class="ss">  - </span>ultimately **limited by GPU memory**</span>
<span id="cb11-943"><a href="#cb11-943"></a><span class="ss">  - </span>model performance limited by size</span>
<span id="cb11-944"><a href="#cb11-944"></a></span>
<span id="cb11-945"><a href="#cb11-945"></a><span class="ss">- </span>âš ï¸ When model does not fit on a single GPU:</span>
<span id="cb11-946"><a href="#cb11-946"></a><span class="ss">  - </span>Offloading (can only get you so far...):</span>
<span id="cb11-947"><a href="#cb11-947"></a><span class="ss">    - </span>{{&lt; iconify logos microsoft-icon &gt;}} <span class="co">[</span><span class="ot">DeepSpeed + `ZeRO`</span><span class="co">](https://www.deepspeed.ai/tutorials/zero/)</span></span>
<span id="cb11-948"><a href="#cb11-948"></a><span class="ss">    - </span>ğŸ”¥ <span class="co">[</span><span class="ot">PyTorch + `FSDP`</span><span class="co">](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)</span></span>
<span id="cb11-949"><a href="#cb11-949"></a><span class="ss">  - </span>Otherwise, resort to <span class="co">[</span><span class="ot">model parallelism strategies</span><span class="co">](#additional-parallelism-strategies)</span></span>
<span id="cb11-950"><a href="#cb11-950"></a></span>
<span id="cb11-951"><a href="#cb11-951"></a><span class="fu">## Going beyond Data Parallelism: {{&lt; iconify logos microsoft-icon &gt;}} DeepSpeed + `ZeRO` {background-color="white"}</span></span>
<span id="cb11-952"><a href="#cb11-952"></a></span>
<span id="cb11-953"><a href="#cb11-953"></a><span class="ss">- </span>Depending on the <span class="in">`ZeRO`</span> stage (1, 2, 3), we can offload:</span>
<span id="cb11-954"><a href="#cb11-954"></a><span class="ss">  1. </span>**Stage 1**: optimizer states $\left(P_{\mathrm{os}}\right)$</span>
<span id="cb11-955"><a href="#cb11-955"></a><span class="ss">  2. </span>**Stage 2**: gradients + opt. states $\left(P_{\mathrm{os}+\mathrm{g}}\right)$</span>
<span id="cb11-956"><a href="#cb11-956"></a><span class="ss">  3. </span>**Stage 3**: model params + grads + opt. states $\left(P_{\mathrm{os}+\mathrm{g}+\mathrm{p}}\right)$</span>
<span id="cb11-957"><a href="#cb11-957"></a></span>
<span id="cb11-958"><a href="#cb11-958"></a>!<span class="co">[</span><span class="ot">{{&lt; iconify logos microsoft-icon &gt;}} [DeepSpeed](deepspeed.ai) + [`ZeRO`](https://www.deepspeed.ai/tutorials/zero-offload/)</span><span class="co">](./assets/zero.png)</span>{#fig-zero .r-stretch}</span>
<span id="cb11-959"><a href="#cb11-959"></a></span>
<span id="cb11-960"><a href="#cb11-960"></a><span class="fu">## Fully Sharded Data Parallel: ğŸ”¥ PyTorch + `FSDP` {.smaller background-color="white"}</span></span>
<span id="cb11-961"><a href="#cb11-961"></a></span>
<span id="cb11-962"><a href="#cb11-962"></a><span class="ss">- </span>Instead of maintaining per-GPU copy of <span class="in">`{params, grads, opt_states}`</span>, FSDP</span>
<span id="cb11-963"><a href="#cb11-963"></a>  shards (distributes) these across data-parallel workers</span>
<span id="cb11-964"><a href="#cb11-964"></a><span class="ss">  - </span>can optionally offload the sharded model params to CPU</span>
<span id="cb11-965"><a href="#cb11-965"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Introducing PyTorch Fully Sharded Data Parallel (FSDP) API | PyTorch</span><span class="co">](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)</span></span>
<span id="cb11-966"><a href="#cb11-966"></a></span>
<span id="cb11-967"><a href="#cb11-967"></a>::: {#fig-fsdp}</span>
<span id="cb11-968"><a href="#cb11-968"></a><span class="al">![](assets/fsdp.png)</span></span>
<span id="cb11-969"><a href="#cb11-969"></a></span>
<span id="cb11-970"><a href="#cb11-970"></a>FSDP Workflow. <span class="co">[</span><span class="ot">Source</span><span class="co">](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)</span></span>
<span id="cb11-971"><a href="#cb11-971"></a>:::</span>
<span id="cb11-972"><a href="#cb11-972"></a></span>
<span id="cb11-973"><a href="#cb11-973"></a><span class="fu"># ğŸ•¸ï¸ Additional Parallelism Strategies {background-color="white"}</span></span>
<span id="cb11-974"><a href="#cb11-974"></a></span>
<span id="cb11-975"><a href="#cb11-975"></a><span class="ss">- </span>**Tensor (/ Model) Parallelism** (<span class="in">`TP`</span>):</span>
<span id="cb11-976"><a href="#cb11-976"></a><span class="ss">    - </span>ğŸ¤— <span class="co">[</span><span class="ot">Tensor Parallelism</span><span class="co">](https://huggingface.co/docs/text-generation-inference/en/conceptual/tensor_parallelism)</span></span>
<span id="cb11-977"><a href="#cb11-977"></a><span class="ss">    - </span>ğŸ”¥ <span class="co">[</span><span class="ot">Large Scale Transformer model training with Tensor Parallel (TP)</span><span class="co">](https://pytorch.org/tutorials/intermediate/TP_tutorial.html)</span></span>
<span id="cb11-978"><a href="#cb11-978"></a><span class="ss">- </span>**Pipeline Parallelism** (<span class="in">`PP`</span>):</span>
<span id="cb11-979"><a href="#cb11-979"></a><span class="ss">    - </span>ğŸ”¥ <span class="co">[</span><span class="ot">PyTorch</span><span class="co">](https://pytorch.org/docs/main/distributed.pipelining.html)</span>, {{<span class="dt">&lt;</span><span class="kw">iconify</span><span class="ot"> logos microsoft-icon </span><span class="dt">&gt;</span>}} <span class="co">[</span><span class="ot">DeepSpeed</span><span class="co">](https://deepspeed.readthedocs.io/en/latest/pipeline.html)</span></span>
<span id="cb11-980"><a href="#cb11-980"></a><span class="ss">- </span>**Sequence Parallelism** (<span class="in">`SP`</span>):</span>
<span id="cb11-981"><a href="#cb11-981"></a><span class="ss">    - </span>{{&lt; iconify logos microsoft-icon &gt;}} <span class="co">[</span><span class="ot">DeepSpeed Ulysses</span><span class="co">](https://github.com/microsoft/DeepSpeed/blob/master/blogs/deepspeed-ulysses/README.md)</span></span>
<span id="cb11-982"><a href="#cb11-982"></a><span class="ss">    - </span><span class="co">[</span><span class="ot">Megatron / Context Parallelism</span><span class="co">](https://docs.nvidia.com/megatron-core/developer-guide/latest/api-guide/context_parallel.html)</span></span>
<span id="cb11-983"><a href="#cb11-983"></a><span class="ss">    - </span><span class="co">[</span><span class="ot">Unified Sequence Parallel (USP)</span><span class="co">](https://arxiv.org/abs/2405.07719v3)</span></span>
<span id="cb11-984"><a href="#cb11-984"></a><span class="ss">        - </span>{{&lt; fa brands github &gt;}} <span class="co">[</span><span class="ot">feifeibear/`long-context-attention`</span><span class="co">](https://github.com/feifeibear/long-context-attention)</span></span>
<span id="cb11-985"><a href="#cb11-985"></a></span>
<span id="cb11-986"><a href="#cb11-986"></a><span class="ss">- </span><span class="va">[x]</span> {{&lt; fa brands github &gt;}} <span class="co">[</span><span class="ot">argonne-lcf/`Megatron-DeepSpeed`</span><span class="co">](https://github.com/argonne-lcf/Megatron-DeepSpeed)</span></span>
<span id="cb11-987"><a href="#cb11-987"></a><span class="ss">  - </span>Supports 4D Parallelism (<span class="in">`DP`</span> + <span class="in">`TP`</span> + <span class="in">`PP`</span> + <span class="in">`SP`</span>)</span>
<span id="cb11-988"><a href="#cb11-988"></a></span>
<span id="cb11-989"><a href="#cb11-989"></a></span>
<span id="cb11-990"><a href="#cb11-990"></a><span class="fu">## Pipeline Parallelism (PP) {background-color="white"}</span></span>
<span id="cb11-991"><a href="#cb11-991"></a></span>
<span id="cb11-992"><a href="#cb11-992"></a>::: {.flex-container style="place-content: end space-evenly;"}</span>
<span id="cb11-993"><a href="#cb11-993"></a></span>
<span id="cb11-994"><a href="#cb11-994"></a>::: {.column style="width:60%;"}</span>
<span id="cb11-995"><a href="#cb11-995"></a><span class="ss">- </span>Model is split up **vertically** (layer-level) across multiple GPUs</span>
<span id="cb11-996"><a href="#cb11-996"></a><span class="ss">- </span>Each GPU:</span>
<span id="cb11-997"><a href="#cb11-997"></a><span class="ss">  - </span>has a portion of the full model</span>
<span id="cb11-998"><a href="#cb11-998"></a><span class="ss">  - </span>processes _in parallel_ different stages of the pipeline</span>
<span id="cb11-999"><a href="#cb11-999"></a>    (on a small chunk of the batch)</span>
<span id="cb11-1000"><a href="#cb11-1000"></a><span class="ss">- </span>See:</span>
<span id="cb11-1001"><a href="#cb11-1001"></a><span class="ss">  - </span>ğŸ”¥ <span class="co">[</span><span class="ot">PyTorch / Pipeline Parallelism</span><span class="co">](https://pytorch.org/docs/main/distributed.pipelining.html)</span></span>
<span id="cb11-1002"><a href="#cb11-1002"></a><span class="ss">  - </span>{{<span class="dt">&lt;</span><span class="kw">iconify</span><span class="ot"> logos microsoft-icon </span><span class="dt">&gt;</span>}} <span class="co">[</span><span class="ot">DeepSpeed / Pipeline Parallelism</span><span class="co">](https://deepspeed.readthedocs.io/en/latest/pipeline.html)</span></span>
<span id="cb11-1003"><a href="#cb11-1003"></a>:::</span>
<span id="cb11-1004"><a href="#cb11-1004"></a></span>
<span id="cb11-1005"><a href="#cb11-1005"></a>::: {.column style="width:40%;"}</span>
<span id="cb11-1006"><a href="#cb11-1006"></a></span>
<span id="cb11-1007"><a href="#cb11-1007"></a>::: {#fig-pipeline-parallelism}</span>
<span id="cb11-1008"><a href="#cb11-1008"></a></span>
<span id="cb11-1011"><a href="#cb11-1011"></a><span class="in">```{mermaid}</span></span>
<span id="cb11-1012"><a href="#cb11-1012"></a>flowchart TB</span>
<span id="cb11-1013"><a href="#cb11-1013"></a>    subgraph G0[<span class="ot">"</span><span class="st">`GPU 0`</span><span class="ot">"</span>]</span>
<span id="cb11-1014"><a href="#cb11-1014"></a>        direction LR</span>
<span id="cb11-1015"><a href="#cb11-1015"></a>        a0(<span class="ot">"</span><span class="st">`Layer 0`</span><span class="ot">"</span>)</span>
<span id="cb11-1016"><a href="#cb11-1016"></a>        b0(<span class="ot">"</span><span class="st">`Layer 1`</span><span class="ot">"</span>)</span>
<span id="cb11-1017"><a href="#cb11-1017"></a>    end</span>
<span id="cb11-1018"><a href="#cb11-1018"></a>    subgraph G1[<span class="ot">"</span><span class="st">`GPU 1`</span><span class="ot">"</span>]</span>
<span id="cb11-1019"><a href="#cb11-1019"></a>        direction LR</span>
<span id="cb11-1020"><a href="#cb11-1020"></a>        a1(<span class="ot">"</span><span class="st">`Layer 2`</span><span class="ot">"</span>)</span>
<span id="cb11-1021"><a href="#cb11-1021"></a>        b1(<span class="ot">"</span><span class="st">`Layer 3`</span><span class="ot">"</span>)</span>
<span id="cb11-1022"><a href="#cb11-1022"></a>    end</span>
<span id="cb11-1023"><a href="#cb11-1023"></a>    a0 -.-&gt; b0</span>
<span id="cb11-1024"><a href="#cb11-1024"></a>    b0 --&gt; a1</span>
<span id="cb11-1025"><a href="#cb11-1025"></a>    a1 -.-&gt; b1</span>
<span id="cb11-1026"><a href="#cb11-1026"></a>classDef block fill:<span class="co">#CCCCCC02,stroke:#838383,stroke-width:1px,color:#838383</span></span>
<span id="cb11-1027"><a href="#cb11-1027"></a>classDef red fill:<span class="co">#ff8181,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-1028"><a href="#cb11-1028"></a>classDef orange fill:<span class="co">#FFC47F,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-1029"><a href="#cb11-1029"></a>classDef yellow fill:<span class="co">#FFFF7F,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-1030"><a href="#cb11-1030"></a>classDef green fill:<span class="co">#98E6A5,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-1031"><a href="#cb11-1031"></a>classDef blue fill:<span class="co">#7DCAFF,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-1032"><a href="#cb11-1032"></a>classDef purple fill:<span class="co">#FFCBE6,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-1033"><a href="#cb11-1033"></a>class G0,G1, block</span>
<span id="cb11-1034"><a href="#cb11-1034"></a>class a0, red</span>
<span id="cb11-1035"><a href="#cb11-1035"></a>class b0, green</span>
<span id="cb11-1036"><a href="#cb11-1036"></a>class a1, blue</span>
<span id="cb11-1037"><a href="#cb11-1037"></a>class b1, yellow</span>
<span id="cb11-1038"><a href="#cb11-1038"></a><span class="in">```</span></span>
<span id="cb11-1039"><a href="#cb11-1039"></a></span>
<span id="cb11-1040"><a href="#cb11-1040"></a>Pipeline Parallelism</span>
<span id="cb11-1041"><a href="#cb11-1041"></a>:::</span>
<span id="cb11-1042"><a href="#cb11-1042"></a></span>
<span id="cb11-1043"><a href="#cb11-1043"></a></span>
<span id="cb11-1044"><a href="#cb11-1044"></a>:::</span>
<span id="cb11-1045"><a href="#cb11-1045"></a></span>
<span id="cb11-1046"><a href="#cb11-1046"></a>:::</span>
<span id="cb11-1047"><a href="#cb11-1047"></a></span>
<span id="cb11-1048"><a href="#cb11-1048"></a><span class="fu">## Tensor Parallel (TP) {background-color="white"}</span></span>
<span id="cb11-1049"><a href="#cb11-1049"></a></span>
<span id="cb11-1050"><a href="#cb11-1050"></a>::: {layout="<span class="co">[</span><span class="ot">50,50</span><span class="co">]</span>"}</span>
<span id="cb11-1051"><a href="#cb11-1051"></a></span>
<span id="cb11-1052"><a href="#cb11-1052"></a>::: {.column}</span>
<span id="cb11-1053"><a href="#cb11-1053"></a><span class="ss">- </span>Each tensor is split up into multiple chunks</span>
<span id="cb11-1054"><a href="#cb11-1054"></a><span class="ss">- </span>Each shard of the tensor resides on its designated GPU</span>
<span id="cb11-1055"><a href="#cb11-1055"></a><span class="ss">- </span>During processing each shard gets processed separately (and in parallel) on</span>
<span id="cb11-1056"><a href="#cb11-1056"></a>  different GPUs</span>
<span id="cb11-1057"><a href="#cb11-1057"></a><span class="ss">    - </span>synced at the end of the step</span>
<span id="cb11-1058"><a href="#cb11-1058"></a></span>
<span id="cb11-1059"><a href="#cb11-1059"></a><span class="ss">- </span>See: <span class="co">[</span><span class="ot">ğŸ¤— Model Parallelism</span><span class="co">](https://huggingface.co/docs/transformers/v4.15.0/parallelism)</span> for additional details</span>
<span id="cb11-1060"><a href="#cb11-1060"></a>:::</span>
<span id="cb11-1061"><a href="#cb11-1061"></a></span>
<span id="cb11-1062"><a href="#cb11-1062"></a>::: {.column}</span>
<span id="cb11-1063"><a href="#cb11-1063"></a></span>
<span id="cb11-1064"><a href="#cb11-1064"></a>::: {#fig-model-parallel-1}</span>
<span id="cb11-1065"><a href="#cb11-1065"></a></span>
<span id="cb11-1068"><a href="#cb11-1068"></a><span class="in">```{mermaid}</span></span>
<span id="cb11-1069"><a href="#cb11-1069"></a>flowchart LR</span>
<span id="cb11-1070"><a href="#cb11-1070"></a>   subgraph G0[<span class="ot">"</span><span class="st">`GPU0`</span><span class="ot">"</span>]</span>
<span id="cb11-1071"><a href="#cb11-1071"></a>    direction TB</span>
<span id="cb11-1072"><a href="#cb11-1072"></a>    a0(<span class="ot">"</span><span class="st">`Layer 0`</span><span class="ot">"</span>)</span>
<span id="cb11-1073"><a href="#cb11-1073"></a>    b0(<span class="ot">"</span><span class="st">`Layer 1`</span><span class="ot">"</span>)</span>
<span id="cb11-1074"><a href="#cb11-1074"></a>    c0(<span class="ot">"</span><span class="st">`Layer 2`</span><span class="ot">"</span>)</span>
<span id="cb11-1075"><a href="#cb11-1075"></a>    d0(<span class="ot">"</span><span class="st">`Layer 3`</span><span class="ot">"</span>)</span>
<span id="cb11-1076"><a href="#cb11-1076"></a>   end</span>
<span id="cb11-1077"><a href="#cb11-1077"></a>   subgraph G1[<span class="ot">"</span><span class="st">`GPU1`</span><span class="ot">"</span>]</span>
<span id="cb11-1078"><a href="#cb11-1078"></a>    direction TB</span>
<span id="cb11-1079"><a href="#cb11-1079"></a>    a1(<span class="ot">"</span><span class="st">`Layer 0`</span><span class="ot">"</span>)</span>
<span id="cb11-1080"><a href="#cb11-1080"></a>    b1(<span class="ot">"</span><span class="st">`Layer 1`</span><span class="ot">"</span>)</span>
<span id="cb11-1081"><a href="#cb11-1081"></a>    c1(<span class="ot">"</span><span class="st">`Layer 2`</span><span class="ot">"</span>)</span>
<span id="cb11-1082"><a href="#cb11-1082"></a>    d1(<span class="ot">"</span><span class="st">`Layer 3`</span><span class="ot">"</span>)</span>
<span id="cb11-1083"><a href="#cb11-1083"></a>   end</span>
<span id="cb11-1084"><a href="#cb11-1084"></a>   a0 &lt;-.-&gt; a1</span>
<span id="cb11-1085"><a href="#cb11-1085"></a>   b0 &lt;-.-&gt; b1</span>
<span id="cb11-1086"><a href="#cb11-1086"></a>   c0 &lt;-.-&gt; c1</span>
<span id="cb11-1087"><a href="#cb11-1087"></a>   d0 &lt;-.-&gt; d1</span>
<span id="cb11-1088"><a href="#cb11-1088"></a>classDef red fill:<span class="co">#ff8181,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-1089"><a href="#cb11-1089"></a>classDef orange fill:<span class="co">#FFC47F,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-1090"><a href="#cb11-1090"></a>classDef yellow fill:<span class="co">#FFFF7F,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-1091"><a href="#cb11-1091"></a>classDef green fill:<span class="co">#98E6A5,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-1092"><a href="#cb11-1092"></a>classDef blue fill:<span class="co">#7DCAFF,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-1093"><a href="#cb11-1093"></a>classDef purple fill:<span class="co">#FFCBE6,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-1094"><a href="#cb11-1094"></a>classDef block fill:<span class="co">#CCCCCC02,stroke:#838383,stroke-width:1px,color:#838383</span></span>
<span id="cb11-1095"><a href="#cb11-1095"></a>class G0,G1, block</span>
<span id="cb11-1096"><a href="#cb11-1096"></a>class a0,a1 red</span>
<span id="cb11-1097"><a href="#cb11-1097"></a>class b0,b1 green</span>
<span id="cb11-1098"><a href="#cb11-1098"></a>class c0,c1 blue</span>
<span id="cb11-1099"><a href="#cb11-1099"></a>class d0,d1 yellow</span>
<span id="cb11-1100"><a href="#cb11-1100"></a><span class="in">```</span></span>
<span id="cb11-1101"><a href="#cb11-1101"></a></span>
<span id="cb11-1102"><a href="#cb11-1102"></a>Tensor Parallel Training</span>
<span id="cb11-1103"><a href="#cb11-1103"></a>:::</span>
<span id="cb11-1104"><a href="#cb11-1104"></a></span>
<span id="cb11-1105"><a href="#cb11-1105"></a>:::</span>
<span id="cb11-1106"><a href="#cb11-1106"></a></span>
<span id="cb11-1107"><a href="#cb11-1107"></a>:::</span>
<span id="cb11-1108"><a href="#cb11-1108"></a></span>
<span id="cb11-1109"><a href="#cb11-1109"></a><span class="fu">## Tensor Parallel (TP) {background-color="white"}</span></span>
<span id="cb11-1110"><a href="#cb11-1110"></a></span>
<span id="cb11-1111"><a href="#cb11-1111"></a>::: {layout="<span class="co">[</span><span class="ot">50,50</span><span class="co">]</span>"}</span>
<span id="cb11-1112"><a href="#cb11-1112"></a></span>
<span id="cb11-1113"><a href="#cb11-1113"></a>::: {.column}</span>
<span id="cb11-1114"><a href="#cb11-1114"></a><span class="ss">- </span>Suitable when the model is too large to fit onto a single device (CPU / GPU)</span>
<span id="cb11-1115"><a href="#cb11-1115"></a><span class="ss">- </span>Typically **more complicated** to implement than data parallel training</span>
<span id="cb11-1116"><a href="#cb11-1116"></a><span class="ss">    - </span>This is what one may call _horizontal parallelism_</span>
<span id="cb11-1117"><a href="#cb11-1117"></a><span class="ss">    - </span>Communication whenever dataflow between two subsets</span>
<span id="cb11-1118"><a href="#cb11-1118"></a><span class="ss">- </span>{{&lt; fa brands github &gt;}} <span class="co">[</span><span class="ot">`argonne-lcf/Megatron-DeepSpeed`</span><span class="co">](https://github.com/argonne-lcf/Megatron-DeepSpeed)</span></span>
<span id="cb11-1119"><a href="#cb11-1119"></a><span class="ss">- </span>ğŸ¤— <span class="co">[</span><span class="ot">`huggingface/nanotron`</span><span class="co">](https://github.com/huggingface/nanotron)</span></span>
<span id="cb11-1120"><a href="#cb11-1120"></a>:::</span>
<span id="cb11-1121"><a href="#cb11-1121"></a></span>
<span id="cb11-1122"><a href="#cb11-1122"></a>::: {.column}</span>
<span id="cb11-1123"><a href="#cb11-1123"></a></span>
<span id="cb11-1124"><a href="#cb11-1124"></a>::: {#fig-model-parallel-1}</span>
<span id="cb11-1125"><a href="#cb11-1125"></a></span>
<span id="cb11-1128"><a href="#cb11-1128"></a><span class="in">```{mermaid}</span></span>
<span id="cb11-1129"><a href="#cb11-1129"></a>flowchart LR</span>
<span id="cb11-1130"><a href="#cb11-1130"></a>   subgraph G0[<span class="ot">"</span><span class="st">`GPU0`</span><span class="ot">"</span>]</span>
<span id="cb11-1131"><a href="#cb11-1131"></a>    direction TB</span>
<span id="cb11-1132"><a href="#cb11-1132"></a>    a0(<span class="ot">"</span><span class="st">`Layer 0`</span><span class="ot">"</span>)</span>
<span id="cb11-1133"><a href="#cb11-1133"></a>    b0(<span class="ot">"</span><span class="st">`Layer 1`</span><span class="ot">"</span>)</span>
<span id="cb11-1134"><a href="#cb11-1134"></a>    c0(<span class="ot">"</span><span class="st">`Layer 2`</span><span class="ot">"</span>)</span>
<span id="cb11-1135"><a href="#cb11-1135"></a>    d0(<span class="ot">"</span><span class="st">`Layer 3`</span><span class="ot">"</span>)</span>
<span id="cb11-1136"><a href="#cb11-1136"></a>   end</span>
<span id="cb11-1137"><a href="#cb11-1137"></a>   subgraph G1[<span class="ot">"</span><span class="st">`GPU1`</span><span class="ot">"</span>]</span>
<span id="cb11-1138"><a href="#cb11-1138"></a>    direction TB</span>
<span id="cb11-1139"><a href="#cb11-1139"></a>    a1(<span class="ot">"</span><span class="st">`Layer 0`</span><span class="ot">"</span>)</span>
<span id="cb11-1140"><a href="#cb11-1140"></a>    b1(<span class="ot">"</span><span class="st">`Layer 1`</span><span class="ot">"</span>)</span>
<span id="cb11-1141"><a href="#cb11-1141"></a>    c1(<span class="ot">"</span><span class="st">`Layer 2`</span><span class="ot">"</span>)</span>
<span id="cb11-1142"><a href="#cb11-1142"></a>    d1(<span class="ot">"</span><span class="st">`Layer 3`</span><span class="ot">"</span>)</span>
<span id="cb11-1143"><a href="#cb11-1143"></a>   end</span>
<span id="cb11-1144"><a href="#cb11-1144"></a>   a0 &lt;-.-&gt; a1</span>
<span id="cb11-1145"><a href="#cb11-1145"></a>   b0 &lt;-.-&gt; b1</span>
<span id="cb11-1146"><a href="#cb11-1146"></a>   c0 &lt;-.-&gt; c1</span>
<span id="cb11-1147"><a href="#cb11-1147"></a>   d0 &lt;-.-&gt; d1</span>
<span id="cb11-1148"><a href="#cb11-1148"></a>classDef red fill:<span class="co">#ff8181,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-1149"><a href="#cb11-1149"></a>classDef orange fill:<span class="co">#FFC47F,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-1150"><a href="#cb11-1150"></a>classDef yellow fill:<span class="co">#FFFF7F,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-1151"><a href="#cb11-1151"></a>classDef green fill:<span class="co">#98E6A5,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-1152"><a href="#cb11-1152"></a>classDef blue fill:<span class="co">#7DCAFF,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-1153"><a href="#cb11-1153"></a>classDef purple fill:<span class="co">#FFCBE6,stroke:#333,stroke-width:1px,color:#000</span></span>
<span id="cb11-1154"><a href="#cb11-1154"></a>classDef block fill:<span class="co">#CCCCCC02,stroke:#838383,stroke-width:1px,color:#838383</span></span>
<span id="cb11-1155"><a href="#cb11-1155"></a>class G0,G1, block</span>
<span id="cb11-1156"><a href="#cb11-1156"></a>class a0,a1 red</span>
<span id="cb11-1157"><a href="#cb11-1157"></a>class b0,b1 green</span>
<span id="cb11-1158"><a href="#cb11-1158"></a>class c0,c1 blue</span>
<span id="cb11-1159"><a href="#cb11-1159"></a>class d0,d1 yellow</span>
<span id="cb11-1160"><a href="#cb11-1160"></a><span class="in">```</span></span>
<span id="cb11-1161"><a href="#cb11-1161"></a></span>
<span id="cb11-1162"><a href="#cb11-1162"></a>Tensor Parallel Training</span>
<span id="cb11-1163"><a href="#cb11-1163"></a>:::</span>
<span id="cb11-1164"><a href="#cb11-1164"></a></span>
<span id="cb11-1165"><a href="#cb11-1165"></a>:::</span>
<span id="cb11-1166"><a href="#cb11-1166"></a></span>
<span id="cb11-1167"><a href="#cb11-1167"></a>:::</span>
<span id="cb11-1168"><a href="#cb11-1168"></a></span>
<span id="cb11-1169"><a href="#cb11-1169"></a><span class="co">&lt;!-- ::: footnote --&gt;</span></span>
<span id="cb11-1170"><a href="#cb11-1170"></a><span class="co">&lt;!----&gt;</span></span>
<span id="cb11-1171"><a href="#cb11-1171"></a><span class="co">&lt;!-- - {{&lt; fa brands github &gt;}} [`argonne-lcf/Megatron-DeepSpeed`](https://github.com/argonne-lcf/Megatron-DeepSpeed) --&gt;</span></span>
<span id="cb11-1172"><a href="#cb11-1172"></a><span class="co">&lt;!-- - ğŸ¤— [`huggingface/nanotron`](https://github.com/huggingface/nanotron) --&gt;</span></span>
<span id="cb11-1173"><a href="#cb11-1173"></a><span class="co">&lt;!----&gt;</span></span>
<span id="cb11-1174"><a href="#cb11-1174"></a><span class="co">&lt;!-- See: [ğŸ¤— Model Parallelism](https://huggingface.co/docs/transformers/v4.15.0/parallelism) for additional details --&gt;</span></span>
<span id="cb11-1175"><a href="#cb11-1175"></a><span class="co">&lt;!-- ::: --&gt;</span></span>
<span id="cb11-1176"><a href="#cb11-1176"></a><span class="co">&lt;!----&gt;</span></span>
<span id="cb11-1177"><a href="#cb11-1177"></a><span class="co">&lt;!-- ::: {.notes} --&gt;</span></span>
<span id="cb11-1178"><a href="#cb11-1178"></a><span class="co">&lt;!----&gt;</span></span>
<span id="cb11-1179"><a href="#cb11-1179"></a><span class="co">&lt;!-- - Split up network over multiple workers --&gt;</span></span>
<span id="cb11-1180"><a href="#cb11-1180"></a><span class="co">&lt;!--   - Each receives disjoint subset --&gt;</span></span>
<span id="cb11-1181"><a href="#cb11-1181"></a><span class="co">&lt;!--   - All communication associated with subsets are distributed --&gt;</span></span>
<span id="cb11-1182"><a href="#cb11-1182"></a><span class="co">&lt;!-- - Communication whenever dataflow between two subsets --&gt;</span></span>
<span id="cb11-1183"><a href="#cb11-1183"></a><span class="co">&lt;!-- - Typically **more complicated** to implement than data parallel training --&gt;</span></span>
<span id="cb11-1184"><a href="#cb11-1184"></a><span class="co">&lt;!-- - Suitable when the model is too large to fit onto a single device (CPU / GPU) --&gt;</span></span>
<span id="cb11-1185"><a href="#cb11-1185"></a><span class="co">&lt;!----&gt;</span></span>
<span id="cb11-1186"><a href="#cb11-1186"></a><span class="co">&lt;!-- ::: --&gt;</span></span>
<span id="cb11-1187"><a href="#cb11-1187"></a></span>
<span id="cb11-1188"><a href="#cb11-1188"></a><span class="fu"># Tensor (/ Model) Parallel Training: Example {background-color="white"}</span></span>
<span id="cb11-1189"><a href="#cb11-1189"></a></span>
<span id="cb11-1190"><a href="#cb11-1190"></a>Want to compute: $y = \sum_{i} x_{i} W_{i} = x_0 * W_0 + x_1 * W_1 + x_2 * W_2$  </span>
<span id="cb11-1191"><a href="#cb11-1191"></a>where each GPU only has only its portion of the full weights as shown below</span>
<span id="cb11-1192"><a href="#cb11-1192"></a></span>
<span id="cb11-1193"><a href="#cb11-1193"></a><span class="ss">1. </span>Compute: $y_{0} = x_{0} * W_{0}\rightarrow$ <span class="in">`GPU1`</span></span>
<span id="cb11-1194"><a href="#cb11-1194"></a><span class="ss">2. </span>Compute: $y_{1} = y_{0} + x_{1} * W_{1}\rightarrow$ <span class="in">`GPU2`</span></span>
<span id="cb11-1195"><a href="#cb11-1195"></a><span class="ss">3. </span>Compute: $y = y_{1} + x_{2} * W_{2} = \sum_{i} x_{i} W_{i}$ âœ…</span>
<span id="cb11-1196"><a href="#cb11-1196"></a></span>
<span id="cb11-1197"><a href="#cb11-1197"></a>::: {#fig-tensor-parallel-example style="width:75%; margin-left: auto; margin-right: auto; text-align:center;"}</span>
<span id="cb11-1198"><a href="#cb11-1198"></a></span>
<span id="cb11-1201"><a href="#cb11-1201"></a><span class="in">```{mermaid}</span></span>
<span id="cb11-1202"><a href="#cb11-1202"></a>flowchart LR</span>
<span id="cb11-1203"><a href="#cb11-1203"></a>    subgraph X0[<span class="ot">"</span><span class="st">`GPU0`</span><span class="ot">"</span>]</span>
<span id="cb11-1204"><a href="#cb11-1204"></a>        direction LR</span>
<span id="cb11-1205"><a href="#cb11-1205"></a>        a(<span class="ot">"</span><span class="st">`W0`</span><span class="ot">"</span>)</span>
<span id="cb11-1206"><a href="#cb11-1206"></a>    end</span>
<span id="cb11-1207"><a href="#cb11-1207"></a>    subgraph X1[<span class="ot">"</span><span class="st">`GPU1`</span><span class="ot">"</span>]</span>
<span id="cb11-1208"><a href="#cb11-1208"></a>        direction LR</span>
<span id="cb11-1209"><a href="#cb11-1209"></a>        b(<span class="ot">"</span><span class="st">`W1`</span><span class="ot">"</span>)</span>
<span id="cb11-1210"><a href="#cb11-1210"></a>    end</span>
<span id="cb11-1211"><a href="#cb11-1211"></a>    subgraph X2[<span class="ot">"</span><span class="st">`GPU2`</span><span class="ot">"</span>]</span>
<span id="cb11-1212"><a href="#cb11-1212"></a>        direction LR</span>
<span id="cb11-1213"><a href="#cb11-1213"></a>        c(<span class="ot">"</span><span class="st">`W2`</span><span class="ot">"</span>)</span>
<span id="cb11-1214"><a href="#cb11-1214"></a>    end</span>
<span id="cb11-1215"><a href="#cb11-1215"></a>  t0(<span class="ot">"</span><span class="st">`xâ‚€`</span><span class="ot">"</span>)--&gt;X0</span>
<span id="cb11-1216"><a href="#cb11-1216"></a>  X0 --&gt;|<span class="ot">"</span><span class="st">`xâ‚€ Wâ‚€`</span><span class="ot">"</span>|X1</span>
<span id="cb11-1217"><a href="#cb11-1217"></a>  X1 --&gt;|<span class="ot">"</span><span class="st">`xâ‚€ Wâ‚€ &lt;br&gt;+ xâ‚ Wâ‚`</span><span class="ot">"</span>|X2</span>
<span id="cb11-1218"><a href="#cb11-1218"></a>  t1(<span class="ot">"</span><span class="st">`xâ‚`</span><span class="ot">"</span>) --&gt; X1</span>
<span id="cb11-1219"><a href="#cb11-1219"></a>  t2(<span class="ot">"</span><span class="st">`xâ‚‚`</span><span class="ot">"</span>) --&gt; X2</span>
<span id="cb11-1220"><a href="#cb11-1220"></a><span class="in">```</span></span>
<span id="cb11-1221"><a href="#cb11-1221"></a></span>
<span id="cb11-1222"><a href="#cb11-1222"></a>:::</span>
<span id="cb11-1223"><a href="#cb11-1223"></a></span>
<span id="cb11-1224"><a href="#cb11-1224"></a><span class="fu">## Tensor (Model) Parallelism[^efficient-large-scale] {background-color="white"}</span></span>
<span id="cb11-1225"><a href="#cb11-1225"></a></span>
<span id="cb11-1226"><a href="#cb11-1226"></a><span class="ss">- </span>In **Tensor Paralleism** each GPU processes only a slice of a tensor and only</span>
<span id="cb11-1227"><a href="#cb11-1227"></a>aggregates the full tensor for operations that require the whole thing.</span>
<span id="cb11-1228"><a href="#cb11-1228"></a><span class="ss">  - </span>The main building block of any transformer is a fully connected</span>
<span id="cb11-1229"><a href="#cb11-1229"></a>  <span class="in">`nn.Linear`</span> followed by a nonlinear activation GeLU.</span>
<span id="cb11-1230"><a href="#cb11-1230"></a><span class="ss">    - </span><span class="in">`Y = GeLU(XA)`</span>, where X and Y are the input and output vectors, and A is the weight matrix.</span>
<span id="cb11-1231"><a href="#cb11-1231"></a><span class="ss">  - </span>If we look at the computation in matrix form, itâ€™s easy to see how the matrix multiplication can be split between multiple GPUs:</span>
<span id="cb11-1232"><a href="#cb11-1232"></a></span>
<span id="cb11-1233"><a href="#cb11-1233"></a><span class="ot">[^efficient-large-scale]: </span><span class="co">[</span><span class="ot">Efficient Large-Scale Language Model Training on GPU Clusters</span><span class="co">](https://arxiv.org/abs/2104.04473)</span></span>
<span id="cb11-1234"><a href="#cb11-1234"></a></span>
<span id="cb11-1235"><a href="#cb11-1235"></a><span class="fu">## Tensor Parallelism {background-color="white"}</span></span>
<span id="cb11-1236"><a href="#cb11-1236"></a></span>
<span id="cb11-1237"><a href="#cb11-1237"></a>::: {#fig-parallel-gemm style="max-width: 80%; margin-left: auto; margin-right: auto;"}</span>
<span id="cb11-1238"><a href="#cb11-1238"></a></span>
<span id="cb11-1239"><a href="#cb11-1239"></a><span class="al">![](assets/parallelism-tp-parallel_gemm.png)</span></span>
<span id="cb11-1240"><a href="#cb11-1240"></a></span>
<span id="cb11-1241"><a href="#cb11-1241"></a>Tensor Parallel GEMM. This information is based on (the much more in-depth)</span>
<span id="cb11-1242"><a href="#cb11-1242"></a><span class="co">[</span><span class="ot">TP Overview</span><span class="co">](https://github.com/huggingface/transformers/issues/10321#issuecomment-783543530)</span> by <span class="co">[</span><span class="ot">\@anton-l</span><span class="co">](https://github.com/anton-l)</span></span>
<span id="cb11-1243"><a href="#cb11-1243"></a>:::</span>
<span id="cb11-1244"><a href="#cb11-1244"></a></span>
<span id="cb11-1245"><a href="#cb11-1245"></a><span class="fu">## 3D Parallelism {background-color="white"}</span></span>
<span id="cb11-1246"><a href="#cb11-1246"></a></span>
<span id="cb11-1247"><a href="#cb11-1247"></a><span class="ss">- </span><span class="in">`DP`</span> + <span class="in">`TP`</span> + <span class="in">`PP`</span> (3D) Parallelism</span>
<span id="cb11-1248"><a href="#cb11-1248"></a></span>
<span id="cb11-1249"><a href="#cb11-1249"></a>::: {#fig-3dparallel style="text-align:center!important;"}</span>
<span id="cb11-1250"><a href="#cb11-1250"></a></span>
<span id="cb11-1251"><a href="#cb11-1251"></a><span class="al">![](assets/parallelism-deepspeed-3d.png)</span></span>
<span id="cb11-1252"><a href="#cb11-1252"></a></span>
<span id="cb11-1253"><a href="#cb11-1253"></a>Figure taken from [3D parallelism: Scaling to trillion-parameter</span>
<span id="cb11-1254"><a href="#cb11-1254"></a>models](https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/)</span>
<span id="cb11-1255"><a href="#cb11-1255"></a></span>
<span id="cb11-1256"><a href="#cb11-1256"></a>:::</span>
<span id="cb11-1257"><a href="#cb11-1257"></a></span>
<span id="cb11-1258"><a href="#cb11-1258"></a><span class="fu">## Deciding on a Parallelism Strategy {background-color="white"}</span></span>
<span id="cb11-1259"><a href="#cb11-1259"></a></span>
<span id="cb11-1260"><a href="#cb11-1260"></a>::: {.panel-tabset}</span>
<span id="cb11-1261"><a href="#cb11-1261"></a></span>
<span id="cb11-1262"><a href="#cb11-1262"></a><span class="fu">### Single GPU</span></span>
<span id="cb11-1263"><a href="#cb11-1263"></a></span>
<span id="cb11-1264"><a href="#cb11-1264"></a><span class="ss">- </span>Model fits onto a single GPU:</span>
<span id="cb11-1265"><a href="#cb11-1265"></a><span class="ss">  - </span>Normal use</span>
<span id="cb11-1266"><a href="#cb11-1266"></a><span class="ss">- </span>Model **DOES NOT** fit on a single GPU:</span>
<span id="cb11-1267"><a href="#cb11-1267"></a><span class="ss">  - </span><span class="in">`ZeRO`</span> + Offload CPU (or, optionally, <span class="in">`NVMe`</span>)</span>
<span id="cb11-1268"><a href="#cb11-1268"></a><span class="ss">- </span>Largest layer **DOES NOT** fit on a single GPU:</span>
<span id="cb11-1269"><a href="#cb11-1269"></a><span class="ss">  - </span><span class="in">`ZeRO`</span> + Enable <span class="co">[</span><span class="ot">Memory Centric Tiling (MCT)</span><span class="co">](https://deepspeed.readthedocs.io/en/latest/zero3.html#memory-centric-tiling)</span></span>
<span id="cb11-1270"><a href="#cb11-1270"></a><span class="ss">    - </span>MCT Allows running of arbitrarily large layers by automatically splitting them and executing them sequentially.</span>
<span id="cb11-1271"><a href="#cb11-1271"></a></span>
<span id="cb11-1272"><a href="#cb11-1272"></a><span class="fu">### Single Node / Multi-GPU</span></span>
<span id="cb11-1273"><a href="#cb11-1273"></a></span>
<span id="cb11-1274"><a href="#cb11-1274"></a>::: {.flex-container}</span>
<span id="cb11-1275"><a href="#cb11-1275"></a></span>
<span id="cb11-1276"><a href="#cb11-1276"></a>::: {.column}</span>
<span id="cb11-1277"><a href="#cb11-1277"></a><span class="ss">- </span>Model fits onto a single GPU</span>
<span id="cb11-1278"><a href="#cb11-1278"></a><span class="ss">  - </span><span class="co">[</span><span class="ot">`DDP`</span><span class="co">](https://pytorch.org/docs/stable/notes/ddp.html)</span></span>
<span id="cb11-1279"><a href="#cb11-1279"></a><span class="ss">  - </span><span class="co">[</span><span class="ot">`ZeRO`</span><span class="co">](https://deepspeed.readthedocs.io/en/latest/zero3.html)</span></span>
<span id="cb11-1280"><a href="#cb11-1280"></a>:::</span>
<span id="cb11-1281"><a href="#cb11-1281"></a></span>
<span id="cb11-1282"><a href="#cb11-1282"></a>::: {.column}</span>
<span id="cb11-1283"><a href="#cb11-1283"></a><span class="ss">- </span>Model **DOES NOT** fit onto a single GPU</span>
<span id="cb11-1284"><a href="#cb11-1284"></a><span class="ss">  1. </span><span class="co">[</span><span class="ot">Pipeline Parallelism (`PP`)</span><span class="co">](https://www.deepspeed.ai/tutorials/pipeline/)</span></span>
<span id="cb11-1285"><a href="#cb11-1285"></a><span class="ss">  2. </span><span class="co">[</span><span class="ot">`ZeRO`</span><span class="co">](https://deepspeed.readthedocs.io/en/latest/zero3.html)</span></span>
<span id="cb11-1286"><a href="#cb11-1286"></a><span class="ss">  3. </span><span class="co">[</span><span class="ot">Tensor Parallelism (`TP`)</span><span class="co">](https://pytorch.org/docs/stable/distributed.tensor.parallel.html)</span></span>
<span id="cb11-1287"><a href="#cb11-1287"></a>:::</span>
<span id="cb11-1288"><a href="#cb11-1288"></a></span>
<span id="cb11-1289"><a href="#cb11-1289"></a>:::</span>
<span id="cb11-1290"><a href="#cb11-1290"></a></span>
<span id="cb11-1291"><a href="#cb11-1291"></a><span class="ss">- </span>With sufficiently fast connectivity between nodes, these three strategies should be comparable.</span>
<span id="cb11-1292"><a href="#cb11-1292"></a></span>
<span id="cb11-1293"><a href="#cb11-1293"></a><span class="ss">  - </span>Otherwise, <span class="in">`PP`</span> $&gt;$ <span class="in">`ZeRO`</span> $\simeq$ <span class="in">`TP`</span>.</span>
<span id="cb11-1294"><a href="#cb11-1294"></a></span>
<span id="cb11-1295"><a href="#cb11-1295"></a><span class="fu">### Multi-Node / Multi-GPU</span></span>
<span id="cb11-1296"><a href="#cb11-1296"></a></span>
<span id="cb11-1297"><a href="#cb11-1297"></a><span class="ss">- </span>When you have fast inter-node connectivity:</span>
<span id="cb11-1298"><a href="#cb11-1298"></a></span>
<span id="cb11-1299"><a href="#cb11-1299"></a><span class="ss">  - </span><span class="in">`ZeRO`</span> (virtually **NO** modifications)</span>
<span id="cb11-1300"><a href="#cb11-1300"></a><span class="ss">  - </span><span class="in">`PP`</span> + <span class="in">`ZeRO`</span> + <span class="in">`TP`</span> + <span class="in">`DP`</span> (less communication, at the cost of **MAJOR** modifications)</span>
<span id="cb11-1301"><a href="#cb11-1301"></a><span class="ss">    - </span>when you have slow inter-node connectivity and still low on GPU memory:</span>
<span id="cb11-1302"><a href="#cb11-1302"></a></span>
<span id="cb11-1303"><a href="#cb11-1303"></a>      <span class="in">```bash</span></span>
<span id="cb11-1304"><a href="#cb11-1304"></a>      <span class="ex">DP</span> + PP + TP + ZeRO-1</span>
<span id="cb11-1305"><a href="#cb11-1305"></a>      <span class="in">```</span></span>
<span id="cb11-1306"><a href="#cb11-1306"></a></span>
<span id="cb11-1307"><a href="#cb11-1307"></a><span class="ss">  - </span>**NOTE**: <span class="in">`TP`</span> is almost _always_ used within a single node, e.g.  </span>
<span id="cb11-1308"><a href="#cb11-1308"></a>    <span class="in">`TP &lt;= GPUS_PER_NODE`</span></span>
<span id="cb11-1309"><a href="#cb11-1309"></a></span>
<span id="cb11-1310"><a href="#cb11-1310"></a>:::</span>
<span id="cb11-1311"><a href="#cb11-1311"></a></span>
<span id="cb11-1312"><a href="#cb11-1312"></a><span class="fu"># ğŸ¦™ Large Language Models {background-color="white"}</span></span>
<span id="cb11-1313"><a href="#cb11-1313"></a></span>
<span id="cb11-1314"><a href="#cb11-1314"></a>::: {#fig-llms}</span>
<span id="cb11-1315"><a href="#cb11-1315"></a><span class="al">![](./assets/llms.gif)</span></span>
<span id="cb11-1316"><a href="#cb11-1316"></a></span>
<span id="cb11-1317"><a href="#cb11-1317"></a>Large Language Models have (LLM)s have taken the ~~NLP community~~ **world** by storm<span class="ot">[^llm-animation]</span>.  </span>
<span id="cb11-1318"><a href="#cb11-1318"></a>:::</span>
<span id="cb11-1319"><a href="#cb11-1319"></a></span>
<span id="cb11-1320"><a href="#cb11-1320"></a><span class="ot">[^llm-animation]: </span>Source: <span class="co">[</span><span class="ot">{{&lt; fa brands github &gt;}} `Hannibal046/Awesome-LLM`</span><span class="co">](https://github.com/Hannibal046/Awesome-LLM)</span></span>
<span id="cb11-1321"><a href="#cb11-1321"></a></span>
<span id="cb11-1322"><a href="#cb11-1322"></a><span class="fu">## ğŸ”® Emergent Abilities {background-color="#FBFBFD"}</span></span>
<span id="cb11-1323"><a href="#cb11-1323"></a></span>
<span id="cb11-1324"><a href="#cb11-1324"></a>::: {#fig-emergent-abilities}</span>
<span id="cb11-1325"><a href="#cb11-1325"></a></span>
<span id="cb11-1326"><a href="#cb11-1326"></a><span class="al">![](./assets/emergent-abilities.gif)</span></span>
<span id="cb11-1327"><a href="#cb11-1327"></a></span>
<span id="cb11-1328"><a href="#cb11-1328"></a>See @wei2022emergentabilitieslargelanguage, @yao2023tree</span>
<span id="cb11-1329"><a href="#cb11-1329"></a>:::</span>
<span id="cb11-1330"><a href="#cb11-1330"></a></span>
<span id="cb11-1331"><a href="#cb11-1331"></a></span>
<span id="cb11-1332"><a href="#cb11-1332"></a><span class="fu">## ğŸš‚ Training LLMs {.smaller background-color="white"}</span></span>
<span id="cb11-1333"><a href="#cb11-1333"></a></span>
<span id="cb11-1334"><a href="#cb11-1334"></a>::: {layout="<span class="co">[</span><span class="ot">15,-2,10</span><span class="co">]</span>" layout-valign="bottom"}</span>
<span id="cb11-1335"><a href="#cb11-1335"></a>!<span class="co">[</span><span class="ot">Visualization from [Hannibal046/Awesome-LLM](https://github.com/Hannibal046/Awesome-LLM)</span><span class="co">](./assets/evolution.gif)</span></span>
<span id="cb11-1336"><a href="#cb11-1336"></a></span>
<span id="cb11-1337"><a href="#cb11-1337"></a><span class="al">![](./assets/it_hungers.jpeg)</span></span>
<span id="cb11-1338"><a href="#cb11-1338"></a>:::</span>
<span id="cb11-1339"><a href="#cb11-1339"></a></span>
<span id="cb11-1340"><a href="#cb11-1340"></a><span class="fu">## â™»ï¸ Life-Cycle of the LLM {auto-animate=true background-color="white"}</span></span>
<span id="cb11-1341"><a href="#cb11-1341"></a></span>
<span id="cb11-1342"><a href="#cb11-1342"></a>::: {.flex-container}</span>
<span id="cb11-1343"><a href="#cb11-1343"></a></span>
<span id="cb11-1344"><a href="#cb11-1344"></a>::: {.column style="width: 40%;"}</span>
<span id="cb11-1345"><a href="#cb11-1345"></a><span class="ss">1. </span>Data collection + preprocessing</span>
<span id="cb11-1346"><a href="#cb11-1346"></a><span class="ss">2. </span>**Pre-training**</span>
<span id="cb11-1347"><a href="#cb11-1347"></a><span class="ss">    - </span>Architecture decisions, model size, etc.</span>
<span id="cb11-1348"><a href="#cb11-1348"></a><span class="ss">3. </span>Supervised Fine-Tuning</span>
<span id="cb11-1349"><a href="#cb11-1349"></a><span class="ss">    - </span>Instruction Tuning</span>
<span id="cb11-1350"><a href="#cb11-1350"></a><span class="ss">    - </span>Alignment</span>
<span id="cb11-1351"><a href="#cb11-1351"></a><span class="ss">4. </span>Deploy (+ monitor, re-evaluate, etc.)</span>
<span id="cb11-1352"><a href="#cb11-1352"></a>:::</span>
<span id="cb11-1353"><a href="#cb11-1353"></a></span>
<span id="cb11-1354"><a href="#cb11-1354"></a>::: {.column style="width:50%;"}</span>
<span id="cb11-1355"><a href="#cb11-1355"></a></span>
<span id="cb11-1356"><a href="#cb11-1356"></a>::: {#fig-pretrain-two}</span>
<span id="cb11-1357"><a href="#cb11-1357"></a><span class="al">![](./assets/gpt3-training-step-back-prop.gif)</span></span>
<span id="cb11-1358"><a href="#cb11-1358"></a></span>
<span id="cb11-1359"><a href="#cb11-1359"></a>**Pre-training**: Virtually _all of the compute_ used during pre-training<span class="ot">[^pretrain-two-source]</span>.</span>
<span id="cb11-1360"><a href="#cb11-1360"></a></span>
<span id="cb11-1361"><a href="#cb11-1361"></a>:::</span>
<span id="cb11-1362"><a href="#cb11-1362"></a></span>
<span id="cb11-1363"><a href="#cb11-1363"></a>:::</span>
<span id="cb11-1364"><a href="#cb11-1364"></a></span>
<span id="cb11-1365"><a href="#cb11-1365"></a>:::</span>
<span id="cb11-1366"><a href="#cb11-1366"></a></span>
<span id="cb11-1367"><a href="#cb11-1367"></a><span class="ot">[^pretrain-two-source]: </span>Figure from <span class="co">[</span><span class="ot">The Illustrated Transformer</span><span class="co">](http://jalammar.github.io/illustrated-transformer/)</span></span>
<span id="cb11-1368"><a href="#cb11-1368"></a></span>
<span id="cb11-1369"><a href="#cb11-1369"></a><span class="fu">## ğŸ€ Life-Cycle of the LLM {auto-animate=true background-color="white"}</span></span>
<span id="cb11-1370"><a href="#cb11-1370"></a></span>
<span id="cb11-1371"><a href="#cb11-1371"></a>::: {.flex-container}</span>
<span id="cb11-1372"><a href="#cb11-1372"></a></span>
<span id="cb11-1373"><a href="#cb11-1373"></a>::: {.column style="width: 50%;"}</span>
<span id="cb11-1374"><a href="#cb11-1374"></a><span class="ss">1. </span>Data collection + preprocessing</span>
<span id="cb11-1375"><a href="#cb11-1375"></a><span class="ss">2. </span>Pre-training</span>
<span id="cb11-1376"><a href="#cb11-1376"></a><span class="ss">    - </span>Architecture decisions, model size, etc.</span>
<span id="cb11-1377"><a href="#cb11-1377"></a><span class="ss">3. </span>**Supervised Fine-Tuning**</span>
<span id="cb11-1378"><a href="#cb11-1378"></a><span class="ss">    - </span>Instruction Tuning</span>
<span id="cb11-1379"><a href="#cb11-1379"></a><span class="ss">    - </span>Alignment</span>
<span id="cb11-1380"><a href="#cb11-1380"></a><span class="ss">4. </span>Deploy (+ monitor, re-evaluate, etc.)</span>
<span id="cb11-1381"><a href="#cb11-1381"></a>:::</span>
<span id="cb11-1382"><a href="#cb11-1382"></a></span>
<span id="cb11-1383"><a href="#cb11-1383"></a>::: {.column style="width:50%;"}</span>
<span id="cb11-1384"><a href="#cb11-1384"></a></span>
<span id="cb11-1385"><a href="#cb11-1385"></a>::: {#fig-finetune-lifecycle}</span>
<span id="cb11-1386"><a href="#cb11-1386"></a><span class="al">![](./assets/gpt3-fine-tuning.gif)</span></span>
<span id="cb11-1387"><a href="#cb11-1387"></a></span>
<span id="cb11-1388"><a href="#cb11-1388"></a>**Fine-tuning**: Fine-tuning actually updates the model's weights to make the</span>
<span id="cb11-1389"><a href="#cb11-1389"></a>model better at a certain task<span class="ot">[^ill-transf1]</span>.</span>
<span id="cb11-1390"><a href="#cb11-1390"></a>:::</span>
<span id="cb11-1391"><a href="#cb11-1391"></a></span>
<span id="cb11-1392"><a href="#cb11-1392"></a>:::</span>
<span id="cb11-1393"><a href="#cb11-1393"></a></span>
<span id="cb11-1394"><a href="#cb11-1394"></a>:::</span>
<span id="cb11-1395"><a href="#cb11-1395"></a></span>
<span id="cb11-1396"><a href="#cb11-1396"></a><span class="ot">[^ill-transf1]: </span>Figure from <span class="co">[</span><span class="ot">The Illustrated Transformer</span><span class="co">](http://jalammar.github.io/illustrated-transformer/)</span></span>
<span id="cb11-1397"><a href="#cb11-1397"></a></span>
<span id="cb11-1398"><a href="#cb11-1398"></a><span class="fu">## â© Forward Pass {background-color="white"}</span></span>
<span id="cb11-1399"><a href="#cb11-1399"></a></span>
<span id="cb11-1400"><a href="#cb11-1400"></a>::: {#fig-hf-assisted-generation style="width:100%;"}</span>
<span id="cb11-1401"><a href="#cb11-1401"></a></span>
<span id="cb11-1402"><a href="#cb11-1402"></a><span class="al">![](./assets/hf_assisted_generation.mov)</span></span>
<span id="cb11-1403"><a href="#cb11-1403"></a></span>
<span id="cb11-1404"><a href="#cb11-1404"></a>Language Model trained for causal language modeling<span class="ot">[^hf-assisted-generation]</span>.</span>
<span id="cb11-1405"><a href="#cb11-1405"></a>:::</span>
<span id="cb11-1406"><a href="#cb11-1406"></a></span>
<span id="cb11-1407"><a href="#cb11-1407"></a><span class="ot">[^hf-assisted-generation]: </span>Video from: <span class="co">[</span><span class="ot">ğŸ¤— Generation with LLMs</span><span class="co">](https://huggingface.co/docs/transformers/main/en/llm_tutorial)</span></span>
<span id="cb11-1408"><a href="#cb11-1408"></a></span>
<span id="cb11-1409"><a href="#cb11-1409"></a><span class="fu">## ğŸ’¬ Generating Text {background-color="white"}</span></span>
<span id="cb11-1410"><a href="#cb11-1410"></a></span>
<span id="cb11-1411"><a href="#cb11-1411"></a>::: {#fig-generating-text style="width: 100%;"}</span>
<span id="cb11-1412"><a href="#cb11-1412"></a></span>
<span id="cb11-1413"><a href="#cb11-1413"></a><span class="al">![](./assets/hf_assisted_generation2.mov)</span></span>
<span id="cb11-1414"><a href="#cb11-1414"></a></span>
<span id="cb11-1415"><a href="#cb11-1415"></a>Language Model trained for causal language modeling<span class="ot">[^generating-text-source]</span>.</span>
<span id="cb11-1416"><a href="#cb11-1416"></a>:::</span>
<span id="cb11-1417"><a href="#cb11-1417"></a></span>
<span id="cb11-1418"><a href="#cb11-1418"></a><span class="ot">[^generating-text-source]: </span>Video from: <span class="co">[</span><span class="ot">ğŸ¤— Generation with LLMs</span><span class="co">](https://huggingface.co/docs/transformers/main/en/llm_tutorial)</span></span>
<span id="cb11-1419"><a href="#cb11-1419"></a></span>
<span id="cb11-1420"><a href="#cb11-1420"></a><span class="fu"># ğŸ‘‹ Hands On {background-color="white"}</span></span>
<span id="cb11-1421"><a href="#cb11-1421"></a></span>
<span id="cb11-1422"><a href="#cb11-1422"></a>{{&lt; fa brands github &gt;}} <span class="co">[</span><span class="ot">ai-science-training-series / 06_parallel_training</span><span class="co">](https://github.com/argonne-lcf/ai-science-training-series/tree/main/06_parallel_training#hands-on)</span></span>
<span id="cb11-1423"><a href="#cb11-1423"></a></span>
<span id="cb11-1424"><a href="#cb11-1424"></a><span class="fu">## ğŸ§‘â€ğŸ’» Hands On: Getting Started {background-color="white"}</span></span>
<span id="cb11-1425"><a href="#cb11-1425"></a></span>
<span id="cb11-1426"><a href="#cb11-1426"></a><span class="ss">1. </span>ğŸŒ± Clone Repo(s):</span>
<span id="cb11-1427"><a href="#cb11-1427"></a></span>
<span id="cb11-1428"><a href="#cb11-1428"></a><span class="ss">    - </span>{{&lt; fa brands github &gt;}} <span class="co">[</span><span class="ot">saforem2/`wordplay`</span><span class="co">](https://github.com/saforem2/wordplay)</span></span>
<span id="cb11-1429"><a href="#cb11-1429"></a></span>
<span id="cb11-1430"><a href="#cb11-1430"></a>        <span class="in">```bash</span></span>
<span id="cb11-1431"><a href="#cb11-1431"></a>        <span class="fu">git</span> clone https://github.com/saforem2/wordplay</span>
<span id="cb11-1432"><a href="#cb11-1432"></a>        <span class="bu">cd</span> wordplay</span>
<span id="cb11-1433"><a href="#cb11-1433"></a>        <span class="in">```</span></span>
<span id="cb11-1434"><a href="#cb11-1434"></a></span>
<span id="cb11-1435"><a href="#cb11-1435"></a><span class="ss">    - </span>{{&lt; fa brands github &gt;}} <span class="co">[</span><span class="ot">saforem2/`ezpz`</span><span class="co">](https://github.com/saforem2/ezpz)</span></span>
<span id="cb11-1436"><a href="#cb11-1436"></a></span>
<span id="cb11-1437"><a href="#cb11-1437"></a>        <span class="in">```bash</span></span>
<span id="cb11-1438"><a href="#cb11-1438"></a>        <span class="fu">git</span> clone https://github.com/saforem2/ezpz deps/ezpz</span>
<span id="cb11-1439"><a href="#cb11-1439"></a>        <span class="in">```</span></span>
<span id="cb11-1440"><a href="#cb11-1440"></a><span class="ss">1. </span>ğŸ Setup Python:</span>
<span id="cb11-1441"><a href="#cb11-1441"></a></span>
<span id="cb11-1442"><a href="#cb11-1442"></a>    <span class="in">```bash</span></span>
<span id="cb11-1443"><a href="#cb11-1443"></a>    <span class="bu">export</span> <span class="va">PBS_O_WORKDIR</span><span class="op">=</span><span class="va">$(</span><span class="bu">pwd</span><span class="va">)</span> <span class="kw">&amp;&amp;</span> <span class="bu">source</span> deps/ezpz/src/ezpz/bin/utils.sh</span>
<span id="cb11-1444"><a href="#cb11-1444"></a>    <span class="ex">ezpz_setup_python</span></span>
<span id="cb11-1445"><a href="#cb11-1445"></a>    <span class="ex">ezpz_setup_job</span></span>
<span id="cb11-1446"><a href="#cb11-1446"></a>    <span class="in">```</span></span>
<span id="cb11-1447"><a href="#cb11-1447"></a></span>
<span id="cb11-1448"><a href="#cb11-1448"></a><span class="fu">## ğŸ“¦ Install {`ezpz`, `wordplay`} {background-color="white"}</span></span>
<span id="cb11-1449"><a href="#cb11-1449"></a></span>
<span id="cb11-1450"><a href="#cb11-1450"></a></span>
<span id="cb11-1451"><a href="#cb11-1451"></a><span class="ss">1. </span>Install Python packages:</span>
<span id="cb11-1452"><a href="#cb11-1452"></a></span>
<span id="cb11-1453"><a href="#cb11-1453"></a><span class="ss">    1. </span>{{&lt; fa brands github &gt;}} <span class="co">[</span><span class="ot">saforem2/`ezpz`</span><span class="co">](https://github.com/saforem2/ezpz)</span>:</span>
<span id="cb11-1454"><a href="#cb11-1454"></a></span>
<span id="cb11-1455"><a href="#cb11-1455"></a>        <span class="in">```bash</span></span>
<span id="cb11-1456"><a href="#cb11-1456"></a>        <span class="ex">python3</span> <span class="at">-m</span> pip install <span class="at">-e</span> <span class="st">"./deps/ezpz"</span> <span class="at">--require-virtualenv</span></span>
<span id="cb11-1457"><a href="#cb11-1457"></a>        <span class="in">```</span></span>
<span id="cb11-1458"><a href="#cb11-1458"></a></span>
<span id="cb11-1459"><a href="#cb11-1459"></a><span class="ss">    1. </span>{{&lt; fa brands github &gt;}} <span class="co">[</span><span class="ot">saforem2/`wordplay`</span><span class="co">](https://github.com/saforem2/ezpz)</span>:</span>
<span id="cb11-1460"><a href="#cb11-1460"></a></span>
<span id="cb11-1461"><a href="#cb11-1461"></a>        <span class="in">```bash</span></span>
<span id="cb11-1462"><a href="#cb11-1462"></a>        <span class="co"># from inside `wordplay/`</span></span>
<span id="cb11-1463"><a href="#cb11-1463"></a>        <span class="ex">python3</span> <span class="at">-m</span> pip install <span class="at">-e</span> . <span class="at">--require-virtualenv</span></span>
<span id="cb11-1464"><a href="#cb11-1464"></a>        <span class="in">```</span></span>
<span id="cb11-1465"><a href="#cb11-1465"></a></span>
<span id="cb11-1466"><a href="#cb11-1466"></a><span class="ss">1. </span>Test distributed setup:</span>
<span id="cb11-1467"><a href="#cb11-1467"></a></span>
<span id="cb11-1468"><a href="#cb11-1468"></a>    <span class="in">```bash</span></span>
<span id="cb11-1469"><a href="#cb11-1469"></a>    <span class="ex">mpirun</span> <span class="at">-n</span> <span class="st">"</span><span class="va">${NGPUS}</span><span class="st">"</span> python3 <span class="at">-m</span> ezpz.test_dist</span>
<span id="cb11-1470"><a href="#cb11-1470"></a>    <span class="in">```</span></span>
<span id="cb11-1471"><a href="#cb11-1471"></a></span>
<span id="cb11-1472"><a href="#cb11-1472"></a>    See: ğŸ‹ <span class="co">[</span><span class="ot">`ezpz/test_dist.py`</span><span class="co">](https://github.com/saforem2/ezpz/blob/main/src/ezpz/test_dist.py)</span></span>
<span id="cb11-1473"><a href="#cb11-1473"></a></span>
<span id="cb11-1474"><a href="#cb11-1474"></a></span>
<span id="cb11-1475"><a href="#cb11-1475"></a><span class="fu">## {{&lt; fa brands github &gt;}} [`ezpz`](https://github.com/saforem2/ezpz): Example \[[video](https://asciinema.org/a/668460)\] {background-color="#121314"}</span></span>
<span id="cb11-1476"><a href="#cb11-1476"></a></span>
<span id="cb11-1477"><a href="#cb11-1477"></a>::: {#fig-ezpz-asciinema}</span>
<span id="cb11-1478"><a href="#cb11-1478"></a></span>
<span id="cb11-1479"><a href="#cb11-1479"></a><span class="dt">&lt;</span><span class="kw">script</span><span class="ot"> src</span><span class="op">=</span><span class="st">"https://asciinema.org/a/668460.js"</span><span class="ot"> id</span><span class="op">=</span><span class="st">"asciicast-668460"</span><span class="ot"> async</span><span class="op">=</span><span class="st">"true"</span><span class="dt">&gt;&lt;/</span><span class="kw">script</span><span class="dt">&gt;</span></span>
<span id="cb11-1480"><a href="#cb11-1480"></a></span>
<span id="cb11-1481"><a href="#cb11-1481"></a>Example: using <span class="co">[</span><span class="ot">ğŸ‹ `ezpz.test_dist`</span><span class="co">](https://github.com/saforem2/ezpz/blob/main/src/ezpz/test_dist.py)</span></span>
<span id="cb11-1482"><a href="#cb11-1482"></a>to train a small model using DDP</span>
<span id="cb11-1483"><a href="#cb11-1483"></a>:::</span>
<span id="cb11-1484"><a href="#cb11-1484"></a></span>
<span id="cb11-1485"><a href="#cb11-1485"></a><span class="fu">## Install [`wordplay` ğŸ®ğŸ’¬ ](https://github.com/saforem2/wordplay) {background-color="white"}</span></span>
<span id="cb11-1486"><a href="#cb11-1486"></a></span>
<span id="cb11-1487"><a href="#cb11-1487"></a>::: {#fig-nanoGPT}</span>
<span id="cb11-1488"><a href="#cb11-1488"></a><span class="al">![](./assets/nanogpt.jpg)</span></span>
<span id="cb11-1489"><a href="#cb11-1489"></a></span>
<span id="cb11-1490"><a href="#cb11-1490"></a>The simplest, fastest repository for training / finetuning GPT based models.</span>
<span id="cb11-1491"><a href="#cb11-1491"></a>Figure from <span class="co">[</span><span class="ot">karpathy/`nanoGPT`</span><span class="co">](https://github.com/karpathy/nanoGPT)</span></span>
<span id="cb11-1492"><a href="#cb11-1492"></a>:::</span>
<span id="cb11-1493"><a href="#cb11-1493"></a></span>
<span id="cb11-1494"><a href="#cb11-1494"></a><span class="fu">## Prepare Data {background-color="white"}</span></span>
<span id="cb11-1495"><a href="#cb11-1495"></a></span>
<span id="cb11-1496"><a href="#cb11-1496"></a><span class="in">```bash</span></span>
<span id="cb11-1497"><a href="#cb11-1497"></a><span class="ex">$</span> python3 wordplay/data/shakespeare_char/prepare.py</span>
<span id="cb11-1498"><a href="#cb11-1498"></a><span class="ex">Using</span> HF_DATASETS_CACHE=/home/foremans/tmp/polaris-talk/2024-07-17-073327/wordplay/data/shakespeare_char/.cache/huggingface</span>
<span id="cb11-1499"><a href="#cb11-1499"></a><span class="ex">length</span> of dataset in characters: 1,115,394</span>
<span id="cb11-1500"><a href="#cb11-1500"></a><span class="ex">all</span> the unique characters:</span>
<span id="cb11-1501"><a href="#cb11-1501"></a> <span class="ex">!$</span><span class="kw">&amp;</span><span class="ex">\',-.3:</span><span class="kw">;</span><span class="ex">?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz</span></span>
<span id="cb11-1502"><a href="#cb11-1502"></a><span class="ex">vocab</span> size: 65</span>
<span id="cb11-1503"><a href="#cb11-1503"></a><span class="ex">train</span> has 1,003,854 tokens</span>
<span id="cb11-1504"><a href="#cb11-1504"></a><span class="ex">val</span> has 111,540 tokens</span>
<span id="cb11-1505"><a href="#cb11-1505"></a><span class="in">```</span></span>
<span id="cb11-1506"><a href="#cb11-1506"></a></span>
<span id="cb11-1507"><a href="#cb11-1507"></a><span class="fu">## Launch Training (DDP) {background-color="white"}</span></span>
<span id="cb11-1508"><a href="#cb11-1508"></a></span>
<span id="cb11-1509"><a href="#cb11-1509"></a><span class="in">```bash</span></span>
<span id="cb11-1510"><a href="#cb11-1510"></a><span class="ex">launch</span> python3 <span class="at">-m</span> wordplay <span class="dt">\</span></span>
<span id="cb11-1511"><a href="#cb11-1511"></a>    train.backend=DDP <span class="dt">\</span></span>
<span id="cb11-1512"><a href="#cb11-1512"></a>    train.eval_interval=100 <span class="dt">\</span></span>
<span id="cb11-1513"><a href="#cb11-1513"></a>    data=shakespeare <span class="dt">\</span></span>
<span id="cb11-1514"><a href="#cb11-1514"></a>    train.dtype=bf16 <span class="dt">\</span></span>
<span id="cb11-1515"><a href="#cb11-1515"></a>    model.batch_size=64 <span class="dt">\</span></span>
<span id="cb11-1516"><a href="#cb11-1516"></a>    model.block_size=1024 <span class="dt">\</span></span>
<span id="cb11-1517"><a href="#cb11-1517"></a>    train.max_iters=1000 <span class="dt">\</span></span>
<span id="cb11-1518"><a href="#cb11-1518"></a>    train.log_interval=10 <span class="dt">\</span></span>
<span id="cb11-1519"><a href="#cb11-1519"></a>    train.compile=false <span class="dt">\</span></span>
<span id="cb11-1520"><a href="#cb11-1520"></a>    <span class="kw">|</span> <span class="fu">tee</span> wordplay-gpt2-DDP.log</span>
<span id="cb11-1521"><a href="#cb11-1521"></a><span class="in">```</span></span>
<span id="cb11-1522"><a href="#cb11-1522"></a></span>
<span id="cb11-1523"><a href="#cb11-1523"></a><span class="fu">## Training: Example Output {background-color="white"}</span></span>
<span id="cb11-1524"><a href="#cb11-1524"></a></span>
<span id="cb11-1525"><a href="#cb11-1525"></a><span class="in">```bash</span></span>
<span id="cb11-1526"><a href="#cb11-1526"></a><span class="ex">$</span> launch python3 <span class="at">-m</span> wordplay <span class="dt">\</span></span>
<span id="cb11-1527"><a href="#cb11-1527"></a>    train.backend=DDP <span class="dt">\</span></span>
<span id="cb11-1528"><a href="#cb11-1528"></a>    train.eval_interval=100 <span class="dt">\</span></span>
<span id="cb11-1529"><a href="#cb11-1529"></a>    data=shakespeare <span class="dt">\</span></span>
<span id="cb11-1530"><a href="#cb11-1530"></a>    train.dtype=bf16 <span class="dt">\</span></span>
<span id="cb11-1531"><a href="#cb11-1531"></a>    model.batch_size=64 <span class="dt">\</span></span>
<span id="cb11-1532"><a href="#cb11-1532"></a>    model.block_size=1024 <span class="dt">\</span></span>
<span id="cb11-1533"><a href="#cb11-1533"></a>    train.max_iters=1000 <span class="dt">\</span></span>
<span id="cb11-1534"><a href="#cb11-1534"></a>    train.log_interval=10 <span class="dt">\</span></span>
<span id="cb11-1535"><a href="#cb11-1535"></a>    train.compile=false <span class="dt">\</span></span>
<span id="cb11-1536"><a href="#cb11-1536"></a>    <span class="kw">|</span> <span class="fu">tee</span> wordplay-gpt2-DDP.log</span>
<span id="cb11-1537"><a href="#cb11-1537"></a><span class="ex">[2024-07-17</span> 07:42:11.746540]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">__init__:156</span><span class="pp">]</span> <span class="at">-</span> Setting logging level to <span class="st">'INFO'</span> on <span class="st">'RANK == 0'</span></span>
<span id="cb11-1538"><a href="#cb11-1538"></a><span class="ex">[2024-07-17</span> 07:42:11.748763]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">__init__:157</span><span class="pp">]</span> <span class="at">-</span> Setting logging level to <span class="st">'CRITICAL'</span> on all others <span class="st">'RANK != 0'</span></span>
<span id="cb11-1539"><a href="#cb11-1539"></a><span class="ex">[2024-07-17</span> 07:42:11.749453]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">__init__:160</span><span class="pp">]</span> <span class="at">-</span> To disable this behavior, and log from ALL ranks <span class="er">(</span><span class="ex">not</span> recommended<span class="kw">)</span><span class="ex">,</span> set: <span class="st">'export LOG_FROM_ALL_RANKS=1'</span>  in your environment, and re-run.</span>
<span id="cb11-1540"><a href="#cb11-1540"></a><span class="ex">[2024-07-17</span> 07:42:11.772718]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">configs:81</span><span class="pp">]</span> <span class="at">-</span> Setting HF_DATASETS_CACHE to /home/foremans/tmp/polaris-talk/2024-07-17-073327/wordplay/.cache/huggingface/datasets</span>
<span id="cb11-1541"><a href="#cb11-1541"></a><span class="ex">[2024-07-17</span> 07:42:15.341532]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist:358</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'cuda'</span><span class="pp">][</span><span class="ss">rank=2/3</span><span class="pp">][</span><span class="ss">local_rank=2/3</span><span class="pp">][</span><span class="ss">node=0/0</span><span class="pp">]</span></span>
<span id="cb11-1542"><a href="#cb11-1542"></a><span class="ex">[2024-07-17</span> 07:42:15.342381]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist:358</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'cuda'</span><span class="pp">][</span><span class="ss">rank=1/3</span><span class="pp">][</span><span class="ss">local_rank=1/3</span><span class="pp">][</span><span class="ss">node=0/0</span><span class="pp">]</span></span>
<span id="cb11-1543"><a href="#cb11-1543"></a><span class="ex">[2024-07-17</span> 07:42:15.342430]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist:358</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'cuda'</span><span class="pp">][</span><span class="ss">rank=3/3</span><span class="pp">][</span><span class="ss">local_rank=3/3</span><span class="pp">][</span><span class="ss">node=0/0</span><span class="pp">]</span></span>
<span id="cb11-1544"><a href="#cb11-1544"></a><span class="ex">[2024-07-17</span> 07:42:15.348657]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist:95</span><span class="pp">]</span> <span class="at">-</span></span>
<span id="cb11-1545"><a href="#cb11-1545"></a></span>
<span id="cb11-1546"><a href="#cb11-1546"></a><span class="ex">[dist_info]:</span></span>
<span id="cb11-1547"><a href="#cb11-1547"></a>  <span class="ex">â€¢</span> DEVICE=cuda</span>
<span id="cb11-1548"><a href="#cb11-1548"></a>  <span class="ex">â€¢</span> DEVICE_ID=cuda:0</span>
<span id="cb11-1549"><a href="#cb11-1549"></a>  <span class="ex">â€¢</span> DISTRIBUTED_BACKEND=nccl</span>
<span id="cb11-1550"><a href="#cb11-1550"></a>  <span class="ex">â€¢</span> GPUS_PER_NODE=4</span>
<span id="cb11-1551"><a href="#cb11-1551"></a>  <span class="ex">â€¢</span> HOSTS=<span class="pp">[</span><span class="st">'x3101c0s13b0n0.hsn.cm.polaris.alcf.anl.gov'</span><span class="pp">]</span></span>
<span id="cb11-1552"><a href="#cb11-1552"></a>  <span class="ex">â€¢</span> HOSTFILE=/var/spool/pbs/aux/2024084.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov</span>
<span id="cb11-1553"><a href="#cb11-1553"></a>  <span class="ex">â€¢</span> HOSTNAME=x3101c0s13b0n0.hsn.cm.polaris.alcf.anl.gov</span>
<span id="cb11-1554"><a href="#cb11-1554"></a>  <span class="ex">â€¢</span> LOCAL_RANK=0</span>
<span id="cb11-1555"><a href="#cb11-1555"></a>  <span class="ex">â€¢</span> MACHINE=Polaris</span>
<span id="cb11-1556"><a href="#cb11-1556"></a>  <span class="ex">â€¢</span> NUM_NODES=1</span>
<span id="cb11-1557"><a href="#cb11-1557"></a>  <span class="ex">â€¢</span> NGPUS=4</span>
<span id="cb11-1558"><a href="#cb11-1558"></a>  <span class="ex">â€¢</span> NGPUS_AVAILABLE=4</span>
<span id="cb11-1559"><a href="#cb11-1559"></a>  <span class="ex">â€¢</span> NODE_ID=0</span>
<span id="cb11-1560"><a href="#cb11-1560"></a>  <span class="ex">â€¢</span> RANK=0</span>
<span id="cb11-1561"><a href="#cb11-1561"></a>  <span class="ex">â€¢</span> SCHEDULER=PBS</span>
<span id="cb11-1562"><a href="#cb11-1562"></a>  <span class="ex">â€¢</span> WORLD_SIZE_TOTAL=4</span>
<span id="cb11-1563"><a href="#cb11-1563"></a>  <span class="ex">â€¢</span> WORLD_SIZE_IN_USE=4</span>
<span id="cb11-1564"><a href="#cb11-1564"></a>  <span class="ex">â€¢</span> LAUNCH_CMD=mpiexec <span class="at">--verbose</span> <span class="at">--envall</span> <span class="at">-n</span> 4 <span class="at">-ppn</span> 4 <span class="at">--hostfile</span> /var/spool/pbs/aux/2024084.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov <span class="at">--cpu-bind</span> depth <span class="at">-d</span> 16</span>
<span id="cb11-1565"><a href="#cb11-1565"></a></span>
<span id="cb11-1566"><a href="#cb11-1566"></a><span class="ex">[2024-07-17</span> 07:42:15.351446]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist:725</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">0/4</span><span class="pp">]</span> Using device=<span class="st">'cuda'</span> with backend=<span class="st">'DDP'</span> + <span class="st">'nccl'</span> for distributed training.</span>
<span id="cb11-1567"><a href="#cb11-1567"></a><span class="ex">[2024-07-17</span> 07:42:15.356169]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist:358</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'cuda'</span><span class="pp">][</span><span class="ss">rank=0/3</span><span class="pp">][</span><span class="ss">local_rank=0/3</span><span class="pp">][</span><span class="ss">node=0/0</span><span class="pp">]</span></span>
<span id="cb11-1568"><a href="#cb11-1568"></a><span class="ex">[2024-07-17</span> 07:42:15.356692]<span class="pp">[</span><span class="ss">WARNING</span><span class="pp">][</span><span class="ss">dist:364</span><span class="pp">]</span> <span class="at">-</span> Using [4 / 4] available <span class="st">"cuda"</span> devices !!</span>
<span id="cb11-1569"><a href="#cb11-1569"></a><span class="ex">[2024-07-17</span> 07:42:15.359571]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">configs:317</span><span class="pp">]</span> <span class="at">-</span> Loading val from /home/foremans/tmp/polaris-talk/2024-07-17-073327/wordplay/data/shakespeare_char/val.bin</span>
<span id="cb11-1570"><a href="#cb11-1570"></a><span class="ex">[2024-07-17</span> 07:42:15.360138]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">configs:317</span><span class="pp">]</span> <span class="at">-</span> Loading train from /home/foremans/tmp/polaris-talk/2024-07-17-073327/wordplay/data/shakespeare_char/train.bin</span>
<span id="cb11-1571"><a href="#cb11-1571"></a><span class="ex">[2024-07-17</span> 07:42:15.361154]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">configs:442</span><span class="pp">]</span> <span class="at">-</span> Tokens per iteration: 262,144</span>
<span id="cb11-1572"><a href="#cb11-1572"></a><span class="ex">[2024-07-17</span> 07:42:15.361574]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">configs:465</span><span class="pp">]</span> <span class="at">-</span> Using self.ptdtype=torch.float16 on self.device_type=<span class="st">'cuda'</span></span>
<span id="cb11-1573"><a href="#cb11-1573"></a><span class="ex">[2024-07-17</span> 07:42:15.362002]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">configs:471</span><span class="pp">]</span> <span class="at">-</span> Initializing a new model from scratch</span>
<span id="cb11-1574"><a href="#cb11-1574"></a><span class="ex">[2024-07-17</span> 07:42:15.362529]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist:874</span><span class="pp">]</span> <span class="at">-</span> Setting up wandb from rank: 0</span>
<span id="cb11-1575"><a href="#cb11-1575"></a><span class="ex">[2024-07-17</span> 07:42:15.362896]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist:875</span><span class="pp">]</span> <span class="at">-</span> Using: WB PROJECT: WordPlay</span>
<span id="cb11-1576"><a href="#cb11-1576"></a><span class="ex">[2024-07-17</span> 07:42:16.451786]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist:905</span><span class="pp">]</span> <span class="at">-</span> W<span class="kw">&amp;</span><span class="ex">B</span> RUN: <span class="pp">[</span><span class="ss">still</span><span class="pp">-</span><span class="ss">frog</span><span class="pp">-</span><span class="ss">17</span><span class="pp">]</span><span class="er">(</span><span class="ex">https://wandb.ai/aurora_gpt/WordPlay/runs/6by9vpcj</span><span class="kw">)</span></span>
<span id="cb11-1577"><a href="#cb11-1577"></a><span class="ex">[2024-07-17</span> 07:42:16.464106]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist:312</span><span class="pp">]</span> <span class="at">-</span> Updating wandb.run: still-frog-17 config with <span class="st">"DIST_INFO"</span></span>
<span id="cb11-1578"><a href="#cb11-1578"></a><span class="ex">[2024-07-17</span> 07:42:16.469424]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist:938</span><span class="pp">]</span> <span class="at">-</span> Running on machine=<span class="st">'Polaris'</span></span>
<span id="cb11-1579"><a href="#cb11-1579"></a><span class="ex">[2024-07-17</span> 07:42:16.471151]<span class="pp">[</span><span class="ss">WARNING</span><span class="pp">][</span><span class="ss">__main__:89</span><span class="pp">]</span> <span class="at">-</span> {</span>
<span id="cb11-1580"><a href="#cb11-1580"></a>    <span class="st">"train"</span><span class="ex">:</span> {</span>
<span id="cb11-1581"><a href="#cb11-1581"></a>        <span class="st">"framework"</span><span class="ex">:</span> <span class="st">"pytorch"</span>,</span>
<span id="cb11-1582"><a href="#cb11-1582"></a>        <span class="st">"backend"</span><span class="ex">:</span> <span class="st">"DDP"</span>,</span>
<span id="cb11-1583"><a href="#cb11-1583"></a>        <span class="st">"device"</span><span class="ex">:</span> null,</span>
<span id="cb11-1584"><a href="#cb11-1584"></a>        <span class="st">"seed"</span><span class="ex">:</span> null,</span>
<span id="cb11-1585"><a href="#cb11-1585"></a>        <span class="st">"port"</span><span class="ex">:</span> null,</span>
<span id="cb11-1586"><a href="#cb11-1586"></a>        <span class="st">"ds_config_path"</span><span class="ex">:</span> null,</span>
<span id="cb11-1587"><a href="#cb11-1587"></a>        <span class="st">"precision"</span><span class="ex">:</span> null,</span>
<span id="cb11-1588"><a href="#cb11-1588"></a>        <span class="st">"ngpus"</span><span class="ex">:</span> null,</span>
<span id="cb11-1589"><a href="#cb11-1589"></a>        <span class="st">"use_wandb"</span><span class="ex">:</span> true,</span>
<span id="cb11-1590"><a href="#cb11-1590"></a>        <span class="st">"eval_interval"</span><span class="ex">:</span> 100,</span>
<span id="cb11-1591"><a href="#cb11-1591"></a>        <span class="st">"log_interval"</span><span class="ex">:</span> 10,</span>
<span id="cb11-1592"><a href="#cb11-1592"></a>        <span class="st">"eval_iters"</span><span class="ex">:</span> 200,</span>
<span id="cb11-1593"><a href="#cb11-1593"></a>        <span class="st">"eval_only"</span><span class="ex">:</span> false,</span>
<span id="cb11-1594"><a href="#cb11-1594"></a>        <span class="st">"always_save_checkpoint"</span><span class="ex">:</span> false,</span>
<span id="cb11-1595"><a href="#cb11-1595"></a>        <span class="st">"init_from"</span><span class="ex">:</span> <span class="st">"scratch"</span>,</span>
<span id="cb11-1596"><a href="#cb11-1596"></a>        <span class="st">"wandb_project"</span><span class="ex">:</span> <span class="st">"WordPlay"</span>,</span>
<span id="cb11-1597"><a href="#cb11-1597"></a>        <span class="st">"max_iters"</span><span class="ex">:</span> 1000,</span>
<span id="cb11-1598"><a href="#cb11-1598"></a>        <span class="st">"warmup_iters"</span><span class="ex">:</span> 100,</span>
<span id="cb11-1599"><a href="#cb11-1599"></a>        <span class="st">"dtype"</span><span class="ex">:</span> <span class="st">"bf16"</span>,</span>
<span id="cb11-1600"><a href="#cb11-1600"></a>        <span class="st">"compile"</span><span class="ex">:</span> false</span>
<span id="cb11-1601"><a href="#cb11-1601"></a>    <span class="er">}</span><span class="ex">,</span></span>
<span id="cb11-1602"><a href="#cb11-1602"></a>    <span class="st">"model"</span><span class="ex">:</span> {</span>
<span id="cb11-1603"><a href="#cb11-1603"></a>        <span class="st">"n_layer"</span><span class="ex">:</span> 12,</span>
<span id="cb11-1604"><a href="#cb11-1604"></a>        <span class="st">"n_head"</span><span class="ex">:</span> 12,</span>
<span id="cb11-1605"><a href="#cb11-1605"></a>        <span class="st">"n_embd"</span><span class="ex">:</span> 768,</span>
<span id="cb11-1606"><a href="#cb11-1606"></a>        <span class="st">"batch_size"</span><span class="ex">:</span> 64,</span>
<span id="cb11-1607"><a href="#cb11-1607"></a>        <span class="st">"block_size"</span><span class="ex">:</span> 1024,</span>
<span id="cb11-1608"><a href="#cb11-1608"></a>        <span class="st">"activation"</span><span class="ex">:</span> <span class="st">"gelu"</span>,</span>
<span id="cb11-1609"><a href="#cb11-1609"></a>        <span class="st">"dropout"</span><span class="ex">:</span> 0.0,</span>
<span id="cb11-1610"><a href="#cb11-1610"></a>        <span class="st">"bias"</span><span class="ex">:</span> false,</span>
<span id="cb11-1611"><a href="#cb11-1611"></a>        <span class="st">"vocab_size"</span><span class="ex">:</span> 65</span>
<span id="cb11-1612"><a href="#cb11-1612"></a>    <span class="er">}</span><span class="ex">,</span></span>
<span id="cb11-1613"><a href="#cb11-1613"></a>    <span class="st">"data"</span><span class="ex">:</span> {</span>
<span id="cb11-1614"><a href="#cb11-1614"></a>        <span class="st">"dataset"</span><span class="ex">:</span> <span class="st">"shakespeare_char"</span>,</span>
<span id="cb11-1615"><a href="#cb11-1615"></a>        <span class="st">"out_dir"</span><span class="ex">:</span> <span class="st">"out-shakespeare-char"</span>,</span>
<span id="cb11-1616"><a href="#cb11-1616"></a>        <span class="st">"root_path"</span><span class="ex">:</span> null</span>
<span id="cb11-1617"><a href="#cb11-1617"></a>    <span class="er">}</span><span class="ex">,</span></span>
<span id="cb11-1618"><a href="#cb11-1618"></a>    <span class="st">"optimizer"</span><span class="ex">:</span> {</span>
<span id="cb11-1619"><a href="#cb11-1619"></a>        <span class="st">"gas"</span><span class="ex">:</span> 1,</span>
<span id="cb11-1620"><a href="#cb11-1620"></a>        <span class="st">"name"</span><span class="ex">:</span> <span class="st">"AdamW"</span>,</span>
<span id="cb11-1621"><a href="#cb11-1621"></a>        <span class="st">"learning_rate"</span><span class="ex">:</span> 0.0006,</span>
<span id="cb11-1622"><a href="#cb11-1622"></a>        <span class="st">"weight_decay"</span><span class="ex">:</span> 0.1,</span>
<span id="cb11-1623"><a href="#cb11-1623"></a>        <span class="st">"beta1"</span><span class="ex">:</span> 0.9,</span>
<span id="cb11-1624"><a href="#cb11-1624"></a>        <span class="st">"beta2"</span><span class="ex">:</span> 0.95,</span>
<span id="cb11-1625"><a href="#cb11-1625"></a>        <span class="st">"grad_clip"</span><span class="ex">:</span> 1.0,</span>
<span id="cb11-1626"><a href="#cb11-1626"></a>        <span class="st">"decay_lr"</span><span class="ex">:</span> true,</span>
<span id="cb11-1627"><a href="#cb11-1627"></a>        <span class="st">"lr_decay_iters"</span><span class="ex">:</span> 600000,</span>
<span id="cb11-1628"><a href="#cb11-1628"></a>        <span class="st">"min_lr"</span><span class="ex">:</span> 6e-05</span>
<span id="cb11-1629"><a href="#cb11-1629"></a>    <span class="er">}</span></span>
<span id="cb11-1630"><a href="#cb11-1630"></a><span class="er">}</span></span>
<span id="cb11-1631"><a href="#cb11-1631"></a><span class="ex">[2024-07-17</span> 07:42:16.474305]<span class="pp">[</span><span class="ss">WARNING</span><span class="pp">][</span><span class="ss">__main__:90</span><span class="pp">]</span> <span class="at">-</span> Output dir: /home/foremans/tmp/polaris-talk/outputs/runs/pytorch/DDP/2024-07-17/07-42-13</span>
<span id="cb11-1632"><a href="#cb11-1632"></a><span class="ex">[2024-07-17</span> 07:42:16.474922]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:246</span><span class="pp">]</span> <span class="at">-</span> Initializing a new model from scratch</span>
<span id="cb11-1633"><a href="#cb11-1633"></a><span class="ex">[2024-07-17</span> 07:42:17.258904]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">model:255</span><span class="pp">]</span> <span class="at">-</span> number of parameters: 85.00M</span>
<span id="cb11-1634"><a href="#cb11-1634"></a><span class="ex">[2024-07-17</span> 07:42:17.290004]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:264</span><span class="pp">]</span> <span class="at">-</span> Model size: num_params=85003776</span>
<span id="cb11-1635"><a href="#cb11-1635"></a><span class="ex">[2024-07-17</span> 07:42:17.292626]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">model:445</span><span class="pp">]</span> <span class="at">-</span> num decayed parameter tensors: 50, with 85,771,008 parameters</span>
<span id="cb11-1636"><a href="#cb11-1636"></a><span class="ex">[2024-07-17</span> 07:42:17.293296]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">model:449</span><span class="pp">]</span> <span class="at">-</span> num non-decayed parameter tensors: 25, with 19,200 parameters</span>
<span id="cb11-1637"><a href="#cb11-1637"></a><span class="ex">[2024-07-17</span> 07:42:17.515324]<span class="pp">[</span><span class="ss">CRITICAL</span><span class="pp">][</span><span class="ss">trainer:316</span><span class="pp">]</span> <span class="at">-</span> <span class="st">"devid='cuda:1'"</span></span>
<span id="cb11-1638"><a href="#cb11-1638"></a><span class="ex">[2024-07-17</span> 07:42:17.515340]<span class="pp">[</span><span class="ss">CRITICAL</span><span class="pp">][</span><span class="ss">trainer:316</span><span class="pp">]</span> <span class="at">-</span> <span class="st">"devid='cuda:2'"</span></span>
<span id="cb11-1639"><a href="#cb11-1639"></a><span class="ex">[2024-07-17</span> 07:42:17.515465]<span class="pp">[</span><span class="ss">CRITICAL</span><span class="pp">][</span><span class="ss">trainer:316</span><span class="pp">]</span> <span class="at">-</span> <span class="st">"devid='cuda:3'"</span></span>
<span id="cb11-1640"><a href="#cb11-1640"></a><span class="ex">[2024-07-17</span> 07:42:18.431814]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">model:465</span><span class="pp">]</span> <span class="at">-</span> using fused AdamW: True</span>
<span id="cb11-1641"><a href="#cb11-1641"></a><span class="ex">[2024-07-17</span> 07:42:18.432620]<span class="pp">[</span><span class="ss">CRITICAL</span><span class="pp">][</span><span class="ss">trainer:316</span><span class="pp">]</span> <span class="at">-</span> <span class="st">"devid='cuda:0'"</span></span>
<span id="cb11-1642"><a href="#cb11-1642"></a><span class="ex">[2024-07-17</span> 07:42:19.951020]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:356</span><span class="pp">]</span> <span class="at">-</span> â€¢ self.model=GPT<span class="er">(</span></span>
<span id="cb11-1643"><a href="#cb11-1643"></a>  <span class="kw">(</span><span class="ex">transformer</span><span class="kw">)</span><span class="bu">:</span> ModuleDict<span class="er">(</span></span>
<span id="cb11-1644"><a href="#cb11-1644"></a>    <span class="kw">(</span><span class="ex">wte</span><span class="kw">)</span><span class="bu">:</span> Embedding<span class="er">(</span><span class="ex">65,</span> 768<span class="kw">)</span></span>
<span id="cb11-1645"><a href="#cb11-1645"></a>    <span class="kw">(</span><span class="ex">wpe</span><span class="kw">)</span><span class="bu">:</span> Embedding<span class="er">(</span><span class="ex">1024,</span> 768<span class="kw">)</span></span>
<span id="cb11-1646"><a href="#cb11-1646"></a>    <span class="kw">(</span><span class="ex">drop</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb11-1647"><a href="#cb11-1647"></a>    <span class="kw">(</span><span class="ex">h</span><span class="kw">)</span><span class="bu">:</span> ModuleList<span class="er">(</span></span>
<span id="cb11-1648"><a href="#cb11-1648"></a>      <span class="kw">(</span><span class="ex">0-11</span><span class="kw">)</span><span class="bu">:</span> 12 x Block<span class="er">(</span></span>
<span id="cb11-1649"><a href="#cb11-1649"></a>        <span class="kw">(</span><span class="ex">ln_1</span><span class="kw">)</span><span class="bu">:</span> LayerNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb11-1650"><a href="#cb11-1650"></a>        <span class="kw">(</span><span class="ex">attn</span><span class="kw">)</span><span class="bu">:</span> CausalSelfAttention<span class="er">(</span></span>
<span id="cb11-1651"><a href="#cb11-1651"></a>          <span class="kw">(</span><span class="ex">c_attn</span><span class="kw">)</span><span class="bu">:</span> Linear<span class="er">(</span><span class="va">in_features</span><span class="op">=</span>768, <span class="va">out_features</span><span class="op">=</span>2304, <span class="va">bias</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb11-1652"><a href="#cb11-1652"></a>          <span class="kw">(</span><span class="ex">c_proj</span><span class="kw">)</span><span class="bu">:</span> Linear<span class="er">(</span><span class="va">in_features</span><span class="op">=</span>768, <span class="va">out_features</span><span class="op">=</span>768, <span class="va">bias</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb11-1653"><a href="#cb11-1653"></a>          <span class="kw">(</span><span class="ex">attn_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb11-1654"><a href="#cb11-1654"></a>          <span class="kw">(</span><span class="ex">resid_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb11-1655"><a href="#cb11-1655"></a>        <span class="kw">)</span></span>
<span id="cb11-1656"><a href="#cb11-1656"></a>        <span class="kw">(</span><span class="ex">ln_2</span><span class="kw">)</span><span class="bu">:</span> LayerNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb11-1657"><a href="#cb11-1657"></a>        <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> MLP<span class="er">(</span></span>
<span id="cb11-1658"><a href="#cb11-1658"></a>          <span class="kw">(</span><span class="ex">c_fc</span><span class="kw">)</span><span class="bu">:</span> Linear<span class="er">(</span><span class="va">in_features</span><span class="op">=</span>768, <span class="va">out_features</span><span class="op">=</span>3072, <span class="va">bias</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb11-1659"><a href="#cb11-1659"></a>          <span class="kw">(</span><span class="ex">act_fn</span><span class="kw">)</span><span class="bu">:</span> GELU<span class="er">(</span><span class="va">approximate</span><span class="op">=</span><span class="st">'none'</span><span class="kw">)</span></span>
<span id="cb11-1660"><a href="#cb11-1660"></a>          <span class="kw">(</span><span class="ex">c_proj</span><span class="kw">)</span><span class="bu">:</span> Linear<span class="er">(</span><span class="va">in_features</span><span class="op">=</span>3072, <span class="va">out_features</span><span class="op">=</span>768, <span class="va">bias</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb11-1661"><a href="#cb11-1661"></a>          <span class="kw">(</span><span class="ex">dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb11-1662"><a href="#cb11-1662"></a>        <span class="kw">)</span></span>
<span id="cb11-1663"><a href="#cb11-1663"></a>      <span class="kw">)</span></span>
<span id="cb11-1664"><a href="#cb11-1664"></a>    <span class="kw">)</span></span>
<span id="cb11-1665"><a href="#cb11-1665"></a>    <span class="kw">(</span><span class="ex">ln_f</span><span class="kw">)</span><span class="bu">:</span> LayerNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb11-1666"><a href="#cb11-1666"></a>  <span class="kw">)</span></span>
<span id="cb11-1667"><a href="#cb11-1667"></a>  <span class="kw">(</span><span class="ex">lm_head</span><span class="kw">)</span><span class="bu">:</span> Linear<span class="er">(</span><span class="va">in_features</span><span class="op">=</span>768, <span class="va">out_features</span><span class="op">=</span>65, <span class="va">bias</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb11-1668"><a href="#cb11-1668"></a><span class="kw">)</span></span>
<span id="cb11-1669"><a href="#cb11-1669"></a><span class="ex">[2024-07-17</span> 07:42:19.955340]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:357</span><span class="pp">]</span> <span class="at">-</span> â€¢ self.grad_scaler=<span class="op">&lt;</span>torch.cuda.amp.grad_scaler.GradScaler object at 0x145a38f0f090<span class="op">&gt;</span></span>
<span id="cb11-1670"><a href="#cb11-1670"></a><span class="ex">[2024-07-17</span> 07:42:19.956897]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:358</span><span class="pp">]</span> <span class="at">-</span> â€¢ self.model_engine=DistributedDataParallel<span class="er">(</span></span>
<span id="cb11-1671"><a href="#cb11-1671"></a>  <span class="kw">(</span><span class="ex">module</span><span class="kw">)</span><span class="bu">:</span> GPT<span class="er">(</span></span>
<span id="cb11-1672"><a href="#cb11-1672"></a>    <span class="kw">(</span><span class="ex">transformer</span><span class="kw">)</span><span class="bu">:</span> ModuleDict<span class="er">(</span></span>
<span id="cb11-1673"><a href="#cb11-1673"></a>      <span class="kw">(</span><span class="ex">wte</span><span class="kw">)</span><span class="bu">:</span> Embedding<span class="er">(</span><span class="ex">65,</span> 768<span class="kw">)</span></span>
<span id="cb11-1674"><a href="#cb11-1674"></a>      <span class="kw">(</span><span class="ex">wpe</span><span class="kw">)</span><span class="bu">:</span> Embedding<span class="er">(</span><span class="ex">1024,</span> 768<span class="kw">)</span></span>
<span id="cb11-1675"><a href="#cb11-1675"></a>      <span class="kw">(</span><span class="ex">drop</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb11-1676"><a href="#cb11-1676"></a>      <span class="kw">(</span><span class="ex">h</span><span class="kw">)</span><span class="bu">:</span> ModuleList<span class="er">(</span></span>
<span id="cb11-1677"><a href="#cb11-1677"></a>        <span class="kw">(</span><span class="ex">0-11</span><span class="kw">)</span><span class="bu">:</span> 12 x Block<span class="er">(</span></span>
<span id="cb11-1678"><a href="#cb11-1678"></a>          <span class="kw">(</span><span class="ex">ln_1</span><span class="kw">)</span><span class="bu">:</span> LayerNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb11-1679"><a href="#cb11-1679"></a>          <span class="kw">(</span><span class="ex">attn</span><span class="kw">)</span><span class="bu">:</span> CausalSelfAttention<span class="er">(</span></span>
<span id="cb11-1680"><a href="#cb11-1680"></a>            <span class="kw">(</span><span class="ex">c_attn</span><span class="kw">)</span><span class="bu">:</span> Linear<span class="er">(</span><span class="va">in_features</span><span class="op">=</span>768, <span class="va">out_features</span><span class="op">=</span>2304, <span class="va">bias</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb11-1681"><a href="#cb11-1681"></a>            <span class="kw">(</span><span class="ex">c_proj</span><span class="kw">)</span><span class="bu">:</span> Linear<span class="er">(</span><span class="va">in_features</span><span class="op">=</span>768, <span class="va">out_features</span><span class="op">=</span>768, <span class="va">bias</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb11-1682"><a href="#cb11-1682"></a>            <span class="kw">(</span><span class="ex">attn_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb11-1683"><a href="#cb11-1683"></a>            <span class="kw">(</span><span class="ex">resid_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb11-1684"><a href="#cb11-1684"></a>          <span class="kw">)</span></span>
<span id="cb11-1685"><a href="#cb11-1685"></a>          <span class="kw">(</span><span class="ex">ln_2</span><span class="kw">)</span><span class="bu">:</span> LayerNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb11-1686"><a href="#cb11-1686"></a>          <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> MLP<span class="er">(</span></span>
<span id="cb11-1687"><a href="#cb11-1687"></a>            <span class="kw">(</span><span class="ex">c_fc</span><span class="kw">)</span><span class="bu">:</span> Linear<span class="er">(</span><span class="va">in_features</span><span class="op">=</span>768, <span class="va">out_features</span><span class="op">=</span>3072, <span class="va">bias</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb11-1688"><a href="#cb11-1688"></a>            <span class="kw">(</span><span class="ex">act_fn</span><span class="kw">)</span><span class="bu">:</span> GELU<span class="er">(</span><span class="va">approximate</span><span class="op">=</span><span class="st">'none'</span><span class="kw">)</span></span>
<span id="cb11-1689"><a href="#cb11-1689"></a>            <span class="kw">(</span><span class="ex">c_proj</span><span class="kw">)</span><span class="bu">:</span> Linear<span class="er">(</span><span class="va">in_features</span><span class="op">=</span>3072, <span class="va">out_features</span><span class="op">=</span>768, <span class="va">bias</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb11-1690"><a href="#cb11-1690"></a>            <span class="kw">(</span><span class="ex">dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb11-1691"><a href="#cb11-1691"></a>          <span class="kw">)</span></span>
<span id="cb11-1692"><a href="#cb11-1692"></a>        <span class="kw">)</span></span>
<span id="cb11-1693"><a href="#cb11-1693"></a>      <span class="kw">)</span></span>
<span id="cb11-1694"><a href="#cb11-1694"></a>      <span class="kw">(</span><span class="ex">ln_f</span><span class="kw">)</span><span class="bu">:</span> LayerNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb11-1695"><a href="#cb11-1695"></a>    <span class="kw">)</span></span>
<span id="cb11-1696"><a href="#cb11-1696"></a>    <span class="kw">(</span><span class="ex">lm_head</span><span class="kw">)</span><span class="bu">:</span> Linear<span class="er">(</span><span class="va">in_features</span><span class="op">=</span>768, <span class="va">out_features</span><span class="op">=</span>65, <span class="va">bias</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb11-1697"><a href="#cb11-1697"></a>  <span class="kw">)</span></span>
<span id="cb11-1698"><a href="#cb11-1698"></a><span class="kw">)</span></span>
<span id="cb11-1699"><a href="#cb11-1699"></a><span class="ex">[2024-07-17</span> 07:42:19.961066]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:359</span><span class="pp">]</span> <span class="at">-</span> â€¢ self.optimizer=AdamW <span class="er">(</span></span>
<span id="cb11-1700"><a href="#cb11-1700"></a><span class="ex">Parameter</span> Group 0</span>
<span id="cb11-1701"><a href="#cb11-1701"></a>    <span class="ex">amsgrad:</span> False</span>
<span id="cb11-1702"><a href="#cb11-1702"></a>    <span class="ex">betas:</span> <span class="er">(</span><span class="ex">0.9,</span> 0.95<span class="kw">)</span></span>
<span id="cb11-1703"><a href="#cb11-1703"></a>    <span class="ex">capturable:</span> False</span>
<span id="cb11-1704"><a href="#cb11-1704"></a>    <span class="ex">differentiable:</span> False</span>
<span id="cb11-1705"><a href="#cb11-1705"></a>    <span class="ex">eps:</span> 1e-08</span>
<span id="cb11-1706"><a href="#cb11-1706"></a>    <span class="ex">foreach:</span> None</span>
<span id="cb11-1707"><a href="#cb11-1707"></a>    <span class="ex">fused:</span> True</span>
<span id="cb11-1708"><a href="#cb11-1708"></a>    <span class="ex">lr:</span> 0.0006</span>
<span id="cb11-1709"><a href="#cb11-1709"></a>    <span class="ex">maximize:</span> False</span>
<span id="cb11-1710"><a href="#cb11-1710"></a>    <span class="ex">weight_decay:</span> 0.1</span>
<span id="cb11-1711"><a href="#cb11-1711"></a></span>
<span id="cb11-1712"><a href="#cb11-1712"></a><span class="ex">Parameter</span> Group 1</span>
<span id="cb11-1713"><a href="#cb11-1713"></a>    <span class="ex">amsgrad:</span> False</span>
<span id="cb11-1714"><a href="#cb11-1714"></a>    <span class="ex">betas:</span> <span class="er">(</span><span class="ex">0.9,</span> 0.95<span class="kw">)</span></span>
<span id="cb11-1715"><a href="#cb11-1715"></a>    <span class="ex">capturable:</span> False</span>
<span id="cb11-1716"><a href="#cb11-1716"></a>    <span class="ex">differentiable:</span> False</span>
<span id="cb11-1717"><a href="#cb11-1717"></a>    <span class="ex">eps:</span> 1e-08</span>
<span id="cb11-1718"><a href="#cb11-1718"></a>    <span class="ex">foreach:</span> None</span>
<span id="cb11-1719"><a href="#cb11-1719"></a>    <span class="ex">fused:</span> True</span>
<span id="cb11-1720"><a href="#cb11-1720"></a>    <span class="ex">lr:</span> 0.0006</span>
<span id="cb11-1721"><a href="#cb11-1721"></a>    <span class="ex">maximize:</span> False</span>
<span id="cb11-1722"><a href="#cb11-1722"></a>    <span class="ex">weight_decay:</span> 0.0</span>
<span id="cb11-1723"><a href="#cb11-1723"></a><span class="kw">)</span></span>
<span id="cb11-1724"><a href="#cb11-1724"></a><span class="ex">[2024-07-17</span> 07:42:19.988827]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:802</span><span class="pp">]</span> <span class="at">-</span> Startup time: 6.7125</span>
<span id="cb11-1725"><a href="#cb11-1725"></a>                <span class="ex">Training</span> Legend</span>
<span id="cb11-1726"><a href="#cb11-1726"></a><span class="ex">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“</span></span>
<span id="cb11-1727"><a href="#cb11-1727"></a><span class="ex">â”ƒ</span>    abbr     â”ƒ desc                           â”ƒ</span>
<span id="cb11-1728"><a href="#cb11-1728"></a><span class="ex">â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©</span></span>
<span id="cb11-1729"><a href="#cb11-1729"></a><span class="ex">â”‚</span>    step     â”‚ Current training iteration     â”‚</span>
<span id="cb11-1730"><a href="#cb11-1730"></a><span class="ex">â”‚</span>    loss     â”‚ Loss value                     â”‚</span>
<span id="cb11-1731"><a href="#cb11-1731"></a><span class="ex">â”‚</span>     dt      â”‚ Elapsed time per training step â”‚</span>
<span id="cb11-1732"><a href="#cb11-1732"></a><span class="ex">â”‚</span>     dtf     â”‚ Elapsed time per forward step  â”‚</span>
<span id="cb11-1733"><a href="#cb11-1733"></a><span class="ex">â”‚</span>     dtb     â”‚ Elapsed time per backward step â”‚</span>
<span id="cb11-1734"><a href="#cb11-1734"></a><span class="ex">â”‚</span>     sps     â”‚ Samples per second             â”‚</span>
<span id="cb11-1735"><a href="#cb11-1735"></a><span class="ex">â”‚</span> sps_per_gpu â”‚ Samples per second <span class="er">(</span><span class="ex">per</span> GPU<span class="kw">)</span>   <span class="ex">â”‚</span></span>
<span id="cb11-1736"><a href="#cb11-1736"></a><span class="ex">â”‚</span>     tps     â”‚ Tokens per second              â”‚</span>
<span id="cb11-1737"><a href="#cb11-1737"></a><span class="ex">â”‚</span> tps_per_gpu â”‚ Tokens per second <span class="er">(</span><span class="ex">per</span> GPU<span class="kw">)</span>    <span class="ex">â”‚</span></span>
<span id="cb11-1738"><a href="#cb11-1738"></a><span class="ex">â”‚</span>     mfu     â”‚ Model flops utilization        â”‚</span>
<span id="cb11-1739"><a href="#cb11-1739"></a><span class="ex">â”‚</span> train_loss  â”‚ Training loss value            â”‚</span>
<span id="cb11-1740"><a href="#cb11-1740"></a><span class="ex">â”‚</span>  val_loss   â”‚ Validation loss value          â”‚</span>
<span id="cb11-1741"><a href="#cb11-1741"></a><span class="ex">â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span></span>
<span id="cb11-1742"><a href="#cb11-1742"></a><span class="ex">[2024-07-17</span> 07:42:21.451865]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:820</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="st">'prompt'</span><span class="pp">]</span>: <span class="st">'What is an LLM?'</span></span>
<span id="cb11-1743"><a href="#cb11-1743"></a><span class="ex">[2024-07-17</span> 07:42:21.452667]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:824</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="st">'response'</span><span class="pp">]</span>:</span>
<span id="cb11-1744"><a href="#cb11-1744"></a><span class="ex">What</span> is an LLM<span class="pp">?</span>eelEl\'<span class="va">$nltPwBSWal</span>,<span class="kw">;</span><span class="ex">PWw</span> bbu<span class="dt">\'</span>HiyP<span class="dt">\'</span>FWwF <span class="kw">&amp;</span><span class="ex">AhW:ygrn</span> kk-<span class="dt">\'\'</span>KFlMwnlEfflkc,elpWaWtgml<span class="va">$Pgglhllw</span> lglhFllzczPAFHpeAAPPSltgkrWPPhlEMgcrN ggPWt-WPSSzHSkkrzzk.FFrtSSkgMll<span class="kw">&amp;</span><span class="ex">gFXr,hghaueaVPW-pHFF-gg,,,FF,,kbApgg</span> gg<span class="dt">\'</span>aWWzzkk<span class="dt">\'</span>a<span class="dt">\'</span>CggHl<span class="va">$bGeA</span>,FFk,,SF<span class="kw">;</span><span class="ex">UF,,aZ</span> <span class="kw">;</span><span class="ex">gglee$,k.US</span><span class="kw">&amp;</span><span class="ex">kg:S,,zVzzc</span></span>
<span id="cb11-1745"><a href="#cb11-1745"></a><span class="ex">[2024-07-17</span> 07:43:01.573073]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=10 loss=3.154310 dt=0.282833 dtf=0.005247 dtb=0.011417 sps=14.142633 sps_per_gpu=3.535658 tps=926851.609409 tps_per_gpu=231712.902352 mfu=46.288281 train_loss=4.125778 val_loss=4.128809</span>
<span id="cb11-1746"><a href="#cb11-1746"></a><span class="ex">[2024-07-17</span> 07:43:04.402750]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=20 loss=2.660851 dt=0.306263 dtf=0.005233 dtb=0.011419 sps=13.060678 sps_per_gpu=3.265170 tps=855944.613638 tps_per_gpu=213986.153409 mfu=45.934162 train_loss=4.125778 val_loss=4.128809</span>
<span id="cb11-1747"><a href="#cb11-1747"></a><span class="ex">[2024-07-17</span> 07:43:07.237507]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=30 loss=2.543283 dt=0.283021 dtf=0.005238 dtb=0.011245 sps=14.133211 sps_per_gpu=3.533303 tps=926234.088226 tps_per_gpu=231558.522057 mfu=45.966490 train_loss=4.125778 val_loss=4.128809</span>
<span id="cb11-1748"><a href="#cb11-1748"></a><span class="ex">[2024-07-17</span> 07:43:10.077248]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=40 loss=2.503963 dt=0.285001 dtf=0.005213 dtb=0.011471 sps=14.035061 sps_per_gpu=3.508765 tps=919801.749941 tps_per_gpu=229950.437485 mfu=45.963461 train_loss=4.125778 val_loss=4.128809</span>
<span id="cb11-1749"><a href="#cb11-1749"></a><span class="ex">[2024-07-17</span> 07:43:12.917039]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=50 loss=2.477469 dt=0.283532 dtf=0.005166 dtb=0.011294 sps=14.107763 sps_per_gpu=3.526941 tps=924566.380009 tps_per_gpu=231141.595002 mfu=45.984530 train_loss=4.125778 val_loss=4.128809</span>
<span id="cb11-1750"><a href="#cb11-1750"></a><span class="ex">[2024-07-17</span> 07:43:15.760749]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=60 loss=2.471083 dt=0.284630 dtf=0.005140 dtb=0.011224 sps=14.053326 sps_per_gpu=3.513332 tps=920998.786204 tps_per_gpu=230249.696551 mfu=45.985675 train_loss=4.125778 val_loss=4.128809</span>
<span id="cb11-1751"><a href="#cb11-1751"></a><span class="ex">[2024-07-17</span> 07:43:18.602785]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=70 loss=2.458894 dt=0.283926 dtf=0.005219 dtb=0.010383 sps=14.088155 sps_per_gpu=3.522039 tps=923281.352698 tps_per_gpu=230820.338174 mfu=45.998106 train_loss=4.125778 val_loss=4.128809</span>
<span id="cb11-1752"><a href="#cb11-1752"></a><span class="ex">[2024-07-17</span> 07:43:21.451433]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=80 loss=2.489088 dt=0.285537 dtf=0.005183 dtb=0.011373 sps=14.008683 sps_per_gpu=3.502171 tps=918073.060430 tps_per_gpu=229518.265108 mfu=45.983282 train_loss=4.125778 val_loss=4.128809</span>
<span id="cb11-1753"><a href="#cb11-1753"></a><span class="ex">[2024-07-17</span> 07:43:24.302241]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=90 loss=2.471990 dt=0.300767 dtf=0.005445 dtb=0.010290 sps=13.299337 sps_per_gpu=3.324834 tps=871585.359388 tps_per_gpu=217896.339847 mfu=45.737774 train_loss=4.125778 val_loss=4.128809</span>
<span id="cb11-1754"><a href="#cb11-1754"></a><span class="ex">[2024-07-17</span> 07:43:27.153275]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=100 loss=2.445556 dt=0.285869 dtf=0.005182 dtb=0.011251 sps=13.992403 sps_per_gpu=3.498101 tps=917006.151328 tps_per_gpu=229251.537832 mfu=45.743655 train_loss=4.125778 val_loss=4.128809</span>
<span id="cb11-1755"><a href="#cb11-1755"></a><span class="ex">[2024-07-17</span> 07:43:28.182553]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:820</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="st">'prompt'</span><span class="pp">]</span>: <span class="st">'What is an LLM?'</span></span>
<span id="cb11-1756"><a href="#cb11-1756"></a><span class="ex">[2024-07-17</span> 07:43:28.183179]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:824</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="st">'response'</span><span class="pp">]</span>:</span>
<span id="cb11-1757"><a href="#cb11-1757"></a></span>
<span id="cb11-1758"><a href="#cb11-1758"></a><span class="ex">What</span> is an LLM<span class="pp">?</span></span>
<span id="cb11-1759"><a href="#cb11-1759"></a></span>
<span id="cb11-1760"><a href="#cb11-1760"></a><span class="ex">Goupay</span> my winghimithell bls ger t bon sinthard ht omind be,</span>
<span id="cb11-1761"><a href="#cb11-1761"></a><span class="ex">And</span> lereind h py balithand frd oforondof wimon me hageas thinero mand,</span>
<span id="cb11-1762"><a href="#cb11-1762"></a><span class="ex">Thacanes,</span></span>
<span id="cb11-1763"><a href="#cb11-1763"></a><span class="ex">An</span> frift ghik med d herthecke ntore thack couthen ale, t thit ang d m t h chy me fache ag, wit my hathan glat ng</span>
<span id="cb11-1764"><a href="#cb11-1764"></a><span class="ex">[2024-07-17</span> 07:44:06.025837]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:760</span><span class="pp">]</span> <span class="at">-</span> Saving checkpoint to: /home/foremans/tmp/polaris-talk/outputs/runs/pytorch/DDP/2024-07-17/07-42-13</span>
<span id="cb11-1765"><a href="#cb11-1765"></a><span class="ex">[2024-07-17</span> 07:44:06.026607]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:761</span><span class="pp">]</span> <span class="at">-</span> Saving model to: /home/foremans/tmp/polaris-talk/outputs/runs/pytorch/DDP/2024-07-17/07-42-13/model.pth</span>
<span id="cb11-1766"><a href="#cb11-1766"></a><span class="ex">[2024-07-17</span> 07:44:07.682968]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">configs:141</span><span class="pp">]</span> <span class="at">-</span> Appending /home/foremans/tmp/polaris-talk/outputs/runs/pytorch/DDP/2024-07-17/07-42-13 to /home/foremans/tmp/polaris-talk/2024-07-17-073327/wordplay/src/ckpts/checkpoints.log</span>
<span id="cb11-1767"><a href="#cb11-1767"></a><span class="ex">[2024-07-17</span> 07:44:10.519506]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=110 loss=2.433923 dt=0.285038 dtf=0.005757 dtb=0.011762 sps=14.033209 sps_per_gpu=3.508302 tps=919680.367894 tps_per_gpu=229920.091974 mfu=45.762304 train_loss=2.439494 val_loss=2.478951</span>
<span id="cb11-1768"><a href="#cb11-1768"></a><span class="ex">[2024-07-17</span> 07:44:13.362148]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=120 loss=2.429014 dt=0.284445 dtf=0.005222 dtb=0.011486 sps=14.062460 sps_per_gpu=3.515615 tps=921597.361532 tps_per_gpu=230399.340383 mfu=45.788661 train_loss=2.439494 val_loss=2.478951</span>
<span id="cb11-1769"><a href="#cb11-1769"></a><span class="ex">[2024-07-17</span> 07:44:16.210694]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=130 loss=2.402059 dt=0.285559 dtf=0.005199 dtb=0.011765 sps=14.007633 sps_per_gpu=3.501908 tps=918004.211586 tps_per_gpu=229501.052897 mfu=45.794438 train_loss=2.439494 val_loss=2.478951</span>
<span id="cb11-1770"><a href="#cb11-1770"></a><span class="ex">[2024-07-17</span> 07:44:19.061546]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=140 loss=2.374062 dt=0.285476 dtf=0.005239 dtb=0.011453 sps=14.011662 sps_per_gpu=3.502916 tps=918268.297093 tps_per_gpu=229567.074273 mfu=45.800956 train_loss=2.439494 val_loss=2.478951</span>
<span id="cb11-1771"><a href="#cb11-1771"></a><span class="ex">[2024-07-17</span> 07:44:21.917283]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=150 loss=2.365385 dt=0.285846 dtf=0.005125 dtb=0.011320 sps=13.993568 sps_per_gpu=3.498392 tps=917082.475791 tps_per_gpu=229270.618948 mfu=45.800900 train_loss=2.439494 val_loss=2.478951</span>
<span id="cb11-1772"><a href="#cb11-1772"></a><span class="ex">[2024-07-17</span> 07:44:24.771924]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=160 loss=2.317337 dt=0.280788 dtf=0.005173 dtb=0.011249 sps=14.245602 sps_per_gpu=3.561401 tps=933599.792506 tps_per_gpu=233399.948127 mfu=45.883340 train_loss=2.439494 val_loss=2.478951</span>
<span id="cb11-1773"><a href="#cb11-1773"></a><span class="ex">[2024-07-17</span> 07:44:27.626812]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=170 loss=2.256231 dt=0.284973 dtf=0.005141 dtb=0.011299 sps=14.036416 sps_per_gpu=3.509104 tps=919890.544506 tps_per_gpu=229972.636126 mfu=45.889069 train_loss=2.439494 val_loss=2.478951</span>
<span id="cb11-1774"><a href="#cb11-1774"></a><span class="ex">[2024-07-17</span> 07:44:30.480952]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=180 loss=2.216419 dt=0.286555 dtf=0.005180 dtb=0.011402 sps=13.958906 sps_per_gpu=3.489726 tps=914810.852170 tps_per_gpu=228702.713043 mfu=45.868857 train_loss=2.439494 val_loss=2.478951</span>
<span id="cb11-1775"><a href="#cb11-1775"></a><span class="ex">[2024-07-17</span> 07:44:33.337342]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=190 loss=2.145123 dt=0.291456 dtf=0.005409 dtb=0.019347 sps=13.724205 sps_per_gpu=3.431051 tps=899429.467247 tps_per_gpu=224857.366812 mfu=45.773849 train_loss=2.439494 val_loss=2.478951</span>
<span id="cb11-1776"><a href="#cb11-1776"></a><span class="ex">[2024-07-17</span> 07:44:36.194584]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=200 loss=2.068149 dt=0.285703 dtf=0.005153 dtb=0.011286 sps=14.000555 sps_per_gpu=3.500139 tps=917540.393411 tps_per_gpu=229385.098353 mfu=45.778791 train_loss=2.439494 val_loss=2.478951</span>
<span id="cb11-1777"><a href="#cb11-1777"></a><span class="ex">[2024-07-17</span> 07:44:37.224149]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:820</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="st">'prompt'</span><span class="pp">]</span>: <span class="st">'What is an LLM?'</span></span>
<span id="cb11-1778"><a href="#cb11-1778"></a><span class="ex">[2024-07-17</span> 07:44:37.224745]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:824</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="st">'response'</span><span class="pp">]</span>:</span>
<span id="cb11-1779"><a href="#cb11-1779"></a></span>
<span id="cb11-1780"><a href="#cb11-1780"></a><span class="ex">What</span> is an LLM<span class="pp">?</span></span>
<span id="cb11-1781"><a href="#cb11-1781"></a></span>
<span id="cb11-1782"><a href="#cb11-1782"></a><span class="ex">LORTESS</span> LA:</span>
<span id="cb11-1783"><a href="#cb11-1783"></a><span class="ex">No,</span> sighappat selace<span class="pp">?</span> don downd sourciceans note cancen up sof liond</span>
<span id="cb11-1784"><a href="#cb11-1784"></a><span class="ex">This</span> and my man, werame, of re thee</span>
<span id="cb11-1785"><a href="#cb11-1785"></a><span class="ex">Thise</span> not will I on land brond sul me a fingore<span class="pp">?</span></span>
<span id="cb11-1786"><a href="#cb11-1786"></a></span>
<span id="cb11-1787"><a href="#cb11-1787"></a><span class="ex">FLER:</span></span>
<span id="cb11-1788"><a href="#cb11-1788"></a><span class="ex">Tisint</span> your not nare lame o igen,-to brorst.</span>
<span id="cb11-1789"><a href="#cb11-1789"></a></span>
<span id="cb11-1790"><a href="#cb11-1790"></a><span class="ex">SamERS:</span></span>
<span id="cb11-1791"><a href="#cb11-1791"></a><span class="ex">Sin:</span></span>
<span id="cb11-1792"><a href="#cb11-1792"></a><span class="ex">I\'l</span> hell she lor hen w</span>
<span id="cb11-1793"><a href="#cb11-1793"></a><span class="ex">[2024-07-17</span> 07:45:14.409129]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:760</span><span class="pp">]</span> <span class="at">-</span> Saving checkpoint to: /home/foremans/tmp/polaris-talk/outputs/runs/pytorch/DDP/2024-07-17/07-42-13</span>
<span id="cb11-1794"><a href="#cb11-1794"></a><span class="ex">[2024-07-17</span> 07:45:14.409820]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:761</span><span class="pp">]</span> <span class="at">-</span> Saving model to: /home/foremans/tmp/polaris-talk/outputs/runs/pytorch/DDP/2024-07-17/07-42-13/model.pth</span>
<span id="cb11-1795"><a href="#cb11-1795"></a><span class="ex">[2024-07-17</span> 07:45:16.366935]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">configs:141</span><span class="pp">]</span> <span class="at">-</span> Appending /home/foremans/tmp/polaris-talk/outputs/runs/pytorch/DDP/2024-07-17/07-42-13 to /home/foremans/tmp/polaris-talk/2024-07-17-073327/wordplay/src/ckpts/checkpoints.log</span>
<span id="cb11-1796"><a href="#cb11-1796"></a><span class="ex">[2024-07-17</span> 07:45:19.245061]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=210 loss=1.982169 dt=0.283305 dtf=0.005223 dtb=0.011284 sps=14.119042 sps_per_gpu=3.529760 tps=925305.515083 tps_per_gpu=231326.378771 mfu=45.822019 train_loss=2.045786 val_loss=2.148510</span>
<span id="cb11-1797"><a href="#cb11-1797"></a><span class="ex">[2024-07-17</span> 07:45:22.092430]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=220 loss=1.897731 dt=0.284759 dtf=0.005217 dtb=0.011187 sps=14.046945 sps_per_gpu=3.511736 tps=920580.608106 tps_per_gpu=230145.152026 mfu=45.837327 train_loss=2.045786 val_loss=2.148510</span>
<span id="cb11-1798"><a href="#cb11-1798"></a><span class="ex">[2024-07-17</span> 07:45:24.942639]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=230 loss=1.817213 dt=0.285266 dtf=0.005208 dtb=0.011446 sps=14.022003 sps_per_gpu=3.505501 tps=918945.985503 tps_per_gpu=229736.496376 mfu=45.842940 train_loss=2.045786 val_loss=2.148510</span>
<span id="cb11-1799"><a href="#cb11-1799"></a><span class="ex">[2024-07-17</span> 07:45:27.797910]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=240 loss=1.779287 dt=0.285465 dtf=0.005189 dtb=0.011220 sps=14.012250 sps_per_gpu=3.503062 tps=918306.793546 tps_per_gpu=229576.698387 mfu=45.844800 train_loss=2.045786 val_loss=2.148510</span>
<span id="cb11-1800"><a href="#cb11-1800"></a><span class="ex">[2024-07-17</span> 07:45:30.653597]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=250 loss=1.704220 dt=0.289284 dtf=0.005471 dtb=0.010346 sps=13.827253 sps_per_gpu=3.456813 tps=906182.836379 tps_per_gpu=226545.709095 mfu=45.785926 train_loss=2.045786 val_loss=2.148510</span>
<span id="cb11-1801"><a href="#cb11-1801"></a><span class="ex">[2024-07-17</span> 07:45:33.512769]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=260 loss=1.671318 dt=0.287679 dtf=0.005125 dtb=0.011250 sps=13.904380 sps_per_gpu=3.476095 tps=911237.442617 tps_per_gpu=227809.360654 mfu=45.758182 train_loss=2.045786 val_loss=2.148510</span>
<span id="cb11-1802"><a href="#cb11-1802"></a><span class="ex">[2024-07-17</span> 07:45:36.373461]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=270 loss=1.650952 dt=0.298661 dtf=0.005118 dtb=0.011520 sps=13.393107 sps_per_gpu=3.348277 tps=877730.651421 tps_per_gpu=219432.662855 mfu=45.565875 train_loss=2.045786 val_loss=2.148510</span>
<span id="cb11-1803"><a href="#cb11-1803"></a><span class="ex">[2024-07-17</span> 07:45:39.236930]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=280 loss=1.573242 dt=0.285970 dtf=0.005171 dtb=0.011290 sps=13.987477 sps_per_gpu=3.496869 tps=916683.279847 tps_per_gpu=229170.819962 mfu=45.587333 train_loss=2.045786 val_loss=2.148510</span>
<span id="cb11-1804"><a href="#cb11-1804"></a><span class="ex">[2024-07-17</span> 07:45:42.100605]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=290 loss=1.533265 dt=0.286487 dtf=0.005432 dtb=0.011288 sps=13.962259 sps_per_gpu=3.490565 tps=915030.617828 tps_per_gpu=228757.654457 mfu=45.598392 train_loss=2.045786 val_loss=2.148510</span>
<span id="cb11-1805"><a href="#cb11-1805"></a><span class="ex">[2024-07-17</span> 07:45:44.964424]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:885</span><span class="pp">]</span> <span class="at">-</span> step=300 loss=1.492064 dt=0.288480 dtf=0.005355 dtb=0.011480 sps=13.865774 sps_per_gpu=3.466443 tps=908707.340870 tps_per_gpu=227176.835218 mfu=45.576766 train_loss=2.045786 val_loss=2.148510</span>
<span id="cb11-1806"><a href="#cb11-1806"></a><span class="ex">[2024-07-17</span> 07:45:45.995833]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:820</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="st">'prompt'</span><span class="pp">]</span>: <span class="st">'What is an LLM?'</span></span>
<span id="cb11-1807"><a href="#cb11-1807"></a><span class="ex">[2024-07-17</span> 07:45:45.996497]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">trainer:824</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="st">'response'</span><span class="pp">]</span>:</span>
<span id="cb11-1808"><a href="#cb11-1808"></a></span>
<span id="cb11-1809"><a href="#cb11-1809"></a><span class="ex">What</span> is an LLM<span class="pp">?</span></span>
<span id="cb11-1810"><a href="#cb11-1810"></a></span>
<span id="cb11-1811"><a href="#cb11-1811"></a><span class="ex">RICHMORD:</span></span>
<span id="cb11-1812"><a href="#cb11-1812"></a><span class="ex">Char</span> stire<span class="pp">?</span> how in those are name the range hone.</span>
<span id="cb11-1813"><a href="#cb11-1813"></a></span>
<span id="cb11-1814"><a href="#cb11-1814"></a><span class="ex">GLOUCESTER:</span></span>
<span id="cb11-1815"><a href="#cb11-1815"></a><span class="ex">Nay,</span> in lond<span class="st">'s time the palt are worder more</span></span>
<span id="cb11-1816"><a href="#cb11-1816"></a><span class="st">That wilt in the purpose be a pey</span></span>
<span id="cb11-1817"><a href="#cb11-1817"></a><span class="st">And thou thine onter hands, and the which broth.</span></span>
<span id="cb11-1818"><a href="#cb11-1818"></a></span>
<span id="cb11-1819"><a href="#cb11-1819"></a><span class="st">ELBOWINCA:</span></span>
<span id="cb11-1820"><a href="#cb11-1820"></a><span class="st">At lie my lord with the me an arms be a s</span></span>
<span id="cb11-1821"><a href="#cb11-1821"></a><span class="st">[2024-07-17 07:46:23.549987][INFO][trainer:760] - Saving checkpoint to: /home/foremans/tmp/polaris-talk/outputs/runs/pytorch/DDP/2024-07-17/07-42-13</span></span>
<span id="cb11-1822"><a href="#cb11-1822"></a><span class="st">[2024-07-17 07:46:23.550696][INFO][trainer:761] - Saving model to: /home/foremans/tmp/polaris-talk/outputs/runs/pytorch/DDP/2024-07-17/07-42-13/model.pth</span></span>
<span id="cb11-1823"><a href="#cb11-1823"></a><span class="st">[2024-07-17 07:46:25.496559][INFO][configs:141] - Appending /home/foremans/tmp/polaris-talk/outputs/runs/pytorch/DDP/2024-07-17/07-42-13 to /home/foremans/tmp/polaris-talk/2024-07-17-073327/wordplay/src/ckpts/checkpoints.log</span></span>
<span id="cb11-1824"><a href="#cb11-1824"></a><span class="st">[2024-07-17 07:46:28.374854][INFO][trainer:885] - step=310 loss=1.444200 dt=0.299907 dtf=0.005333 dtb=0.010637 sps=13.337481 sps_per_gpu=3.334370 tps=874085.133345 tps_per_gpu=218521.283336 mfu=45.384395 train_loss=1.495372 val_loss=1.713714</span></span>
<span id="cb11-1825"><a href="#cb11-1825"></a><span class="st">[2024-07-17 07:46:31.223079][INFO][trainer:885] - step=320 loss=1.429350 dt=0.285238 dtf=0.005245 dtb=0.011485 sps=14.023353 sps_per_gpu=3.505838 tps=919034.479880 tps_per_gpu=229758.619970 mfu=45.435743 train_loss=1.495372 val_loss=1.713714</span></span>
<span id="cb11-1826"><a href="#cb11-1826"></a><span class="st">[2024-07-17 07:46:34.074957][INFO][trainer:885] - step=330 loss=1.362220 dt=0.285027 dtf=0.005165 dtb=0.011407 sps=14.033736 sps_per_gpu=3.508434 tps=919714.904826 tps_per_gpu=229928.726207 mfu=45.485355 train_loss=1.495372 val_loss=1.713714</span></span>
<span id="cb11-1827"><a href="#cb11-1827"></a><span class="st">[2024-07-17 07:46:36.929464][INFO][trainer:885] - step=340 loss=1.350888 dt=0.284436 dtf=0.005199 dtb=0.011287 sps=14.062893 sps_per_gpu=3.515723 tps=921625.744709 tps_per_gpu=230406.436177 mfu=45.539549 train_loss=1.495372 val_loss=1.713714</span></span>
<span id="cb11-1828"><a href="#cb11-1828"></a><span class="st">```</span></span>
<span id="cb11-1829"><a href="#cb11-1829"></a></span>
<span id="cb11-1830"><a href="#cb11-1830"></a><span class="st">## {{&lt; fa brands github &gt;}} [`wordplay`](https://github.com/saforem2/wordplay): Example \[[video](https://asciinema.org/a/668462)\] {background-color="#121314"}</span></span>
<span id="cb11-1831"><a href="#cb11-1831"></a></span>
<span id="cb11-1832"><a href="#cb11-1832"></a><span class="st">::: {#fig-wordplay-asciinema}</span></span>
<span id="cb11-1833"><a href="#cb11-1833"></a></span>
<span id="cb11-1834"><a href="#cb11-1834"></a><span class="st">&lt;script src="https://asciinema.org/a/668462.js" id="asciicast-668462" async="true"&gt;&lt;/script&gt;</span></span>
<span id="cb11-1835"><a href="#cb11-1835"></a></span>
<span id="cb11-1836"><a href="#cb11-1836"></a><span class="st">Training a LLM to talk like Shakespeare using [saforem2/`wordplay` ğŸ®ğŸ’¬](https://github.com/saforem2/wordplay)</span></span>
<span id="cb11-1837"><a href="#cb11-1837"></a><span class="st">:::</span></span>
<span id="cb11-1838"><a href="#cb11-1838"></a></span>
<span id="cb11-1839"><a href="#cb11-1839"></a><span class="st"># â¤ï¸ Thank you! {background-color="white"}</span></span>
<span id="cb11-1840"><a href="#cb11-1840"></a></span>
<span id="cb11-1841"><a href="#cb11-1841"></a><span class="st">- Organizers</span></span>
<span id="cb11-1842"><a href="#cb11-1842"></a><span class="st">- Feel free to reach out!</span></span>
<span id="cb11-1843"><a href="#cb11-1843"></a></span>
<span id="cb11-1844"><a href="#cb11-1844"></a><span class="st">    &lt;split even&gt;</span></span>
<span id="cb11-1845"><a href="#cb11-1845"></a></span>
<span id="cb11-1846"><a href="#cb11-1846"></a><span class="st">    [&lt;i class="fas fa-home"&gt;&lt;/i&gt;](https://samforeman.me)</span></span>
<span id="cb11-1847"><a href="#cb11-1847"></a><span class="st">    [&lt;i class="far fa-paper-plane"&gt;&lt;/i&gt;](mailto:foremans@anl.gov)</span></span>
<span id="cb11-1848"><a href="#cb11-1848"></a><span class="st">    [&lt;i class="fab fa-twitter"&gt;&lt;/i&gt;](https://www.twitter.com/saforem2)</span></span>
<span id="cb11-1849"><a href="#cb11-1849"></a></span>
<span id="cb11-1850"><a href="#cb11-1850"></a><span class="st">    &lt;/split&gt;</span></span>
<span id="cb11-1851"><a href="#cb11-1851"></a></span>
<span id="cb11-1852"><a href="#cb11-1852"></a><span class="st">::: {.callout-note icon=false title="Acknowledgements" collapse="false"}</span></span>
<span id="cb11-1853"><a href="#cb11-1853"></a><span class="st">This research used resources of the Argonne Leadership Computing Facility,</span></span>
<span id="cb11-1854"><a href="#cb11-1854"></a><span class="st">which is a DOE Office of Science User Facility supported under Contract</span></span>
<span id="cb11-1855"><a href="#cb11-1855"></a><span class="st">DE-AC02-06CH11357</span></span>
<span id="cb11-1856"><a href="#cb11-1856"></a><span class="st">:::</span></span>
<span id="cb11-1857"><a href="#cb11-1857"></a></span>
<span id="cb11-1858"><a href="#cb11-1858"></a></span>
<span id="cb11-1859"><a href="#cb11-1859"></a><span class="st"># ğŸ““ References {background-color="white"}</span></span>
<span id="cb11-1860"><a href="#cb11-1860"></a></span>
<span id="cb11-1861"><a href="#cb11-1861"></a><span class="st">- Title slide (Tetris animation) from: &lt;https://emilhvitfeldt.github.io/quarto-iframe-examples/tetris/index.html&gt;</span></span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/saforem2/personal_site/blob/main/talks/ai-for-science-2024/index.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/saforem2/personal_site/edit/main/talks/ai-for-science-2024/index.qmd" class="toc-action"><i class="bi empty"></i>Edit this page</a></li><li><a href="https://github.com/saforem2/personal_site/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div></div></footer><script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>