# üßëüèª‚Äçüíª Sam Foreman‚Äôs R√©sum√©
Sam Foreman
2025-04-26

- [üë§ About](#bust_in_silhouette-about)
- [üéì Education](#mortar_board-education)
- [üëî Professional Experience](#necktie-professional-experience)
- [üìö Publications](#books-publications)
- [üèÜ Awards and Honors](#trophy-awards-and-honors)
- [ü¶ú Talks](#parrot-talks)
- [üé™ Events](#circus_tent-events)
- [üìì References](#notebook-references)

## üë§ About

Computational Scientist at Argonne National Laboratory.  
Scaling AI for science on supercomputers.  
[samforeman.me](https://samforeman.me)
[GitHub](https://github.com/saforem2) ‚Ä¢ [Google
Scholar](https://scholar.google.com/citations?user=vV_1zDwAAAAJ&hl=en) ‚Ä¢
[ORCID](https://orcid.org/0000-0002-9981-0876) ‚Ä¢
[Twitter](https://twitter.com/saforem2)

## üéì Education

- **Ph.D., Physics**  
  *University of Iowa* \| 2015‚Äì2019
  - [*Learning Better Physics: A Machine Learning Approach to Lattice
    Gauge
    Theory*](https://www.proquest.com/openview/95d7f7c12da8da8aa5ead3ac0f6ca0e8/1?cbl=18750&diss=y&pq-origsite=gscholar)
- **B.S. in Engineering Physics**  
  *University of Illinois at Urbana-Champaign* \| 2010‚Äì2015
  - [Energy Storage in Quantum Resonators (US Patent
    \#US9741492B2)](https://patents.google.com/patent/US9741492B2/en)
- **B.S. in Applied Mathematics**  
  *University of Illinois at Urbana-Champaign* \| 2010‚Äì2015

## üëî Professional Experience

- **Assistant Computational Scientist**
  - *Argonne National Laboratory*, Leadership Computing Facility (ALCF)
    Lemont, IL \| 2022‚ÄìPresent
    - Research lead on scaling large language models (LLMs) and
      generative AI for science on supercomputers (Aurora, Frontier,
      LUMI, Leonardo, ‚Ä¶).
      - Co-lead the Models and Pretraining team of the
        [AuroraGPT](https://auroragpt.anl.gov) project
    - Optimize large-scale training of foundation models and language
      models for scientific applications.
    - Collaborate with interdisciplinary teams to enhance simulation
      efficiency and scalability
    - Focus on AI and HPC for scientific applications, including:
      - Training large language models on supercomputers
      - Genome scale language models (GenSLMs) for studying SARS-CoV-2
        evolutionary dynamics
      - Direct Preference Optimization (DPO) for multimodal protein
        design workflows
      - Climate modeling and weather forecasting using foundation models
      - Developing improved sampling algorithms for lattice quantum
        chromodynamics (QCD)
    - <https://www.alcf.anl.gov/about/people/sam-foreman>
- **Postdoctoral Researcher**
  - *Argonne National Laboratory*, Leadership Computing Facility (ALCF)
    Lemont, IL \| 2019 ‚Äì 2022
    - Applied deep learning to lattice gauge theory and quantum field
      simulations.
    - Developed ML-enhanced Monte Carlo methods for QCD
      ([l2hmc-qcd](https://github.com/saforem2/l2hmc-qcd)).
    - Engaged in AI-for-Science collaborations with national labs and
      university partners.
- **Graduate Researcher (DOE SCGSR Fellowship)**
  - *Argonne National Laboratory*, Mathematics and Computer Sciences
    Division (MCS)  
    Lemont, IL \| 2018 ‚Äì 2019
    - Development of [l2hmc-qcd](https://github.com/saforem2/l2hmc-qcd)
      in collaboration with ALCF for my PhD Thesis research

## üìö Publications

> [!NOTE]
>
> <span style="color:#4582ec;">You can find a full list of my
> publications on my [Google
> Scholar](https://scholar.google.com/citations?user=vV_1zDwAAAAJ&hl=en)</span>

1.  üåé [<span class="highlight-green">**AERIS**</span>: **Argonne Earth
    Systems Model for Reliable and Skillful
    Predictions**](https://arxiv.org/abs/2509.13523) (Hatanp√§√§ et al.
    (2025))
    - ‚ú® [*2025 ACM Gordon Bell Prize for Climate Modeling
      Finalist*](https://awards.acm.org/bell-climate)
2.  [HiPerRAG: High-Performance Retrieval Augmented Generation for
    Scientific Insights](https://arxiv.org/abs/2505.04846) (Gokdemir et
    al. (2025))
3.  [MOFA: Discovering Materials for Carbon Capture with a GenAI and
    Simulation-Based Workflow](https://arxiv.org/abs/2501.10651) (Yan et
    al. (2025))
4.  üß™ [<span class="highlight-pink">**MProt-DPO**</span>: **Breaking
    the ExaFLOPS Barrier for Multimodal Protein Design with
    DPO**](https://doi.org/10.1109/SC41406.2024.00013) (Dharuman et al.
    (2024))
    - üåü [*2024 ACM Gordon Bell
      Finalist*](https://sc24.supercomputing.org/2024/10/presenting-the-finalists-for-the-2024-gordon-bell-prize/)
5.  [Intro to HPC Bootcamp: Engaging New Communities Through Energy
    Justice Projects](https://jocse.org/downloads/jocse-15-1-10.pdf)
    (Leung et al. (2024))
6.  [Thorough Characterization and Analysis of Large Transformer Model
    Training At-Scale](https://doi.org/10.1145/3639034) (Cheng et al.
    (2024))
7.  [MLMC: Machine Learning Monte Carlo for Lattice Gauge
    Theory](https://arxiv.org/abs/2312.08936) (Sam Foreman, Jin, and
    Osborn (2023))
8.  [Protein Generation via Genome-scale Language Models with
    Bio-physical
    Scoring](https://dl.acm.org/doi/abs/10.1145/3624062.3626087)
    (Dharuman et al. (2023))
9.   [DeepSpeed4Science Initiative: Enabling Large-Scale Scientific
    Discovery](https://arxiv.org/abs/2310.04610) (Song et al. (2023))
    - [üì∞ DeepSpeed4Science.ai Blog
      Post](https://www.deepspeed.ai/deepspeed4science/#new-megatron-deepspeed-for-large-scale-ai4science-model-training)
    - [üöÇ Loooooooong Sequence
      Lengths](../../posts/AuroraGPT/long-sequences/index.qmd)
10. [Comprehensive Performance Study of LLMs on Novel AI
    Accelerators](https://arxiv.org/abs/2310.04607) (Emani et al.
    (2023))
11. [Exploratory Analysis of Climate Data with
    `ClimRR`](https://saforem2.github.io/climate-analysis), [Intro to
    HPC Bootcamp @
    NERSC](https://github.com/NERSC/intro-HPC-bootcamp-2023) (Sam
    Foreman (2023))
12. üß¨ [<span class="highlight">**GenSLMs**</span>: **Genome-scale
    language models reveal SARS-Cov-2 evolutionary
    dynamics**](https://www.biorxiv.org/content/10.1101/2022.10.10.511571v1.abstract)
    (Zvyagin et al. (2023))
    - Winner of the [üèÜ *ACM Gordon Bell Special Prize for High
      Performance Computing-Based COVID-19
      Research*](https://www.acm.org/media-center/2022/november/gordon-bell-special-prize-covid-research-2022)
13. [Lattice QCD and Particle Physics](https://arxiv.org/abs/2207.07641)
    (Kronfeld et al. (2022))
14. [Applications of ML to Lattice
    QFT](https://arxiv.org/abs/2202.05838) (Boyda et al. (2022))
15. [LeapFrogLayers: Trainable Framework for Effective
    Sampling](https://arxiv.org/abs/2112.01582) (Sam Foreman et al.
    (2021))
16. [HMC with Normalizing Flows](https://arxiv.org/abs/2112.01586)
    \[[slides](https://indico.cern.ch/event/1006302/contributions/4380743/)\]
    (Sam Foreman et al. (2021))
17. [Deep Learning Hamiltonian Monte
    Carlo](https://arxiv.org/abs/2105.03418) \[[+
    poster](https://simdl.github.io/posters/57-supp_DLHMC_Foreman_SimDL-ICLR2021_poster1.pdf)\]
    (Sam Foreman, Jin, and C. (2021))
18. [Machine Learning and Neural Networks for Field
    Theory](https://bit.ly/snowmass_ml2020) (Sam Foreman, Jin, and
    Osborn (2020))
19. [Examples of renormalization group transformations for image
    sets](https://journals.aps.org/pre/abstract/10.1103/PhysRevE.98.052129)
    (Samuel Foreman et al. (2018))
20. [RG inspired Machine Learning for lattice field
    theory](https://arxiv.org/abs/1710.02079) (Sam Foreman et al.
    (2018))
21. [Large Energy Density in Three-Plate Nanocapacitors due to Coulomb
    Blockade](https://doi.org/10.1063/1.5009698) (Hubler et al. (2018))
22. [Superconductivity of In and Sn
    Samples](https://doi.org/10.1063/1.4896340) (Deamont and Foreman
    (2014))

## üèÜ Awards and Honors

- Nominated to serve on the US [**Coordinating Panel for Software and
  Computing**](https://imfisk.github.io/CPSC/) by the Division of
  Particles and Fields of the American Physical Society (APS).

- **Finalist, ACM Gordon Bell Prize in Climate Modeling**, 2025

  - Recognized for our work on  
    üåé **AERIS** (Hatanp√§√§ et al. (2025)): The first billion-parameter
    pixel-level diffusion model for global weather and
    subseasonal-to-seasonal forecasting. Trained efficiently at scales
    from 1.3‚Äì80B parameters with our sequence-window parallelism (SWiPe)
    strategy, we achieve a sustained mixed-precision performance of
    10.21 ExaFLOPS and peak performance of 11.21 ExaFLOPS, scaling to
    10,080 nodes (120,960 GPUs) on the Aurora supercomputer.

- **Finalist, ACM Gordon Bell Prize**, 2024

  - Acknowledged for the MProt-DPO (Dharuman et al. (2024)) project,
    which achieved over 4 ExaFLOP sustained performance in multimodal
    protein design workflows using Direct Preference Optimization.
    - [Argonne team breaks new ground in AI-driven protein design ‚Äì
      Argonne @
      SC](https://sc.cels.anl.gov/gordon-bell-argonne-team-breaks-new-ground-in-ai-driven-protein-design/)

- **ACM Gordon Bell Special Prize for High Performance Computing-Based
  COVID-19 Research**, 2022

  - Recognized for contributions to the GenSLMs (Zvyagin et al. (2023))
    project, which developed genome-scale language models to study
    SARS-CoV-2 evolutionary dynamics.
    - [ACM Gordon Bell Special Prize for HPC-Based COVID-19 Research
      Awarded to Team for Modelling How Pandemic-Causing Viruses,
      Especially SARS-CoV-2, are Identified and
      Classified](https://www.acm.org/media-center/2022/november/gordon-bell-special-prize-covid-research-2022)

- **DOE Office of Science Graduate Student Research Fellow**, 2018

  - Awarded by the Department of Energy for outstanding research
    contributions during graduate studies.

## ü¶ú Talks

> [!NOTE]
>
> \[You can see all of my talks online at <https://samforeman.me/talks/>

- 2025-:
  - 09: [Scientific AI at Scale: AI for
    Science](../../talks/openskai25/ai4science/index.html) @ [Open SkAI
    2025](https://www.openskai-conference.org)
  - 09: [Scientific AI at Scale: Distributed
    Training](../../talks/openskai25/training/index.html) @ [Open SkAI
    2025](https://www.openskai-conference.org/)
  - 07: [Large Scale Training on Diverse
    Accelerators](../../talks/AuroraGPT-SIAM25/index.html) @ [Scalable
    Deep Learning, SIAM
    AN2025](https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=84772)
  - 05: [LLMs on Aurora: üåå
    AuroraGPT](../../talks/incite-hackathon-2025/AuroraGPT/index.html) @
    [2025 ALCF INCITE GPU
    Hackathon](https://www.alcf.anl.gov/events/alcf-incite-gpu-hackathon)
  - 05: [LLMs on Aurora: üçã
    ezpz](../../talks/incite-hackathon-2025/ezpz/index.html) @ [2025
    ALCF INCITE GPU
    Hackathon](https://www.alcf.anl.gov/events/alcf-incite-gpu-hackathon)
  - 02: [AuroraGPT: Foundation Models for
    Science](../../talks/aurora-gpt-fm-for-electric-grid/index.html) @
    [Foundation Models for the Electric
    Grid](https://www.alcf.anl.gov/alcf-ai-science-training-series)
- 2024-:
  - 11: [Parallel Training
    Methods](../../talks/ai-for-science-2024/index.html) @
    [AI-for-Science on
    Supercomputers](https://www.alcf.anl.gov/alcf-ai-science-training-series)
  - 10:
    [AuroraGPT](../../talks/AuroraGPT/alcf-hpc-workshop-2024/index.html)
    @ [2024 ALCF Hands-On HPC
    Workshop](https://www.alcf.anl.gov/events/2024-alcf-hands-hpc-workshop)
  - 10: [Machine Learning and Foundation Models at
    Scale](../../talks/alcf-hpc-workshop-2024/index.html) @ [2024 ALCF
    Hands-On HPC
    Workshop](https://www.alcf.anl.gov/events/2024-alcf-hands-hpc-workshop)
  - 09: [AuroraGPT](../../talks/hpc-user-forum/index.html) @ [HPC User
    Forum, 2024](https://www.hpcuserforum.com/hpc-user-forum-fall-2024/)
  - 08: [Training LLMs at Scale](../../talks/llms-at-scale/) @ [ATPESC,
    2024](https://extremecomputingtraining.anl.gov/atpesc-2024/)
  - 07: [LLMs on
    Polaris](https://samforeman.me/talks/llms-on-polaris/slides) @
    [Center for Scientific Foundation Models, Summer School
    24‚Äô](https://scifm.ai/summer_school.html)
  - 03: [Parallel Training
    Techniques](https://github.com/saforem2/parallel-training-slides) @
    [AI-4-Science Training
    Series](https://github.com/argonne-lcf/ai-science-training-series/tree/main/06_parallel_training)
  - 02: [LLMs from
    Scratch](https://saforem2.github.io/llm-workshop-talk) @ [LLM
    Tutorial Workshop](https://github.com/argonne-lcf/llm-workshop)
- 2023-:
  - 11: [Creating Small(-ish)
    LLMs](https://saforem2.github.io/LLM-tutorial) @ [LLM Tutorial
    Workshop (1)](https://github.com/brettin/llm_tutorial)
  - 10: [Exascale Science on
    Aurora](https://saforem2.github.io/oneapi-talk) @ [Intel oneAPI
    Workshop @
    UIC](https://www.alcf.anl.gov/events/alcf-hands-hpc-workshop)
  - 10: [LLM Lunch Talk](https://saforem2.github.io/llm-lunch-talk) @
    [ALCF Hands On HPC
    Workshop](https://www.alcf.anl.gov/events/alcf-hands-hpc-workshop)
  - 08: [Scaling LLMs for
    Science](https://saforem2.github.io/scaling4science) @
    [Data-Intensive Computing + AI/ML at
    Scale](https://events.cels.anl.gov/event/426/overview)
  - 07: [MLMC: Machine Learning Monte
    Carlo](https://saforem2.github.io/lattice23) @ [Lattice
    2023](https://indico.fnal.gov/event/57249/contributions/271305/)
  - 07: [Generative Modeling and Efficient
    Sampling](https://saforem2.github.io/lqcd-pasc23/) @
    [PASC23](https://pasc23.pasc-conference.org/)
  - 04: [Efficient Sampling for
    LGT](https://saforem2.github.io/deep-fridays) @ [Deep Fridays @ U.
    Bologna](https://www.cs.unibo.it/~asperti/deep_fridays.html)
- 2022-:
  - 11: [Large Scale
    Training](https://saforem2.github.io/ai4sci-large-scale-training) @
    [AI4Science on Supercomputers
    (ALCF)](https://github.com/argonne-lcf/ai-science-training-series)
  - 10: [Hyperparameter
    Management](https://saforem2.github.io/hparam-management-sdl2022/) @
    [ALCF SDL
    Workshop](https://www.alcf.anl.gov/events/2022-alcf-simulation-data-and-learning-workshop)
  - 08: [Statistical
    Learning](https://saforem2.github.io/ATPESC-StatisticalLearning) @
    [ATPESC 2022](https://extremecomputingtraining.anl.gov/)
  - 05: [Scientific Data Science: An Emerging
    Symbiosis](https://saforem2.github.io/anl-job-talk/) @ ANL (05/2022)
  - 03: [Machine Learning in
    HEP](https://saforem2.github.io/physicsSeminar) @ UNC Greensboro
- 2021-:
  - 12: [Accelerated Sampling Methods for
    LGT](https://saforem2.github.io/l2hmc-dwq25/), @ [DWQ @ 25
    \[BNL\]](https://indico.bnl.gov/event/13576/)
  - 09: [Training Topological Samplers for
    LGT](https://saforem2.github.io/l2hmc_talk_ect2021) @ [ML4HEP, ECT\*
    Trento](https://indico.ectstar.eu/event/77/contributions/2349/)
  - 05: [Deep Learning HMC for Improved Gauge
    Generation](https://bit.ly/mainz21) @ [ML in LQCD
    Workshop](https://bit.ly/mainz21_overview) \[2021\]
- 2020:
  - 02: [Machine Learning for Lattice
    QCD](https://slides.com/samforeman/l2hmc-qcd/embed) @ U. Iowa
    \[2020\]

## üé™ Events

- Organizer for:
  - [SC25 Workshop: High Performance Python for Science at Scale
    (HPPSS)](https://hppss.github.io/SC25/), November 2025
  - [SC25 Tutorial: Accelerating and Scaling Python for
    HPC](https://sc25.conference-program.com/presentation/?id=tut121&sess=sess255)
  - [SC24 Workshop: High Performance Python for Science at Scale
    (HPPSS)](https://hppss.github.io/SC24/), November 2024
  - [SC23 Workshop: High Performance Python for Science at Scale
    (HPPSS)](https://hppss.github.io/SC23/), November 2023
  - [\[Machine\](2025-09-20_machine.md) Learning and Quantum Computing
    for Earth Sciences](https://17.usnccm.org/702) at 17th U. S.
    National Congress on Computational Mechanics, July 2023

## üìì References

<div id="refs" class="references csl-bib-body hanging-indent"
entry-spacing="0">

<div id="ref-boyda2022applications" class="csl-entry">

Boyda, Denis, Salvatore Calƒ±ÃÄ, Sam Foreman, Lena Funcke, Daniel C
Hackett, Yin Lin, Gert Aarts, et al. 2022. ‚ÄúApplications of Machine
Learning to Lattice Quantum Field Theory.‚Äù *arXiv Preprint
arXiv:2202.05838*. <https://arxiv.org/abs/2202.05838>.

</div>

<div id="ref-cheng2024thorough" class="csl-entry">

Cheng, Scott, Jun-Liang Lin, Murali Emani, Siddhisanket Raskar, Sam
Foreman, Zhen Xie, Venkatram Vishwanath, and Mahmut Taylan Kandemir.
2024. ‚ÄúThorough Characterization and Analysis of Large Transformer Model
Training at-Scale.‚Äù *Proc. ACM Meas. Anal. Comput. Syst.* 8 (1).
<https://doi.org/10.1145/3639034>.

</div>

<div id="ref-deamont2014superconductivity" class="csl-entry">

Deamont, George, and Sam Foreman. 2014. ‚ÄúSuperconductivity of in and Sn
Samples.‚Äù

</div>

<div id="ref-mprot-dpo2024" class="csl-entry">

Dharuman, Gautham, Kyle Hippe, Alexander Brace, Sam Foreman, V√§in√∂
Hatanp√§√§, Varuni K. Sastry, Huihuo Zheng, et al. 2024. ‚ÄúMProt-DPO:
Breaking the ExaFLOPS Barrier for Multimodal Protein Design Workflows
with Direct Preference Optimization.‚Äù In *Proceedings of the
International Conference for High Performance Computing, Networking,
Storage, and Analysis*. SC ‚Äô24. Atlanta, GA, USA: IEEE Press.
<https://doi.org/10.1109/SC41406.2024.00013>.

</div>

<div id="ref-dharuman2023protein" class="csl-entry">

Dharuman, Gautham, Logan Ward, Heng Ma, Priyanka V Setty, Ozan Gokdemir,
Sam Foreman, Murali Emani, et al. 2023. ‚ÄúProtein Generation via
Genome-Scale Language Models with Bio-Physical Scoring.‚Äù In *Proceedings
of the SC‚Äô23 Workshops of the International Conference on High
Performance Computing, Network, Storage, and Analysis*, 95‚Äì101.

</div>

<div id="ref-emani2023comprehensive" class="csl-entry">

Emani, Murali, Sam Foreman, Varuni Sastry, Zhen Xie, Siddhisanket
Raskar, William Arnold, Rajeev Thakur, Venkatram Vishwanath, and Michael
E Papka. 2023. ‚ÄúA Comprehensive Performance Study of Large Language
Models on Novel AI Accelerators.‚Äù *arXiv Preprint arXiv:2310.04607*.
<https://arxiv.org/abs/2310.04607>.

</div>

<div id="ref-foreman2023climrr" class="csl-entry">

Foreman, Sam. 2023. ‚ÄúEnergy Justice Analysis of Climate Data with
ClimRR.‚Äù August 7, 2023. <https://saforem2.github.io/climate-analysis>.

</div>

<div id="ref-foreman2018rg" class="csl-entry">

Foreman, Sam, Joel Giedt, Yannick Meurice, and Judah Unmuth-Yockey.
2018. ‚Äú<span class="nocase">RG-inspired machine learning for lattice
field theory</span>.‚Äù In *European Physical Journal Web of Conferences*,
175:11025. European Physical Journal Web of Conferences.
<https://doi.org/10.1051/epjconf/201817511025>.

</div>

<div id="ref-foreman2021hmc" class="csl-entry">

Foreman, Sam, Taku Izubuchi, Luchang Jin, Xiao-Yong Jin, James C Osborn,
and Akio Tomiya. 2021. ‚ÄúHMC with Normalizing Flows.‚Äù *arXiv Preprint
arXiv:2112.01586*. <https://arxiv.org/abs/2112.01586>.

</div>

<div id="ref-foreman2021deep" class="csl-entry">

Foreman, Sam, Xiao-Yong Jin, and Osborn James C. 2021. ‚ÄúDeep Learning
Hamiltonian Monte Carlo.‚Äù <https://arxiv.org/abs/2105.03418>.

</div>

<div id="ref-foreman2020machine" class="csl-entry">

Foreman, Sam, Xiao-Yong Jin, and James C Osborn. 2020. ‚ÄúMachine Learning
and Neural Networks for Field Theory.‚Äù

</div>

<div id="ref-foreman2023mlmc" class="csl-entry">

Foreman, Sam, Xiao-Yong Jin, and James C. Osborn. 2023. ‚ÄúMLMC: Machine
Learning Monte Carlo for Lattice Gauge Theory.‚Äù
<https://arxiv.org/abs/2312.08936>.

</div>

<div id="ref-foreman2018examples" class="csl-entry">

Foreman, Samuel, Joel Giedt, Yannick Meurice, and Judah Unmuth-Yockey.
2018. ‚ÄúExamples of Renormalization Group Transformations for Image
Sets.‚Äù *Physical Review E* 98 (5): 052129.

</div>

<div id="ref-gokdemir2025hiperrag" class="csl-entry">

Gokdemir, Ozan, Carlo Siebenschuh, Alexander Brace, Azton Wells, Brian
Hsu, Kyle Hippe, Priyanka V. Setty, et al. 2025. ‚ÄúHiPerRAG:
High-Performance Retrieval Augmented Generation for Scientific
Insights.‚Äù <https://arxiv.org/abs/2505.04846>.

</div>

<div id="ref-stock2025aeris" class="csl-entry">

Hatanp√§√§, V√§in√∂, Eugene Ku, Jason Stock, Murali Emani, Sam Foreman,
Chunyong Jung, Sandeep Madireddy, et al. 2025. ‚ÄúAERIS: Argonne Earth
Systems Model for Reliable and Skillful Predictions.‚Äù
<https://arxiv.org/abs/2509.13523>.

</div>

<div id="ref-hubler2018large" class="csl-entry">

Hubler, A, S Foreman, J Liu, and L Wortsmann. 2018. ‚ÄúLarge Energy
Density in Three-Plate Nanocapacitors Due to Coulomb Blockade.‚Äù *Journal
of Applied Physics* 123 (10).

</div>

<div id="ref-kronfeld2022lattice" class="csl-entry">

Kronfeld, Andreas S, Tanmoy Bhattacharya, Thomas Blum, Norman H Christ,
Carleton DeTar, William Detmold, Robert Edwards, et al. 2022. ‚ÄúLattice
QCD and Particle Physics.‚Äù *arXiv Preprint arXiv:2207.07641*.
<https://arxiv.org/abs/2207.07641>.

</div>

<div id="ref-leung2024intro" class="csl-entry">

Leung, Mary Ann, Katharine Cahill, Rebecca Hartman-Baker, Paige Kinsley,
Lois Curfman McInnes, Suzanne Parete-Koon, Sreeranjani Ramprakash, et
al. 2024. ‚ÄúIntro to HPC Bootcamp: Engaging New Communities Through
Energy Justice Projects.‚Äù *Journal of Computational Science Education*
15 (1). <https://doi.org/10.22369/issn.2153-4136/15/1/10>.

</div>

<div id="ref-song2023deepspeed4science" class="csl-entry">

Song, Shuaiwen Leon, Bonnie Kruft, Minjia Zhang, Conglong Li, Shiyang
Chen, Chengming Zhang, Masahiro Tanaka, et al. 2023. ‚ÄúDeepSpeed4Science
Initiative: Enabling Large-Scale Scientific Discovery Through
Sophisticated AI System Technologies.‚Äù *arXiv Preprint
arXiv:2310.04610*. <https://arxiv.org/abs/2310.04610>.

</div>

<div id="ref-yan2025mofa" class="csl-entry">

Yan, Xiaoli, Nathaniel Hudson, Hyun Park, Daniel Grzenda, J. Gregory
Pauloski, Marcus Schwarting, Haochen Pan, et al. 2025. ‚ÄúMOFA:
Discovering Materials for Carbon Capture with a GenAI- and
Simulation-Based Workflow.‚Äù <https://arxiv.org/abs/2501.10651>.

</div>

<div id="ref-zvyagin2023genslms" class="csl-entry">

Zvyagin, Maxim, Alexander Brace, Kyle Hippe, Yuntian Deng, Bin Zhang,
Cindy Orozco Bohorquez, Austin Clyde, et al. 2023. ‚ÄúGenSLMs:
Genome-Scale Language Models Reveal SARS-CoV-2 Evolutionary Dynamics.‚Äù
*The International Journal of High Performance Computing Applications*
37 (6): 683‚Äì705.

</div>

</div>
