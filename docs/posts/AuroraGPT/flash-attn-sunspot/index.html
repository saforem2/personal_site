<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.2">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Sam Foreman">
<meta name="dcterms.date" content="2024-06-17">

<title>📸 flash-attn on Sunspot – Sam Foreman</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../posts/AuroraGPT/long-sequences/index.html" rel="next">
<link href="../../../posts/AuroraGPT/determinstic-flash-attn/index.html" rel="prev">
<link href="../../../assets/favicon.svg" rel="icon" type="image/svg+xml">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-7a61aea021b1646dfdd8de545d6de203.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark-83f0e58f0214840f6ba4a41a8a112a92.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script src="../../../site_libs/quarto-contrib/iconify-2.1.0/iconify-icon.min.js"></script>
<script src="../../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "?",
    "H"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-E72QE53WSY"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-E72QE53WSY', { 'anonymize_ip': true});
</script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
<link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&amp;family=IBM+Plex+Sans+Condensed:ital,wght@0,400;0,500;0,600;0,700&amp;family=IBM+Plex+Mono:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700&amp;display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=IBM+Plex+Sans&amp;family=IBM+Plex+Sans+Condensed&amp;family=IBM+Plex+Mono&amp;display=swap" rel="stylesheet">
<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-TC329HJ');</script>
<!-- End Google Tag Manager -->
<link rel="preconnect" href="https://fonts.googleapis.com">

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../css/colors-oklch.min.css">
<link rel="stylesheet" href="../../../css/custom.css">
<link rel="stylesheet" href="../../../css/svgbob.css">
<link rel="stylesheet" href="../../../static/fonts/MIosevkaQp/MIosevkaQp.css">
<link rel="stylesheet" href="../../../static/fonts/MIosevkaterm/MIosevkaTerm.css">
<meta property="og:title" content="Sam Foreman">
<meta property="og:description" content="Talks, Posts, Projects and More">
<meta property="og:image" content="https://github.com/saforem2/personal_site/blob/main/assets/thumbnail.png?raw=true">
<meta property="og:site_name" content="Sam Foreman">
<meta name="twitter:title" content="Sam Foreman">
<meta name="twitter:description" content="Talks, Posts, Projects and More">
<meta name="twitter:image" content="https://github.com/saforem2/personal_site/blob/main/assets/thumbnail.png?raw=true">
<meta name="twitter:creator" content="@saforem2">
<meta name="twitter:site" content="@saforem2">
<meta name="twitter:card" content="summary_large_image">
<meta name="citation_title" content="📸 `flash-attn` on Sunspot">
<meta name="citation_author" content="Sam Foreman">
<meta name="citation_publication_date" content="2024-06-17">
<meta name="citation_cover_date" content="2024-06-17">
<meta name="citation_year" content="2024">
<meta name="citation_online_date" content="2024-06-17">
<meta name="citation_fulltext_html_url" content="https://samforeman.me/posts/AuroraGPT/flash-attn-sunspot/">
<meta name="citation_language" content="en">
<meta name="citation_reference" content="citation_title=MProt-DPO: Breaking the ExaFLOPS barrier for multimodal protein design workflows with direct preference optimization;,citation_abstract=We present a scalable, end-to-end workflow for protein design. By augmenting protein sequences with natural language descriptions of their biochemical properties, we train generative models that can be preferentially aligned with protein fitness landscapes. Through complex experimental- and simulation-based observations, we integrate these measures as preferred parameters for generating new protein variants and demonstrate our workflow on five diverse supercomputers. We achieve &amp;amp;amp;gt;1 ExaFLOPS sustained performance in mixed precision on each supercomputer and a maximum sustained performance of 4.11 ExaFLOPS and peak performance of 5.57 ExaFLOPS. We establish the scientific performance of our model on two tasks: (1) across a predetermined benchmark dataset of deep mutational scanning experiments to optimize the fitness-determining mutations in the yeast protein HIS7, and (2) in optimizing the design of the enzyme malate dehydrogenase to achieve lower activation barriers (and therefore increased catalytic rates) using simulation data. Our implementation thus sets high watermarks for multimodal protein design workflows.;,citation_author=Gautham Dharuman;,citation_author=Kyle Hippe;,citation_author=Alexander Brace;,citation_author=Sam Foreman;,citation_author=Väinö Hatanpää;,citation_author=Varuni K. Sastry;,citation_author=Huihuo Zheng;,citation_author=Logan Ward;,citation_author=Servesh Muralidharan;,citation_author=Archit Vasan;,citation_author=Bharat Kale;,citation_author=Carla M. Mann;,citation_author=Heng Ma;,citation_author=Yun-Hsuan Cheng;,citation_author=Yuliana Zamora;,citation_author=Shengchao Liu;,citation_author=Chaowei Xiao;,citation_author=Murali Emani;,citation_author=Tom Gibbs;,citation_author=Mahidhar Tatineni;,citation_author=Deepak Canchi;,citation_author=Jerome Mitchell;,citation_author=Koichi Yamada;,citation_author=Maria Garzaran;,citation_author=Michael E. Papka;,citation_author=Ian Foster;,citation_author=Rick Stevens;,citation_author=Anima Anandkumar;,citation_author=Venkatram Vishwanath;,citation_author=Arvind Ramanathan;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://doi.org/10.1109/SC41406.2024.00013;,citation_doi=10.1109/SC41406.2024.00013;,citation_isbn=9798350352917;,citation_conference_title=Proceedings of the international conference for high performance computing, networking, storage, and analysis;,citation_conference=IEEE Press;,citation_series_title=SC ’24;">
<meta name="citation_reference" content="citation_title=Quality measures for dynamic graph generative models;,citation_author=Ryien Hosseini;,citation_author=Filippo Simini;,citation_author=Venkatram Vishwanath;,citation_author=Rebecca Willett;,citation_author=Henry Hoffmann;,citation_publication_date=2025;,citation_cover_date=2025;,citation_year=2025;,citation_fulltext_html_url=https://openreview.net/forum?id=8bjspmAMBk;,citation_conference_title=The thirteenth international conference on learning representations;">
<meta name="citation_reference" content="citation_title=Superconductivity of in and sn samples;,citation_author=George Deamont;,citation_author=Sam Foreman;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;">
<meta name="citation_reference" content="citation_title=RG-inspired machine learning for lattice field theory;,citation_author=Sam Foreman;,citation_author=Joel Giedt;,citation_author=Yannick Meurice;,citation_author=Judah Unmuth-Yockey;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_volume=175;,citation_conference_title=EPJ web of conferences;,citation_conference=EDP Sciences;">
<meta name="citation_reference" content="citation_title=Large energy density in three-plate nanocapacitors due to coulomb blockade;,citation_author=A Hubler;,citation_author=S Foreman;,citation_author=J Liu;,citation_author=L Wortsmann;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_issue=10;,citation_volume=123;,citation_journal_title=Journal of Applied Physics;,citation_publisher=AIP Publishing;">
<meta name="citation_reference" content="citation_title=Examples of renormalization group transformations for image sets;,citation_author=Samuel Foreman;,citation_author=Joel Giedt;,citation_author=Yannick Meurice;,citation_author=Judah Unmuth-Yockey;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_issue=5;,citation_volume=98;,citation_journal_title=Physical Review E;,citation_publisher=American Physical Society;">
<meta name="citation_reference" content="citation_title=Machine learning inspired analysis of the Ising model transition;,citation_author=Samuel Foreman;,citation_author=Joel Giedt;,citation_author=Yannick Meurice;,citation_author=Judah Unmuth-Yockey;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_doi=10.22323/1.334.0245;,citation_volume=LATTICE2018;,citation_journal_title=PoS;">
<meta name="citation_reference" content="citation_title=Machine learning inspired analysis of the ising model transition;,citation_author=Samuel Foreman;,citation_author=Joel Giedt;,citation_author=Yannick Meurice;,citation_author=Judah Unmuth-Yockey;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_conference_title=Lattice 2018;">
<meta name="citation_reference" content="citation_title=Learning better physics: A machine learning approach to lattice gauge theory;,citation_author=Samuel Alfred Foreman;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_dissertation_institution=University of Iowa;">
<meta name="citation_reference" content="citation_title=Machine learning and neural networks for field theory;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C Osborn;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;">
<meta name="citation_reference" content="citation_title=HMC with normalizing flows;,citation_author=Sam Foreman;,citation_author=Taku Izubuchi;,citation_author=Luchang Jin;,citation_author=Xiao-Yong Jin;,citation_author=James C Osborn;,citation_author=Akio Tomiya;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_journal_title=arXiv preprint arXiv:2112.01586;">
<meta name="citation_reference" content="citation_title=LeapfrogLayers: A trainable framework for effective topological sampling;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C Osborn;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_journal_title=arXiv preprint arXiv:2112.01582;">
<meta name="citation_reference" content="citation_title=Energy storage in quantum resonators;,citation_author=Jiaqi Liu;,citation_author=Alfred W Hubler;,citation_author=Samuel Alfred Foreman;,citation_author=Katharina Ott;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;">
<meta name="citation_reference" content="citation_title=Applications of machine learning to lattice quantum field theory;,citation_author=Denis Boyda;,citation_author=Salvatore Calı̀;,citation_author=Sam Foreman;,citation_author=Lena Funcke;,citation_author=Daniel C Hackett;,citation_author=Yin Lin;,citation_author=Gert Aarts;,citation_author=Andrei Alexandru;,citation_author=Xiao-Yong Jin;,citation_author=Biagio Lucini;,citation_author=others;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_journal_title=arXiv preprint arXiv:2202.05838;">
<meta name="citation_reference" content="citation_title=Lattice QCD and particle physics;,citation_author=Andreas S Kronfeld;,citation_author=Tanmoy Bhattacharya;,citation_author=Thomas Blum;,citation_author=Norman H Christ;,citation_author=Carleton DeTar;,citation_author=William Detmold;,citation_author=Robert Edwards;,citation_author=Anna Hasenfratz;,citation_author=Huey-Wen Lin;,citation_author=Swagato Mukherjee;,citation_author=others;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_journal_title=arXiv preprint arXiv:2207.07641;">
<meta name="citation_reference" content="citation_title=GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics;,citation_author=Maxim Zvyagin;,citation_author=Alexander Brace;,citation_author=Kyle Hippe;,citation_author=Yuntian Deng;,citation_author=Bin Zhang;,citation_author=Cindy Orozco Bohorquez;,citation_author=Austin Clyde;,citation_author=Bharat Kale;,citation_author=Danilo Perez-Rivera;,citation_author=Heng Ma;,citation_author=others;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_issue=6;,citation_volume=37;,citation_journal_title=The International Journal of High Performance Computing Applications;,citation_publisher=SAGE Publications Sage UK: London, England;">
<meta name="citation_reference" content="citation_title=MLMC: Machine learning monte carlo;,citation_author=Sam Foreman;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_conference_title=The international symposium on lattice field theory;">
<meta name="citation_reference" content="citation_title=Superconductivity of in and sn samples;,citation_author=George Deamont;,citation_author=Sam Foreman;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;">
<meta name="citation_reference" content="citation_title=A comprehensive performance study of large language models on novel AI accelerators;,citation_author=Murali Emani;,citation_author=Sam Foreman;,citation_author=Varuni Sastry;,citation_author=Zhen Xie;,citation_author=Siddhisanket Raskar;,citation_author=William Arnold;,citation_author=Rajeev Thakur;,citation_author=Venkatram Vishwanath;,citation_author=Michael E Papka;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_journal_title=arXiv preprint arXiv:2310.04607;">
<meta name="citation_reference" content="citation_title=DeepSpeed4Science initiative: Enabling large-scale scientific discovery through sophisticated AI system technologies;,citation_author=Shuaiwen Leon Song;,citation_author=Bonnie Kruft;,citation_author=Minjia Zhang;,citation_author=Conglong Li;,citation_author=Shiyang Chen;,citation_author=Chengming Zhang;,citation_author=Masahiro Tanaka;,citation_author=Xiaoxia Wu;,citation_author=Jeff Rasley;,citation_author=Ammar Ahmad Awan;,citation_author=others;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_journal_title=arXiv preprint arXiv:2310.04610;">
<meta name="citation_reference" content="citation_title=Protein generation via genome-scale language models with bio-physical scoring;,citation_author=Gautham Dharuman;,citation_author=Logan Ward;,citation_author=Heng Ma;,citation_author=Priyanka V Setty;,citation_author=Ozan Gokdemir;,citation_author=Sam Foreman;,citation_author=Murali Emani;,citation_author=Kyle Hippe;,citation_author=Alexander Brace;,citation_author=Kristopher Keipert;,citation_author=others;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_conference_title=Proceedings of the SC’23 workshops of the international conference on high performance computing, network, storage, and analysis;">
<meta name="citation_reference" content="citation_title=MLMC: Machine learning monte carlo for lattice gauge theory;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C Osborn;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_journal_title=arXiv preprint arXiv:2312.08936;">
<meta name="citation_reference" content="citation_title=Snowmass 2021 computational frontier CompF03 topical group report: Machine learning;,citation_author=Phiala Shanahan;,citation_author=Kazuhiro Terao;,citation_author=Daniel Whiteson;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_journal_title=arXiv preprint arXiv:2209.07559;">
<meta name="citation_reference" content="citation_title=Thorough characterization and analysis of large transformer model training at-scale;,citation_author=Scott Cheng;,citation_author=Jun-Liang Lin;,citation_author=Murali Emani;,citation_author=Siddhisanket Raskar;,citation_author=Sam Foreman;,citation_author=Zhen Xie;,citation_author=Venkatram Vishwanath;,citation_author=Mahmut Taylan Kandemir;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_issue=1;,citation_volume=8;,citation_journal_title=Proceedings of the ACM on Measurement and Analysis of Computing Systems;,citation_publisher=ACM New York, NY, USA;">
<meta name="citation_reference" content="citation_title=Communities through energy justice projects;,citation_author=Mary Ann Leung;,citation_author=Katharine Cahill;,citation_author=Rebecca Hartman-Baker;,citation_author=Paige Kinsley;,citation_author=Lois Curfman McInnes;,citation_author=Suzanne Parete-Koon;,citation_author=Subil Abraham;,citation_author=Lacy Beach Barrier;,citation_author=Gladys Chen;,citation_author=Lizanne DeStefano;,citation_author=others;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_issue=1;,citation_volume=15;,citation_journal_title=Journal of Computational Science;">
<meta name="citation_reference" content="citation_title=Applications of a foundation model approach for weather and climate;,citation_author=Troy Arcomano;,citation_author=Alexander Wikner;,citation_author=Romit Maulik;,citation_author=Veerabhadra Rao Kotamarthi;,citation_author=Sam Foreman;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_volume=2023;,citation_conference_title=AGU fall meeting abstracts;">
<meta name="citation_reference" content="citation_title=Toward a holistic performance evaluation of large language models across diverse ai accelerators;,citation_author=Murali Emani;,citation_author=Sam Foreman;,citation_author=Varuni Sastry;,citation_author=Zhen Xie;,citation_author=Siddhisanket Raskar;,citation_author=William Arnold;,citation_author=Rajeev Thakur;,citation_author=Venkatram Vishwanath;,citation_author=Michael E Papka;,citation_author=Sanjif Shanmugavelu;,citation_author=others;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_conference_title=2024 IEEE international parallel and distributed processing symposium workshops (IPDPSW);,citation_conference=IEEE;">
<meta name="citation_reference" content="citation_title=Intro to HPC bootcamp: Engaging new communities through energy justice projects;,citation_author=Suzanne Parete-Koon;,citation_author=Michael Sandoval;,citation_author=Kellen Leland;,citation_author=Subil Abraham;,citation_author=Mary Ann Leung;,citation_author=Rebecca Hartman-Baker;,citation_author=Paige Kinsley;,citation_author=Lois McInnes;,citation_author=Sreeranjani Ramprakash;,citation_author=Lacy Beach Barrier;,citation_author=others;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_issue=1;,citation_volume=15;,citation_journal_title=Journal of Computational Science Education;,citation_publisher=Oak Ridge National Laboratory (ORNL), Oak Ridge, TN (United States);">
<meta name="citation_reference" content="citation_title=MProt-DPO: Breaking the ExaFLOPS barrier for multimodal protein design workflows with direct preference optimization;,citation_author=Gautham Dharuman;,citation_author=Kyle Hippe;,citation_author=Alexander Brace;,citation_author=Sam Foreman;,citation_author=Väinä Hatanpää;,citation_author=Varuni K Sastry;,citation_author=Huihuo Zheng;,citation_author=Logan Ward;,citation_author=Servesh Muralidharan;,citation_author=Archit Vasan;,citation_author=others;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_conference_title=2024 SC24: International conference for high performance computing, networking, storage and analysis SC;,citation_conference=IEEE Computer Society;">
<meta name="citation_reference" content="citation_title=Emergent abilities of large language models;,citation_author=Jason Wei;,citation_author=Yi Tay;,citation_author=Rishi Bommasani;,citation_author=Colin Raffel;,citation_author=Barret Zoph;,citation_author=Sebastian Borgeaud;,citation_author=Dani Yogatama;,citation_author=Maarten Bosma;,citation_author=Denny Zhou;,citation_author=Donald Metzler;,citation_author=Ed H. Chi;,citation_author=Tatsunori Hashimoto;,citation_author=Oriol Vinyals;,citation_author=Percy Liang;,citation_author=Jeff Dean;,citation_author=William Fedus;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2206.07682;">
<meta name="citation_reference" content="citation_title=DeepSpeed4Science initiative: Enabling large-scale scientific discovery through sophisticated AI system technologies;,citation_author=Shuaiwen Leon Song;,citation_author=Bonnie Kruft;,citation_author=Minjia Zhang;,citation_author=Conglong Li;,citation_author=Shiyang Chen;,citation_author=Chengming Zhang;,citation_author=Masahiro Tanaka;,citation_author=Xiaoxia Wu;,citation_author=Jeff Rasley;,citation_author=Ammar Ahmad Awan;,citation_author=Connor Holmes;,citation_author=Martin Cai;,citation_author=Adam Ghanem;,citation_author=Zhongzhu Zhou;,citation_author=Yuxiong He;,citation_author=Pete Luferenko;,citation_author=Divya Kumar;,citation_author=Jonathan Weyn;,citation_author=Ruixiong Zhang;,citation_author=Sylwester Klocek;,citation_author=Volodymyr Vragov;,citation_author=Mohammed AlQuraishi;,citation_author=Gustaf Ahdritz;,citation_author=Christina Floristean;,citation_author=Cristina Negri;,citation_author=Rao Kotamarthi;,citation_author=Venkatram Vishwanath;,citation_author=Arvind Ramanathan;,citation_author=Sam Foreman;,citation_author=Kyle Hippe;,citation_author=Troy Arcomano;,citation_author=Romit Maulik;,citation_author=Maxim Zvyagin;,citation_author=Alexander Brace;,citation_author=Bin Zhang;,citation_author=Cindy Orozco Bohorquez;,citation_author=Austin Clyde;,citation_author=Bharat Kale;,citation_author=Danilo Perez-Rivera;,citation_author=Heng Ma;,citation_author=Carla M. Mann;,citation_author=Michael Irvin;,citation_author=J. Gregory Pauloski;,citation_author=Logan Ward;,citation_author=Valerie Hayot;,citation_author=Murali Emani;,citation_author=Zhen Xie;,citation_author=Diangen Lin;,citation_author=Maulik Shukla;,citation_author=Ian Foster;,citation_author=James J. Davis;,citation_author=Michael E. Papka;,citation_author=Thomas Brettin;,citation_author=Prasanna Balaprakash;,citation_author=Gina Tourassi;,citation_author=John Gounley;,citation_author=Heidi Hanson;,citation_author=Thomas E Potok;,citation_author=Massimiliano Lupo Pasini;,citation_author=Kate Evans;,citation_author=Dan Lu;,citation_author=Dalton Lunga;,citation_author=Junqi Yin;,citation_author=Sajal Dash;,citation_author=Feiyi Wang;,citation_author=Mallikarjun Shankar;,citation_author=Isaac Lyngaas;,citation_author=Xiao Wang;,citation_author=Guojing Cong;,citation_author=Pei Zhang;,citation_author=Ming Fan;,citation_author=Siyan Liu;,citation_author=Adolfy Hoisie;,citation_author=Shinjae Yoo;,citation_author=Yihui Ren;,citation_author=William Tang;,citation_author=Kyle Felker;,citation_author=Alexey Svyatkovskiy;,citation_author=Hang Liu;,citation_author=Ashwin Aji;,citation_author=Angela Dalton;,citation_author=Michael Schulte;,citation_author=Karl Schulz;,citation_author=Yuntian Deng;,citation_author=Weili Nie;,citation_author=Josh Romero;,citation_author=Christian Dallago;,citation_author=Arash Vahdat;,citation_author=Chaowei Xiao;,citation_author=Thomas Gibbs;,citation_author=Anima Anandkumar;,citation_author=Rick Stevens;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2310.04610;">
<meta name="citation_reference" content="citation_title=Emergent abilities of large language models;,citation_author=Jason Wei;,citation_author=Yi Tay;,citation_author=Rishi Bommasani;,citation_author=Colin Raffel;,citation_author=Barret Zoph;,citation_author=Sebastian Borgeaud;,citation_author=Dani Yogatama;,citation_author=Maarten Bosma;,citation_author=Denny Zhou;,citation_author=Donald Metzler;,citation_author=Ed H. Chi;,citation_author=Tatsunori Hashimoto;,citation_author=Oriol Vinyals;,citation_author=Percy Liang;,citation_author=Jeff Dean;,citation_author=William Fedus;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2206.07682;">
<meta name="citation_reference" content="citation_title=The climate risk &amp;amp;amp; resilience portal (ClimRR) metadata and data dictionary;,citation_author=C. Burdi;,citation_author=Wall. T Branham;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://dub.sh/ClimRR-Metadata;">
<meta name="citation_reference" content="citation_title=Progress on $(g-2)_\mu$ from lattice QCD;,citation_author=Hartmut Wittig;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2306.04165;">
<meta name="citation_reference" content="citation_title=Hybrid Monte Carlo;,citation_author=S. Duane;,citation_author=A. D. Kennedy;,citation_author=B. J. Pendleton;,citation_author=D. Roweth;,citation_publication_date=1987;,citation_cover_date=1987;,citation_year=1987;,citation_doi=10.1016/0370-2693(87)91197-X;,citation_volume=195;,citation_journal_title=Phys. Lett. B;">
<meta name="citation_reference" content="citation_title=Snowmass 2021 Computational Frontier CompF03 Topical Group Report: Machine Learning;,citation_author=Phiala Shanahan;,citation_author=others;,citation_publication_date=2022-09;,citation_cover_date=2022-09;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2209.07559;">
<meta name="citation_reference" content="citation_title=Applications of Machine Learning to Lattice Quantum Field Theory;,citation_author=Denis Boyda;,citation_author=others;,citation_publication_date=2022-02;,citation_cover_date=2022-02;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2202.05838;,citation_conference_title=Snowmass 2021;">
<meta name="citation_reference" content="citation_title=HMC with Normalizing Flows;,citation_author=Sam Foreman;,citation_author=Taku Izubuchi;,citation_author=Luchang Jin;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_author=Akio Tomiya;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01586;,citation_doi=10.22323/1.396.0073;,citation_volume=LATTICE2021;,citation_journal_title=PoS;">
<meta name="citation_reference" content="citation_title=LeapfrogLayers: A Trainable Framework for Effective Topological Sampling;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01582;,citation_doi=10.22323/1.396.0508;,citation_volume=LATTICE2021;,citation_journal_title=PoS;">
<meta name="citation_reference" content="citation_title=Deep Learning Hamiltonian Monte Carlo;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_publication_date=2021-05;,citation_cover_date=2021-05;,citation_year=2021;,citation_fulltext_html_url=https://arxiv.org/abs/2105.03418;,citation_conference_title=9th International Conference on Learning Representations;">
<meta name="citation_reference" content="citation_title=Deep learning hamiltonian monte carlo;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://arxiv.org/abs/2105.03418;">
<meta name="citation_reference" content="citation_title=Deep Learning Hamiltonian Monte Carlo;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_publication_date=2021-05;,citation_cover_date=2021-05;,citation_year=2021;,citation_fulltext_html_url=https://arxiv.org/abs/2105.03418;,citation_conference_title=9th International Conference on Learning Representations;">
<meta name="citation_reference" content="citation_title=Deep learning hamiltonian monte carlo;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C Osborn;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_journal_title=arXiv preprint arXiv:2105.03418;">
<meta name="citation_reference" content="citation_title=LeapfrogLayers: A trainable framework for effective topological sampling;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C Osborn;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_journal_title=arXiv preprint arXiv:2112.01582;">
<meta name="citation_reference" content="citation_title=Energy Justice Analysis of Climate Data with ClimRR;,citation_author=Sam Foreman;,citation_publication_date=2023-08-07;,citation_cover_date=2023-08-07;,citation_year=2023;,citation_fulltext_html_url=https://saforem2.github.io/climate-analysis;,citation_language=en;">
<meta name="citation_reference" content="citation_author=Sam Foreman;,citation_publication_date=2023-08-19;,citation_cover_date=2023-08-19;,citation_year=2023;,citation_fulltext_html_url=https://saforem2.github.io/l2hmc-qcd;,citation_language=en;">
<meta name="citation_reference" content="citation_title=MLMC: Machine learning monte carlo for lattice gauge theory;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James Osborn;,citation_publication_date=00;,citation_cover_date=00;,citation_year=0;,citation_conference_title=40th international symposium on lattice field theory (lattice 2023) (batavia, IL, united states, 07/31/2023 - 08/04/2023);">
<meta name="citation_reference" content="citation_title=Progress on $(g-2)_\mu$ from lattice QCD;,citation_author=Hartmut Wittig;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2306.04165;">
<meta name="citation_reference" content="citation_title=Hybrid Monte Carlo;,citation_author=S. Duane;,citation_author=A. D. Kennedy;,citation_author=B. J. Pendleton;,citation_author=D. Roweth;,citation_publication_date=1987;,citation_cover_date=1987;,citation_year=1987;,citation_doi=10.1016/0370-2693(87)91197-X;,citation_volume=195;,citation_journal_title=Phys. Lett. B;">
<meta name="citation_reference" content="citation_title=Snowmass 2021 Computational Frontier CompF03 Topical Group Report: Machine Learning;,citation_author=Phiala Shanahan;,citation_author=others;,citation_publication_date=2022-09;,citation_cover_date=2022-09;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2209.07559;">
<meta name="citation_reference" content="citation_title=Applications of Machine Learning to Lattice Quantum Field Theory;,citation_author=Denis Boyda;,citation_author=others;,citation_publication_date=2022-02;,citation_cover_date=2022-02;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2202.05838;,citation_conference_title=Snowmass 2021;">
<meta name="citation_reference" content="citation_title=LeapfrogLayers: A Trainable Framework for Effective Topological Sampling;,citation_author=S. Foreman;,citation_author=X. Jin;,citation_author=J. Osborn;,citation_publication_date=2022-07;,citation_cover_date=2022-07;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01582;,citation_doi=10.22323/1.396.0508;,citation_conference_title=The 38th international symposium on lattice field theory;">
<meta name="citation_reference" content="citation_title=HMC with Normalizing Flows;,citation_author=Sam Foreman;,citation_author=Taku Izubuchi;,citation_author=Luchang Jin;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_author=Akio Tomiya;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01586;,citation_doi=10.22323/1.396.0073;,citation_volume=LATTICE2021;,citation_journal_title=PoS;">
<meta name="citation_reference" content="citation_title=Mastering language models;,citation_author=Samuel Montgomery;,citation_publication_date=2023-10;,citation_cover_date=2023-10;,citation_year=2023;,citation_fulltext_html_url=https://towardsdatascience.com/mastering-language-models-32e1d891511a
           ;,citation_journal_title=Medium;,citation_publisher=Towards Data Science;">
<meta name="citation_reference" content="citation_title=Harnessing the power of LLMs in practice: A survey on ChatGPT and beyond;,citation_author=Jingfeng Yang;,citation_author=Hongye Jin;,citation_author=Ruixiang Tang;,citation_author=Xiaotian Han;,citation_author=Qizhang Feng;,citation_author=Haoming Jiang;,citation_author=Bing Yin;,citation_author=Xia Hu;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2304.13712;">
<meta name="citation_reference" content="citation_title=Training tips for the transformer model;,citation_author=Martin Popel;,citation_author=Ondřej Bojar;,citation_publication_date=2018-04;,citation_cover_date=2018-04;,citation_year=2018;,citation_fulltext_html_url=https://doi.org/10.2478%2Fpralin-2018-0002;,citation_issue=1;,citation_doi=10.2478/pralin-2018-0002;,citation_volume=110;,citation_journal_title=The Prague Bulletin of Mathematical Linguistics;,citation_publisher=Charles University in Prague, Karolinum Press;">
<meta name="citation_reference" content="citation_title=Attention is all you need;,citation_author=Ashish Vaswani;,citation_author=Noam Shazeer;,citation_author=Niki Parmar;,citation_author=Jakob Uszkoreit;,citation_author=Llion Jones;,citation_author=Aidan N. Gomez;,citation_author=Lukasz Kaiser;,citation_author=Illia Polosukhin;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_fulltext_html_url=https://arxiv.org/abs/1706.03762;">
<meta name="citation_reference" content="citation_title=Tree of thoughts: Deliberate problem solving with large language models;,citation_author=Shunyu Yao;,citation_author=Dian Yu;,citation_author=Jeffrey Zhao;,citation_author=Izhak Shafran;,citation_author=Thomas L. Griffiths;,citation_author=Yuan Cao;,citation_author=Karthik Narasimhan;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2305.10601;">
<meta name="citation_reference" content="citation_title=GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics;,citation_abstract=We seek to transform how new and emergent variants of pandemiccausing viruses, specifically SARS-CoV-2, are identified and classified. By adapting large language models (LLMs) for genomic data, we build genome-scale language models (GenSLMs) which can learn the evolutionary landscape of SARS-CoV-2 genomes. By pretraining on over 110 million prokaryotic gene sequences and finetuning a SARS-CoV-2-specific model on 1.5 million genomes, we show that GenSLMs can accurately and rapidly identify variants of concern. Thus, to our knowledge, GenSLMs represents one of the first whole genome scale foundation models which can generalize to other prediction tasks. We demonstrate scaling of GenSLMs on GPU-based supercomputers and AI-hardware accelerators utilizing 1.63 Zettaflops in training runs with a sustained performance of 121 PFLOPS in mixed precision and peak of 850 PFLOPS. We present initial scientific insights from examining GenSLMs in tracking evolutionary dynamics of SARS-CoV-2, paving the path to realizing this on large biological data.Competing Interest StatementThe authors have declared no competing interest.;,citation_author=Maxim Zvyagin;,citation_author=Alexander Brace;,citation_author=Kyle Hippe;,citation_author=Yuntian Deng;,citation_author=Bin Zhang;,citation_author=Cindy Orozco Bohorquez;,citation_author=Austin Clyde;,citation_author=Bharat Kale;,citation_author=Danilo Perez-Rivera;,citation_author=Heng Ma;,citation_author=Carla M. Mann;,citation_author=Michael Irvin;,citation_author=J. Gregory Pauloski;,citation_author=Logan Ward;,citation_author=Valerie Hayot-Sasson;,citation_author=Murali Emani;,citation_author=Sam Foreman;,citation_author=Zhen Xie;,citation_author=Diangen Lin;,citation_author=Maulik Shukla;,citation_author=Weili Nie;,citation_author=Josh Romero;,citation_author=Christian Dallago;,citation_author=Arash Vahdat;,citation_author=Chaowei Xiao;,citation_author=Thomas Gibbs;,citation_author=Ian Foster;,citation_author=James J. Davis;,citation_author=Michael E. Papka;,citation_author=Thomas Brettin;,citation_author=Rick Stevens;,citation_author=Anima Anandkumar;,citation_author=Venkatram Vishwanath;,citation_author=Arvind Ramanathan;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://www.biorxiv.org/content/early/2022/11/23/2022.10.10.511571;,citation_doi=10.1101/2022.10.10.511571;,citation_journal_title=bioRxiv;,citation_publisher=Cold Spring Harbor Laboratory;">
</head>

<body class="nav-sidebar floating nav-fixed quarto-dark"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = true;
    const queryPrefersDark = window.matchMedia('(prefers-color-scheme: dark)');
    const darkModeDefault = queryPrefersDark.matches;
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    queryPrefersDark.addEventListener("change", e => {
      if(window.localStorage.getItem("quarto-color-scheme") !== null)
        return;
      const alternate = e.matches
      toggleColorMode(alternate);
      localAlternateSentinel = e.matches ? 'alternate' : 'default'; // this is used alongside local storage!
      toggleGiscusIfUsed(alternate, darkModeDefault);
    });
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../assets/signature-navbar-orig.svg" alt="Sam Foreman" class="navbar-logo">
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link active" href="../../../talks/index.html" aria-current="page"> 
<span class="menu-text">talks</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../posts/index.html"> 
<span class="menu-text">posts</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-projects" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">projects</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-projects">    
        <li>
    <a class="dropdown-item" href="../../../projects/index.html">
 <span class="dropdown-text">📚 All Projects</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://saforem2.github.io/ezpz">
 <span class="dropdown-text">🍋 <code>ezpz</code></span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://saforem2.github.io/l2hmc-qcd">
 <span class="dropdown-text">🟥 <code>l2hmc-qcd</code></span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/argonne-lcf/Megatron-DeepSpeed)">
 <span class="dropdown-text">🤖 <code>Megatron-DeepSpeed</code></span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://saforem2.github.io/wordplay">
 <span class="dropdown-text">💬 <code>wordplay</code> 🎮</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://www.alcf.anl.gov/alcf-ai-science-training-series?">
 <span class="dropdown-text">🎓 <code>ai-science-training</code></span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://saforem2.github.io/enrich">
 <span class="dropdown-text">💸 <code>enrich</code></span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://saforem2.github.io/ambivalent">
 <span class="dropdown-text">🤷🏻‍♂️<code>ambivalent</code></span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://saforem2.github.io/climate-analysis">
 <span class="dropdown-text">🌍 <code>climate-analysis</code></span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/saforem2/glitz">
 <span class="dropdown-text">🎨 <code>glitz</code></span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/saforem2/personal_site">
 <span class="dropdown-text">🙋🏻<code>personal_site</code></span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/saforem2/notes-demo">
 <span class="dropdown-text">🗒️ <code>Notes-Demo</code></span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/saforem2/personal_site"> 
<span class="menu-text"><span class="icon" style="font-size: 1.25rem; color:var(--bs-nav-link-color);"><iconify-icon role="img" inline="" icon="ph:github-logo" aria-label="Icon github-logo from ph Iconify.design set." title="Icon github-logo from ph Iconify.design set."></iconify-icon></span></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../index.xml"> 
<span class="menu-text"><span class="icon" style="font-size: 1.25rem; color:var(--bs-nav-link-color);"><iconify-icon role="img" inline="" icon="ph:rss" aria-label="Icon rss from ph Iconify.design set." title="Icon rss from ph Iconify.design set."></iconify-icon></span></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../posts/index.html">📬 Posts</a></li><li class="breadcrumb-item"><a href="../../../posts/AuroraGPT/index.html">🤖 AuroraGPT</a></li><li class="breadcrumb-item"><a href="../../../posts/AuroraGPT/flash-attn-sunspot/index.html">📸 <code>flash-attn</code> on Sunspot</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../projects/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">📚 Projects</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../posts/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">📬 Posts</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/aurora-frameworks-2025-numpy-2/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">🚧 Frameworks Issue with <code>numpy &gt; 2</code></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/dope-slides/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">💅 How to Make Dope Slides</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/ezpz-at-alcf/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">🍋 <code>ezpz</code> @ ALCF</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/ezpz-v1/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">📝 ezpz-v1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/installing-pytorch-on-aurora/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">🔥 Building PyTorch from Source on Aurora</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/resume/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">👤 Résumé</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/svgbob/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">🫥 svgbob</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/torchtune-aurora/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">🪛 Torchtune on Aurora</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/torchtune-patch-aurora/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">🚑 Torchtune Patch on Aurora</span></a>
  </div>
</li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../posts/AuroraGPT/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">🤖 AuroraGPT</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth2 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/AuroraGPT/aurora-gpt/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">🏎️ Megatron-DeepSpeed on Intel XPU</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/AuroraGPT/checkpoints/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">💾 Converting Checkpoints</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/AuroraGPT/determinstic-flash-attn/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">🎰 Deterministic <code>flash-attn</code></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/AuroraGPT/flash-attn-sunspot/index.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">📸 <code>flash-attn</code> on Sunspot</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/AuroraGPT/long-sequences/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">🚂 Loooooooong Sequence Lengths</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/AuroraGPT/mpi4py-reproducer/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">🐛 <code>mpi4py</code> bug on Sunspot</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/AuroraGPT/spike-skipper/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">🏔️ Spike Skipper</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/AuroraGPT/startup-times/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">🐢 Starting Up Distributed Training on Aurora</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">⚛️ AI for Physics</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/ai-for-physics/diffusion/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">🎲 MCMC + Diffusion Sampling</span></a>
  </div>
</li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false">
 <span class="menu-text">🎢 L2HMC for LQCD</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth3 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/ai-for-physics/l2hmc-qcd/2dU1/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">🎢 <code>l2hmc-qcd</code> Example: 2D U(1)</span></a>
  </div>
</li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false">
 <span class="menu-text">4dSU3nb</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth4 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/ai-for-physics/l2hmc-qcd/4dSU3nb/index-broken.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">🕸️ l2hmc-qcd Example: 4D SU(3)</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false">
 <span class="menu-text">📗 Jupyter</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/jupyter/test/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">🏁 <code>l2hmc</code> Example: 2D <span class="math inline">U(1)</span></span></a>
  </div>
</li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false">
 <span class="menu-text">L2hmc</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth3 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/jupyter/l2hmc/4dSU3/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">🔳 <code>l2hmc-qcd</code> Example: 4D SU(3)</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../talks/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">🎙️ Talks</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../talks/ai-for-science-2024/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Parallel Training Methods</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../talks/alcf-hpc-workshop-2024/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deep Learning and Foundation Models at Scale</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../talks/aurora-gpt-fm-for-electric-grid/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AuroraGPT</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../talks/hpc-user-forum/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AuroraGPT</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../talks/incite-hackathon-2025/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Large Language Models on Aurora</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../talks/lattice23/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">MLMC: Machine Learning Monte Carlo</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../talks/llms-at-scale/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Training LLMs at Scale</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../talks/llms-on-polaris/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">LLMs on Polaris</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../talks/test/slides.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Test Rendering on Mobile</span></a>
  </div>
</li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false">
 <span class="menu-text">AuroraGPT</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../talks/AuroraGPT/alcf-hpc-workshop-2024/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AuroraGPT</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#update-2024-06-16" id="toc-update-2024-06-16" class="nav-link active" data-scroll-target="#update-2024-06-16">Update: 2024-06-16</a></li>
  <li><a href="#impact-on-loss-bug" id="toc-impact-on-loss-bug" class="nav-link" data-scroll-target="#impact-on-loss-bug">🐛 Impact on Loss [Bug?]</a></li>
  <li><a href="#llm-framework-release" id="toc-llm-framework-release" class="nav-link" data-scroll-target="#llm-framework-release">📦 LLM Framework Release</a>
  <ul class="collapse">
  <li><a href="#flash-no-flash" id="toc-flash-no-flash" class="nav-link" data-scroll-target="#flash-no-flash">📸 flash 🤝 📷 no-flash</a></li>
  <li><a href="#broken-mpigremlins" id="toc-broken-mpigremlins" class="nav-link" data-scroll-target="#broken-mpigremlins">🚧 Broken MPI</a></li>
  </ul></li>
  <li><a href="#framework-comparison" id="toc-framework-comparison" class="nav-link" data-scroll-target="#framework-comparison">🕵🏻‍ Framework Comparison</a>
  <ul class="collapse">
  <li><a href="#fix-in-disguise" id="toc-fix-in-disguise" class="nav-link" data-scroll-target="#fix-in-disguise">🥸 Fix in Disguise</a></li>
  <li><a href="#fix" id="toc-fix" class="nav-link" data-scroll-target="#fix">✅ <code>2024.0</code> Fix</a></li>
  <li><a href="#lr-decay-iters-comparison" id="toc-lr-decay-iters-comparison" class="nav-link" data-scroll-target="#lr-decay-iters-comparison">📊 <code>lr-decay-iters</code> Comparison</a></li>
  </ul></li>
  <li><a href="#lr-decay-iters-dependence" id="toc-lr-decay-iters-dependence" class="nav-link" data-scroll-target="#lr-decay-iters-dependence">📈 <code>lr-decay-iters</code> dependence</a></li>
  <li><a href="#performance-improvement-in-2024.1" id="toc-performance-improvement-in-2024.1" class="nav-link" data-scroll-target="#performance-improvement-in-2024.1">🏎️ Performance Improvement in <code>2024.1</code></a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/saforem2/personal_site/blob/main/posts/AuroraGPT/flash-attn-sunspot/index.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/saforem2/personal_site/edit/main/posts/AuroraGPT/flash-attn-sunspot/index.qmd" class="toc-action"><i class="bi empty"></i>Edit this page</a></li><li><a href="https://github.com/saforem2/personal_site/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-TC329HJ" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../posts/index.html">📬 Posts</a></li><li class="breadcrumb-item"><a href="../../../posts/AuroraGPT/index.html">🤖 AuroraGPT</a></li><li class="breadcrumb-item"><a href="../../../posts/AuroraGPT/flash-attn-sunspot/index.html">📸 <code>flash-attn</code> on Sunspot</a></li></ol></nav>
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">📸 <code>flash-attn</code> on Sunspot</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
  <div class="quarto-categories">
    <div class="quarto-category">ALCF</div>
    <div class="quarto-category">AuroraGPT</div>
    <div class="quarto-category">Aurora</div>
    <div class="quarto-category">Megatron-DeepSpeed</div>
    <div class="quarto-category">XPU</div>
  </div>
  </div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author"><a href="https://samforeman.me">Sam Foreman</a> <a href="mailto:foremans@anl.gov" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            <a href="https://alcf.anl.gov/about/people/sam-foreman">
            </a><a href="https://alcf.anl.gov/about/people/sam-foreman">ALCF</a>
            
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">June 17, 2024</p>
    </div>
  </div>
  
    <div>
    <div class="quarto-title-meta-heading">Modified</div>
    <div class="quarto-title-meta-contents">
      <p class="date-modified">April 26, 2025</p>
    </div>
  </div>
    
  </div>
  


</header>


<section id="update-2024-06-16" class="level2">
<h2 class="anchored" data-anchor-id="update-2024-06-16">Update: 2024-06-16</h2>
<p>After an interactive debug session with Intel, the root behavior of the apparent discrepancy was identified.</p>
<p>In particular, we found that the <a href="https://github.com/argonne-lcf/Megatron-DeepSpeed">ALCF/Megatron-DeepSpeed</a> repo was <strong>NOT</strong> explicitly setting the dropout values to <code>0.0</code> (and so, was using the default values of <code>0.1</code>) for both <code>--attention-dropout</code> and <code>--hidden-dropout</code>.</p>
<p>After making this change, the losses were observed to agree, as can be seen below in</p>
<div id="fig-flash-attn-dropout-fix" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-flash-attn-dropout-fix-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./assets/flash-attn/flash-attn-dropout-fix.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;1: After correctly setting the dropout values, the loss curves were observed to agree."><img src="./assets/flash-attn/flash-attn-dropout-fix.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-flash-attn-dropout-fix-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: After correctly setting the dropout values, the loss curves were observed to agree.
</figcaption>
</figure>
</div>
</section>
<section id="impact-on-loss-bug" class="level2">
<h2 class="anchored" data-anchor-id="impact-on-loss-bug">🐛 Impact on Loss [Bug?]</h2>
<p>In the <code>q4-drop</code>, it was observed that toggling <code>flash-attn</code> on / off seemed to produce different loss curves (with otherwise <em>identical configs</em>)</p>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>shared-config.yaml</strong></pre>
</div>
<div class="sourceCode" id="cb1" data-filename="shared-config.yaml"><pre class="sourceCode numberSource yaml number-lines code-with-copy"><code class="sourceCode yaml"><span id="cb1-1"><a href="#cb1-1"></a><span class="fu">TP</span><span class="kw">:</span><span class="at"> </span><span class="dv">1</span></span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="fu">PP</span><span class="kw">:</span><span class="at"> </span><span class="dv">1</span></span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="fu">GAS</span><span class="kw">:</span><span class="at"> </span><span class="dv">1</span></span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="fu">OPT</span><span class="kw">:</span><span class="at"> adamw</span></span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="fu">dtype</span><span class="kw">:</span><span class="at"> bf16</span></span>
<span id="cb1-6"><a href="#cb1-6"></a><span class="fu">NLAYERS</span><span class="kw">:</span><span class="at"> </span><span class="dv">10</span></span>
<span id="cb1-7"><a href="#cb1-7"></a><span class="fu">MICRO_BATCH</span><span class="kw">:</span><span class="at"> </span><span class="dv">2</span></span>
<span id="cb1-8"><a href="#cb1-8"></a><span class="fu">WORLD_SIZE</span><span class="kw">:</span><span class="at"> </span><span class="dv">24</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This can be seen clearly in the figure below:</p>
<p><a href="./assets/flash-attn/flash-attn-bug-q4-drop-sunspot-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2"><img src="./assets/flash-attn/flash-attn-bug-q4-drop-sunspot-1.png" class="img-fluid"></a></p>
<p>This was identified, and to be addressed in upcoming release.</p>
</section>
<section id="llm-framework-release" class="level2">
<h2 class="anchored" data-anchor-id="llm-framework-release">📦 LLM Framework Release</h2>
<p>On 05/14/2024, Intel dropped their new LLM frameworks release:</p>
<details closed="">
<summary>
🎁 <code>frameworks_2024_5_v2</code> Announcement:
</summary>
<p>Hi Venkat,</p>
<p>We have shared the official Q2 release in two different forms :</p>
<p>Manual Setup: <code>/gila/Aurora_deployment/anl_24_q2_release.tar.gz</code></p>
<p>and</p>
<p>Module:</p>
<p><code>module use -a /home/jmitche1/anl_release/2024/q2</code></p>
<p><code>module load frameworks_2024_5_v2</code></p>
<p>&nbsp;Instructions on how to use modules with Q2 build are anl_24_q2_release/README</p>
<ul>
<li><strong>The release includes :</strong>
<ul>
<li>Megatron-DeepSpeed 0.14.2 (with patch)</li>
<li>Intel® Extension for PyTorch* v2.1.30+xpu</li>
<li>TorchCCL 2.1.300</li>
<li>ONEAPI 2024.1.0.596.PUBLIC_IDP_2024.1.0_723</li>
<li>Agama driver: 803.29</li>
</ul></li>
<li>The release provides following key features:
<ul>
<li>Scaleup Performance improvement from the <code>TorchCCl</code> prototype feature enabled by <code>TORCH_LLM_ALLREDUCE=1</code> &nbsp;<a href="https://urldefense.us/v3/__https://github.com/intel/torch-ccl/releases/tag/v2.1.300*2Bxpu__;JQ!!G_uCfscf7eWS!ZDMnN0Oxp1sCv06MkdlBqFIq0NMAXaCBOtl3fEtBq8Fn4-3iYY5-kPEKr-q4vZIL_i6f2wQbULxAIFJAthJyu3VvNA$">details</a></li>
<li>Auto TP inference support for more workloads</li>
<li>Flash Attention V2 improvement for 256 head dimension support; MiCS support.</li>
<li>Latest Features and Optimizations from DeepSpeed&nbsp;<a href="https://github.com/microsoft/DeepSpeed/releases/tag/v0.14.2">0.14.2</a>&nbsp;and Intel® Extension for PyTorch*&nbsp;<a href="https://github.com/intel/intel-extension-for-pytorch/tree/v2.1.30*2Bxpu">2.1.30</a>.</li>
</ul></li>
</ul>
<p>Thanks,&nbsp;<br>
Jerome</p>
</details>
<section id="flash-no-flash" class="level3">
<h3 class="anchored" data-anchor-id="flash-no-flash">📸 flash 🤝 📷 no-flash</h3>
<p>With this new release, Intel observed that the loss curves agreed exactly for <code>flash</code> / no-<code>flash</code>, using the learning rate settings below:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode numberSource yaml number-lines code-with-copy"><code class="sourceCode yaml"><span id="cb2-1"><a href="#cb2-1"></a><span class="fu">lr</span><span class="kw">:</span><span class="at"> </span><span class="fl">0.00015</span></span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="fu">lr_warmup_frac</span><span class="kw">:</span><span class="at"> </span><span class="fl">0.01</span></span>
<span id="cb2-3"><a href="#cb2-3"></a><span class="fu">lr_decay_iters</span><span class="kw">:</span><span class="at"> </span><span class="dv">320000</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Testing with Jerome’s new release:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode numberSource sh number-lines code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1"></a><span class="ex">module</span> use <span class="at">-a</span> /home/jmitche1/anl_release/2024/q2</span>
<span id="cb3-2"><a href="#cb3-2"></a><span class="ex">module</span> load frameworks_2024_5_v2</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>I was able to independently confirm these results, shown in <a href="#📸%20flash%20🤝%20📷%20no-flash">📸 <code>flash</code> 🤝 📷 <code>no-flash</code></a> below.</p>
<details closed="">
<summary>
🔗 <code>wandb</code> links:
</summary>
<ul>
<li>[📸 <code>flash</code>] W&amp;B Run: <a href="https://wandb.ai/aurora_gpt/AuroraGPT/runs/716r5rnq/overview?nw=nwuserforemans">youthful-river-1832</a></li>
<li>[📷 no-<code>flash</code>] W&amp;B Run: <a href="https://wandb.ai/aurora_gpt/AuroraGPT/runs/120ln0b4/overview?nw=nwuserforemans">earthy-wave-1830</a></li>
</ul>
</details>
<details closed="">
<summary>
📸 <code>flash</code> vs.&nbsp;📷 <code>no-flash</code>
</summary>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./assets/flash-attn/flash-attn-sunspot-compare-new-release.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="flash 📸 🤝 📷 no-flash"><img src="./assets/flash-attn/flash-attn-sunspot-compare-new-release.png" class="img-fluid figure-img" alt="flash 📸 🤝 📷 no-flash"></a></p>
<figcaption><code>flash</code> 📸 🤝 📷 no-<code>flash</code></figcaption>
</figure>
</div>
</details>
</section>
<section id="broken-mpigremlins" class="level3">
<h3 class="anchored" data-anchor-id="broken-mpigremlins">🚧 Broken MPI<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></h3>
<p>For whatever reason, things seemed to have spontaneously broken on the night of 2024-04-14 ??</p>
<p>When trying to run experiments the following day (05/15/2024) I was met with this<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode numberSource sh number-lines code-with-copy"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1"></a><span class="ex">Abort</span><span class="er">(</span><span class="ex">15</span><span class="kw">)</span><span class="bu">:</span> Fatal error in internal_Init_thread: Other MPI error</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>which was discussed further in <a href="https://github.com/pmodels/mpich/pull/7001">this thread</a> on slack.</p>
<p>It seems Subrata also encountered a similar issue [see: <a href="https://cels-anl.slack.com/archives/C047E7ZTUUF/p1715985162970119">slack thread</a>]</p>
<details closed="">
<summary>
✅ <code>mpi4py</code> fix
</summary>
<p>To resolve this</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode numberSource sh number-lines code-with-copy"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1"></a><span class="ex">Abort</span><span class="er">(</span><span class="ex">15</span><span class="kw">)</span><span class="bu">:</span> Fatal error in internal_Init_thread: Other MPI error</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>issue we can simply load the correct modules:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode numberSource sh number-lines code-with-copy"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1"></a><span class="ex">module</span> use <span class="at">-a</span> /home/jmitche1/anl_release/2024/q2</span>
<span id="cb6-2"><a href="#cb6-2"></a><span class="ex">module</span> load frameworks_2024_5_v2</span>
<span id="cb6-3"><a href="#cb6-3"></a><span class="ex">module</span> use /home/ftartagl/graphics-compute-runtime/modulefiles</span>
<span id="cb6-4"><a href="#cb6-4"></a><span class="ex">module</span> load graphics-compute-runtime/agama-ci-devel-803.29 </span>
<span id="cb6-5"><a href="#cb6-5"></a><span class="ex">module</span> load spack-pe-gcc/0.6.1-23.275.2 gcc/12.2.0</span>
<span id="cb6-6"><a href="#cb6-6"></a><span class="ex">module</span> use /soft/preview-modulefiles/24.086.0</span>
<span id="cb6-7"><a href="#cb6-7"></a><span class="ex">module</span> load oneapi/release/2024.04.15.001</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>For full details see <a href="../../../posts/AuroraGPT/mpi4py-reproducer/index.html">mpi4py-reproducer</a>, and this [<a href="https://cels-anl.slack.com/archives/C05V0SRAVB6/p1715867557424879">slack thread</a>].</p>
</details>
</section>
</section>
<section id="framework-comparison" class="level2">
<h2 class="anchored" data-anchor-id="framework-comparison">🕵🏻‍ Framework Comparison</h2>
<p>As I was re-building MPI, and after talking to Jerome, I realized that <em>most</em> of the dependencies are already present in the provided <code>frameworks/</code> modules on Sunspot.</p>
<p>As a simple test, I tried building a new environment built on the base <code>conda</code> environment<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> provided by the<code>frameworks/2023.12.15.001</code> module, which worked without modification and had ) <em>most</em> of what I needed already installed:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">import</span> torch</span>
<span id="cb7-2"><a href="#cb7-2"></a><span class="op">&gt;&gt;&gt;</span> torch.__version__</span>
<span id="cb7-3"><a href="#cb7-3"></a><span class="co">'2.1.0a0+cxx11.abi'</span></span>
<span id="cb7-4"><a href="#cb7-4"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">import</span> intel_extension_for_pytorch <span class="im">as</span> ipex</span>
<span id="cb7-5"><a href="#cb7-5"></a><span class="op">&gt;&gt;&gt;</span> ipex.__version__</span>
<span id="cb7-6"><a href="#cb7-6"></a><span class="co">'2.1.10+xpu'</span></span>
<span id="cb7-7"><a href="#cb7-7"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">from</span> mpi4py <span class="im">import</span> MPI</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The remaining dependencies were installed according to the instructions from the <strong>new</strong> release <code>frameworks_2024_5_v2</code>.</p>
<p>Details included below.</p>
<details closed="">
<summary>
📦 <code>pip</code> Install Dependencies
</summary>
<p>Unfortunately, the <code>frameworks/**</code> don’t appear to provide DeepSpeed.</p>
<p>We can create a virtual environment on top of the base conda by</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode numberSource sh number-lines code-with-copy"><code class="sourceCode bash"><span id="cb8-1"><a href="#cb8-1"></a><span class="ex">$</span> module use frameworks/2023.12.15.001</span>
<span id="cb8-2"><a href="#cb8-2"></a><span class="ex">$</span> export PBS_O_WORKDIR=<span class="va">$(</span><span class="bu">pwd</span><span class="va">)</span> <span class="kw">;</span> <span class="bu">source</span> ALCF/helpers.sh <span class="kw">&amp;&amp;</span> <span class="ex">setup_venv_from_conda</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Once the <code>venv</code> has been created and activated, we can install the remaining dependencies:</p>
<p>To build / install DeepSpeed, along with its required dependencies:</p>
<ul>
<li><p><code>intel-extension-for-deepspeed</code>:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode numberSource sh number-lines code-with-copy"><code class="sourceCode bash"><span id="cb9-1"><a href="#cb9-1"></a><span class="ex">python3</span> <span class="at">-m</span> pip install intel_extension_for_pytorch_deepspeed<span class="dt">\=\=</span>2.1.30 <span class="at">-f</span> <span class="st">"https://pytorch-extension.intel.com/release-whl/stable/xpu/us/intel-extension-for-pytorch-deepspeed/"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p><code>DeepSpeed</code>:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode numberSource sh number-lines code-with-copy"><code class="sourceCode bash"><span id="cb10-1"><a href="#cb10-1"></a><span class="bu">echo</span> <span class="st">"build deepspeed"</span></span>
<span id="cb10-2"><a href="#cb10-2"></a><span class="fu">git</span> clone https://github.com/microsoft/DeepSpeed.git</span>
<span id="cb10-3"><a href="#cb10-3"></a><span class="bu">cd</span> DeepSpeed</span>
<span id="cb10-4"><a href="#cb10-4"></a><span class="fu">git</span> remote add yizhou_ds https://github.com/YizhouZ/DeepSpeed.git</span>
<span id="cb10-5"><a href="#cb10-5"></a><span class="fu">git</span> fetch yizhou_ds</span>
<span id="cb10-6"><a href="#cb10-6"></a><span class="fu">git</span> checkout yizhou/kernel_path</span>
<span id="cb10-7"><a href="#cb10-7"></a><span class="ex">pip</span> install <span class="at">-r</span> requirements/requirements.txt</span>
<span id="cb10-8"><a href="#cb10-8"></a><span class="ex">python</span> setup.py develop <span class="kw">|&amp;</span> <span class="fu">tee</span> build.log</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>Extras:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode numberSource sh number-lines code-with-copy"><code class="sourceCode bash"><span id="cb11-1"><a href="#cb11-1"></a><span class="ex">python3</span> <span class="at">-m</span> pip install transformers datasets python-etcd tensorboardX packaging sentencepiece bitsandbytes tiktoken neural-speed einops intel-extension-for-transformers</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ul>
</details>
<p>Looking around the available modules a bit, I noticed a newer frameworks release (<code>frameworks/2024.04.15.002</code>) that had a newer version of both <code>torch</code> and <code>ipex</code>:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode numberSource sh number-lines code-with-copy"><code class="sourceCode bash"><span id="cb12-1"><a href="#cb12-1"></a><span class="ex">module</span> use /soft/preview-modulefiles/24.086.0</span>
<span id="cb12-2"><a href="#cb12-2"></a><span class="ex">module</span> load frameworks/2024.04.15.002.lua</span>
<span id="cb12-3"><a href="#cb12-3"></a><span class="ex">python3</span> <span class="at">-c</span> <span class="st">'from mpi4py import MPI; print(MPI.__file__)'</span></span>
<span id="cb12-4"><a href="#cb12-4"></a><span class="co"># /soft/datascience/aurora_nre_models_frameworks-2024.1_preview_u1/lib/python3.9/site-packages/mpi4py/MPI.cpython-39-x86_64-linux-gnu.so</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">import</span> torch</span>
<span id="cb13-2"><a href="#cb13-2"></a><span class="op">&gt;&gt;&gt;</span> torch.__version__</span>
<span id="cb13-3"><a href="#cb13-3"></a><span class="co">'2.1.0.post2+cxx11.abi'</span></span>
<span id="cb13-4"><a href="#cb13-4"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">import</span> intel_extension_for_pytorch <span class="im">as</span> ipex</span>
<span id="cb13-5"><a href="#cb13-5"></a><span class="op">&gt;&gt;&gt;</span> ipex.__version__</span>
<span id="cb13-6"><a href="#cb13-6"></a><span class="co">'2.1.30+xpu'</span></span>
<span id="cb13-7"><a href="#cb13-7"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">from</span> mpi4py <span class="im">import</span> MPI<span class="op">;</span> <span class="bu">print</span>(MPI.<span class="va">__file__</span>)</span>
<span id="cb13-8"><a href="#cb13-8"></a><span class="op">/</span>soft<span class="op">/</span>datascience<span class="op">/</span>aurora_nre_models_frameworks<span class="op">-</span><span class="fl">2024.1</span><span class="er">_preview_u1</span><span class="op">/</span>lib<span class="op">/</span>python3<span class="fl">.9</span><span class="op">/</span>site<span class="op">-</span>packages<span class="op">/</span>mpi4py<span class="op">/</span>MPI.cpython<span class="op">-</span><span class="dv">39</span><span class="op">-</span>x86_64<span class="op">-</span>linux<span class="op">-</span>gnu.so</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The remaining dependencies were installed identically to what was just done previously for the <code>frameworks/2023.12.15.001</code> module.</p>
<p><strong>NOTE</strong>: In the figures below, we denote these two environments as:</p>
<ul>
<li><code>2024.0</code>:
<ul>
<li><code>module load frameworks/2023.12.15.001</code></li>
</ul></li>
<li><code>2024.1</code>:
<ul>
<li><code>module use /soft/preview-modulefiles/24.086.0</code></li>
<li><code>module load frameworks/2024.04.15.002.lua</code></li>
</ul></li>
<li><code>anl_24_q2_release</code>:
<ul>
<li><code>eval "$(~/miniconda3/bin/conda shell.zsh hook)"</code></li>
<li><code>conda activate anl_24_q2_release</code></li>
</ul></li>
</ul>
<section id="fix-in-disguise" class="level3">
<h3 class="anchored" data-anchor-id="fix-in-disguise">🥸 Fix in Disguise</h3>
<p>Armed now with functional environment(s) for <a href="https://github.com/argonne-lcf/Megatron-DeepSpeed"><code>argonne-lcf/Megatron-DeepSpeed</code></a>, I was able to resume my previous experiments.</p>
<p>From the discussion with Intel, it was hard to understand / reason about <em>why</em> the <code>flash-attn</code> fix would have <em>any</em> dependence on the learning rate schedule (warmup + decay).</p>
<p>If the <code>flash-attn</code> fix works for a particular learning rate schedule, you would reasonably expect that it should work for <em>any</em> learning rate schedule.</p>
<p>An additional source of confusion for me was that the discrepancy in the loss curves (seemingly) disappeared when using the learning rate settings provided by Intel<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>, but not when using the ALCF defaults<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>.</p>
<p>After thinking about it for a bit and trying to reason about possible causes, I wondered if it might not be a mix of multiple different factors:</p>
<ol type="1">
<li>Small learning rate</li>
<li>Very long decay</li>
<li>[maybe ?] somehow dependent on the learning rate warmup fraction
<ol type="1">
<li>preliminary experiments seemed to suggest this was not the case</li>
</ol></li>
</ol>
<p>So, I was curious what would happen if I used the (larger) learning rate value from the ALCF defaults (<code>lr=0.003</code>) with the very long <code>lr-decay-iters: 320000</code> from Intel.</p>
<p>These results are shown below.</p>
<p>In particular, for all three experiments the following learning rate settings were used:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode numberSource yml number-lines code-with-copy"><code class="sourceCode yaml"><span id="cb14-1"><a href="#cb14-1"></a><span class="fu">lr</span><span class="kw">:</span><span class="at"> </span><span class="fl">0.0003</span></span>
<span id="cb14-2"><a href="#cb14-2"></a><span class="fu">lr-warmup-frac</span><span class="kw">:</span><span class="at"> </span><span class="fl">0.05</span></span>
<span id="cb14-3"><a href="#cb14-3"></a><span class="fu">lr-decay-iters</span><span class="kw">:</span><span class="at"> </span><span class="dv">320000</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="./assets/flash-attn/flash-attn-disguise-decay10000-1.png" class="img-fluid" alt="flash-attn-disguise-decay10000-1"> Looking at this figure ^, it appears that up until the very very end, all three loss curves agree <em>identically</em>.</p>
<p>However, if we look closely at the very end, it looks like there <em>might</em> be a <em>slight</em> difference beginning to appear between the <code>2024.0</code> (brown line) and <code>{anl_24_q2_release, 2024.1}</code> ({dark, light} blue lines, respectively).</p>
<p>Thinking that I might be onto something, I then tried again with a smaller <code>lr-decay-iters: 5000</code>.</p>
<p>This result is shown below:</p>
<p><img src="./assets/flash-attn/flash-attn-disguise-decay5000.png" class="img-fluid" alt="flash-attn-disguise-decay5000"> In particular, we can now more clearly see the difference beginning to appear between the <code>2024.0</code> and <code>2024.1</code> loss curves.</p>
<p>Continuing on, we see this effect become increasingly dramatic with even smaller values of <code>lr-decay-iters</code>:</p>
<p><img src="./assets/flash-attn/flash-attn-disguise-decay-2000.png" class="img-fluid" alt="flash-attn-disguise-decay-2000"> <img src="./assets/flash-attn/flash-attn-disguise-decay1500.png" class="img-fluid" alt="flash-attn-disguise-decay1500"></p>
<p><img src="./assets/flash-attn/flash-attn-disguise-decay1000-1.png" class="img-fluid" alt="flash-attn-disguise-decay1000-1"> In each of these experiments, it appears that:</p>
<ul>
<li><code>2024.0</code>:
<ul>
<li>Not impacted by this <code>lr-decay-iters</code> dependence</li>
<li>Continue to decrease for the duration of training</li>
</ul></li>
<li><code>2024.1</code>:
<ul>
<li>Impacted by the <code>lr-decay-iters</code> dependence</li>
<li>Plateaus towards the end of training</li>
</ul></li>
</ul>
<details closed="">
<summary>
Older Figs
</summary>
<p><img src="./assets/flash-attn/disguised-fix-2.png" class="img-fluid" alt="disguised-fix-2"> <img src="./assets/flash-attn/disguised-fix-1.png" class="img-fluid" alt="disguised-fix-1"></p>
</details>
</section>
<section id="fix" class="level3">
<h3 class="anchored" data-anchor-id="fix">✅ <code>2024.0</code> Fix</h3>
<p><strong>Everything</strong> seems to work with</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode numberSource sh number-lines code-with-copy"><code class="sourceCode bash"><span id="cb15-1"><a href="#cb15-1"></a><span class="ex">module</span> load frameworks/2023.12.15.001</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="./assets/flash-attn/flash-attn-2024-0-fix.png" class="img-fluid"> <img src="./assets/flash-attn/flash-attn-fix-frameworks-comparison.png" class="img-fluid"></p>
</section>
<section id="lr-decay-iters-comparison" class="level3">
<h3 class="anchored" data-anchor-id="lr-decay-iters-comparison">📊 <code>lr-decay-iters</code> Comparison</h3>
<ul>
<li><code>2024.0</code>:
<ul>
<li><img src="./assets/flash-attn/decay-experiment-2024-0-1.png" class="img-fluid"></li>
</ul></li>
<li><code>2024.1</code>:
<ul>
<li><img src="./assets/flash-attn/decay-experiment-2024-1.png" class="img-fluid"></li>
</ul></li>
</ul>
</section>
</section>
<section id="lr-decay-iters-dependence" class="level2">
<h2 class="anchored" data-anchor-id="lr-decay-iters-dependence">📈 <code>lr-decay-iters</code> dependence</h2>
<p><a href="./assets/lr-decay-iters-dependence-2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4"><img src="./assets/lr-decay-iters-dependence-2.png" class="img-fluid"></a></p>
<p><img src="./assets/lr-schedule-dependence.png" class="img-fluid"> <img src="./assets/lr-decay-iters-dependence-1.png" class="img-fluid"></p>
</section>
<section id="performance-improvement-in-2024.1" class="level2">
<h2 class="anchored" data-anchor-id="performance-improvement-in-2024.1">🏎️ Performance Improvement in <code>2024.1</code></h2>
<p><a href="./assets/performance-2024-0.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5"><img src="./assets/performance-2024-0.png" class="img-fluid"></a></p>
<p><a href="./assets/performance-2024-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6"><img src="./assets/performance-2024-1.png" class="img-fluid"></a></p>
<div class="sourceCode" id="cb16"><pre class="sourceCode numberSource yml number-lines code-with-copy"><code class="sourceCode yaml"><span id="cb16-1"><a href="#cb16-1"></a><span class="fu">lr</span><span class="kw">:</span><span class="at"> </span><span class="fl">0.0003</span></span>
<span id="cb16-2"><a href="#cb16-2"></a><span class="fu">lr-warmup-frac</span><span class="kw">:</span><span class="at"> </span><span class="fl">0.05</span></span>
<span id="cb16-3"><a href="#cb16-3"></a><span class="fu">lr-decay-iters</span><span class="kw">:</span><span class="at"> </span><span class="ch">null</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>



</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p><a href="https://en.wikipedia.org/wiki/Gremlin">Gremlins</a>, likely<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>https://github.com/pmodels/mpich/pull/7001<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Explicitly, <code>aurora_nre_models_frameworks-2024.0</code>, abbreviated as <code>2024.0</code><a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Intel used the following learning rate schedule in their experiments <code>yml   lr: 0.00015   lr-warmup-frac: 0.01   lr-decay-iters: 320000</code><a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>ALCF used the following learning rate schedule in their experiments<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{foreman2024,
  author = {Foreman, Sam},
  title = {📸 `Flash-Attn` on {Sunspot}},
  date = {2024-06-17},
  url = {https://samforeman.me/posts/AuroraGPT/flash-attn-sunspot/},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-foreman2024" class="csl-entry quarto-appendix-citeas" role="listitem">
Foreman, Sam. 2024. <span>“📸 `Flash-Attn` on Sunspot.”</span> June 17,
2024. <a href="https://samforeman.me/posts/AuroraGPT/flash-attn-sunspot/">https://samforeman.me/posts/AuroraGPT/flash-attn-sunspot/</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/samforeman\.me");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<input type="hidden" id="giscus-base-theme" value="dark">
<input type="hidden" id="giscus-alt-theme" value="light">
<script>
  function loadGiscus() {
    // Function to get the theme based on body class
    const getTheme = () => {
      let baseTheme = document.getElementById('giscus-base-theme').value;
      let altTheme = document.getElementById('giscus-alt-theme').value;
      if (authorPrefersDark) {
          [baseTheme, altTheme] = [altTheme, baseTheme];
      }
      return document.body.classList.contains('quarto-dark') ? altTheme : baseTheme;
    };
    const script = document.createElement("script");
    script.src = "https://giscus.app/client.js";
    script.async = true;
    script.dataset.repo = "saforem2/personal_site";
    script.dataset.repoId = "R_kgDOGbjyRw";
    script.dataset.category = "General";
    script.dataset.categoryId = "DIC_kwDOGbjyR84CjWfk";
    script.dataset.mapping = "title";
    script.dataset.reactionsEnabled = "1";
    script.dataset.emitMetadata = "0";
    script.dataset.inputPosition = "top";
    script.dataset.theme = getTheme();
    script.dataset.lang = "en";
    script.crossOrigin = "anonymous";
    // Append the script to the desired div instead of at the end of the body
    document.getElementById("quarto-content").appendChild(script);
  }
  loadGiscus();
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../../posts/AuroraGPT/determinstic-flash-attn/index.html" class="pagination-link" aria-label="🎰 Deterministic `flash-attn`">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">🎰 Deterministic <code>flash-attn</code></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../posts/AuroraGPT/long-sequences/index.html" class="pagination-link" aria-label="🚂 Loooooooong Sequence Lengths">
        <span class="nav-page-text">🚂 Loooooooong Sequence Lengths</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb17" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb17-1"><a href="#cb17-1"></a><span class="co">---</span></span>
<span id="cb17-2"><a href="#cb17-2"></a><span class="an">title:</span><span class="co"> "📸 `flash-attn` on Sunspot"</span></span>
<span id="cb17-3"><a href="#cb17-3"></a><span class="an">toc:</span><span class="co"> true</span></span>
<span id="cb17-4"><a href="#cb17-4"></a><span class="an">date:</span><span class="co"> 2024-06-17</span></span>
<span id="cb17-5"><a href="#cb17-5"></a><span class="an">format:</span></span>
<span id="cb17-6"><a href="#cb17-6"></a><span class="co">  html: default</span></span>
<span id="cb17-7"><a href="#cb17-7"></a><span class="co"># format:</span></span>
<span id="cb17-8"><a href="#cb17-8"></a><span class="co">#   html: default</span></span>
<span id="cb17-9"><a href="#cb17-9"></a><span class="co">#  # ipynb: default</span></span>
<span id="cb17-10"><a href="#cb17-10"></a><span class="co">#   gfm:</span></span>
<span id="cb17-11"><a href="#cb17-11"></a><span class="co">#     output-file: "README.md"</span></span>
<span id="cb17-12"><a href="#cb17-12"></a><span class="an">format-links:</span><span class="co"> false</span></span>
<span id="cb17-13"><a href="#cb17-13"></a><span class="an">categories:</span></span>
<span id="cb17-14"><a href="#cb17-14"></a><span class="co">  - ALCF</span></span>
<span id="cb17-15"><a href="#cb17-15"></a><span class="co">  - AuroraGPT</span></span>
<span id="cb17-16"><a href="#cb17-16"></a><span class="co">  - Aurora</span></span>
<span id="cb17-17"><a href="#cb17-17"></a><span class="co">  - Megatron-DeepSpeed</span></span>
<span id="cb17-18"><a href="#cb17-18"></a><span class="co">  - XPU</span></span>
<span id="cb17-19"><a href="#cb17-19"></a><span class="co">---</span></span>
<span id="cb17-20"><a href="#cb17-20"></a></span>
<span id="cb17-21"><a href="#cb17-21"></a><span class="fu">## Update: 2024-06-16</span></span>
<span id="cb17-22"><a href="#cb17-22"></a></span>
<span id="cb17-23"><a href="#cb17-23"></a>After an interactive debug session with Intel, the root behavior of the apparent</span>
<span id="cb17-24"><a href="#cb17-24"></a>discrepancy was identified.</span>
<span id="cb17-25"><a href="#cb17-25"></a></span>
<span id="cb17-26"><a href="#cb17-26"></a>In particular, we found that the</span>
<span id="cb17-27"><a href="#cb17-27"></a><span class="co">[</span><span class="ot">ALCF/Megatron-DeepSpeed</span><span class="co">](https://github.com/argonne-lcf/Megatron-DeepSpeed)</span></span>
<span id="cb17-28"><a href="#cb17-28"></a>repo was **NOT** explicitly setting the dropout values to <span class="in">`0.0`</span> (and so, was</span>
<span id="cb17-29"><a href="#cb17-29"></a>using the default values of <span class="in">`0.1`</span>) for both <span class="in">`--attention-dropout`</span> and</span>
<span id="cb17-30"><a href="#cb17-30"></a><span class="in">`--hidden-dropout`</span>.</span>
<span id="cb17-31"><a href="#cb17-31"></a></span>
<span id="cb17-32"><a href="#cb17-32"></a>After making this change, the losses were observed to agree, as can be seen</span>
<span id="cb17-33"><a href="#cb17-33"></a>below in </span>
<span id="cb17-34"><a href="#cb17-34"></a></span>
<span id="cb17-35"><a href="#cb17-35"></a>::: {#fig-flash-attn-dropout-fix}</span>
<span id="cb17-36"><a href="#cb17-36"></a></span>
<span id="cb17-37"><a href="#cb17-37"></a><span class="al">![](./assets/flash-attn/flash-attn-dropout-fix.png)</span></span>
<span id="cb17-38"><a href="#cb17-38"></a></span>
<span id="cb17-39"><a href="#cb17-39"></a>After correctly setting the dropout values, the loss curves were observed to</span>
<span id="cb17-40"><a href="#cb17-40"></a>agree.</span>
<span id="cb17-41"><a href="#cb17-41"></a></span>
<span id="cb17-42"><a href="#cb17-42"></a>:::</span>
<span id="cb17-43"><a href="#cb17-43"></a></span>
<span id="cb17-44"><a href="#cb17-44"></a><span class="fu">## 🐛 Impact on Loss \[Bug?\]</span></span>
<span id="cb17-45"><a href="#cb17-45"></a></span>
<span id="cb17-46"><a href="#cb17-46"></a>In the <span class="in">`q4-drop`</span>, it was observed that toggling <span class="in">`flash-attn`</span> on / off seemed to produce different loss curves (with otherwise *identical configs*)</span>
<span id="cb17-47"><a href="#cb17-47"></a></span>
<span id="cb17-48"><a href="#cb17-48"></a><span class="in">```yaml {filename="shared-config.yaml" code-line-numbers="true"}</span></span>
<span id="cb17-49"><a href="#cb17-49"></a><span class="fu">TP</span><span class="kw">:</span><span class="at"> </span><span class="dv">1</span></span>
<span id="cb17-50"><a href="#cb17-50"></a><span class="fu">PP</span><span class="kw">:</span><span class="at"> </span><span class="dv">1</span></span>
<span id="cb17-51"><a href="#cb17-51"></a><span class="fu">GAS</span><span class="kw">:</span><span class="at"> </span><span class="dv">1</span></span>
<span id="cb17-52"><a href="#cb17-52"></a><span class="fu">OPT</span><span class="kw">:</span><span class="at"> adamw</span></span>
<span id="cb17-53"><a href="#cb17-53"></a><span class="fu">dtype</span><span class="kw">:</span><span class="at"> bf16</span></span>
<span id="cb17-54"><a href="#cb17-54"></a><span class="fu">NLAYERS</span><span class="kw">:</span><span class="at"> </span><span class="dv">10</span></span>
<span id="cb17-55"><a href="#cb17-55"></a><span class="fu">MICRO_BATCH</span><span class="kw">:</span><span class="at"> </span><span class="dv">2</span></span>
<span id="cb17-56"><a href="#cb17-56"></a><span class="fu">WORLD_SIZE</span><span class="kw">:</span><span class="at"> </span><span class="dv">24</span></span>
<span id="cb17-57"><a href="#cb17-57"></a><span class="in">```</span></span>
<span id="cb17-58"><a href="#cb17-58"></a></span>
<span id="cb17-59"><a href="#cb17-59"></a>This can be seen clearly in the figure below:</span>
<span id="cb17-60"><a href="#cb17-60"></a></span>
<span id="cb17-61"><a href="#cb17-61"></a><span class="al">![](./assets/flash-attn/flash-attn-bug-q4-drop-sunspot-1.png)</span></span>
<span id="cb17-62"><a href="#cb17-62"></a></span>
<span id="cb17-63"><a href="#cb17-63"></a>This was identified, and to be addressed in upcoming release.</span>
<span id="cb17-64"><a href="#cb17-64"></a></span>
<span id="cb17-65"><a href="#cb17-65"></a><span class="fu">## 📦 LLM Framework Release</span></span>
<span id="cb17-66"><a href="#cb17-66"></a></span>
<span id="cb17-67"><a href="#cb17-67"></a>On 05/14/2024, Intel dropped their new LLM frameworks release:</span>
<span id="cb17-68"><a href="#cb17-68"></a></span>
<span id="cb17-69"><a href="#cb17-69"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="ot"> closed</span><span class="dt">&gt;&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>🎁 <span class="dt">&lt;</span><span class="kw">code</span><span class="dt">&gt;</span>frameworks_2024_5_v2<span class="dt">&lt;/</span><span class="kw">code</span><span class="dt">&gt;</span> Announcement:<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb17-70"><a href="#cb17-70"></a></span>
<span id="cb17-71"><a href="#cb17-71"></a>Hi Venkat,</span>
<span id="cb17-72"><a href="#cb17-72"></a></span>
<span id="cb17-73"><a href="#cb17-73"></a>We have shared the official Q2 release in two different forms :</span>
<span id="cb17-74"><a href="#cb17-74"></a></span>
<span id="cb17-75"><a href="#cb17-75"></a>Manual Setup: <span class="in">`/gila/Aurora_deployment/anl_24_q2_release.tar.gz`</span></span>
<span id="cb17-76"><a href="#cb17-76"></a></span>
<span id="cb17-77"><a href="#cb17-77"></a>and</span>
<span id="cb17-78"><a href="#cb17-78"></a></span>
<span id="cb17-79"><a href="#cb17-79"></a>Module:</span>
<span id="cb17-80"><a href="#cb17-80"></a></span>
<span id="cb17-81"><a href="#cb17-81"></a><span class="in">`module use -a /home/jmitche1/anl_release/2024/q2`</span></span>
<span id="cb17-82"><a href="#cb17-82"></a></span>
<span id="cb17-83"><a href="#cb17-83"></a><span class="in">`module load frameworks_2024_5_v2`</span></span>
<span id="cb17-84"><a href="#cb17-84"></a></span>
<span id="cb17-85"><a href="#cb17-85"></a>&nbsp;Instructions on how to use modules with Q2 build are anl_24_q2_release/README</span>
<span id="cb17-86"><a href="#cb17-86"></a></span>
<span id="cb17-87"><a href="#cb17-87"></a><span class="ss">- </span>**The release includes :**</span>
<span id="cb17-88"><a href="#cb17-88"></a><span class="ss">    - </span>Megatron-DeepSpeed 0.14.2 (with patch)</span>
<span id="cb17-89"><a href="#cb17-89"></a><span class="ss">    - </span>Intel® Extension for PyTorch* v2.1.30+xpu</span>
<span id="cb17-90"><a href="#cb17-90"></a><span class="ss">    - </span>TorchCCL 2.1.300</span>
<span id="cb17-91"><a href="#cb17-91"></a><span class="ss">    - </span>ONEAPI 2024.1.0.596.PUBLIC_IDP_2024.1.0_723</span>
<span id="cb17-92"><a href="#cb17-92"></a><span class="ss">    - </span>Agama driver: 803.29</span>
<span id="cb17-93"><a href="#cb17-93"></a></span>
<span id="cb17-94"><a href="#cb17-94"></a><span class="ss">- </span>The release provides following key features:</span>
<span id="cb17-95"><a href="#cb17-95"></a><span class="ss">    - </span>Scaleup Performance improvement from the <span class="in">`TorchCCl`</span> prototype feature enabled by <span class="in">`TORCH_LLM_ALLREDUCE=1`</span> &nbsp;<span class="co">[</span><span class="ot">details</span><span class="co">](https://urldefense.us/v3/__https://github.com/intel/torch-ccl/releases/tag/v2.1.300*2Bxpu__;JQ!!G_uCfscf7eWS!ZDMnN0Oxp1sCv06MkdlBqFIq0NMAXaCBOtl3fEtBq8Fn4-3iYY5-kPEKr-q4vZIL_i6f2wQbULxAIFJAthJyu3VvNA$)</span></span>
<span id="cb17-96"><a href="#cb17-96"></a><span class="ss">    - </span>Auto TP inference support for more workloads</span>
<span id="cb17-97"><a href="#cb17-97"></a><span class="ss">    - </span>Flash Attention V2 improvement for 256 head dimension support; MiCS support.</span>
<span id="cb17-98"><a href="#cb17-98"></a><span class="ss">    - </span>Latest Features and Optimizations from DeepSpeed&nbsp;<span class="co">[</span><span class="ot">0.14.2</span><span class="co">](https://github.com/microsoft/DeepSpeed/releases/tag/v0.14.2)</span>&nbsp;and Intel® Extension for PyTorch*&nbsp;<span class="co">[</span><span class="ot">2.1.30</span><span class="co">](https://github.com/intel/intel-extension-for-pytorch/tree/v2.1.30*2Bxpu)</span>.</span>
<span id="cb17-99"><a href="#cb17-99"></a></span>
<span id="cb17-100"><a href="#cb17-100"></a>Thanks,&nbsp;  </span>
<span id="cb17-101"><a href="#cb17-101"></a>Jerome</span>
<span id="cb17-102"><a href="#cb17-102"></a></span>
<span id="cb17-103"><a href="#cb17-103"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb17-104"><a href="#cb17-104"></a></span>
<span id="cb17-105"><a href="#cb17-105"></a><span class="fu">### 📸 flash 🤝 📷 no-flash</span></span>
<span id="cb17-106"><a href="#cb17-106"></a></span>
<span id="cb17-107"><a href="#cb17-107"></a>With this new release, Intel observed that the loss curves agreed exactly for <span class="in">`flash`</span> / no-<span class="in">`flash`</span>, using the learning rate settings below:</span>
<span id="cb17-108"><a href="#cb17-108"></a></span>
<span id="cb17-109"><a href="#cb17-109"></a><span class="in">```yaml</span></span>
<span id="cb17-110"><a href="#cb17-110"></a><span class="fu">lr</span><span class="kw">:</span><span class="at"> </span><span class="fl">0.00015</span></span>
<span id="cb17-111"><a href="#cb17-111"></a><span class="fu">lr_warmup_frac</span><span class="kw">:</span><span class="at"> </span><span class="fl">0.01</span></span>
<span id="cb17-112"><a href="#cb17-112"></a><span class="fu">lr_decay_iters</span><span class="kw">:</span><span class="at"> </span><span class="dv">320000</span></span>
<span id="cb17-113"><a href="#cb17-113"></a><span class="in">```</span></span>
<span id="cb17-114"><a href="#cb17-114"></a></span>
<span id="cb17-115"><a href="#cb17-115"></a>Testing with Jerome's new release:</span>
<span id="cb17-116"><a href="#cb17-116"></a></span>
<span id="cb17-117"><a href="#cb17-117"></a><span class="in">```sh</span></span>
<span id="cb17-118"><a href="#cb17-118"></a><span class="ex">module</span> use <span class="at">-a</span> /home/jmitche1/anl_release/2024/q2</span>
<span id="cb17-119"><a href="#cb17-119"></a><span class="ex">module</span> load frameworks_2024_5_v2</span>
<span id="cb17-120"><a href="#cb17-120"></a><span class="in">```</span></span>
<span id="cb17-121"><a href="#cb17-121"></a></span>
<span id="cb17-122"><a href="#cb17-122"></a>I was able to independently confirm these results, shown in <span class="co">[</span><span class="ot">📸 `flash` 🤝 📷 `no-flash`</span><span class="co">](#📸%20flash%20🤝%20📷%20no-flash)</span> below.</span>
<span id="cb17-123"><a href="#cb17-123"></a></span>
<span id="cb17-124"><a href="#cb17-124"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="ot"> closed</span><span class="dt">&gt;&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>🔗 <span class="dt">&lt;</span><span class="kw">code</span><span class="dt">&gt;</span>wandb<span class="dt">&lt;/</span><span class="kw">code</span><span class="dt">&gt;</span> links:<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb17-125"><a href="#cb17-125"></a></span>
<span id="cb17-126"><a href="#cb17-126"></a><span class="ss">- </span><span class="sc">\[</span>📸 <span class="in">`flash`</span><span class="sc">\]</span> W&amp;B Run: <span class="co">[</span><span class="ot">youthful-river-1832</span><span class="co">](https://wandb.ai/aurora_gpt/AuroraGPT/runs/716r5rnq/overview?nw=nwuserforemans)</span></span>
<span id="cb17-127"><a href="#cb17-127"></a><span class="ss">- </span><span class="sc">\[</span>📷 no-<span class="in">`flash`</span><span class="sc">\]</span> W&amp;B Run: <span class="co">[</span><span class="ot">earthy-wave-1830</span><span class="co">](https://wandb.ai/aurora_gpt/AuroraGPT/runs/120ln0b4/overview?nw=nwuserforemans)</span></span>
<span id="cb17-128"><a href="#cb17-128"></a></span>
<span id="cb17-129"><a href="#cb17-129"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb17-130"><a href="#cb17-130"></a></span>
<span id="cb17-131"><a href="#cb17-131"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="ot"> closed</span><span class="dt">&gt;&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>📸 <span class="dt">&lt;</span><span class="kw">code</span><span class="dt">&gt;</span>flash<span class="dt">&lt;/</span><span class="kw">code</span><span class="dt">&gt;</span> vs. 📷 <span class="dt">&lt;</span><span class="kw">code</span><span class="dt">&gt;</span>no-flash<span class="dt">&lt;/</span><span class="kw">code</span><span class="dt">&gt;&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb17-132"><a href="#cb17-132"></a></span>
<span id="cb17-133"><a href="#cb17-133"></a><span class="al">![`flash` 📸 🤝 📷 no-`flash`](./assets/flash-attn/flash-attn-sunspot-compare-new-release.png)</span></span>
<span id="cb17-134"><a href="#cb17-134"></a></span>
<span id="cb17-135"><a href="#cb17-135"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb17-136"><a href="#cb17-136"></a></span>
<span id="cb17-137"><a href="#cb17-137"></a><span class="fu">### 🚧 Broken MPI[^gremlins]</span></span>
<span id="cb17-138"><a href="#cb17-138"></a></span>
<span id="cb17-139"><a href="#cb17-139"></a>For whatever reason, things seemed to have spontaneously broken on the night of</span>
<span id="cb17-140"><a href="#cb17-140"></a>2024-04-14 ??</span>
<span id="cb17-141"><a href="#cb17-141"></a></span>
<span id="cb17-142"><a href="#cb17-142"></a><span class="ot">[^gremlins]: </span><span class="co">[</span><span class="ot">Gremlins</span><span class="co">](https://en.wikipedia.org/wiki/Gremlin)</span>, likely</span>
<span id="cb17-143"><a href="#cb17-143"></a></span>
<span id="cb17-144"><a href="#cb17-144"></a>When trying to run experiments the following day (05/15/2024) I was met with</span>
<span id="cb17-145"><a href="#cb17-145"></a>this<span class="ot">[^mpi-issue]</span>:</span>
<span id="cb17-146"><a href="#cb17-146"></a></span>
<span id="cb17-147"><a href="#cb17-147"></a><span class="in">```sh</span></span>
<span id="cb17-148"><a href="#cb17-148"></a><span class="ex">Abort</span><span class="er">(</span><span class="ex">15</span><span class="kw">)</span><span class="bu">:</span> Fatal error in internal_Init_thread: Other MPI error</span>
<span id="cb17-149"><a href="#cb17-149"></a><span class="in">```</span></span>
<span id="cb17-150"><a href="#cb17-150"></a></span>
<span id="cb17-151"><a href="#cb17-151"></a><span class="ot">[^mpi-issue]: https://github.com/pmodels/mpich/pull/7001</span></span>
<span id="cb17-152"><a href="#cb17-152"></a></span>
<span id="cb17-153"><a href="#cb17-153"></a>which was discussed further in <span class="co">[</span><span class="ot">this thread</span><span class="co">](https://github.com/pmodels/mpich/pull/7001)</span> on slack.</span>
<span id="cb17-154"><a href="#cb17-154"></a></span>
<span id="cb17-155"><a href="#cb17-155"></a>It seems Subrata also encountered a similar issue <span class="sc">\[</span>see: <span class="co">[</span><span class="ot">slack thread</span><span class="co">](https://cels-anl.slack.com/archives/C047E7ZTUUF/p1715985162970119)</span><span class="sc">\]</span></span>
<span id="cb17-156"><a href="#cb17-156"></a></span>
<span id="cb17-157"><a href="#cb17-157"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="ot"> closed</span><span class="dt">&gt;&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>✅ <span class="dt">&lt;</span><span class="kw">code</span><span class="dt">&gt;</span>mpi4py<span class="dt">&lt;/</span><span class="kw">code</span><span class="dt">&gt;</span> fix<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb17-158"><a href="#cb17-158"></a></span>
<span id="cb17-159"><a href="#cb17-159"></a><span class="co">&lt;!-- `mpi4py` fix --&gt;</span></span>
<span id="cb17-160"><a href="#cb17-160"></a></span>
<span id="cb17-161"><a href="#cb17-161"></a>To resolve this</span>
<span id="cb17-162"><a href="#cb17-162"></a></span>
<span id="cb17-163"><a href="#cb17-163"></a><span class="in">```sh</span></span>
<span id="cb17-164"><a href="#cb17-164"></a><span class="ex">Abort</span><span class="er">(</span><span class="ex">15</span><span class="kw">)</span><span class="bu">:</span> Fatal error in internal_Init_thread: Other MPI error</span>
<span id="cb17-165"><a href="#cb17-165"></a><span class="in">```</span></span>
<span id="cb17-166"><a href="#cb17-166"></a></span>
<span id="cb17-167"><a href="#cb17-167"></a>issue we can simply load the correct modules:</span>
<span id="cb17-168"><a href="#cb17-168"></a></span>
<span id="cb17-169"><a href="#cb17-169"></a><span class="in">```sh</span></span>
<span id="cb17-170"><a href="#cb17-170"></a><span class="ex">module</span> use <span class="at">-a</span> /home/jmitche1/anl_release/2024/q2</span>
<span id="cb17-171"><a href="#cb17-171"></a><span class="ex">module</span> load frameworks_2024_5_v2</span>
<span id="cb17-172"><a href="#cb17-172"></a><span class="ex">module</span> use /home/ftartagl/graphics-compute-runtime/modulefiles</span>
<span id="cb17-173"><a href="#cb17-173"></a><span class="ex">module</span> load graphics-compute-runtime/agama-ci-devel-803.29 </span>
<span id="cb17-174"><a href="#cb17-174"></a><span class="ex">module</span> load spack-pe-gcc/0.6.1-23.275.2 gcc/12.2.0</span>
<span id="cb17-175"><a href="#cb17-175"></a><span class="ex">module</span> use /soft/preview-modulefiles/24.086.0</span>
<span id="cb17-176"><a href="#cb17-176"></a><span class="ex">module</span> load oneapi/release/2024.04.15.001</span>
<span id="cb17-177"><a href="#cb17-177"></a><span class="in">```</span></span>
<span id="cb17-178"><a href="#cb17-178"></a></span>
<span id="cb17-179"><a href="#cb17-179"></a>For full details see <span class="co">[</span><span class="ot">mpi4py-reproducer</span><span class="co">](../mpi4py-reproducer/index.qmd)</span>, and</span>
<span id="cb17-180"><a href="#cb17-180"></a>this <span class="sc">\[</span>[slack</span>
<span id="cb17-181"><a href="#cb17-181"></a>thread](https://cels-anl.slack.com/archives/C05V0SRAVB6/p1715867557424879)<span class="sc">\]</span>.</span>
<span id="cb17-182"><a href="#cb17-182"></a></span>
<span id="cb17-183"><a href="#cb17-183"></a><span class="co">&lt;!-- ::: --&gt;</span></span>
<span id="cb17-184"><a href="#cb17-184"></a></span>
<span id="cb17-185"><a href="#cb17-185"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb17-186"><a href="#cb17-186"></a></span>
<span id="cb17-187"><a href="#cb17-187"></a><span class="fu">## 🕵🏻‍  Framework Comparison</span></span>
<span id="cb17-188"><a href="#cb17-188"></a></span>
<span id="cb17-189"><a href="#cb17-189"></a>As I was re-building MPI, and after talking to Jerome, I realized that _most_ of</span>
<span id="cb17-190"><a href="#cb17-190"></a>the dependencies are already present in the provided <span class="in">`frameworks/`</span> modules on</span>
<span id="cb17-191"><a href="#cb17-191"></a>Sunspot.</span>
<span id="cb17-192"><a href="#cb17-192"></a></span>
<span id="cb17-193"><a href="#cb17-193"></a>As a simple test, I tried building a new environment built on the base <span class="in">`conda`</span></span>
<span id="cb17-194"><a href="#cb17-194"></a>environment<span class="ot">[^2024-0]</span> provided by the<span class="in">`frameworks/2023.12.15.001`</span> module, which worked without modification and had ) _most_ of what I needed already installed:</span>
<span id="cb17-195"><a href="#cb17-195"></a></span>
<span id="cb17-196"><a href="#cb17-196"></a><span class="in">```python</span></span>
<span id="cb17-197"><a href="#cb17-197"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">import</span> torch</span>
<span id="cb17-198"><a href="#cb17-198"></a><span class="op">&gt;&gt;&gt;</span> torch.__version__</span>
<span id="cb17-199"><a href="#cb17-199"></a><span class="co">'2.1.0a0+cxx11.abi'</span></span>
<span id="cb17-200"><a href="#cb17-200"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">import</span> intel_extension_for_pytorch <span class="im">as</span> ipex</span>
<span id="cb17-201"><a href="#cb17-201"></a><span class="op">&gt;&gt;&gt;</span> ipex.__version__</span>
<span id="cb17-202"><a href="#cb17-202"></a><span class="co">'2.1.10+xpu'</span></span>
<span id="cb17-203"><a href="#cb17-203"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">from</span> mpi4py <span class="im">import</span> MPI</span>
<span id="cb17-204"><a href="#cb17-204"></a><span class="in">```</span></span>
<span id="cb17-205"><a href="#cb17-205"></a></span>
<span id="cb17-206"><a href="#cb17-206"></a><span class="ot">[^2024-0]: </span>Explicitly, <span class="in">`aurora_nre_models_frameworks-2024.0`</span>, abbreviated as <span class="in">`2024.0`</span></span>
<span id="cb17-207"><a href="#cb17-207"></a></span>
<span id="cb17-208"><a href="#cb17-208"></a>The remaining dependencies were installed according to the instructions from the **new** release <span class="in">`frameworks_2024_5_v2`</span>.</span>
<span id="cb17-209"><a href="#cb17-209"></a></span>
<span id="cb17-210"><a href="#cb17-210"></a>Details included below.</span>
<span id="cb17-211"><a href="#cb17-211"></a></span>
<span id="cb17-212"><a href="#cb17-212"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="ot"> closed</span><span class="dt">&gt;&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>📦 <span class="dt">&lt;</span><span class="kw">code</span><span class="dt">&gt;</span>pip<span class="dt">&lt;/</span><span class="kw">code</span><span class="dt">&gt;</span> Install Dependencies<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb17-213"><a href="#cb17-213"></a></span>
<span id="cb17-214"><a href="#cb17-214"></a></span>
<span id="cb17-215"><a href="#cb17-215"></a>Unfortunately, the <span class="in">`frameworks/**`</span> don't appear to provide DeepSpeed.</span>
<span id="cb17-216"><a href="#cb17-216"></a></span>
<span id="cb17-217"><a href="#cb17-217"></a>We can create a virtual environment on top of the base conda by</span>
<span id="cb17-218"><a href="#cb17-218"></a></span>
<span id="cb17-219"><a href="#cb17-219"></a><span class="in">```sh</span></span>
<span id="cb17-220"><a href="#cb17-220"></a><span class="ex">$</span> module use frameworks/2023.12.15.001</span>
<span id="cb17-221"><a href="#cb17-221"></a><span class="ex">$</span> export PBS_O_WORKDIR=<span class="va">$(</span><span class="bu">pwd</span><span class="va">)</span> <span class="kw">;</span> <span class="bu">source</span> ALCF/helpers.sh <span class="kw">&amp;&amp;</span> <span class="ex">setup_venv_from_conda</span></span>
<span id="cb17-222"><a href="#cb17-222"></a><span class="in">```</span></span>
<span id="cb17-223"><a href="#cb17-223"></a></span>
<span id="cb17-224"><a href="#cb17-224"></a>Once the <span class="in">`venv`</span> has been created and activated, we can install the remaining dependencies:</span>
<span id="cb17-225"><a href="#cb17-225"></a></span>
<span id="cb17-226"><a href="#cb17-226"></a>To build / install DeepSpeed, along with its required dependencies:</span>
<span id="cb17-227"><a href="#cb17-227"></a></span>
<span id="cb17-228"><a href="#cb17-228"></a><span class="ss">- </span><span class="in">`intel-extension-for-deepspeed`</span>:</span>
<span id="cb17-229"><a href="#cb17-229"></a></span>
<span id="cb17-230"><a href="#cb17-230"></a>    <span class="in">```sh</span></span>
<span id="cb17-231"><a href="#cb17-231"></a>    <span class="ex">python3</span> <span class="at">-m</span> pip install intel_extension_for_pytorch_deepspeed<span class="dt">\=\=</span>2.1.30 <span class="at">-f</span> <span class="st">"https://pytorch-extension.intel.com/release-whl/stable/xpu/us/intel-extension-for-pytorch-deepspeed/"</span></span>
<span id="cb17-232"><a href="#cb17-232"></a>    <span class="in">```</span></span>
<span id="cb17-233"><a href="#cb17-233"></a></span>
<span id="cb17-234"><a href="#cb17-234"></a><span class="ss">- </span><span class="in">`DeepSpeed`</span>:</span>
<span id="cb17-235"><a href="#cb17-235"></a></span>
<span id="cb17-236"><a href="#cb17-236"></a>    <span class="in">```sh</span></span>
<span id="cb17-237"><a href="#cb17-237"></a>    <span class="bu">echo</span> <span class="st">"build deepspeed"</span></span>
<span id="cb17-238"><a href="#cb17-238"></a>    <span class="fu">git</span> clone https://github.com/microsoft/DeepSpeed.git</span>
<span id="cb17-239"><a href="#cb17-239"></a>    <span class="bu">cd</span> DeepSpeed</span>
<span id="cb17-240"><a href="#cb17-240"></a>    <span class="fu">git</span> remote add yizhou_ds https://github.com/YizhouZ/DeepSpeed.git</span>
<span id="cb17-241"><a href="#cb17-241"></a>    <span class="fu">git</span> fetch yizhou_ds</span>
<span id="cb17-242"><a href="#cb17-242"></a>    <span class="fu">git</span> checkout yizhou/kernel_path</span>
<span id="cb17-243"><a href="#cb17-243"></a>    <span class="ex">pip</span> install <span class="at">-r</span> requirements/requirements.txt</span>
<span id="cb17-244"><a href="#cb17-244"></a>    <span class="ex">python</span> setup.py develop <span class="kw">|&amp;</span> <span class="fu">tee</span> build.log</span>
<span id="cb17-245"><a href="#cb17-245"></a>    <span class="in">```</span></span>
<span id="cb17-246"><a href="#cb17-246"></a></span>
<span id="cb17-247"><a href="#cb17-247"></a></span>
<span id="cb17-248"><a href="#cb17-248"></a><span class="ss">- </span>Extras:</span>
<span id="cb17-249"><a href="#cb17-249"></a></span>
<span id="cb17-250"><a href="#cb17-250"></a>    <span class="in">```sh</span></span>
<span id="cb17-251"><a href="#cb17-251"></a>    <span class="ex">python3</span> <span class="at">-m</span> pip install transformers datasets python-etcd tensorboardX packaging sentencepiece bitsandbytes tiktoken neural-speed einops intel-extension-for-transformers</span>
<span id="cb17-252"><a href="#cb17-252"></a>    <span class="in">```</span></span>
<span id="cb17-253"><a href="#cb17-253"></a></span>
<span id="cb17-254"><a href="#cb17-254"></a></span>
<span id="cb17-255"><a href="#cb17-255"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb17-256"><a href="#cb17-256"></a></span>
<span id="cb17-257"><a href="#cb17-257"></a>Looking around the available modules a bit, I noticed a newer frameworks release (<span class="in">`frameworks/2024.04.15.002`</span>) that had a newer version of both <span class="in">`torch`</span> and <span class="in">`ipex`</span>:</span>
<span id="cb17-258"><a href="#cb17-258"></a></span>
<span id="cb17-259"><a href="#cb17-259"></a><span class="in">```sh</span></span>
<span id="cb17-260"><a href="#cb17-260"></a><span class="ex">module</span> use /soft/preview-modulefiles/24.086.0</span>
<span id="cb17-261"><a href="#cb17-261"></a><span class="ex">module</span> load frameworks/2024.04.15.002.lua</span>
<span id="cb17-262"><a href="#cb17-262"></a><span class="ex">python3</span> <span class="at">-c</span> <span class="st">'from mpi4py import MPI; print(MPI.__file__)'</span></span>
<span id="cb17-263"><a href="#cb17-263"></a><span class="co"># /soft/datascience/aurora_nre_models_frameworks-2024.1_preview_u1/lib/python3.9/site-packages/mpi4py/MPI.cpython-39-x86_64-linux-gnu.so</span></span>
<span id="cb17-264"><a href="#cb17-264"></a><span class="in">```</span></span>
<span id="cb17-265"><a href="#cb17-265"></a></span>
<span id="cb17-266"><a href="#cb17-266"></a><span class="in">```python</span></span>
<span id="cb17-267"><a href="#cb17-267"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">import</span> torch</span>
<span id="cb17-268"><a href="#cb17-268"></a><span class="op">&gt;&gt;&gt;</span> torch.__version__</span>
<span id="cb17-269"><a href="#cb17-269"></a><span class="co">'2.1.0.post2+cxx11.abi'</span></span>
<span id="cb17-270"><a href="#cb17-270"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">import</span> intel_extension_for_pytorch <span class="im">as</span> ipex</span>
<span id="cb17-271"><a href="#cb17-271"></a><span class="op">&gt;&gt;&gt;</span> ipex.__version__</span>
<span id="cb17-272"><a href="#cb17-272"></a><span class="co">'2.1.30+xpu'</span></span>
<span id="cb17-273"><a href="#cb17-273"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">from</span> mpi4py <span class="im">import</span> MPI<span class="op">;</span> <span class="bu">print</span>(MPI.<span class="va">__file__</span>)</span>
<span id="cb17-274"><a href="#cb17-274"></a><span class="op">/</span>soft<span class="op">/</span>datascience<span class="op">/</span>aurora_nre_models_frameworks<span class="op">-</span><span class="fl">2024.1</span><span class="er">_preview_u1</span><span class="op">/</span>lib<span class="op">/</span>python3<span class="fl">.9</span><span class="op">/</span>site<span class="op">-</span>packages<span class="op">/</span>mpi4py<span class="op">/</span>MPI.cpython<span class="op">-</span><span class="dv">39</span><span class="op">-</span>x86_64<span class="op">-</span>linux<span class="op">-</span>gnu.so</span>
<span id="cb17-275"><a href="#cb17-275"></a><span class="in">```</span></span>
<span id="cb17-276"><a href="#cb17-276"></a></span>
<span id="cb17-277"><a href="#cb17-277"></a>The remaining dependencies were installed identically to what was just done</span>
<span id="cb17-278"><a href="#cb17-278"></a>previously for the <span class="in">`frameworks/2023.12.15.001`</span> module.</span>
<span id="cb17-279"><a href="#cb17-279"></a></span>
<span id="cb17-280"><a href="#cb17-280"></a>**NOTE**: In the figures below, we denote these two environments as:</span>
<span id="cb17-281"><a href="#cb17-281"></a></span>
<span id="cb17-282"><a href="#cb17-282"></a><span class="ss">- </span><span class="in">`2024.0`</span>: </span>
<span id="cb17-283"><a href="#cb17-283"></a><span class="ss">    - </span><span class="in">`module load frameworks/2023.12.15.001`</span></span>
<span id="cb17-284"><a href="#cb17-284"></a><span class="ss">- </span><span class="in">`2024.1`</span>: </span>
<span id="cb17-285"><a href="#cb17-285"></a><span class="ss">    - </span><span class="in">`module use /soft/preview-modulefiles/24.086.0`</span></span>
<span id="cb17-286"><a href="#cb17-286"></a><span class="ss">    - </span><span class="in">`module load frameworks/2024.04.15.002.lua`</span></span>
<span id="cb17-287"><a href="#cb17-287"></a><span class="ss">- </span><span class="in">`anl_24_q2_release`</span>:</span>
<span id="cb17-288"><a href="#cb17-288"></a><span class="ss">    - </span><span class="in">`eval "$(~/miniconda3/bin/conda shell.zsh hook)"`</span></span>
<span id="cb17-289"><a href="#cb17-289"></a><span class="ss">    - </span><span class="in">`conda activate anl_24_q2_release`</span></span>
<span id="cb17-290"><a href="#cb17-290"></a></span>
<span id="cb17-291"><a href="#cb17-291"></a><span class="fu">### 🥸 Fix in Disguise</span></span>
<span id="cb17-292"><a href="#cb17-292"></a></span>
<span id="cb17-293"><a href="#cb17-293"></a>Armed now with functional environment(s) for</span>
<span id="cb17-294"><a href="#cb17-294"></a><span class="co">[</span><span class="ot">`argonne-lcf/Megatron-DeepSpeed`</span><span class="co">](https://github.com/argonne-lcf/Megatron-DeepSpeed)</span>,</span>
<span id="cb17-295"><a href="#cb17-295"></a>I was able to resume my previous experiments.</span>
<span id="cb17-296"><a href="#cb17-296"></a></span>
<span id="cb17-297"><a href="#cb17-297"></a>From the discussion with Intel, it was hard to understand / reason about _why_</span>
<span id="cb17-298"><a href="#cb17-298"></a>the <span class="in">`flash-attn`</span> fix would have _any_ dependence on the learning rate schedule</span>
<span id="cb17-299"><a href="#cb17-299"></a>(warmup + decay).</span>
<span id="cb17-300"><a href="#cb17-300"></a></span>
<span id="cb17-301"><a href="#cb17-301"></a>If the <span class="in">`flash-attn`</span> fix works for a particular learning rate schedule, you would</span>
<span id="cb17-302"><a href="#cb17-302"></a>reasonably expect that it should work for _any_ learning rate schedule.</span>
<span id="cb17-303"><a href="#cb17-303"></a></span>
<span id="cb17-304"><a href="#cb17-304"></a>An additional source of confusion for me was that the discrepancy in the loss</span>
<span id="cb17-305"><a href="#cb17-305"></a>curves (seemingly) disappeared when using the learning rate settings provided by</span>
<span id="cb17-306"><a href="#cb17-306"></a>Intel<span class="ot">[^intel-lr]</span>, but not when using the ALCF defaults<span class="ot">[^alcf-lr]</span>.</span>
<span id="cb17-307"><a href="#cb17-307"></a></span>
<span id="cb17-308"><a href="#cb17-308"></a>After thinking about it for a bit and trying to reason about possible causes, I</span>
<span id="cb17-309"><a href="#cb17-309"></a>wondered if it might not be a mix of multiple different factors:</span>
<span id="cb17-310"><a href="#cb17-310"></a></span>
<span id="cb17-311"><a href="#cb17-311"></a><span class="ss">1. </span>Small learning rate </span>
<span id="cb17-312"><a href="#cb17-312"></a><span class="ss">2. </span>Very long decay</span>
<span id="cb17-313"><a href="#cb17-313"></a><span class="ss">3. </span><span class="sc">\[</span>maybe ?<span class="sc">\]</span> somehow dependent on the learning rate warmup fraction</span>
<span id="cb17-314"><a href="#cb17-314"></a><span class="ss">    1. </span>preliminary experiments seemed to suggest this was not the case</span>
<span id="cb17-315"><a href="#cb17-315"></a></span>
<span id="cb17-316"><a href="#cb17-316"></a></span>
<span id="cb17-317"><a href="#cb17-317"></a>So, I was curious what would happen if I used the (larger) learning rate value from the ALCF defaults (<span class="in">`lr=0.003`</span>) with the very long <span class="in">`lr-decay-iters: 320000`</span> from Intel.</span>
<span id="cb17-318"><a href="#cb17-318"></a></span>
<span id="cb17-319"><a href="#cb17-319"></a>These results are shown below.</span>
<span id="cb17-320"><a href="#cb17-320"></a></span>
<span id="cb17-321"><a href="#cb17-321"></a>In particular, for all three experiments the following learning rate settings were used:</span>
<span id="cb17-322"><a href="#cb17-322"></a></span>
<span id="cb17-323"><a href="#cb17-323"></a><span class="in">```yml</span></span>
<span id="cb17-324"><a href="#cb17-324"></a><span class="fu">lr</span><span class="kw">:</span><span class="at"> </span><span class="fl">0.0003</span></span>
<span id="cb17-325"><a href="#cb17-325"></a><span class="fu">lr-warmup-frac</span><span class="kw">:</span><span class="at"> </span><span class="fl">0.05</span></span>
<span id="cb17-326"><a href="#cb17-326"></a><span class="fu">lr-decay-iters</span><span class="kw">:</span><span class="at"> </span><span class="dv">320000</span></span>
<span id="cb17-327"><a href="#cb17-327"></a><span class="in">```</span></span>
<span id="cb17-328"><a href="#cb17-328"></a></span>
<span id="cb17-329"><a href="#cb17-329"></a></span>
<span id="cb17-330"><a href="#cb17-330"></a><span class="al">![flash-attn-disguise-decay10000-1](./assets/flash-attn/flash-attn-disguise-decay10000-1.png)</span></span>
<span id="cb17-331"><a href="#cb17-331"></a>Looking at this figure ^, it appears that up until the very very end, all three loss curves agree _identically_.</span>
<span id="cb17-332"><a href="#cb17-332"></a></span>
<span id="cb17-333"><a href="#cb17-333"></a>However, if we look closely at the very end, it looks like there _might_ be a</span>
<span id="cb17-334"><a href="#cb17-334"></a>_slight_ difference beginning to appear between the <span class="in">`2024.0`</span> (brown line) and</span>
<span id="cb17-335"><a href="#cb17-335"></a><span class="in">`{anl_24_q2_release, 2024.1}`</span> ({dark, light} blue lines, respectively).</span>
<span id="cb17-336"><a href="#cb17-336"></a></span>
<span id="cb17-337"><a href="#cb17-337"></a>Thinking that I might be onto something, I then tried again with a smaller <span class="in">`lr-decay-iters: 5000`</span>.</span>
<span id="cb17-338"><a href="#cb17-338"></a></span>
<span id="cb17-339"><a href="#cb17-339"></a>This result is shown below:</span>
<span id="cb17-340"><a href="#cb17-340"></a></span>
<span id="cb17-341"><a href="#cb17-341"></a><span class="al">![flash-attn-disguise-decay5000](./assets/flash-attn/flash-attn-disguise-decay5000.png)</span></span>
<span id="cb17-342"><a href="#cb17-342"></a>In particular, we can now more clearly see the difference beginning to appear between the <span class="in">`2024.0`</span> and <span class="in">`2024.1`</span> loss curves.</span>
<span id="cb17-343"><a href="#cb17-343"></a></span>
<span id="cb17-344"><a href="#cb17-344"></a>Continuing on, we see this effect become increasingly dramatic with even smaller values of <span class="in">`lr-decay-iters`</span>:</span>
<span id="cb17-345"><a href="#cb17-345"></a></span>
<span id="cb17-346"><a href="#cb17-346"></a><span class="al">![flash-attn-disguise-decay-2000](./assets/flash-attn/flash-attn-disguise-decay-2000.png)</span></span>
<span id="cb17-347"><a href="#cb17-347"></a><span class="al">![flash-attn-disguise-decay1500](./assets/flash-attn/flash-attn-disguise-decay1500.png)</span></span>
<span id="cb17-348"><a href="#cb17-348"></a></span>
<span id="cb17-349"><a href="#cb17-349"></a><span class="al">![flash-attn-disguise-decay1000-1](./assets/flash-attn/flash-attn-disguise-decay1000-1.png)</span></span>
<span id="cb17-350"><a href="#cb17-350"></a>In each of these experiments, it appears that:</span>
<span id="cb17-351"><a href="#cb17-351"></a></span>
<span id="cb17-352"><a href="#cb17-352"></a><span class="ss">- </span><span class="in">`2024.0`</span>:  </span>
<span id="cb17-353"><a href="#cb17-353"></a><span class="ss">    - </span>Not impacted by this <span class="in">`lr-decay-iters`</span> dependence</span>
<span id="cb17-354"><a href="#cb17-354"></a><span class="ss">    - </span>Continue to decrease for the duration of training</span>
<span id="cb17-355"><a href="#cb17-355"></a><span class="ss">- </span><span class="in">`2024.1`</span>:</span>
<span id="cb17-356"><a href="#cb17-356"></a><span class="ss">    - </span>Impacted by the <span class="in">`lr-decay-iters`</span> dependence</span>
<span id="cb17-357"><a href="#cb17-357"></a><span class="ss">    - </span>Plateaus towards the end of training</span>
<span id="cb17-358"><a href="#cb17-358"></a></span>
<span id="cb17-359"><a href="#cb17-359"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="ot"> closed</span><span class="dt">&gt;&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Older Figs<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb17-360"><a href="#cb17-360"></a></span>
<span id="cb17-361"><a href="#cb17-361"></a><span class="al">![disguised-fix-2](./assets/flash-attn/disguised-fix-2.png)</span></span>
<span id="cb17-362"><a href="#cb17-362"></a><span class="al">![disguised-fix-1](./assets/flash-attn/disguised-fix-1.png)</span></span>
<span id="cb17-363"><a href="#cb17-363"></a></span>
<span id="cb17-364"><a href="#cb17-364"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb17-365"><a href="#cb17-365"></a></span>
<span id="cb17-366"><a href="#cb17-366"></a><span class="fu">### ✅ `2024.0` Fix</span></span>
<span id="cb17-367"><a href="#cb17-367"></a></span>
<span id="cb17-368"><a href="#cb17-368"></a>**Everything** seems to work with</span>
<span id="cb17-369"><a href="#cb17-369"></a></span>
<span id="cb17-370"><a href="#cb17-370"></a><span class="in">```sh</span></span>
<span id="cb17-371"><a href="#cb17-371"></a><span class="ex">module</span> load frameworks/2023.12.15.001</span>
<span id="cb17-372"><a href="#cb17-372"></a><span class="in">```</span></span>
<span id="cb17-373"><a href="#cb17-373"></a></span>
<span id="cb17-374"><a href="#cb17-374"></a><span class="al">![](./assets/flash-attn/flash-attn-2024-0-fix.png)</span> <span class="al">![](./assets/flash-attn/flash-attn-fix-frameworks-comparison.png)</span></span>
<span id="cb17-375"><a href="#cb17-375"></a></span>
<span id="cb17-376"><a href="#cb17-376"></a><span class="fu">### 📊 `lr-decay-iters` Comparison</span></span>
<span id="cb17-377"><a href="#cb17-377"></a></span>
<span id="cb17-378"><a href="#cb17-378"></a><span class="ss">- </span><span class="in">`2024.0`</span>:</span>
<span id="cb17-379"><a href="#cb17-379"></a><span class="ss">    - </span><span class="al">![](./assets/flash-attn/decay-experiment-2024-0-1.png)</span> </span>
<span id="cb17-380"><a href="#cb17-380"></a></span>
<span id="cb17-381"><a href="#cb17-381"></a><span class="ss">- </span><span class="in">`2024.1`</span>:</span>
<span id="cb17-382"><a href="#cb17-382"></a><span class="ss">    - </span><span class="al">![](./assets/flash-attn/decay-experiment-2024-1.png)</span></span>
<span id="cb17-383"><a href="#cb17-383"></a></span>
<span id="cb17-384"><a href="#cb17-384"></a><span class="fu">## 📈 `lr-decay-iters` dependence</span></span>
<span id="cb17-385"><a href="#cb17-385"></a></span>
<span id="cb17-386"><a href="#cb17-386"></a><span class="al">![](./assets/lr-decay-iters-dependence-2.png)</span></span>
<span id="cb17-387"><a href="#cb17-387"></a></span>
<span id="cb17-388"><a href="#cb17-388"></a><span class="al">![](./assets/lr-schedule-dependence.png)</span> <span class="al">![](./assets/lr-decay-iters-dependence-1.png)</span>  </span>
<span id="cb17-389"><a href="#cb17-389"></a></span>
<span id="cb17-390"><a href="#cb17-390"></a><span class="fu">## 🏎️ Performance Improvement in `2024.1`</span></span>
<span id="cb17-391"><a href="#cb17-391"></a></span>
<span id="cb17-392"><a href="#cb17-392"></a><span class="al">![](./assets/performance-2024-0.png)</span></span>
<span id="cb17-393"><a href="#cb17-393"></a></span>
<span id="cb17-394"><a href="#cb17-394"></a><span class="al">![](./assets/performance-2024-1.png)</span></span>
<span id="cb17-395"><a href="#cb17-395"></a></span>
<span id="cb17-396"><a href="#cb17-396"></a><span class="ot">[^intel-lr]: </span>Intel used the following learning rate schedule in their</span>
<span id="cb17-397"><a href="#cb17-397"></a>    experiments</span>
<span id="cb17-398"><a href="#cb17-398"></a>  <span class="in">```yml</span></span>
<span id="cb17-399"><a href="#cb17-399"></a><span class="at">  </span><span class="fu">lr</span><span class="kw">:</span><span class="at"> </span><span class="fl">0.00015</span></span>
<span id="cb17-400"><a href="#cb17-400"></a><span class="at">  </span><span class="fu">lr-warmup-frac</span><span class="kw">:</span><span class="at"> </span><span class="fl">0.01</span></span>
<span id="cb17-401"><a href="#cb17-401"></a><span class="at">  </span><span class="fu">lr-decay-iters</span><span class="kw">:</span><span class="at"> </span><span class="dv">320000</span></span>
<span id="cb17-402"><a href="#cb17-402"></a><span class="at">  </span><span class="in">```</span></span>
<span id="cb17-403"><a href="#cb17-403"></a></span>
<span id="cb17-404"><a href="#cb17-404"></a><span class="ot">[^alcf-lr]: </span>ALCF used the following learning rate schedule in their experiments</span>
<span id="cb17-405"><a href="#cb17-405"></a></span>
<span id="cb17-406"><a href="#cb17-406"></a>  <span class="in">```yml</span></span>
<span id="cb17-407"><a href="#cb17-407"></a><span class="at">  </span><span class="fu">lr</span><span class="kw">:</span><span class="at"> </span><span class="fl">0.0003</span></span>
<span id="cb17-408"><a href="#cb17-408"></a><span class="at">  </span><span class="fu">lr-warmup-frac</span><span class="kw">:</span><span class="at"> </span><span class="fl">0.05</span></span>
<span id="cb17-409"><a href="#cb17-409"></a><span class="at">  </span><span class="fu">lr-decay-iters</span><span class="kw">:</span><span class="at"> </span><span class="ch">null</span></span>
<span id="cb17-410"><a href="#cb17-410"></a><span class="at">  </span><span class="in">```</span></span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/saforem2/personal_site/blob/main/posts/AuroraGPT/flash-attn-sunspot/index.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/saforem2/personal_site/edit/main/posts/AuroraGPT/flash-attn-sunspot/index.qmd" class="toc-action"><i class="bi empty"></i>Edit this page</a></li><li><a href="https://github.com/saforem2/personal_site/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div></div></footer><script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>