<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.5">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Sam Foreman">
<meta name="dcterms.date" content="2024-10-17">

<title>💾 Converting Checkpoints – Sam Foreman</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../assets/favicon.svg" rel="icon" type="image/svg+xml">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-f9a0c9feee0ed64b2a0eec8864ba50a5.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark-537e1a037d449d4c7d9c42284896e1e0.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script src="../../../site_libs/quarto-contrib/iconify-2.1.0/iconify-icon.min.js"></script>
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "?",
    "H"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-XVM2Y822Y1"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-XVM2Y822Y1', { 'anonymize_ip': true});
</script>
<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-TC329HJ');</script>
<!-- End Google Tag Manager -->


<link rel="stylesheet" href="../../../css/custom.css">
<link rel="stylesheet" href="../../../css/custom.css">
<link rel="stylesheet" href="../../../css/svgbob.css">
<link rel="stylesheet" href="../../../static/fonts/IosevkaSansTerminalss15Custom/IosevkaSansTerminalss15Custom.css">
<link rel="stylesheet" href="../../../static/fonts/IosevkaSansQp/IosevkaSansQp.css">
<meta property="og:title" content="Converting Checkpoints">
<meta property="og:description" content="My ramblings about science and computers">
<meta property="og:image" content="https://github.com/saforem2/personal_site/blob/main/assets/thumbnail.png?raw=true">
<meta property="og:site_name" content="Sam Foreman">
<meta name="twitter:title" content="Converting Checkpoints">
<meta name="twitter:description" content="My ramblings about science and computers">
<meta name="twitter:image" content="https://github.com/saforem2/personal_site/blob/main/assets/thumbnail.png?raw=true">
<meta name="twitter:creator" content="saforem2">
<meta name="twitter:site" content="saforem2">
<meta name="twitter:card" content="summary_large_image">
<meta name="citation_title" content="Converting Checkpoints">
<meta name="citation_author" content="Sam Foreman">
<meta name="citation_publication_date" content="2024-10-17">
<meta name="citation_cover_date" content="2024-10-17">
<meta name="citation_year" content="2024">
<meta name="citation_online_date" content="2024-10-17">
<meta name="citation_fulltext_html_url" content="https://samforeman.me/posts/AuroraGPT/checkpoints">
<meta name="citation_language" content="en">
<meta name="citation_reference" content="citation_title=Superconductivity of in and sn samples;,citation_author=George Deamont;,citation_author=Sam Foreman;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;">
<meta name="citation_reference" content="citation_title=RG-inspired machine learning for lattice field theory;,citation_author=Sam Foreman;,citation_author=Joel Giedt;,citation_author=Yannick Meurice;,citation_author=Judah Unmuth-Yockey;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_volume=175;,citation_conference_title=EPJ web of conferences;,citation_conference=EDP Sciences;">
<meta name="citation_reference" content="citation_title=Large energy density in three-plate nanocapacitors due to coulomb blockade;,citation_author=A Hubler;,citation_author=S Foreman;,citation_author=J Liu;,citation_author=L Wortsmann;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_issue=10;,citation_volume=123;,citation_journal_title=Journal of Applied Physics;,citation_publisher=AIP Publishing;">
<meta name="citation_reference" content="citation_title=Examples of renormalization group transformations for image sets;,citation_author=Samuel Foreman;,citation_author=Joel Giedt;,citation_author=Yannick Meurice;,citation_author=Judah Unmuth-Yockey;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_issue=5;,citation_volume=98;,citation_journal_title=Physical Review E;,citation_publisher=American Physical Society;">
<meta name="citation_reference" content="citation_title=Machine learning inspired analysis of the Ising model transition;,citation_author=Samuel Foreman;,citation_author=Joel Giedt;,citation_author=Yannick Meurice;,citation_author=Judah Unmuth-Yockey;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_doi=10.22323/1.334.0245;,citation_volume=LATTICE2018;,citation_journal_title=PoS;">
<meta name="citation_reference" content="citation_title=Machine learning inspired analysis of the ising model transition;,citation_author=Samuel Foreman;,citation_author=Joel Giedt;,citation_author=Yannick Meurice;,citation_author=Judah Unmuth-Yockey;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_conference_title=Lattice 2018;">
<meta name="citation_reference" content="citation_title=Learning better physics: A machine learning approach to lattice gauge theory;,citation_author=Samuel Alfred Foreman;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_dissertation_institution=University of Iowa;">
<meta name="citation_reference" content="citation_title=Machine learning and neural networks for field theory;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C Osborn;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;">
<meta name="citation_reference" content="citation_title=HMC with normalizing flows;,citation_author=Sam Foreman;,citation_author=Taku Izubuchi;,citation_author=Luchang Jin;,citation_author=Xiao-Yong Jin;,citation_author=James C Osborn;,citation_author=Akio Tomiya;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_journal_title=arXiv preprint arXiv:2112.01586;">
<meta name="citation_reference" content="citation_title=LeapfrogLayers: A trainable framework for effective topological sampling;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C Osborn;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_journal_title=arXiv preprint arXiv:2112.01582;">
<meta name="citation_reference" content="citation_title=Energy storage in quantum resonators;,citation_author=Jiaqi Liu;,citation_author=Alfred W Hubler;,citation_author=Samuel Alfred Foreman;,citation_author=Katharina Ott;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;">
<meta name="citation_reference" content="citation_title=Applications of machine learning to lattice quantum field theory;,citation_author=Denis Boyda;,citation_author=Salvatore Calı̀;,citation_author=Sam Foreman;,citation_author=Lena Funcke;,citation_author=Daniel C Hackett;,citation_author=Yin Lin;,citation_author=Gert Aarts;,citation_author=Andrei Alexandru;,citation_author=Xiao-Yong Jin;,citation_author=Biagio Lucini;,citation_author=others;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_journal_title=arXiv preprint arXiv:2202.05838;">
<meta name="citation_reference" content="citation_title=Lattice QCD and particle physics;,citation_author=Andreas S Kronfeld;,citation_author=Tanmoy Bhattacharya;,citation_author=Thomas Blum;,citation_author=Norman H Christ;,citation_author=Carleton DeTar;,citation_author=William Detmold;,citation_author=Robert Edwards;,citation_author=Anna Hasenfratz;,citation_author=Huey-Wen Lin;,citation_author=Swagato Mukherjee;,citation_author=others;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_journal_title=arXiv preprint arXiv:2207.07641;">
<meta name="citation_reference" content="citation_title=GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics;,citation_author=Maxim Zvyagin;,citation_author=Alexander Brace;,citation_author=Kyle Hippe;,citation_author=Yuntian Deng;,citation_author=Bin Zhang;,citation_author=Cindy Orozco Bohorquez;,citation_author=Austin Clyde;,citation_author=Bharat Kale;,citation_author=Danilo Perez-Rivera;,citation_author=Heng Ma;,citation_author=others;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_issue=6;,citation_volume=37;,citation_journal_title=The International Journal of High Performance Computing Applications;,citation_publisher=SAGE Publications Sage UK: London, England;">
<meta name="citation_reference" content="citation_title=MLMC: Machine learning monte carlo;,citation_author=Sam Foreman;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_conference_title=The international symposium on lattice field theory;">
<meta name="citation_reference" content="citation_title=Superconductivity of in and sn samples;,citation_author=George Deamont;,citation_author=Sam Foreman;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;">
<meta name="citation_reference" content="citation_title=A comprehensive performance study of large language models on novel AI accelerators;,citation_author=Murali Emani;,citation_author=Sam Foreman;,citation_author=Varuni Sastry;,citation_author=Zhen Xie;,citation_author=Siddhisanket Raskar;,citation_author=William Arnold;,citation_author=Rajeev Thakur;,citation_author=Venkatram Vishwanath;,citation_author=Michael E Papka;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_journal_title=arXiv preprint arXiv:2310.04607;">
<meta name="citation_reference" content="citation_title=DeepSpeed4Science initiative: Enabling large-scale scientific discovery through sophisticated AI system technologies;,citation_author=Shuaiwen Leon Song;,citation_author=Bonnie Kruft;,citation_author=Minjia Zhang;,citation_author=Conglong Li;,citation_author=Shiyang Chen;,citation_author=Chengming Zhang;,citation_author=Masahiro Tanaka;,citation_author=Xiaoxia Wu;,citation_author=Jeff Rasley;,citation_author=Ammar Ahmad Awan;,citation_author=others;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_journal_title=arXiv preprint arXiv:2310.04610;">
<meta name="citation_reference" content="citation_title=Protein generation via genome-scale language models with bio-physical scoring;,citation_author=Gautham Dharuman;,citation_author=Logan Ward;,citation_author=Heng Ma;,citation_author=Priyanka V Setty;,citation_author=Ozan Gokdemir;,citation_author=Sam Foreman;,citation_author=Murali Emani;,citation_author=Kyle Hippe;,citation_author=Alexander Brace;,citation_author=Kristopher Keipert;,citation_author=others;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_conference_title=Proceedings of the SC’23 workshops of the international conference on high performance computing, network, storage, and analysis;">
<meta name="citation_reference" content="citation_title=MLMC: Machine learning monte carlo for lattice gauge theory;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C Osborn;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_journal_title=arXiv preprint arXiv:2312.08936;">
<meta name="citation_reference" content="citation_title=Snowmass 2021 computational frontier CompF03 topical group report: Machine learning;,citation_author=Phiala Shanahan;,citation_author=Kazuhiro Terao;,citation_author=Daniel Whiteson;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_journal_title=arXiv preprint arXiv:2209.07559;">
<meta name="citation_reference" content="citation_title=Thorough characterization and analysis of large transformer model training at-scale;,citation_author=Scott Cheng;,citation_author=Jun-Liang Lin;,citation_author=Murali Emani;,citation_author=Siddhisanket Raskar;,citation_author=Sam Foreman;,citation_author=Zhen Xie;,citation_author=Venkatram Vishwanath;,citation_author=Mahmut Taylan Kandemir;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_issue=1;,citation_volume=8;,citation_journal_title=Proceedings of the ACM on Measurement and Analysis of Computing Systems;,citation_publisher=ACM New York, NY, USA;">
<meta name="citation_reference" content="citation_title=Communities through energy justice projects;,citation_author=Mary Ann Leung;,citation_author=Katharine Cahill;,citation_author=Rebecca Hartman-Baker;,citation_author=Paige Kinsley;,citation_author=Lois Curfman McInnes;,citation_author=Suzanne Parete-Koon;,citation_author=Subil Abraham;,citation_author=Lacy Beach Barrier;,citation_author=Gladys Chen;,citation_author=Lizanne DeStefano;,citation_author=others;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_issue=1;,citation_volume=15;,citation_journal_title=Journal of Computational Science;">
<meta name="citation_reference" content="citation_title=Applications of a foundation model approach for weather and climate;,citation_author=Troy Arcomano;,citation_author=Alexander Wikner;,citation_author=Romit Maulik;,citation_author=Veerabhadra Rao Kotamarthi;,citation_author=Sam Foreman;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_volume=2023;,citation_conference_title=AGU fall meeting abstracts;">
<meta name="citation_reference" content="citation_title=Toward a holistic performance evaluation of large language models across diverse ai accelerators;,citation_author=Murali Emani;,citation_author=Sam Foreman;,citation_author=Varuni Sastry;,citation_author=Zhen Xie;,citation_author=Siddhisanket Raskar;,citation_author=William Arnold;,citation_author=Rajeev Thakur;,citation_author=Venkatram Vishwanath;,citation_author=Michael E Papka;,citation_author=Sanjif Shanmugavelu;,citation_author=others;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_conference_title=2024 IEEE international parallel and distributed processing symposium workshops (IPDPSW);,citation_conference=IEEE;">
<meta name="citation_reference" content="citation_title=Intro to HPC bootcamp: Engaging new communities through energy justice projects;,citation_author=Suzanne Parete-Koon;,citation_author=Michael Sandoval;,citation_author=Kellen Leland;,citation_author=Subil Abraham;,citation_author=Mary Ann Leung;,citation_author=Rebecca Hartman-Baker;,citation_author=Paige Kinsley;,citation_author=Lois McInnes;,citation_author=Sreeranjani Ramprakash;,citation_author=Lacy Beach Barrier;,citation_author=others;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_issue=1;,citation_volume=15;,citation_journal_title=Journal of Computational Science Education;,citation_publisher=Oak Ridge National Laboratory (ORNL), Oak Ridge, TN (United States);">
<meta name="citation_reference" content="citation_title=MProt-DPO: Breaking the ExaFLOPS barrier for multimodal protein design workflows with direct preference optimization;,citation_author=Gautham Dharuman;,citation_author=Kyle Hippe;,citation_author=Alexander Brace;,citation_author=Sam Foreman;,citation_author=Väinä Hatanpää;,citation_author=Varuni K Sastry;,citation_author=Huihuo Zheng;,citation_author=Logan Ward;,citation_author=Servesh Muralidharan;,citation_author=Archit Vasan;,citation_author=others;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_conference_title=2024 SC24: International conference for high performance computing, networking, storage and analysis SC;,citation_conference=IEEE Computer Society;">
<meta name="citation_reference" content="citation_title=Emergent abilities of large language models;,citation_author=Jason Wei;,citation_author=Yi Tay;,citation_author=Rishi Bommasani;,citation_author=Colin Raffel;,citation_author=Barret Zoph;,citation_author=Sebastian Borgeaud;,citation_author=Dani Yogatama;,citation_author=Maarten Bosma;,citation_author=Denny Zhou;,citation_author=Donald Metzler;,citation_author=Ed H. Chi;,citation_author=Tatsunori Hashimoto;,citation_author=Oriol Vinyals;,citation_author=Percy Liang;,citation_author=Jeff Dean;,citation_author=William Fedus;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2206.07682;">
<meta name="citation_reference" content="citation_title=DeepSpeed4Science initiative: Enabling large-scale scientific discovery through sophisticated AI system technologies;,citation_author=Shuaiwen Leon Song;,citation_author=Bonnie Kruft;,citation_author=Minjia Zhang;,citation_author=Conglong Li;,citation_author=Shiyang Chen;,citation_author=Chengming Zhang;,citation_author=Masahiro Tanaka;,citation_author=Xiaoxia Wu;,citation_author=Jeff Rasley;,citation_author=Ammar Ahmad Awan;,citation_author=Connor Holmes;,citation_author=Martin Cai;,citation_author=Adam Ghanem;,citation_author=Zhongzhu Zhou;,citation_author=Yuxiong He;,citation_author=Pete Luferenko;,citation_author=Divya Kumar;,citation_author=Jonathan Weyn;,citation_author=Ruixiong Zhang;,citation_author=Sylwester Klocek;,citation_author=Volodymyr Vragov;,citation_author=Mohammed AlQuraishi;,citation_author=Gustaf Ahdritz;,citation_author=Christina Floristean;,citation_author=Cristina Negri;,citation_author=Rao Kotamarthi;,citation_author=Venkatram Vishwanath;,citation_author=Arvind Ramanathan;,citation_author=Sam Foreman;,citation_author=Kyle Hippe;,citation_author=Troy Arcomano;,citation_author=Romit Maulik;,citation_author=Maxim Zvyagin;,citation_author=Alexander Brace;,citation_author=Bin Zhang;,citation_author=Cindy Orozco Bohorquez;,citation_author=Austin Clyde;,citation_author=Bharat Kale;,citation_author=Danilo Perez-Rivera;,citation_author=Heng Ma;,citation_author=Carla M. Mann;,citation_author=Michael Irvin;,citation_author=J. Gregory Pauloski;,citation_author=Logan Ward;,citation_author=Valerie Hayot;,citation_author=Murali Emani;,citation_author=Zhen Xie;,citation_author=Diangen Lin;,citation_author=Maulik Shukla;,citation_author=Ian Foster;,citation_author=James J. Davis;,citation_author=Michael E. Papka;,citation_author=Thomas Brettin;,citation_author=Prasanna Balaprakash;,citation_author=Gina Tourassi;,citation_author=John Gounley;,citation_author=Heidi Hanson;,citation_author=Thomas E Potok;,citation_author=Massimiliano Lupo Pasini;,citation_author=Kate Evans;,citation_author=Dan Lu;,citation_author=Dalton Lunga;,citation_author=Junqi Yin;,citation_author=Sajal Dash;,citation_author=Feiyi Wang;,citation_author=Mallikarjun Shankar;,citation_author=Isaac Lyngaas;,citation_author=Xiao Wang;,citation_author=Guojing Cong;,citation_author=Pei Zhang;,citation_author=Ming Fan;,citation_author=Siyan Liu;,citation_author=Adolfy Hoisie;,citation_author=Shinjae Yoo;,citation_author=Yihui Ren;,citation_author=William Tang;,citation_author=Kyle Felker;,citation_author=Alexey Svyatkovskiy;,citation_author=Hang Liu;,citation_author=Ashwin Aji;,citation_author=Angela Dalton;,citation_author=Michael Schulte;,citation_author=Karl Schulz;,citation_author=Yuntian Deng;,citation_author=Weili Nie;,citation_author=Josh Romero;,citation_author=Christian Dallago;,citation_author=Arash Vahdat;,citation_author=Chaowei Xiao;,citation_author=Thomas Gibbs;,citation_author=Anima Anandkumar;,citation_author=Rick Stevens;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2310.04610;">
<meta name="citation_reference" content="citation_title=Emergent abilities of large language models;,citation_author=Jason Wei;,citation_author=Yi Tay;,citation_author=Rishi Bommasani;,citation_author=Colin Raffel;,citation_author=Barret Zoph;,citation_author=Sebastian Borgeaud;,citation_author=Dani Yogatama;,citation_author=Maarten Bosma;,citation_author=Denny Zhou;,citation_author=Donald Metzler;,citation_author=Ed H. Chi;,citation_author=Tatsunori Hashimoto;,citation_author=Oriol Vinyals;,citation_author=Percy Liang;,citation_author=Jeff Dean;,citation_author=William Fedus;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2206.07682;">
<meta name="citation_reference" content="citation_title=The climate risk &amp;amp;amp; resilience portal (ClimRR) metadata and data dictionary;,citation_author=C. Burdi;,citation_author=Wall. T Branham;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://dub.sh/ClimRR-Metadata;">
<meta name="citation_reference" content="citation_title=Progress on $(g-2)_\mu$ from lattice QCD;,citation_author=Hartmut Wittig;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2306.04165;">
<meta name="citation_reference" content="citation_title=Hybrid Monte Carlo;,citation_author=S. Duane;,citation_author=A. D. Kennedy;,citation_author=B. J. Pendleton;,citation_author=D. Roweth;,citation_publication_date=1987;,citation_cover_date=1987;,citation_year=1987;,citation_doi=10.1016/0370-2693(87)91197-X;,citation_volume=195;,citation_journal_title=Phys. Lett. B;">
<meta name="citation_reference" content="citation_title=Snowmass 2021 Computational Frontier CompF03 Topical Group Report: Machine Learning;,citation_author=Phiala Shanahan;,citation_author=others;,citation_publication_date=2022-09;,citation_cover_date=2022-09;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2209.07559;">
<meta name="citation_reference" content="citation_title=Applications of Machine Learning to Lattice Quantum Field Theory;,citation_author=Denis Boyda;,citation_author=others;,citation_publication_date=2022-02;,citation_cover_date=2022-02;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2202.05838;,citation_conference_title=Snowmass 2021;">
<meta name="citation_reference" content="citation_title=HMC with Normalizing Flows;,citation_author=Sam Foreman;,citation_author=Taku Izubuchi;,citation_author=Luchang Jin;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_author=Akio Tomiya;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01586;,citation_doi=10.22323/1.396.0073;,citation_volume=LATTICE2021;,citation_journal_title=PoS;">
<meta name="citation_reference" content="citation_title=LeapfrogLayers: A Trainable Framework for Effective Topological Sampling;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01582;,citation_doi=10.22323/1.396.0508;,citation_volume=LATTICE2021;,citation_journal_title=PoS;">
<meta name="citation_reference" content="citation_title=Deep Learning Hamiltonian Monte Carlo;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_publication_date=2021-05;,citation_cover_date=2021-05;,citation_year=2021;,citation_fulltext_html_url=https://arxiv.org/abs/2105.03418;,citation_conference_title=9th International Conference on Learning Representations;">
<meta name="citation_reference" content="citation_title=Deep learning hamiltonian monte carlo;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://arxiv.org/abs/2105.03418;">
<meta name="citation_reference" content="citation_title=Deep Learning Hamiltonian Monte Carlo;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_publication_date=2021-05;,citation_cover_date=2021-05;,citation_year=2021;,citation_fulltext_html_url=https://arxiv.org/abs/2105.03418;,citation_conference_title=9th International Conference on Learning Representations;">
<meta name="citation_reference" content="citation_title=Deep learning hamiltonian monte carlo;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C Osborn;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_journal_title=arXiv preprint arXiv:2105.03418;">
<meta name="citation_reference" content="citation_title=LeapfrogLayers: A trainable framework for effective topological sampling;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C Osborn;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_journal_title=arXiv preprint arXiv:2112.01582;">
<meta name="citation_reference" content="citation_title=Energy Justice Analysis of Climate Data with ClimRR;,citation_author=Sam Foreman;,citation_publication_date=2023-08-07;,citation_cover_date=2023-08-07;,citation_year=2023;,citation_fulltext_html_url=https://saforem2.github.io/climate-analysis;,citation_language=en;">
<meta name="citation_reference" content="citation_author=Sam Foreman;,citation_publication_date=2023-08-19;,citation_cover_date=2023-08-19;,citation_year=2023;,citation_fulltext_html_url=https://saforem2.github.io/l2hmc-qcd;,citation_language=en;">
<meta name="citation_reference" content="citation_title=MLMC: Machine learning monte carlo for lattice gauge theory;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James Osborn;,citation_publication_date=00;,citation_cover_date=00;,citation_year=0;,citation_conference_title=40th international symposium on lattice field theory (lattice 2023) (batavia, IL, united states, 07/31/2023 - 08/04/2023);">
<meta name="citation_reference" content="citation_title=Progress on $(g-2)_\mu$ from lattice QCD;,citation_author=Hartmut Wittig;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2306.04165;">
<meta name="citation_reference" content="citation_title=Hybrid Monte Carlo;,citation_author=S. Duane;,citation_author=A. D. Kennedy;,citation_author=B. J. Pendleton;,citation_author=D. Roweth;,citation_publication_date=1987;,citation_cover_date=1987;,citation_year=1987;,citation_doi=10.1016/0370-2693(87)91197-X;,citation_volume=195;,citation_journal_title=Phys. Lett. B;">
<meta name="citation_reference" content="citation_title=Snowmass 2021 Computational Frontier CompF03 Topical Group Report: Machine Learning;,citation_author=Phiala Shanahan;,citation_author=others;,citation_publication_date=2022-09;,citation_cover_date=2022-09;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2209.07559;">
<meta name="citation_reference" content="citation_title=Applications of Machine Learning to Lattice Quantum Field Theory;,citation_author=Denis Boyda;,citation_author=others;,citation_publication_date=2022-02;,citation_cover_date=2022-02;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2202.05838;,citation_conference_title=Snowmass 2021;">
<meta name="citation_reference" content="citation_title=LeapfrogLayers: A Trainable Framework for Effective Topological Sampling;,citation_author=S. Foreman;,citation_author=X. Jin;,citation_author=J. Osborn;,citation_publication_date=2022-07;,citation_cover_date=2022-07;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01582;,citation_doi=10.22323/1.396.0508;,citation_conference_title=The 38th international symposium on lattice field theory;">
<meta name="citation_reference" content="citation_title=HMC with Normalizing Flows;,citation_author=Sam Foreman;,citation_author=Taku Izubuchi;,citation_author=Luchang Jin;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_author=Akio Tomiya;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01586;,citation_doi=10.22323/1.396.0073;,citation_volume=LATTICE2021;,citation_journal_title=PoS;">
<meta name="citation_reference" content="citation_title=Mastering language models;,citation_author=Samuel Montgomery;,citation_publication_date=2023-10;,citation_cover_date=2023-10;,citation_year=2023;,citation_fulltext_html_url=https://towardsdatascience.com/mastering-language-models-32e1d891511a
           ;,citation_journal_title=Medium;,citation_publisher=Towards Data Science;">
<meta name="citation_reference" content="citation_title=Harnessing the power of LLMs in practice: A survey on ChatGPT and beyond;,citation_author=Jingfeng Yang;,citation_author=Hongye Jin;,citation_author=Ruixiang Tang;,citation_author=Xiaotian Han;,citation_author=Qizhang Feng;,citation_author=Haoming Jiang;,citation_author=Bing Yin;,citation_author=Xia Hu;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2304.13712;">
<meta name="citation_reference" content="citation_title=Training tips for the transformer model;,citation_author=Martin Popel;,citation_author=Ondřej Bojar;,citation_publication_date=2018-04;,citation_cover_date=2018-04;,citation_year=2018;,citation_fulltext_html_url=https://doi.org/10.2478%2Fpralin-2018-0002;,citation_issue=1;,citation_doi=10.2478/pralin-2018-0002;,citation_volume=110;,citation_journal_title=The Prague Bulletin of Mathematical Linguistics;,citation_publisher=Charles University in Prague, Karolinum Press;">
<meta name="citation_reference" content="citation_title=Attention is all you need;,citation_author=Ashish Vaswani;,citation_author=Noam Shazeer;,citation_author=Niki Parmar;,citation_author=Jakob Uszkoreit;,citation_author=Llion Jones;,citation_author=Aidan N. Gomez;,citation_author=Lukasz Kaiser;,citation_author=Illia Polosukhin;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_fulltext_html_url=https://arxiv.org/abs/1706.03762;">
<meta name="citation_reference" content="citation_title=Tree of thoughts: Deliberate problem solving with large language models;,citation_author=Shunyu Yao;,citation_author=Dian Yu;,citation_author=Jeffrey Zhao;,citation_author=Izhak Shafran;,citation_author=Thomas L. Griffiths;,citation_author=Yuan Cao;,citation_author=Karthik Narasimhan;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2305.10601;">
<meta name="citation_reference" content="citation_title=GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics;,citation_abstract=We seek to transform how new and emergent variants of pandemiccausing viruses, specifically SARS-CoV-2, are identified and classified. By adapting large language models (LLMs) for genomic data, we build genome-scale language models (GenSLMs) which can learn the evolutionary landscape of SARS-CoV-2 genomes. By pretraining on over 110 million prokaryotic gene sequences and finetuning a SARS-CoV-2-specific model on 1.5 million genomes, we show that GenSLMs can accurately and rapidly identify variants of concern. Thus, to our knowledge, GenSLMs represents one of the first whole genome scale foundation models which can generalize to other prediction tasks. We demonstrate scaling of GenSLMs on GPU-based supercomputers and AI-hardware accelerators utilizing 1.63 Zettaflops in training runs with a sustained performance of 121 PFLOPS in mixed precision and peak of 850 PFLOPS. We present initial scientific insights from examining GenSLMs in tracking evolutionary dynamics of SARS-CoV-2, paving the path to realizing this on large biological data.Competing Interest StatementThe authors have declared no competing interest.;,citation_author=Maxim Zvyagin;,citation_author=Alexander Brace;,citation_author=Kyle Hippe;,citation_author=Yuntian Deng;,citation_author=Bin Zhang;,citation_author=Cindy Orozco Bohorquez;,citation_author=Austin Clyde;,citation_author=Bharat Kale;,citation_author=Danilo Perez-Rivera;,citation_author=Heng Ma;,citation_author=Carla M. Mann;,citation_author=Michael Irvin;,citation_author=J. Gregory Pauloski;,citation_author=Logan Ward;,citation_author=Valerie Hayot-Sasson;,citation_author=Murali Emani;,citation_author=Sam Foreman;,citation_author=Zhen Xie;,citation_author=Diangen Lin;,citation_author=Maulik Shukla;,citation_author=Weili Nie;,citation_author=Josh Romero;,citation_author=Christian Dallago;,citation_author=Arash Vahdat;,citation_author=Chaowei Xiao;,citation_author=Thomas Gibbs;,citation_author=Ian Foster;,citation_author=James J. Davis;,citation_author=Michael E. Papka;,citation_author=Thomas Brettin;,citation_author=Rick Stevens;,citation_author=Anima Anandkumar;,citation_author=Venkatram Vishwanath;,citation_author=Arvind Ramanathan;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://www.biorxiv.org/content/early/2022/11/23/2022.10.10.511571;,citation_doi=10.1101/2022.10.10.511571;,citation_journal_title=bioRxiv;,citation_publisher=Cold Spring Harbor Laboratory;">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../assets/signature12.svg" alt="Sam Foreman" class="navbar-logo">
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-talks" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">talks</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-talks">    
        <li>
    <a class="dropdown-item" href="../../../talks/index.html">
 <span class="dropdown-text">📢 All Talks</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../talks/ai-for-science-2024/slides.html">
 <span class="dropdown-text">Parallel Training Methods</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../talks/alcf-hpc-workshop-2024/slides.html">
 <span class="dropdown-text">AuroraGPT (ALCF Hands-On HPC)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../talks/alcf-hpc-workshop-2024/slides.html">
 <span class="dropdown-text">ML + Foundation Models at Scale</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../talks/hpc-user-forum/slides.html">
 <span class="dropdown-text">AuroraGPT (HPC User Forum)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../talks/llms-at-scale/slides.html">
 <span class="dropdown-text">Training LLMs at Scale</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../talks/llms-on-polaris/slides.html">
 <span class="dropdown-text">Polaris Overview + LLMs</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://saforem2.github.io/parallel-training-slides/">
 <span class="dropdown-text">Parallel Training Techniques</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://saforem2.github.io/llm-workshop-talk/">
 <span class="dropdown-text">LLMs from Scratch</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://saforem2.github.io/LLM-tutorial">
 <span class="dropdown-text">Creating Small(-ish) LLMs</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://saforem2.github.io/oneapi-talk/#0">
 <span class="dropdown-text">Exascale Science on Aurora</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://saforem2.github.io/llm-lunch-talk/">
 <span class="dropdown-text">LLM Lunch Talk</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://saforem2.github.io/scaling4science">
 <span class="dropdown-text">Scaling LLMs for Science</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://saforem2.github.io/lattice23">
 <span class="dropdown-text">MLMC (for LQCD)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://saforem2.github.io/lqcd-pasc23">
 <span class="dropdown-text">Generative Modeling</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://saforem2.github.io/deep-fridays">
 <span class="dropdown-text">Efficient Sampling for LGT</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://saforem2.github.io/ai4sci-large-scale-training">
 <span class="dropdown-text">Large Scale Training</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://saforem2.github.io/hparam-management-sdl2022">
 <span class="dropdown-text">Hyperparameter Management</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://saforem2.github.io/ATPESC-StatisticalLearning">
 <span class="dropdown-text">Statistical Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://saforem2.github.io/anl-job-talk/">
 <span class="dropdown-text">Scientific Data Science</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://saforem2.github.io/physicsSeminar">
 <span class="dropdown-text">Machine Learning in HEP</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://bit.ly/mainz21">
 <span class="dropdown-text">DLHMC for Improved Gauge Generation</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://slides.com/samforeman/l2hmc-qcd-93bc0c">
 <span class="dropdown-text">ML for LQCD</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://bit.ly/mainz21_overview">
 <span class="dropdown-text">ML Techniques in LQCD</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://saforem2.github.io/physicsSeminar">
 <span class="dropdown-text">ML for HEP</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-posts" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">posts</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-posts">    
        <li>
    <a class="dropdown-item" href="../../../posts/index.html">
 <span class="dropdown-text">📬 All Posts</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../posts/AuroraGPT/spike-skipper/index.html">
 <span class="dropdown-text">🏔️ Spike Skipper</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../posts/ezpz-at-alcf/index.html">
 <span class="dropdown-text">🍋 <code>ezpz</code> at ALCF</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../posts/AuroraGPT/determinstic-flash-attn/index.html">
 <span class="dropdown-text">🎰 Deterministic Flash Attention</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../posts/AuroraGPT/flash-attn-sunspot/index.html">
 <span class="dropdown-text">📸 <code>flash-attn</code> on Sunspot</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../posts/AuroraGPT/mpi4py-reproducer/index.html">
 <span class="dropdown-text">🐛 <code>mpi4py</code> bug on Sunspot</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../posts/ai-for-physics/diffusion/index.html">
 <span class="dropdown-text">🎲 MCMC + Diffusion Sampling</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../posts/dope-slides/index.html">
 <span class="dropdown-text">💅 How to Make Dope Slides</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../posts/AuroraGPT/startup-times/index.html">
 <span class="dropdown-text">⏰ Starting Up Distributed Training</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../posts/AuroraGPT/long-sequences/index.html">
 <span class="dropdown-text">🚂 Loooooooong Sequence Lengths</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../posts/AuroraGPT/aurora-gpt/index.html">
 <span class="dropdown-text">🏎️ <code>Megatron-DeepSpeed</code> + Intel XPU</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../posts/ai-for-physics/l2hmc-qcd/2dU1/index.html">
 <span class="dropdown-text">🎢 <code>l2hmc-qcd</code> Example: 2D U(1)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../posts/ai-for-physics/l2hmc-qcd/4dSU3/index.html">
 <span class="dropdown-text">🔳 <code>l2hmc-qcd</code> Example: 4D SU(3)</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-projects" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">projects</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-projects">    
        <li>
    <a class="dropdown-item" href="../../../qmd/projects/index.html">
 <span class="dropdown-text">📚 All Projects</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://saforem2.github.io/ezpz">
 <span class="dropdown-text">🍋 <code>ezpz</code></span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://saforem2.github.io/l2hmc-qcd">
 <span class="dropdown-text">🟥 <code>l2hmc-qcd</code></span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/argonne-lcf/Megatron-DeepSpeed)">
 <span class="dropdown-text">🤖 <code>Megatron-DeepSpeed</code></span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://saforem2.github.io/wordplay">
 <span class="dropdown-text">💬 <code>wordplay</code> 🎮</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://www.alcf.anl.gov/alcf-ai-science-training-series?">
 <span class="dropdown-text">🎓 <code>ai-science-training</code></span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://saforem2.github.io/enrich">
 <span class="dropdown-text">💸 <code>enrich</code></span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://saforem2.github.io/ambivalent">
 <span class="dropdown-text">🤷🏻‍♂️<code>ambivalent</code></span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://saforem2.github.io/climate-analysis">
 <span class="dropdown-text">🌍 <code>climate-analysis</code></span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/saforem2/glitz">
 <span class="dropdown-text">🎨 <code>glitz</code></span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/saforem2/personal_site">
 <span class="dropdown-text">🙋🏻<code>personal_site</code></span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/saforem2/notes-demo">
 <span class="dropdown-text">🗒️ <code>Notes-Demo</code></span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/saforem2/personal_site"> 
<span class="menu-text"><span class="icon dim-text" style="font-size: 1.25rem;"><iconify-icon role="img" inline="" icon="ph:github-logo" aria-label="Icon github-logo from ph Iconify.design set." title="Icon github-logo from ph Iconify.design set."></iconify-icon></span></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../index.xml"> 
<span class="menu-text"><span class="icon dim-text" style="font-size: 1.25rem;"><iconify-icon role="img" inline="" icon="ph:rss" aria-label="Icon rss from ph Iconify.design set." title="Icon rss from ph Iconify.design set."></iconify-icon></span></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#mds-hf" id="toc-mds-hf" class="nav-link active" data-scroll-target="#mds-hf">MDS –&gt; HF</a></li>
  <li><a href="#hf-to-meg-ds" id="toc-hf-to-meg-ds" class="nav-link" data-scroll-target="#hf-to-meg-ds">🚧 HF to Meg-DS</a>
  <ul class="collapse">
  <li><a href="#section" id="toc-section" class="nav-link" data-scroll-target="#section">2024-10-17</a></li>
  <li><a href="#older" id="toc-older" class="nav-link" data-scroll-target="#older">Older</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/saforem2/personal_site/blob/main/posts/AuroraGPT/checkpoints/index.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/saforem2/personal_site/edit/main/posts/AuroraGPT/checkpoints/index.qmd" class="toc-action"><i class="bi empty"></i>Edit this page</a></li><li><a href="https://github.com/saforem2/personal_site/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div><div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="index.md"><i class="bi bi-file-code"></i>Github (GFM)</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-TC329HJ" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">💾 Converting Checkpoints</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
  <div class="quarto-categories">
    <div class="quarto-category">AuroraGPT</div>
  </div>
  </div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author"><a href="https://samforeman.me">Sam Foreman</a> <a href="mailto:foremans@anl.gov" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            <a href="https://alcf.anl.gov/about/people/sam-foreman">
            </a><a href="https://alcf.anl.gov/about/people/sam-foreman">ALCF</a>
            
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">October 17, 2024</p>
    </div>
  </div>
  
    <div>
    <div class="quarto-title-meta-heading">Modified</div>
    <div class="quarto-title-meta-contents">
      <p class="date-modified">November 17, 2024</p>
    </div>
  </div>
    
  </div>
  


</header>


<section id="mds-hf" class="level2">
<h2 class="anchored" data-anchor-id="mds-hf">MDS –&gt; HF</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode numberSource bash number-lines code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1"></a><span class="fu">convert_mds_to_hf()</span> <span class="kw">{</span></span>
<span id="cb1-2"><a href="#cb1-2"></a> <span class="co"># GLOBAL_STEP=$1</span></span>
<span id="cb1-3"><a href="#cb1-3"></a> <span class="va">CKPT_ROOT</span><span class="op">=</span><span class="va">$2</span></span>
<span id="cb1-4"><a href="#cb1-4"></a></span>
<span id="cb1-5"><a href="#cb1-5"></a> <span class="va">CKPT_ROOT</span><span class="op">=</span><span class="st">"/flare/Aurora_deployment/AuroraGPT-Testing/foremans/rollback-41k8/Megatron-DeepSpeed-41800/checkpoints/ws768_ds_stage1_nl32_hs4096_mb4_seq4096_gb3072_sp1_pp1_tp1_bf16_optadamw_lr0.00020_lwf0.05/"</span><span class="kw">;</span></span>
<span id="cb1-6"><a href="#cb1-6"></a> <span class="va">SRC</span><span class="op">=</span><span class="st">"</span><span class="va">${CKPT_ROOT}</span><span class="st">/global_step</span><span class="va">${GLOBAL_STEP}</span><span class="st">"</span></span>
<span id="cb1-7"><a href="#cb1-7"></a> <span class="cf">if</span> <span class="kw">[[</span> <span class="ot">-d</span> <span class="st">"</span><span class="va">${SRC}</span><span class="st">"</span> <span class="kw">]];</span> <span class="cf">then</span></span>
<span id="cb1-8"><a href="#cb1-8"></a>        <span class="bu">echo</span> <span class="st">"Converting checkpoint @ global step </span><span class="va">${GLOBAL_STEP}</span><span class="st">"</span></span>
<span id="cb1-9"><a href="#cb1-9"></a>        <span class="bu">echo</span> <span class="st">"\tsrc=</span><span class="va">${SRC}</span><span class="st">"</span></span>
<span id="cb1-10"><a href="#cb1-10"></a>        <span class="va">DST</span><span class="op">=</span><span class="st">"/flare/Aurora_deployment/AuroraGPT-Checkpoints/Megatron-DeepSpeed/checkpoints-to-convert/ws768_ds_stage1_nl32_hs4096_mb4_seq4096_gb3072_sp1_pp1_tp1_bf16_optadamw_lr0.00020_lwf0.05/global_step</span><span class="va">${GLOBAL_STEP}</span><span class="st">_hf"</span></span>
<span id="cb1-11"><a href="#cb1-11"></a>        <span class="bu">echo</span> <span class="st">"\tdst=</span><span class="va">${DST}</span><span class="st">"</span></span>
<span id="cb1-12"><a href="#cb1-12"></a>        <span class="ex">python3</span> mds_to_hf.py <span class="at">--mds_checkpoint</span> <span class="st">"</span><span class="va">${SRC}</span><span class="st">/mp_rank_00_model_states.pt"</span> <span class="at">--output_dir</span> <span class="st">"</span><span class="va">${DST}</span><span class="st">"</span> <span class="at">--cache_dir</span> <span class="st">"./.cache"</span></span>
<span id="cb1-13"><a href="#cb1-13"></a> <span class="cf">else</span></span>
<span id="cb1-14"><a href="#cb1-14"></a>        <span class="bu">echo</span> <span class="st">"Unable to locate directory </span><span class="va">${SRC}</span><span class="st">. Exiting"</span></span>
<span id="cb1-15"><a href="#cb1-15"></a>        <span class="bu">exit</span> 1</span>
<span id="cb1-16"><a href="#cb1-16"></a> <span class="cf">fi</span></span>
<span id="cb1-17"><a href="#cb1-17"></a><span class="kw">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="hf-to-meg-ds" class="level1">
<h1>🚧 HF to Meg-DS</h1>
<section id="section" class="level2">
<h2 class="anchored" data-anchor-id="section">2024-10-17</h2>
<div class="sourceCode" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="im">import</span> os</span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="im">import</span> ezpz <span class="im">as</span> ez</span>
<span id="cb2-3"><a href="#cb2-3"></a><span class="im">import</span> torch</span>
<span id="cb2-4"><a href="#cb2-4"></a><span class="im">import</span> deepspeed</span>
<span id="cb2-5"><a href="#cb2-5"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb2-6"><a href="#cb2-6"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer, AutoConfig, AutoModelForCausalLM</span>
<span id="cb2-7"><a href="#cb2-7"></a><span class="im">from</span> transformers.integrations <span class="im">import</span> HfDeepSpeedConfig</span>
<span id="cb2-8"><a href="#cb2-8"></a><span class="co"># distributed setup</span></span>
<span id="cb2-9"><a href="#cb2-9"></a></span>
<span id="cb2-10"><a href="#cb2-10"></a>os.environ[<span class="st">'WORLD_SIZE'</span>] <span class="op">=</span> <span class="st">'12'</span></span>
<span id="cb2-11"><a href="#cb2-11"></a>rank <span class="op">=</span> ez.setup_torch(backend<span class="op">=</span><span class="st">'deepspeed'</span>)</span>
<span id="cb2-12"><a href="#cb2-12"></a>deepspeed.init_distributed()</span>
<span id="cb2-13"><a href="#cb2-13"></a>model_name <span class="op">=</span> <span class="st">"meta-llama/Llama-3.2-1B"</span></span>
<span id="cb2-14"><a href="#cb2-14"></a>config <span class="op">=</span> AutoConfig.from_pretrained(model_name)</span>
<span id="cb2-15"><a href="#cb2-15"></a>ds_config <span class="op">=</span> {</span>
<span id="cb2-16"><a href="#cb2-16"></a>    <span class="st">"steps_per_print"</span>: <span class="dv">1</span>,</span>
<span id="cb2-17"><a href="#cb2-17"></a>    <span class="st">"train_batch_size"</span>: <span class="dv">1</span>,</span>
<span id="cb2-18"><a href="#cb2-18"></a>    <span class="st">"train_micro_batch_size_per_gpu"</span>: <span class="dv">1</span>,</span>
<span id="cb2-19"><a href="#cb2-19"></a>    <span class="st">"bf16"</span>: {</span>
<span id="cb2-20"><a href="#cb2-20"></a>        <span class="st">"enabled"</span>: <span class="va">True</span></span>
<span id="cb2-21"><a href="#cb2-21"></a>    },</span>
<span id="cb2-22"><a href="#cb2-22"></a>    <span class="st">"optimizer"</span>: {</span>
<span id="cb2-23"><a href="#cb2-23"></a>        <span class="st">"type"</span>: <span class="st">"Adam"</span>,</span>
<span id="cb2-24"><a href="#cb2-24"></a>    },</span>
<span id="cb2-25"><a href="#cb2-25"></a>    <span class="st">"zero_optimization"</span>: {</span>
<span id="cb2-26"><a href="#cb2-26"></a>        <span class="st">"stage"</span>: <span class="dv">3</span>,</span>
<span id="cb2-27"><a href="#cb2-27"></a>    },</span>
<span id="cb2-28"><a href="#cb2-28"></a>}</span>
<span id="cb2-29"><a href="#cb2-29"></a></span>
<span id="cb2-30"><a href="#cb2-30"></a>dschf <span class="op">=</span> HfDeepSpeedConfig(ds_config)  <span class="co"># keep this object alive</span></span>
<span id="cb2-31"><a href="#cb2-31"></a><span class="co"># now a model can be loaded.</span></span>
<span id="cb2-32"><a href="#cb2-32"></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(model_name).to(ez.get_torch_device()).to(torch.bfloat16)</span>
<span id="cb2-33"><a href="#cb2-33"></a><span class="co"># initialise Deepspeed ZeRO and store only the engine object</span></span>
<span id="cb2-34"><a href="#cb2-34"></a>ds_engine <span class="op">=</span> deepspeed.initialize(model<span class="op">=</span>model, config_params<span class="op">=</span>ds_config)[<span class="dv">0</span>]</span>
<span id="cb2-35"><a href="#cb2-35"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(model_name)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="kw">def</span> tokenize_function(examples):</span>
<span id="cb3-2"><a href="#cb3-2"></a>    <span class="cf">return</span> tokenizer(examples[<span class="st">"text"</span>], truncation<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-3"><a href="#cb3-3"></a></span>
<span id="cb3-4"><a href="#cb3-4"></a>dataset <span class="op">=</span> load_dataset(<span class="st">"yelp_review_full"</span>)</span>
<span id="cb3-5"><a href="#cb3-5"></a>tokenized_datasets <span class="op">=</span> dataset.<span class="bu">map</span>(tokenize_function, batched<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-6"><a href="#cb3-6"></a>small_train_dataset <span class="op">=</span> tokenized_datasets[<span class="st">"train"</span>].shuffle(seed<span class="op">=</span><span class="dv">42</span>).select(<span class="bu">range</span>(<span class="dv">1000</span>))</span>
<span id="cb3-7"><a href="#cb3-7"></a>small_eval_dataset <span class="op">=</span> tokenized_datasets[<span class="st">"test"</span>].shuffle(seed<span class="op">=</span><span class="dv">42</span>).select(<span class="bu">range</span>(<span class="dv">1000</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="im">from</span> transformers <span class="im">import</span> Trainer</span>
<span id="cb4-2"><a href="#cb4-2"></a>training_args <span class="op">=</span> TrainingArguments(output_dir<span class="op">=</span><span class="st">"llama-3.2-1B"</span>, deepspeed<span class="op">=</span>ds_config)</span>
<span id="cb4-3"><a href="#cb4-3"></a>trainer <span class="op">=</span> Trainer(model, training_args, train_dataset<span class="op">=</span>small_train_dataset, eval_dataset<span class="op">=</span>small_eval_dataset, tokenizer<span class="op">=</span>tokenizer)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="older" class="level2">
<h2 class="anchored" data-anchor-id="older">Older</h2>
<p>Current status:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode numberSource bash number-lines code-with-copy"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1"></a><span class="co">#[🐍 aurora_nre_models_frameworks-2024.2.1_u1][👻 aurora_nre_models_frameworks-2024.2.1_u1]</span></span>
<span id="cb5-2"><a href="#cb5-2"></a><span class="co">#[🌌][03:39:06 PM][foremans]@[x4407c6s7b0n0][/f/A/f/p/a/Megatron-DeepSpeed][🌱hzheng-data-fix][📝🤷🏎💨]</span></span>
<span id="cb5-3"><a href="#cb5-3"></a><span class="ex">$</span> _conversion_args=<span class="er">(</span><span class="st">"--hf-ckpt-num-shards 1"</span></span>
<span id="cb5-4"><a href="#cb5-4"></a> <span class="st">"--hf-ckpt-dir /flare/Aurora_deployment/foremans/Meta-Llama-3.1-8B-128512"</span></span>
<span id="cb5-5"><a href="#cb5-5"></a> <span class="st">"--load-mode auto"</span></span>
<span id="cb5-6"><a href="#cb5-6"></a> <span class="st">"--save ckpt-mds-llama-3/"</span></span>
<span id="cb5-7"><a href="#cb5-7"></a> <span class="st">"--tensor-model-parallel-size 1"</span></span>
<span id="cb5-8"><a href="#cb5-8"></a> <span class="st">"--pipeline-model-parallel-size 1"</span></span>
<span id="cb5-9"><a href="#cb5-9"></a> <span class="st">"--lr-warmup-iters 2000"</span></span>
<span id="cb5-10"><a href="#cb5-10"></a> <span class="st">"--weight-decay 0.1"</span></span>
<span id="cb5-11"><a href="#cb5-11"></a> <span class="st">"--clip-grad 1"</span></span>
<span id="cb5-12"><a href="#cb5-12"></a> <span class="st">"--num-layers 32"</span></span>
<span id="cb5-13"><a href="#cb5-13"></a> <span class="st">"--hidden-size 4096"</span></span>
<span id="cb5-14"><a href="#cb5-14"></a> <span class="st">"--num-attention-heads 32"</span></span>
<span id="cb5-15"><a href="#cb5-15"></a> <span class="st">"--ffn-hidden-size 14336"</span></span>
<span id="cb5-16"><a href="#cb5-16"></a> <span class="st">"--attention-dropout 0"</span></span>
<span id="cb5-17"><a href="#cb5-17"></a> <span class="st">"--hidden-dropout 0"</span></span>
<span id="cb5-18"><a href="#cb5-18"></a> <span class="st">"--no-query-key-layer-scaling"</span></span>
<span id="cb5-19"><a href="#cb5-19"></a> <span class="st">"--num-key-value-heads 8"</span></span>
<span id="cb5-20"><a href="#cb5-20"></a> <span class="st">"--disable-bias-linear"</span></span>
<span id="cb5-21"><a href="#cb5-21"></a> <span class="st">"--normalization rmsnorm"</span></span>
<span id="cb5-22"><a href="#cb5-22"></a> <span class="st">"--use-rotary-position-embeddings"</span></span>
<span id="cb5-23"><a href="#cb5-23"></a> <span class="st">"--untie-embeddings-and-output-weights"</span></span>
<span id="cb5-24"><a href="#cb5-24"></a> <span class="st">"--swiglu"</span></span>
<span id="cb5-25"><a href="#cb5-25"></a> <span class="st">"--seq-length 2048"</span></span>
<span id="cb5-26"><a href="#cb5-26"></a> <span class="st">"--max-position-embeddings 2048"</span></span>
<span id="cb5-27"><a href="#cb5-27"></a> <span class="st">"--micro-batch-size 1"</span></span>
<span id="cb5-28"><a href="#cb5-28"></a> <span class="st">"--global-batch-size 24"</span></span>
<span id="cb5-29"><a href="#cb5-29"></a> <span class="st">"--train-iters 3500"</span></span>
<span id="cb5-30"><a href="#cb5-30"></a> <span class="st">"--lr 2e-5"</span></span>
<span id="cb5-31"><a href="#cb5-31"></a> <span class="st">"--tensorboard-dir tensorboard_output"</span></span>
<span id="cb5-32"><a href="#cb5-32"></a> <span class="st">"--lr-decay-iters 320000"</span></span>
<span id="cb5-33"><a href="#cb5-33"></a> <span class="st">"--lr-decay-style cosine"</span></span>
<span id="cb5-34"><a href="#cb5-34"></a> <span class="st">"--log-interval 1"</span></span>
<span id="cb5-35"><a href="#cb5-35"></a> <span class="st">"--eval-iters 100"</span></span>
<span id="cb5-36"><a href="#cb5-36"></a> <span class="st">"--eval-interval 100"</span></span>
<span id="cb5-37"><a href="#cb5-37"></a> <span class="st">"--data-path /lus/flare/projects/candle_aesp_CNDA/azton/data/v2/megatron/dataset_v2_wtextbooks_text_document"</span></span>
<span id="cb5-38"><a href="#cb5-38"></a> <span class="st">"--save-interval 1500"</span></span>
<span id="cb5-39"><a href="#cb5-39"></a> <span class="st">"--split 100,0,0"</span></span>
<span id="cb5-40"><a href="#cb5-40"></a> <span class="st">"--bf16"</span></span>
<span id="cb5-41"><a href="#cb5-41"></a> <span class="st">"--tokenizer-type HFTokenizer"</span></span>
<span id="cb5-42"><a href="#cb5-42"></a> <span class="st">"--tokenizer-model ALCF/custom_tokenizer.model"</span></span>
<span id="cb5-43"><a href="#cb5-43"></a> <span class="st">"--deepspeed_config ./examples_deepspeed/finetune_hf_llama/ds_config.json"</span></span>
<span id="cb5-44"><a href="#cb5-44"></a> <span class="st">"--deepspeed"</span></span>
<span id="cb5-45"><a href="#cb5-45"></a> <span class="st">"--distributed-backend ccl"</span></span>
<span id="cb5-46"><a href="#cb5-46"></a> <span class="st">"--no-masked-softmax-fusion"</span></span>
<span id="cb5-47"><a href="#cb5-47"></a> <span class="st">"--no-bias-gelu-fusion"</span></span>
<span id="cb5-48"><a href="#cb5-48"></a> <span class="st">"--no-bias-dropout-fusion"</span></span>
<span id="cb5-49"><a href="#cb5-49"></a> <span class="st">"--no-gradient-accumulation-fusion"</span></span>
<span id="cb5-50"><a href="#cb5-50"></a> <span class="st">"--repeated-dataloader"</span></span>
<span id="cb5-51"><a href="#cb5-51"></a> <span class="st">"--data-cache-path ./.cache"</span></span>
<span id="cb5-52"><a href="#cb5-52"></a> <span class="st">"--make-vocab-size-divisible-by 128512"</span></span>
<span id="cb5-53"><a href="#cb5-53"></a> <span class="st">"--vocab-size 128512"</span></span>
<span id="cb5-54"><a href="#cb5-54"></a><span class="kw">)</span></span>
<span id="cb5-55"><a href="#cb5-55"></a></span>
<span id="cb5-56"><a href="#cb5-56"></a><span class="va">conversion_flags</span><span class="op">=</span><span class="va">($(</span><span class="bu">printf</span> <span class="st">'%s\n'</span> <span class="st">"</span><span class="va">${_conversion_args</span><span class="op">[@]</span><span class="va">}</span><span class="st">"</span> <span class="kw">|</span> <span class="fu">sort</span><span class="va">))</span></span>
<span id="cb5-57"><a href="#cb5-57"></a><span class="bu">echo</span> <span class="st">"</span><span class="va">${conversion_flags</span><span class="op">[@]</span><span class="va">}</span><span class="st">"</span></span>
<span id="cb5-58"><a href="#cb5-58"></a><span class="ex">--attention-dropout</span> 0 <span class="at">--bf16</span> <span class="at">--clip-grad</span> 1 <span class="at">--data-cache-path</span> ./.cache <span class="at">--data-path</span> /lus/flare/projects/candle_aesp_CNDA/azton/data/v2/megatron/dataset_v2_wtextbooks_text_document <span class="at">--deepspeed</span> <span class="at">--deepspeed_config</span> ./examples_deepspeed/finetune_hf_llama/ds_config.json <span class="at">--disable-bias-linear</span> <span class="at">--distributed-backend</span> ccl <span class="at">--eval-interval</span> 100 <span class="at">--eval-iters</span> 100 <span class="at">--ffn-hidden-size</span> 14336 <span class="at">--global-batch-size</span> 24 <span class="at">--hf-ckpt-dir</span> /flare/Aurora_deployment/foremans/Meta-Llama-3.1-8B-128512 <span class="at">--hf-ckpt-num-shards</span> 1 <span class="at">--hidden-dropout</span> 0 <span class="at">--hidden-size</span> 4096 <span class="at">--load-mode</span> auto <span class="at">--log-interval</span> 1 <span class="at">--lr</span> 2e-5 <span class="at">--lr-decay-iters</span> 320000 <span class="at">--lr-decay-style</span> cosine <span class="at">--lr-warmup-iters</span> 2000 <span class="at">--make-vocab-size-divisible-by</span> 128512 <span class="at">--max-position-embeddings</span> 2048 <span class="at">--micro-batch-size</span> 1 <span class="at">--no-bias-dropout-fusion</span> <span class="at">--no-bias-gelu-fusion</span> <span class="at">--no-gradient-accumulation-fusion</span> <span class="at">--no-masked-softmax-fusion</span> <span class="at">--no-query-key-layer-scaling</span> <span class="at">--normalization</span> rmsnorm <span class="at">--num-attention-heads</span> 32 <span class="at">--num-key-value-heads</span> 8 <span class="at">--num-layers</span> 32 <span class="at">--pipeline-model-parallel-size</span> 1 <span class="at">--repeated-dataloader</span> <span class="at">--save</span> ckpt-mds-llama-3/ <span class="at">--save-interval</span> 1500 <span class="at">--seq-length</span> 2048 <span class="at">--split</span> 100,0,0 <span class="at">--swiglu</span> <span class="at">--tensorboard-dir</span> tensorboard_output <span class="at">--tensor-model-parallel-size</span> 1 <span class="at">--tokenizer-model</span> ALCF/custom_tokenizer.model <span class="at">--tokenizer-type</span> HFTokenizer <span class="at">--train-iters</span> 3500 <span class="at">--untie-embeddings-and-output-weights</span> <span class="at">--use-rotary-position-embeddings</span> <span class="at">--vocab-size</span> 128512 <span class="at">--weight-decay</span> 0.1</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb6"><pre class="sourceCode numberSource bash number-lines code-with-copy"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1"></a><span class="co">#[🐍 aurora_nre_models_frameworks-2024.2.1_u1][👻 aurora_nre_models_frameworks-2024.2.1_u1]</span></span>
<span id="cb6-2"><a href="#cb6-2"></a><span class="co">#[🌌][03:39:18 PM][foremans]@[x4407c6s7b0n0][/f/A/f/p/a/Megatron-DeepSpeed][🌱hzheng-data-fix][📝🤷🏎💨]</span></span>
<span id="cb6-3"><a href="#cb6-3"></a><span class="ex">$</span> launch python3 tools/hf2megads_weight_converter.py <span class="st">"</span><span class="va">${conversion_flags</span><span class="op">[@]</span><span class="va">}</span><span class="st">"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<details closed="">
<summary>
output:
</summary>
<div class="sourceCode" id="cb7"><pre class="sourceCode numberSource bash number-lines code-with-copy"><code class="sourceCode bash"><span id="cb7-1"><a href="#cb7-1"></a><span class="ex">Disabling</span> local launch: multi-node application</span>
<span id="cb7-2"><a href="#cb7-2"></a><span class="ex">Connected</span> to tcp://x4407c6s7b0n0.hostmgmt2407.cm.aurora.alcf.anl.gov:7919</span>
<span id="cb7-3"><a href="#cb7-3"></a><span class="ex">Found</span> executable /flare/Aurora_deployment/foremans/projects/argonne-lcf/Megatron-DeepSpeed/venvs/aurora_nre_models_frameworks-2024.2.1_u1/bin/python3</span>
<span id="cb7-4"><a href="#cb7-4"></a><span class="ex">Launching</span> application bdbd987f-b27b-4922-928b-5d1a166e800b</span>
<span id="cb7-5"><a href="#cb7-5"></a><span class="ex">[2024-10-16</span> 15:39:30,424] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb7-6"><a href="#cb7-6"></a><span class="ex">[2024-10-16</span> 15:39:30,456] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb7-7"><a href="#cb7-7"></a><span class="ex">[2024-10-16</span> 15:39:30,456] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb7-8"><a href="#cb7-8"></a><span class="ex">[2024-10-16</span> 15:39:30,456] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb7-9"><a href="#cb7-9"></a><span class="ex">[2024-10-16</span> 15:39:30,457] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb7-10"><a href="#cb7-10"></a><span class="ex">[2024-10-16</span> 15:39:30,568] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb7-11"><a href="#cb7-11"></a><span class="ex">[2024-10-16</span> 15:39:30,571] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb7-12"><a href="#cb7-12"></a><span class="ex">[2024-10-16</span> 15:39:30,575] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb7-13"><a href="#cb7-13"></a><span class="ex">[2024-10-16</span> 15:39:30,578] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb7-14"><a href="#cb7-14"></a><span class="ex">[2024-10-16</span> 15:39:30,584] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb7-15"><a href="#cb7-15"></a><span class="ex">[2024-10-16</span> 15:39:30,608] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb7-16"><a href="#cb7-16"></a><span class="ex">[2024-10-16</span> 15:39:30,613] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb7-17"><a href="#cb7-17"></a><span class="ex">[2024-10-16</span> 15:39:30,613] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb7-18"><a href="#cb7-18"></a><span class="ex">[2024-10-16</span> 15:39:30,614] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb7-19"><a href="#cb7-19"></a><span class="ex">[2024-10-16</span> 15:39:30,615] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb7-20"><a href="#cb7-20"></a><span class="ex">[2024-10-16</span> 15:39:30,630] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb7-21"><a href="#cb7-21"></a><span class="ex">[2024-10-16</span> 15:39:30,686] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb7-22"><a href="#cb7-22"></a><span class="ex">[2024-10-16</span> 15:39:30,698] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb7-23"><a href="#cb7-23"></a><span class="ex">[2024-10-16</span> 15:39:30,711] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb7-24"><a href="#cb7-24"></a><span class="ex">[2024-10-16</span> 15:39:30,715] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb7-25"><a href="#cb7-25"></a><span class="ex">[2024-10-16</span> 15:39:30,716] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb7-26"><a href="#cb7-26"></a><span class="ex">[2024-10-16</span> 15:39:30,723] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb7-27"><a href="#cb7-27"></a><span class="ex">[2024-10-16</span> 15:39:30,724] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb7-28"><a href="#cb7-28"></a><span class="ex">[2024-10-16</span> 15:39:30,726] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb7-29"><a href="#cb7-29"></a><span class="ex">[2024-10-16</span> 15:39:30,731] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb7-30"><a href="#cb7-30"></a><span class="ex">[2024-10-16</span> 15:39:30,737] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb7-31"><a href="#cb7-31"></a><span class="ex">[2024-10-16</span> 15:39:30,755] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb7-32"><a href="#cb7-32"></a><span class="ex">[2024-10-16</span> 15:39:30,766] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb7-33"><a href="#cb7-33"></a><span class="ex">[2024-10-16</span> 15:39:30,768] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb7-34"><a href="#cb7-34"></a><span class="ex">[2024-10-16</span> 15:39:30,780] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb7-35"><a href="#cb7-35"></a><span class="ex">[2024-10-16</span> 15:39:30,798] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb7-36"><a href="#cb7-36"></a><span class="ex">[2024-10-16</span> 15:39:30,840] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb7-37"><a href="#cb7-37"></a><span class="ex">[2024-10-16</span> 15:39:30,870] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb7-38"><a href="#cb7-38"></a><span class="ex">[2024-10-16</span> 15:39:30,870] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb7-39"><a href="#cb7-39"></a><span class="ex">[2024-10-16</span> 15:39:30,872] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb7-40"><a href="#cb7-40"></a><span class="ex">[2024-10-16</span> 15:39:30,873] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb7-41"><a href="#cb7-41"></a><span class="ex">[2024-10-16</span> 15:39:30,877] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb7-42"><a href="#cb7-42"></a><span class="ex">[2024-10-16</span> 15:39:30,888] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb7-43"><a href="#cb7-43"></a><span class="ex">[2024-10-16</span> 15:39:30,896] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb7-44"><a href="#cb7-44"></a><span class="ex">[2024-10-16</span> 15:39:30,902] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb7-45"><a href="#cb7-45"></a><span class="ex">[2024-10-16</span> 15:39:30,932] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb7-46"><a href="#cb7-46"></a><span class="ex">[2024-10-16</span> 15:39:30,934] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb7-47"><a href="#cb7-47"></a><span class="ex">[2024-10-16</span> 15:39:30,957] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb7-48"><a href="#cb7-48"></a><span class="ex">[2024-10-16</span> 15:39:30,984] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb7-49"><a href="#cb7-49"></a><span class="ex">[2024-10-16</span> 15:39:31,028] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb7-50"><a href="#cb7-50"></a><span class="ex">[2024-10-16</span> 15:39:31,029] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb7-51"><a href="#cb7-51"></a><span class="ex">[2024-10-16</span> 15:39:31,062] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb7-52"><a href="#cb7-52"></a><span class="ex">[2024-10-16</span> 15:39:31,150] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb7-53"><a href="#cb7-53"></a><span class="ex">[2024-10-16</span> 15:39:33,079] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:161:init_deepspeed_backend</span><span class="pp">]</span> Initialize ccl backend</span>
<span id="cb7-54"><a href="#cb7-54"></a><span class="ex">[2024-10-16</span> 15:39:33,079] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:652:init_distributed</span><span class="pp">]</span> cdb=None</span>
<span id="cb7-55"><a href="#cb7-55"></a><span class="ex">[2024-10-16</span> 15:39:33,079] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:667:init_distributed</span><span class="pp">]</span> Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...</span>
<span id="cb7-56"><a href="#cb7-56"></a><span class="ex">[2024-10-16</span> 15:39:33,481] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:161:init_deepspeed_backend</span><span class="pp">]</span> Initialize ccl backend</span>
<span id="cb7-57"><a href="#cb7-57"></a><span class="ex">[2024-10-16</span> 15:39:33,481] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:652:init_distributed</span><span class="pp">]</span> cdb=None</span>
<span id="cb7-58"><a href="#cb7-58"></a><span class="ex">[2024-10-16</span> 15:39:33,481] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:667:init_distributed</span><span class="pp">]</span> Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...</span>
<span id="cb7-59"><a href="#cb7-59"></a><span class="ex">[2024-10-16</span> 15:39:33,982] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:161:init_deepspeed_backend</span><span class="pp">]</span> Initialize ccl backend</span>
<span id="cb7-60"><a href="#cb7-60"></a><span class="ex">[2024-10-16</span> 15:39:33,982] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:652:init_distributed</span><span class="pp">]</span> cdb=None</span>
<span id="cb7-61"><a href="#cb7-61"></a><span class="ex">[2024-10-16</span> 15:39:33,982] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:667:init_distributed</span><span class="pp">]</span> Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...</span>
<span id="cb7-62"><a href="#cb7-62"></a><span class="ex">[2024-10-16</span> 15:39:33,983] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:161:init_deepspeed_backend</span><span class="pp">]</span> Initialize ccl backend</span>
<span id="cb7-63"><a href="#cb7-63"></a><span class="ex">[2024-10-16</span> 15:39:33,983] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:652:init_distributed</span><span class="pp">]</span> cdb=None</span>
<span id="cb7-64"><a href="#cb7-64"></a><span class="ex">[2024-10-16</span> 15:39:33,983] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:667:init_distributed</span><span class="pp">]</span> Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...</span>
<span id="cb7-65"><a href="#cb7-65"></a><span class="ex">[2024-10-16</span> 15:39:33,984] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:161:init_deepspeed_backend</span><span class="pp">]</span> Initialize ccl backend</span>
<span id="cb7-66"><a href="#cb7-66"></a><span class="ex">[2024-10-16</span> 15:39:33,984] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:652:init_distributed</span><span class="pp">]</span> cdb=None</span>
<span id="cb7-67"><a href="#cb7-67"></a><span class="ex">[2024-10-16</span> 15:39:33,984] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:667:init_distributed</span><span class="pp">]</span> Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...</span>
<span id="cb7-68"><a href="#cb7-68"></a><span class="ex">[2024-10-16</span> 15:39:34,133] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:161:init_deepspeed_backend</span><span class="pp">]</span> Initialize ccl backend</span>
<span id="cb7-69"><a href="#cb7-69"></a><span class="ex">[2024-10-16</span> 15:39:34,133] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:652:init_distributed</span><span class="pp">]</span> cdb=None</span>
<span id="cb7-70"><a href="#cb7-70"></a><span class="ex">[2024-10-16</span> 15:39:34,133] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:667:init_distributed</span><span class="pp">]</span> Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...</span>
<span id="cb7-71"><a href="#cb7-71"></a><span class="ex">[2024-10-16</span> 15:39:34,165] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:161:init_deepspeed_backend</span><span class="pp">]</span> Initialize ccl backend</span>
<span id="cb7-72"><a href="#cb7-72"></a><span class="ex">[2024-10-16</span> 15:39:34,165] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:652:init_distributed</span><span class="pp">]</span> cdb=None</span>
<span id="cb7-73"><a href="#cb7-73"></a><span class="ex">[2024-10-16</span> 15:39:34,165] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:667:init_distributed</span><span class="pp">]</span> Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...</span>
<span id="cb7-74"><a href="#cb7-74"></a><span class="ex">[2024-10-16</span> 15:39:34,207] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:161:init_deepspeed_backend</span><span class="pp">]</span> Initialize ccl backend</span>
<span id="cb7-75"><a href="#cb7-75"></a><span class="ex">[2024-10-16</span> 15:39:34,207] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:652:init_distributed</span><span class="pp">]</span> cdb=None</span>
<span id="cb7-76"><a href="#cb7-76"></a><span class="ex">[2024-10-16</span> 15:39:34,207] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:667:init_distributed</span><span class="pp">]</span> Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...</span>
<span id="cb7-77"><a href="#cb7-77"></a><span class="ex">[2024-10-16</span> 15:39:34,210] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:161:init_deepspeed_backend</span><span class="pp">]</span> Initialize ccl backend</span>
<span id="cb7-78"><a href="#cb7-78"></a><span class="ex">[2024-10-16</span> 15:39:34,210] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:652:init_distributed</span><span class="pp">]</span> cdb=None</span>
<span id="cb7-79"><a href="#cb7-79"></a><span class="ex">[2024-10-16</span> 15:39:34,210] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:667:init_distributed</span><span class="pp">]</span> Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...</span>
<span id="cb7-80"><a href="#cb7-80"></a><span class="ex">[2024-10-16</span> 15:39:34,216] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:161:init_deepspeed_backend</span><span class="pp">]</span> Initialize ccl backend</span>
<span id="cb7-81"><a href="#cb7-81"></a><span class="ex">[2024-10-16</span> 15:39:34,216] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:652:init_distributed</span><span class="pp">]</span> cdb=None</span>
<span id="cb7-82"><a href="#cb7-82"></a><span class="ex">[2024-10-16</span> 15:39:34,216] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:667:init_distributed</span><span class="pp">]</span> Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...</span>
<span id="cb7-83"><a href="#cb7-83"></a><span class="ex">[2024-10-16</span> 15:39:34,218] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:161:init_deepspeed_backend</span><span class="pp">]</span> Initialize ccl backend</span>
<span id="cb7-84"><a href="#cb7-84"></a><span class="ex">[2024-10-16</span> 15:39:34,218] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:652:init_distributed</span><span class="pp">]</span> cdb=None</span>
<span id="cb7-85"><a href="#cb7-85"></a><span class="ex">[2024-10-16</span> 15:39:34,219] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:667:init_distributed</span><span class="pp">]</span> Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...</span>
<span id="cb7-86"><a href="#cb7-86"></a><span class="ex">[2024-10-16</span> 15:39:34,271] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:161:init_deepspeed_backend</span><span class="pp">]</span> Initialize ccl backend</span>
<span id="cb7-87"><a href="#cb7-87"></a><span class="ex">[2024-10-16</span> 15:39:34,271] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:652:init_distributed</span><span class="pp">]</span> cdb=None</span>
<span id="cb7-88"><a href="#cb7-88"></a><span class="ex">[2024-10-16</span> 15:39:34,271] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:667:init_distributed</span><span class="pp">]</span> Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...</span>
<span id="cb7-89"><a href="#cb7-89"></a><span class="ex">[2024-10-16</span> 15:39:34,768] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:161:init_deepspeed_backend</span><span class="pp">]</span> Initialize ccl backend</span>
<span id="cb7-90"><a href="#cb7-90"></a><span class="ex">[2024-10-16</span> 15:39:34,768] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:652:init_distributed</span><span class="pp">]</span> cdb=None</span>
<span id="cb7-91"><a href="#cb7-91"></a><span class="ex">[2024-10-16</span> 15:39:34,768] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:667:init_distributed</span><span class="pp">]</span> Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...</span>
<span id="cb7-92"><a href="#cb7-92"></a><span class="ex">[2024-10-16</span> 15:39:34,772] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:161:init_deepspeed_backend</span><span class="pp">]</span> Initialize ccl backend</span>
<span id="cb7-93"><a href="#cb7-93"></a><span class="ex">[2024-10-16</span> 15:39:34,772] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:652:init_distributed</span><span class="pp">]</span> cdb=None</span>
<span id="cb7-94"><a href="#cb7-94"></a><span class="ex">[2024-10-16</span> 15:39:34,772] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:667:init_distributed</span><span class="pp">]</span> Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...</span>
<span id="cb7-95"><a href="#cb7-95"></a><span class="ex">[2024-10-16</span> 15:39:34,784] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:161:init_deepspeed_backend</span><span class="pp">]</span> Initialize ccl backend</span>
<span id="cb7-96"><a href="#cb7-96"></a><span class="ex">[2024-10-16</span> 15:39:34,784] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:652:init_distributed</span><span class="pp">]</span> cdb=None</span>
<span id="cb7-97"><a href="#cb7-97"></a><span class="ex">[2024-10-16</span> 15:39:34,784] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:667:init_distributed</span><span class="pp">]</span> Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...</span>
<span id="cb7-98"><a href="#cb7-98"></a><span class="ex">[2024-10-16</span> 15:39:34,795] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:161:init_deepspeed_backend</span><span class="pp">]</span> Initialize ccl backend</span>
<span id="cb7-99"><a href="#cb7-99"></a><span class="ex">[2024-10-16</span> 15:39:34,795] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:652:init_distributed</span><span class="pp">]</span> cdb=None</span>
<span id="cb7-100"><a href="#cb7-100"></a><span class="ex">[2024-10-16</span> 15:39:34,795] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:667:init_distributed</span><span class="pp">]</span> Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...</span>
<span id="cb7-101"><a href="#cb7-101"></a><span class="ex">[2024-10-16</span> 15:39:35,038] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:161:init_deepspeed_backend</span><span class="pp">]</span> Initialize ccl backend</span>
<span id="cb7-102"><a href="#cb7-102"></a><span class="ex">[2024-10-16</span> 15:39:35,038] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:652:init_distributed</span><span class="pp">]</span> cdb=None</span>
<span id="cb7-103"><a href="#cb7-103"></a><span class="ex">[2024-10-16</span> 15:39:35,038] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:667:init_distributed</span><span class="pp">]</span> Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...</span>
<span id="cb7-104"><a href="#cb7-104"></a><span class="ex">[2024-10-16</span> 15:39:35,048] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:161:init_deepspeed_backend</span><span class="pp">]</span> Initialize ccl backend</span>
<span id="cb7-105"><a href="#cb7-105"></a><span class="ex">[2024-10-16</span> 15:39:35,048] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:652:init_distributed</span><span class="pp">]</span> cdb=None</span>
<span id="cb7-106"><a href="#cb7-106"></a><span class="ex">[2024-10-16</span> 15:39:35,048] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:667:init_distributed</span><span class="pp">]</span> Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...</span>
<span id="cb7-107"><a href="#cb7-107"></a><span class="ex">[2024-10-16</span> 15:39:35,062] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:161:init_deepspeed_backend</span><span class="pp">]</span> Initialize ccl backend</span>
<span id="cb7-108"><a href="#cb7-108"></a><span class="ex">[2024-10-16</span> 15:39:35,062] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:652:init_distributed</span><span class="pp">]</span> cdb=None</span>
<span id="cb7-109"><a href="#cb7-109"></a><span class="ex">[2024-10-16</span> 15:39:35,062] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:667:init_distributed</span><span class="pp">]</span> Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...</span>
<span id="cb7-110"><a href="#cb7-110"></a><span class="ex">[2024-10-16</span> 15:39:35,070] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:161:init_deepspeed_backend</span><span class="pp">]</span> Initialize ccl backend</span>
<span id="cb7-111"><a href="#cb7-111"></a><span class="ex">[2024-10-16</span> 15:39:35,070] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:652:init_distributed</span><span class="pp">]</span> cdb=None</span>
<span id="cb7-112"><a href="#cb7-112"></a><span class="ex">[2024-10-16</span> 15:39:35,070] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:667:init_distributed</span><span class="pp">]</span> Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...</span>
<span id="cb7-113"><a href="#cb7-113"></a><span class="ex">[2024-10-16</span> 15:39:35,073] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:161:init_deepspeed_backend</span><span class="pp">]</span> Initialize ccl backend</span>
<span id="cb7-114"><a href="#cb7-114"></a><span class="ex">[2024-10-16</span> 15:39:35,073] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:652:init_distributed</span><span class="pp">]</span> cdb=None</span>
<span id="cb7-115"><a href="#cb7-115"></a><span class="ex">[2024-10-16</span> 15:39:35,073] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:667:init_distributed</span><span class="pp">]</span> Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...</span>
<span id="cb7-116"><a href="#cb7-116"></a><span class="ex">[2024-10-16</span> 15:39:35,077] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:161:init_deepspeed_backend</span><span class="pp">]</span> Initialize ccl backend</span>
<span id="cb7-117"><a href="#cb7-117"></a><span class="ex">[2024-10-16</span> 15:39:35,078] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:652:init_distributed</span><span class="pp">]</span> cdb=None</span>
<span id="cb7-118"><a href="#cb7-118"></a><span class="ex">[2024-10-16</span> 15:39:35,078] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:667:init_distributed</span><span class="pp">]</span> Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...</span>
<span id="cb7-119"><a href="#cb7-119"></a><span class="ex">[2024-10-16</span> 15:39:35,103] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:161:init_deepspeed_backend</span><span class="pp">]</span> Initialize ccl backend</span>
<span id="cb7-120"><a href="#cb7-120"></a><span class="ex">[2024-10-16</span> 15:39:35,104] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:652:init_distributed</span><span class="pp">]</span> cdb=None</span>
<span id="cb7-121"><a href="#cb7-121"></a><span class="ex">[2024-10-16</span> 15:39:35,104] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:667:init_distributed</span><span class="pp">]</span> Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...</span>
<span id="cb7-122"><a href="#cb7-122"></a><span class="ex">[2024-10-16</span> 15:39:35,106] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:161:init_deepspeed_backend</span><span class="pp">]</span> Initialize ccl backend</span>
<span id="cb7-123"><a href="#cb7-123"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:652:init_distributed</span><span class="pp">]</span> cdb=None</span>
<span id="cb7-124"><a href="#cb7-124"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:667:init_distributed</span><span class="pp">]</span> Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...</span>
<span id="cb7-125"><a href="#cb7-125"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:718:mpi_discovery</span><span class="pp">]</span> Discovered MPI settings of world_rank=13, local_rank=1, world_size=24, master_addr=10.115.45.184, master_port=29500</span>
<span id="cb7-126"><a href="#cb7-126"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:718:mpi_discovery</span><span class="pp">]</span> Discovered MPI settings of world_rank=15, local_rank=3, world_size=24, master_addr=10.115.45.184, master_port=29500</span>
<span id="cb7-127"><a href="#cb7-127"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:718:mpi_discovery</span><span class="pp">]</span> Discovered MPI settings of world_rank=16, local_rank=4, world_size=24, master_addr=10.115.45.184, master_port=29500</span>
<span id="cb7-128"><a href="#cb7-128"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:718:mpi_discovery</span><span class="pp">]</span> Discovered MPI settings of world_rank=17, local_rank=5, world_size=24, master_addr=10.115.45.184, master_port=29500</span>
<span id="cb7-129"><a href="#cb7-129"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:718:mpi_discovery</span><span class="pp">]</span> Discovered MPI settings of world_rank=12, local_rank=0, world_size=24, master_addr=10.115.45.184, master_port=29500</span>
<span id="cb7-130"><a href="#cb7-130"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:718:mpi_discovery</span><span class="pp">]</span> Discovered MPI settings of world_rank=14, local_rank=2, world_size=24, master_addr=10.115.45.184, master_port=29500</span>
<span id="cb7-131"><a href="#cb7-131"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:718:mpi_discovery</span><span class="pp">]</span> Discovered MPI settings of world_rank=0, local_rank=0, world_size=24, master_addr=10.115.45.184, master_port=29500</span>
<span id="cb7-132"><a href="#cb7-132"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:683:init_distributed</span><span class="pp">]</span> Initializing TorchBackend in DeepSpeed with backend ccl</span>
<span id="cb7-133"><a href="#cb7-133"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:718:mpi_discovery</span><span class="pp">]</span> Discovered MPI settings of world_rank=18, local_rank=6, world_size=24, master_addr=10.115.45.184, master_port=29500</span>
<span id="cb7-134"><a href="#cb7-134"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:718:mpi_discovery</span><span class="pp">]</span> Discovered MPI settings of world_rank=19, local_rank=7, world_size=24, master_addr=10.115.45.184, master_port=29500</span>
<span id="cb7-135"><a href="#cb7-135"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:718:mpi_discovery</span><span class="pp">]</span> Discovered MPI settings of world_rank=20, local_rank=8, world_size=24, master_addr=10.115.45.184, master_port=29500</span>
<span id="cb7-136"><a href="#cb7-136"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:718:mpi_discovery</span><span class="pp">]</span> Discovered MPI settings of world_rank=21, local_rank=9, world_size=24, master_addr=10.115.45.184, master_port=29500</span>
<span id="cb7-137"><a href="#cb7-137"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:718:mpi_discovery</span><span class="pp">]</span> Discovered MPI settings of world_rank=22, local_rank=10, world_size=24, master_addr=10.115.45.184, master_port=29500</span>
<span id="cb7-138"><a href="#cb7-138"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:718:mpi_discovery</span><span class="pp">]</span> Discovered MPI settings of world_rank=23, local_rank=11, world_size=24, master_addr=10.115.45.184, master_port=29500</span>
<span id="cb7-139"><a href="#cb7-139"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:718:mpi_discovery</span><span class="pp">]</span> Discovered MPI settings of world_rank=1, local_rank=1, world_size=24, master_addr=10.115.45.184, master_port=29500</span>
<span id="cb7-140"><a href="#cb7-140"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:718:mpi_discovery</span><span class="pp">]</span> Discovered MPI settings of world_rank=2, local_rank=2, world_size=24, master_addr=10.115.45.184, master_port=29500</span>
<span id="cb7-141"><a href="#cb7-141"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:718:mpi_discovery</span><span class="pp">]</span> Discovered MPI settings of world_rank=3, local_rank=3, world_size=24, master_addr=10.115.45.184, master_port=29500</span>
<span id="cb7-142"><a href="#cb7-142"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:718:mpi_discovery</span><span class="pp">]</span> Discovered MPI settings of world_rank=4, local_rank=4, world_size=24, master_addr=10.115.45.184, master_port=29500</span>
<span id="cb7-143"><a href="#cb7-143"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:718:mpi_discovery</span><span class="pp">]</span> Discovered MPI settings of world_rank=5, local_rank=5, world_size=24, master_addr=10.115.45.184, master_port=29500</span>
<span id="cb7-144"><a href="#cb7-144"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:718:mpi_discovery</span><span class="pp">]</span> Discovered MPI settings of world_rank=6, local_rank=6, world_size=24, master_addr=10.115.45.184, master_port=29500</span>
<span id="cb7-145"><a href="#cb7-145"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:718:mpi_discovery</span><span class="pp">]</span> Discovered MPI settings of world_rank=7, local_rank=7, world_size=24, master_addr=10.115.45.184, master_port=29500</span>
<span id="cb7-146"><a href="#cb7-146"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:718:mpi_discovery</span><span class="pp">]</span> Discovered MPI settings of world_rank=8, local_rank=8, world_size=24, master_addr=10.115.45.184, master_port=29500</span>
<span id="cb7-147"><a href="#cb7-147"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:718:mpi_discovery</span><span class="pp">]</span> Discovered MPI settings of world_rank=9, local_rank=9, world_size=24, master_addr=10.115.45.184, master_port=29500</span>
<span id="cb7-148"><a href="#cb7-148"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:718:mpi_discovery</span><span class="pp">]</span> Discovered MPI settings of world_rank=10, local_rank=10, world_size=24, master_addr=10.115.45.184, master_port=29500</span>
<span id="cb7-149"><a href="#cb7-149"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:718:mpi_discovery</span><span class="pp">]</span> Discovered MPI settings of world_rank=11, local_rank=11, world_size=24, master_addr=10.115.45.184, master_port=29500</span>
<span id="cb7-150"><a href="#cb7-150"></a><span class="ex">[2024-10-16</span> 15:39:35.116490]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:348</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'xpu'</span><span class="pp">][</span><span class="ss">rank=15/23</span><span class="pp">][</span><span class="ss">local_rank=3/11</span><span class="pp">][</span><span class="ss">node=1/1</span><span class="pp">]</span></span>
<span id="cb7-151"><a href="#cb7-151"></a><span class="ex">[2024-10-16</span> 15:39:35.117455]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:348</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'xpu'</span><span class="pp">][</span><span class="ss">rank=1/23</span><span class="pp">][</span><span class="ss">local_rank=1/11</span><span class="pp">][</span><span class="ss">node=1/1</span><span class="pp">]</span></span>
<span id="cb7-152"><a href="#cb7-152"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">34466</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">MPI</span> was initialized externally, CCL-MPI specific environment is ignored</span>
<span id="cb7-153"><a href="#cb7-153"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">169066</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">MPI</span> was initialized externally, CCL-MPI specific environment is ignored</span>
<span id="cb7-154"><a href="#cb7-154"></a><span class="ex">[2024-10-16</span> 15:39:35.120281]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:348</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'xpu'</span><span class="pp">][</span><span class="ss">rank=10/23</span><span class="pp">][</span><span class="ss">local_rank=10/11</span><span class="pp">][</span><span class="ss">node=0/1</span><span class="pp">]</span></span>
<span id="cb7-155"><a href="#cb7-155"></a><span class="ex">[2024-10-16</span> 15:39:35.120280]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:348</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'xpu'</span><span class="pp">][</span><span class="ss">rank=11/23</span><span class="pp">][</span><span class="ss">local_rank=11/11</span><span class="pp">][</span><span class="ss">node=1/1</span><span class="pp">]</span></span>
<span id="cb7-156"><a href="#cb7-156"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">169075</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">MPI</span> was initialized externally, CCL-MPI specific environment is ignored</span>
<span id="cb7-157"><a href="#cb7-157"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">169076</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">MPI</span> was initialized externally, CCL-MPI specific environment is ignored</span>
<span id="cb7-158"><a href="#cb7-158"></a><span class="ex">[2024-10-16</span> 15:39:35.124730]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:348</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'xpu'</span><span class="pp">][</span><span class="ss">rank=4/23</span><span class="pp">][</span><span class="ss">local_rank=4/11</span><span class="pp">][</span><span class="ss">node=0/1</span><span class="pp">]</span></span>
<span id="cb7-159"><a href="#cb7-159"></a><span class="ex">[2024-10-16</span> 15:39:35.125260]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:348</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'xpu'</span><span class="pp">][</span><span class="ss">rank=6/23</span><span class="pp">][</span><span class="ss">local_rank=6/11</span><span class="pp">][</span><span class="ss">node=0/1</span><span class="pp">]</span></span>
<span id="cb7-160"><a href="#cb7-160"></a><span class="ex">[2024-10-16</span> 15:39:35.125427]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:348</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'xpu'</span><span class="pp">][</span><span class="ss">rank=3/23</span><span class="pp">][</span><span class="ss">local_rank=3/11</span><span class="pp">][</span><span class="ss">node=1/1</span><span class="pp">]</span></span>
<span id="cb7-161"><a href="#cb7-161"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">169069</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">MPI</span> was initialized externally, CCL-MPI specific environment is ignored</span>
<span id="cb7-162"><a href="#cb7-162"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">169068</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">MPI</span> was initialized externally, CCL-MPI specific environment is ignored</span>
<span id="cb7-163"><a href="#cb7-163"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">169071</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">MPI</span> was initialized externally, CCL-MPI specific environment is ignored</span>
<span id="cb7-164"><a href="#cb7-164"></a><span class="ex">[2024-10-16</span> 15:39:35.127167]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:348</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'xpu'</span><span class="pp">][</span><span class="ss">rank=8/23</span><span class="pp">][</span><span class="ss">local_rank=8/11</span><span class="pp">][</span><span class="ss">node=0/1</span><span class="pp">]</span></span>
<span id="cb7-165"><a href="#cb7-165"></a><span class="ex">[2024-10-16</span> 15:39:35.127199]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:348</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'xpu'</span><span class="pp">][</span><span class="ss">rank=21/23</span><span class="pp">][</span><span class="ss">local_rank=9/11</span><span class="pp">][</span><span class="ss">node=1/1</span><span class="pp">]</span></span>
<span id="cb7-166"><a href="#cb7-166"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">169073</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">MPI</span> was initialized externally, CCL-MPI specific environment is ignored</span>
<span id="cb7-167"><a href="#cb7-167"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">34472</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">MPI</span> was initialized externally, CCL-MPI specific environment is ignored</span>
<span id="cb7-168"><a href="#cb7-168"></a><span class="ex">[2024-10-16</span> 15:39:35.129097]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:348</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'xpu'</span><span class="pp">][</span><span class="ss">rank=14/23</span><span class="pp">][</span><span class="ss">local_rank=2/11</span><span class="pp">][</span><span class="ss">node=0/1</span><span class="pp">]</span></span>
<span id="cb7-169"><a href="#cb7-169"></a><span class="ex">[2024-10-16</span> 15:39:35.129281]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:348</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'xpu'</span><span class="pp">][</span><span class="ss">rank=9/23</span><span class="pp">][</span><span class="ss">local_rank=9/11</span><span class="pp">][</span><span class="ss">node=1/1</span><span class="pp">]</span></span>
<span id="cb7-170"><a href="#cb7-170"></a><span class="ex">[2024-10-16</span> 15:39:35.129345]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:348</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'xpu'</span><span class="pp">][</span><span class="ss">rank=22/23</span><span class="pp">][</span><span class="ss">local_rank=10/11</span><span class="pp">][</span><span class="ss">node=0/1</span><span class="pp">]</span></span>
<span id="cb7-171"><a href="#cb7-171"></a><span class="ex">[2024-10-16</span> 15:39:35.129461]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:348</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'xpu'</span><span class="pp">][</span><span class="ss">rank=20/23</span><span class="pp">][</span><span class="ss">local_rank=8/11</span><span class="pp">][</span><span class="ss">node=0/1</span><span class="pp">]</span></span>
<span id="cb7-172"><a href="#cb7-172"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">34465</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">MPI</span> was initialized externally, CCL-MPI specific environment is ignored</span>
<span id="cb7-173"><a href="#cb7-173"></a><span class="ex">[2024-10-16</span> 15:39:35.130518]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:348</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'xpu'</span><span class="pp">][</span><span class="ss">rank=13/23</span><span class="pp">][</span><span class="ss">local_rank=1/11</span><span class="pp">][</span><span class="ss">node=1/1</span><span class="pp">]</span></span>
<span id="cb7-174"><a href="#cb7-174"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">34473</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">MPI</span> was initialized externally, CCL-MPI specific environment is ignored</span>
<span id="cb7-175"><a href="#cb7-175"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">169074</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">MPI</span> was initialized externally, CCL-MPI specific environment is ignored</span>
<span id="cb7-176"><a href="#cb7-176"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">34471</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">MPI</span> was initialized externally, CCL-MPI specific environment is ignored</span>
<span id="cb7-177"><a href="#cb7-177"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">34464</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">MPI</span> was initialized externally, CCL-MPI specific environment is ignored</span>
<span id="cb7-178"><a href="#cb7-178"></a><span class="ex">[2024-10-16</span> 15:39:35.131822]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:348</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'xpu'</span><span class="pp">][</span><span class="ss">rank=18/23</span><span class="pp">][</span><span class="ss">local_rank=6/11</span><span class="pp">][</span><span class="ss">node=0/1</span><span class="pp">]</span></span>
<span id="cb7-179"><a href="#cb7-179"></a><span class="ex">[2024-10-16</span> 15:39:35.131816]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:348</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'xpu'</span><span class="pp">][</span><span class="ss">rank=19/23</span><span class="pp">][</span><span class="ss">local_rank=7/11</span><span class="pp">][</span><span class="ss">node=1/1</span><span class="pp">]</span></span>
<span id="cb7-180"><a href="#cb7-180"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">34469</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">MPI</span> was initialized externally, CCL-MPI specific environment is ignored</span>
<span id="cb7-181"><a href="#cb7-181"></a><span class="ex">[2024-10-16</span> 15:39:35.133183]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:348</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'xpu'</span><span class="pp">][</span><span class="ss">rank=7/23</span><span class="pp">][</span><span class="ss">local_rank=7/11</span><span class="pp">][</span><span class="ss">node=1/1</span><span class="pp">]</span></span>
<span id="cb7-182"><a href="#cb7-182"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">34470</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">MPI</span> was initialized externally, CCL-MPI specific environment is ignored</span>
<span id="cb7-183"><a href="#cb7-183"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">169072</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">MPI</span> was initialized externally, CCL-MPI specific environment is ignored</span>
<span id="cb7-184"><a href="#cb7-184"></a><span class="ex">[2024-10-16</span> 15:39:35.192746]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:348</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'xpu'</span><span class="pp">][</span><span class="ss">rank=2/23</span><span class="pp">][</span><span class="ss">local_rank=2/11</span><span class="pp">][</span><span class="ss">node=0/1</span><span class="pp">]</span></span>
<span id="cb7-185"><a href="#cb7-185"></a><span class="ex">[2024-10-16</span> 15:39:35.192682]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:348</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'xpu'</span><span class="pp">][</span><span class="ss">rank=5/23</span><span class="pp">][</span><span class="ss">local_rank=5/11</span><span class="pp">][</span><span class="ss">node=1/1</span><span class="pp">]</span></span>
<span id="cb7-186"><a href="#cb7-186"></a><span class="ex">[2024-10-16</span> 15:39:35.193446]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:348</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'xpu'</span><span class="pp">][</span><span class="ss">rank=17/23</span><span class="pp">][</span><span class="ss">local_rank=5/11</span><span class="pp">][</span><span class="ss">node=1/1</span><span class="pp">]</span></span>
<span id="cb7-187"><a href="#cb7-187"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">169067</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">MPI</span> was initialized externally, CCL-MPI specific environment is ignored</span>
<span id="cb7-188"><a href="#cb7-188"></a><span class="ex">[2024-10-16</span> 15:39:35.193506]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:348</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'xpu'</span><span class="pp">][</span><span class="ss">rank=12/23</span><span class="pp">][</span><span class="ss">local_rank=0/11</span><span class="pp">][</span><span class="ss">node=0/1</span><span class="pp">]</span></span>
<span id="cb7-189"><a href="#cb7-189"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">169070</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">MPI</span> was initialized externally, CCL-MPI specific environment is ignored</span>
<span id="cb7-190"><a href="#cb7-190"></a><span class="ex">[2024-10-16</span> 15:39:35.194435]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:348</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'xpu'</span><span class="pp">][</span><span class="ss">rank=23/23</span><span class="pp">][</span><span class="ss">local_rank=11/11</span><span class="pp">][</span><span class="ss">node=1/1</span><span class="pp">]</span></span>
<span id="cb7-191"><a href="#cb7-191"></a><span class="ex">[2024-10-16</span> 15:39:35.194529]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:348</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'xpu'</span><span class="pp">][</span><span class="ss">rank=16/23</span><span class="pp">][</span><span class="ss">local_rank=4/11</span><span class="pp">][</span><span class="ss">node=0/1</span><span class="pp">]</span></span>
<span id="cb7-192"><a href="#cb7-192"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">34468</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">MPI</span> was initialized externally, CCL-MPI specific environment is ignored</span>
<span id="cb7-193"><a href="#cb7-193"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">34463</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">MPI</span> was initialized externally, CCL-MPI specific environment is ignored</span>
<span id="cb7-194"><a href="#cb7-194"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">34467</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">MPI</span> was initialized externally, CCL-MPI specific environment is ignored</span>
<span id="cb7-195"><a href="#cb7-195"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">34474</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">MPI</span> was initialized externally, CCL-MPI specific environment is ignored</span>
<span id="cb7-196"><a href="#cb7-196"></a><span class="ex">[2024-10-16</span> 15:39:35.198383]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:92</span><span class="pp">]</span> <span class="at">-</span></span>
<span id="cb7-197"><a href="#cb7-197"></a></span>
<span id="cb7-198"><a href="#cb7-198"></a><span class="ex">[dist_info]:</span></span>
<span id="cb7-199"><a href="#cb7-199"></a>  <span class="ex">•</span> DEVICE=xpu</span>
<span id="cb7-200"><a href="#cb7-200"></a>  <span class="ex">•</span> DEVICE_ID=xpu:0</span>
<span id="cb7-201"><a href="#cb7-201"></a>  <span class="ex">•</span> DISTRIBUTED_BACKEND=ccl</span>
<span id="cb7-202"><a href="#cb7-202"></a>  <span class="ex">•</span> GPUS_PER_NODE=12</span>
<span id="cb7-203"><a href="#cb7-203"></a>  <span class="ex">•</span> HOSTS=[<span class="st">'x4407c6s7b0n0.hostmgmt2407.cm.aurora.alcf.anl.gov'</span>, <span class="st">'x4308c2s5b0n0.hostmgmt2308.cm.aurora.alcf.anl.gov'</span>]</span>
<span id="cb7-204"><a href="#cb7-204"></a>  <span class="ex">•</span> HOSTFILE=/var/spool/pbs/aux/886439.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov</span>
<span id="cb7-205"><a href="#cb7-205"></a>  <span class="ex">•</span> HOSTNAME=x4407c6s7b0n0.hostmgmt2407.cm.aurora.alcf.anl.gov</span>
<span id="cb7-206"><a href="#cb7-206"></a>  <span class="ex">•</span> LOCAL_RANK=0</span>
<span id="cb7-207"><a href="#cb7-207"></a>  <span class="ex">•</span> MACHINE=Aurora</span>
<span id="cb7-208"><a href="#cb7-208"></a>  <span class="ex">•</span> NUM_NODES=2</span>
<span id="cb7-209"><a href="#cb7-209"></a>  <span class="ex">•</span> NGPUS=24</span>
<span id="cb7-210"><a href="#cb7-210"></a>  <span class="ex">•</span> NGPUS_AVAILABLE=24</span>
<span id="cb7-211"><a href="#cb7-211"></a>  <span class="ex">•</span> NODE_ID=0</span>
<span id="cb7-212"><a href="#cb7-212"></a>  <span class="ex">•</span> RANK=0</span>
<span id="cb7-213"><a href="#cb7-213"></a>  <span class="ex">•</span> SCHEDULER=PBS</span>
<span id="cb7-214"><a href="#cb7-214"></a>  <span class="ex">•</span> WORLD_SIZE_TOTAL=24</span>
<span id="cb7-215"><a href="#cb7-215"></a>  <span class="ex">•</span> WORLD_SIZE_IN_USE=24</span>
<span id="cb7-216"><a href="#cb7-216"></a>  <span class="ex">•</span> LAUNCH_CMD=mpiexec <span class="at">--verbose</span> <span class="at">--envall</span> <span class="at">-n</span> 24 <span class="at">-ppn</span> 12 <span class="at">--hostfile</span> /var/spool/pbs/aux/886439.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov <span class="at">--cpu-bind</span> depth <span class="at">-d</span> 16</span>
<span id="cb7-217"><a href="#cb7-217"></a></span>
<span id="cb7-218"><a href="#cb7-218"></a></span>
<span id="cb7-219"><a href="#cb7-219"></a><span class="ex">--------------------------------------------------</span></span>
<span id="cb7-220"><a href="#cb7-220"></a><span class="ex">DeepSpeed</span> C++/CUDA extension op report</span>
<span id="cb7-221"><a href="#cb7-221"></a><span class="ex">--------------------------------------------------</span></span>
<span id="cb7-222"><a href="#cb7-222"></a><span class="ex">NOTE:</span> Ops not installed will be just-in-time <span class="er">(</span><span class="ex">JIT</span><span class="kw">)</span> <span class="ex">compiled</span> at</span>
<span id="cb7-223"><a href="#cb7-223"></a>      <span class="ex">runtime</span> if needed. Op compatibility means that your system</span>
<span id="cb7-224"><a href="#cb7-224"></a>      <span class="ex">meet</span> the required dependencies to JIT install the op.</span>
<span id="cb7-225"><a href="#cb7-225"></a><span class="ex">--------------------------------------------------</span></span>
<span id="cb7-226"><a href="#cb7-226"></a><span class="ex">JIT</span> compiled ops requires ninja</span>
<span id="cb7-227"><a href="#cb7-227"></a><span class="ex">ninja</span> .................. <span class="pp">[</span><span class="ss">OKAY</span><span class="pp">]</span></span>
<span id="cb7-228"><a href="#cb7-228"></a><span class="ex">--------------------------------------------------</span></span>
<span id="cb7-229"><a href="#cb7-229"></a><span class="ex">op</span> name ................ installed .. compatible</span>
<span id="cb7-230"><a href="#cb7-230"></a><span class="ex">--------------------------------------------------</span></span>
<span id="cb7-231"><a href="#cb7-231"></a><span class="ex">deepspeed_not_implemented</span>  <span class="pp">[</span><span class="ss">NO</span><span class="pp">]</span> ....... <span class="pp">[</span><span class="ss">OKAY</span><span class="pp">]</span></span>
<span id="cb7-232"><a href="#cb7-232"></a> <span class="ex">[WARNING]</span>  async_io requires the dev libaio .so object and headers but these were not found.</span>
<span id="cb7-233"><a href="#cb7-233"></a> <span class="ex">[WARNING]</span>  If libaio is already installed <span class="er">(</span><span class="ex">perhaps</span> from source<span class="kw">)</span><span class="ex">,</span> try setting the CFLAGS and LDFLAGS environment variables to where it can be found.</span>
<span id="cb7-234"><a href="#cb7-234"></a><span class="ex">async_io</span> ............... <span class="pp">[</span><span class="ss">NO</span><span class="pp">]</span> ....... <span class="pp">[</span><span class="ss">NO</span><span class="pp">]</span></span>
<span id="cb7-235"><a href="#cb7-235"></a><span class="ex">cpu_adagrad</span> ............ <span class="pp">[</span><span class="ss">NO</span><span class="pp">]</span> ....... <span class="pp">[</span><span class="ss">OKAY</span><span class="pp">]</span></span>
<span id="cb7-236"><a href="#cb7-236"></a><span class="ex">cpu_adam</span> ............... <span class="pp">[</span><span class="ss">NO</span><span class="pp">]</span> ....... <span class="pp">[</span><span class="ss">OKAY</span><span class="pp">]</span></span>
<span id="cb7-237"><a href="#cb7-237"></a><span class="ex">flash_attn</span> ............. <span class="pp">[</span><span class="ss">NO</span><span class="pp">]</span> ....... <span class="pp">[</span><span class="ss">OKAY</span><span class="pp">]</span></span>
<span id="cb7-238"><a href="#cb7-238"></a><span class="ex">fused_adam</span> ............. <span class="pp">[</span><span class="ss">NO</span><span class="pp">]</span> ....... <span class="pp">[</span><span class="ss">OKAY</span><span class="pp">]</span></span>
<span id="cb7-239"><a href="#cb7-239"></a><span class="ex">transformer_inference</span> .. <span class="pp">[</span><span class="ss">NO</span><span class="pp">]</span> ....... <span class="pp">[</span><span class="ss">OKAY</span><span class="pp">]</span></span>
<span id="cb7-240"><a href="#cb7-240"></a><span class="ex">pack_bits</span> .............. <span class="pp">[</span><span class="ss">NO</span><span class="pp">]</span> ....... <span class="pp">[</span><span class="ss">OKAY</span><span class="pp">]</span></span>
<span id="cb7-241"><a href="#cb7-241"></a><span class="ex">--------------------------------------------------</span></span>
<span id="cb7-242"><a href="#cb7-242"></a><span class="ex">DeepSpeed</span> general environment info:</span>
<span id="cb7-243"><a href="#cb7-243"></a><span class="ex">torch</span> install path ............... <span class="pp">[</span><span class="st">'/opt/aurora/24.180.0/frameworks/aurora_nre_models_frameworks-2024.2.1_u1/lib/python3.10/site-packages/torch'</span><span class="pp">]</span></span>
<span id="cb7-244"><a href="#cb7-244"></a><span class="ex">torch</span> version .................... 2.3.1+cxx11.abi</span>
<span id="cb7-245"><a href="#cb7-245"></a><span class="ex">deepspeed</span> install path ........... <span class="pp">[</span><span class="st">'/lus/flare/projects/Aurora_deployment/foremans/projects/argonne-lcf/Megatron-DeepSpeed/deps/DeepSpeed/deepspeed'</span><span class="pp">]</span></span>
<span id="cb7-246"><a href="#cb7-246"></a><span class="ex">deepspeed</span> info ................... 0.15.3+unknown, unknown, unknown</span>
<span id="cb7-247"><a href="#cb7-247"></a><span class="ex">deepspeed</span> wheel compiled w. ...... torch 2.3</span>
<span id="cb7-248"><a href="#cb7-248"></a><span class="ex">shared</span> memory <span class="er">(</span><span class="ex">/dev/shm</span><span class="kw">)</span> <span class="fu">size</span> .... 503.18 GB</span>
<span id="cb7-249"><a href="#cb7-249"></a><span class="ex">[2024-10-16</span> 15:39:35.319255]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">configs.py:272</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">****</span> Git info for DeepSpeed: git_hash=7ef26bf git_branch=hzheng-data-fix <span class="pp">****</span></span>
<span id="cb7-250"><a href="#cb7-250"></a><span class="ex">[2024-10-16</span> 15:39:35.319927]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:725</span><span class="pp">]</span> <span class="at">-</span> Using oneccl_bindings from: /opt/aurora/24.180.0/frameworks/aurora_nre_models_frameworks-2024.2.1_u1/lib/python3.10/site-packages/oneccl_bindings_for_pytorch/__init__.py</span>
<span id="cb7-251"><a href="#cb7-251"></a><span class="ex">[2024-10-16</span> 15:39:35.320347]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:727</span><span class="pp">]</span> <span class="at">-</span> Using ipex from: /opt/aurora/24.180.0/frameworks/aurora_nre_models_frameworks-2024.2.1_u1/lib/python3.10/site-packages/intel_extension_for_pytorch/__init__.py</span>
<span id="cb7-252"><a href="#cb7-252"></a><span class="ex">[2024-10-16</span> 15:39:35.320734]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:728</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">0/24</span><span class="pp">]</span> Using device=<span class="st">'xpu'</span> with backend=<span class="st">'deepspeed'</span> + <span class="st">'ccl'</span> for distributed training.</span>
<span id="cb7-253"><a href="#cb7-253"></a><span class="ex">[2024-10-16</span> 15:39:35.325748]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:348</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'xpu'</span><span class="pp">][</span><span class="ss">rank=0/23</span><span class="pp">][</span><span class="ss">local_rank=0/11</span><span class="pp">][</span><span class="ss">node=0/1</span><span class="pp">]</span></span>
<span id="cb7-254"><a href="#cb7-254"></a><span class="ex">[2024-10-16</span> 15:39:35.326291]<span class="pp">[</span><span class="ss">WARNING</span><span class="pp">][</span><span class="ss">_logger.py:68</span><span class="pp">]</span> <span class="at">-</span> Using [24 / 24] available <span class="st">"xpu"</span> devices !!</span>
<span id="cb7-255"><a href="#cb7-255"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">169065</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">value</span> of CCL_KVS_MODE changed to be mpi <span class="er">(</span><span class="ex">default:pmi</span><span class="kw">)</span></span>
<span id="cb7-256"><a href="#cb7-256"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">169065</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">value</span> of CCL_KVS_CONNECTION_TIMEOUT changed to be 3600 <span class="er">(</span><span class="ex">default:120</span><span class="kw">)</span></span>
<span id="cb7-257"><a href="#cb7-257"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">169065</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">value</span> of CCL_BCAST changed to be double_tree <span class="er">(</span><span class="ex">default:</span><span class="kw">)</span></span>
<span id="cb7-258"><a href="#cb7-258"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">169065</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">value</span> of CCL_ENABLE_SYCL_KERNELS changed to be 1 <span class="er">(</span><span class="ex">default:0</span><span class="kw">)</span></span>
<span id="cb7-259"><a href="#cb7-259"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">169065</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">value</span> of CCL_SYCL_ESIMD changed to be 1 <span class="er">(</span><span class="ex">default:0</span><span class="kw">)</span></span>
<span id="cb7-260"><a href="#cb7-260"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">169065</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">value</span> of CCL_PROCESS_LAUNCHER changed to be pmix <span class="er">(</span><span class="ex">default:hydra</span><span class="kw">)</span></span>
<span id="cb7-261"><a href="#cb7-261"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">169065</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">value</span> of CCL_ZE_CACHE_OPEN_IPC_HANDLES_THRESHOLD changed to be 32768 <span class="er">(</span><span class="ex">default:1000</span><span class="kw">)</span></span>
<span id="cb7-262"><a href="#cb7-262"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">169065</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="va">CCL_ALLGATHERV_MEDIUM_SIZE_THRESHOLD</span><span class="op">=</span>0 <span class="ex">is</span> unknown to and unused by oneCCL code but is present in the environment, check if it is not mistyped.</span>
<span id="cb7-263"><a href="#cb7-263"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">169065</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="va">CCL_SKIP_SCHEDULER</span><span class="op">=</span>1 <span class="ex">is</span> unknown to and unused by oneCCL code but is present in the environment, check if it is not mistyped.</span>
<span id="cb7-264"><a href="#cb7-264"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">169065</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">MPI</span> was initialized externally, CCL-MPI specific environment is ignored</span>
<span id="cb7-265"><a href="#cb7-265"></a><span class="ex">using</span> world size: 24, data-parallel-size: 24, sequence-parallel size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1</span>
<span id="cb7-266"><a href="#cb7-266"></a><span class="ex">accumulate</span> and all-reduce gradients in fp32 for bfloat16 data type.</span>
<span id="cb7-267"><a href="#cb7-267"></a><span class="ex">using</span> torch.bfloat16 for parameters ...</span>
<span id="cb7-268"><a href="#cb7-268"></a><span class="ex">------------------------</span> arguments <span class="at">------------------------</span></span>
<span id="cb7-269"><a href="#cb7-269"></a>  <span class="ex">accumulate_allreduce_grads_in_fp32</span> .............. True</span>
<span id="cb7-270"><a href="#cb7-270"></a>  <span class="ex">adam_beta1</span> ...................................... 0.9</span>
<span id="cb7-271"><a href="#cb7-271"></a>  <span class="ex">adam_beta2</span> ...................................... 0.999</span>
<span id="cb7-272"><a href="#cb7-272"></a>  <span class="ex">adam_eps</span> ........................................ 1e-08</span>
<span id="cb7-273"><a href="#cb7-273"></a>  <span class="ex">add_bias_linear</span> ................................. False</span>
<span id="cb7-274"><a href="#cb7-274"></a>  <span class="ex">add_position_embedding</span> .......................... False</span>
<span id="cb7-275"><a href="#cb7-275"></a>  <span class="ex">adlr_autoresume</span> ................................. False</span>
<span id="cb7-276"><a href="#cb7-276"></a>  <span class="ex">adlr_autoresume_interval</span> ........................ 1000</span>
<span id="cb7-277"><a href="#cb7-277"></a>  <span class="ex">aml_data_download_path</span> .......................... None</span>
<span id="cb7-278"><a href="#cb7-278"></a>  <span class="ex">apply_layernorm_1p</span> .............................. False</span>
<span id="cb7-279"><a href="#cb7-279"></a>  <span class="ex">apply_query_key_layer_scaling</span> ................... False</span>
<span id="cb7-280"><a href="#cb7-280"></a>  <span class="ex">apply_residual_connection_post_layernorm</span> ........ False</span>
<span id="cb7-281"><a href="#cb7-281"></a>  <span class="ex">async_tensor_model_parallel_allreduce</span> ........... False</span>
<span id="cb7-282"><a href="#cb7-282"></a>  <span class="ex">attention_dropout</span> ............................... 0.0</span>
<span id="cb7-283"><a href="#cb7-283"></a>  <span class="ex">attention_softmax_in_fp32</span> ....................... False</span>
<span id="cb7-284"><a href="#cb7-284"></a>  <span class="ex">barrier_with_L1_time</span> ............................ True</span>
<span id="cb7-285"><a href="#cb7-285"></a>  <span class="ex">bert_binary_head</span> ................................ True</span>
<span id="cb7-286"><a href="#cb7-286"></a>  <span class="ex">bert_embedder_type</span> .............................. megatron</span>
<span id="cb7-287"><a href="#cb7-287"></a>  <span class="ex">bert_load</span> ....................................... None</span>
<span id="cb7-288"><a href="#cb7-288"></a>  <span class="ex">bf16</span> ............................................ True</span>
<span id="cb7-289"><a href="#cb7-289"></a>  <span class="ex">bias_dropout_fusion</span> ............................. False</span>
<span id="cb7-290"><a href="#cb7-290"></a>  <span class="ex">bias_gelu_fusion</span> ................................ False</span>
<span id="cb7-291"><a href="#cb7-291"></a>  <span class="ex">biencoder_projection_dim</span> ........................ 0</span>
<span id="cb7-292"><a href="#cb7-292"></a>  <span class="ex">biencoder_shared_query_context_model</span> ............ False</span>
<span id="cb7-293"><a href="#cb7-293"></a>  <span class="ex">block_data_path</span> ................................. None</span>
<span id="cb7-294"><a href="#cb7-294"></a>  <span class="ex">checkpoint_activations</span> .......................... False</span>
<span id="cb7-295"><a href="#cb7-295"></a>  <span class="ex">checkpoint_in_cpu</span> ............................... False</span>
<span id="cb7-296"><a href="#cb7-296"></a>  <span class="ex">checkpoint_num_layers</span> ........................... 1</span>
<span id="cb7-297"><a href="#cb7-297"></a>  <span class="ex">classes_fraction</span> ................................ 1.0</span>
<span id="cb7-298"><a href="#cb7-298"></a>  <span class="ex">clip_grad</span> ....................................... 1.0</span>
<span id="cb7-299"><a href="#cb7-299"></a>  <span class="ex">compression_training</span> ............................ False</span>
<span id="cb7-300"><a href="#cb7-300"></a>  <span class="ex">consumed_train_samples</span> .......................... 0</span>
<span id="cb7-301"><a href="#cb7-301"></a>  <span class="ex">consumed_train_tokens</span> ........................... 0</span>
<span id="cb7-302"><a href="#cb7-302"></a>  <span class="ex">consumed_valid_samples</span> .......................... 0</span>
<span id="cb7-303"><a href="#cb7-303"></a>  <span class="ex">contigious_checkpointing</span> ........................ False</span>
<span id="cb7-304"><a href="#cb7-304"></a>  <span class="ex">cpu_optimizer</span> ................................... False</span>
<span id="cb7-305"><a href="#cb7-305"></a>  <span class="ex">cpu_torch_adam</span> .................................. False</span>
<span id="cb7-306"><a href="#cb7-306"></a>  <span class="ex">create_moe_param_group</span> .......................... False</span>
<span id="cb7-307"><a href="#cb7-307"></a>  <span class="ex">curriculum_learning_legacy</span> ...................... False</span>
<span id="cb7-308"><a href="#cb7-308"></a>  <span class="ex">data_cache_path</span> ................................. ./.cache</span>
<span id="cb7-309"><a href="#cb7-309"></a>  <span class="ex">data_efficiency_curriculum_learning</span> ............. False</span>
<span id="cb7-310"><a href="#cb7-310"></a>  <span class="ex">data_file_list</span> .................................. None</span>
<span id="cb7-311"><a href="#cb7-311"></a>  <span class="ex">data_impl</span> ....................................... infer</span>
<span id="cb7-312"><a href="#cb7-312"></a>  <span class="ex">data_parallel_random_init</span> ....................... False</span>
<span id="cb7-313"><a href="#cb7-313"></a>  <span class="ex">data_parallel_size</span> .............................. 24</span>
<span id="cb7-314"><a href="#cb7-314"></a>  <span class="ex">data_path</span> ....................................... <span class="pp">[</span><span class="st">'/lus/flare/projects/candle_aesp_CNDA/azton/data/v2/megatron/dataset_v2_wtextbooks_text_document'</span><span class="pp">]</span></span>
<span id="cb7-315"><a href="#cb7-315"></a>  <span class="ex">data_per_class_fraction</span> ......................... 1.0</span>
<span id="cb7-316"><a href="#cb7-316"></a>  <span class="ex">data_sharding</span> ................................... True</span>
<span id="cb7-317"><a href="#cb7-317"></a>  <span class="ex">dataloader_type</span> ................................. single</span>
<span id="cb7-318"><a href="#cb7-318"></a>  <span class="ex">DDP_impl</span> ........................................ local</span>
<span id="cb7-319"><a href="#cb7-319"></a>  <span class="ex">decoder_num_layers</span> .............................. None</span>
<span id="cb7-320"><a href="#cb7-320"></a>  <span class="ex">decoder_seq_length</span> .............................. None</span>
<span id="cb7-321"><a href="#cb7-321"></a>  <span class="ex">deepscale</span> ....................................... False</span>
<span id="cb7-322"><a href="#cb7-322"></a>  <span class="ex">deepscale_config</span> ................................ None</span>
<span id="cb7-323"><a href="#cb7-323"></a>  <span class="ex">deepspeed</span> ....................................... True</span>
<span id="cb7-324"><a href="#cb7-324"></a>  <span class="ex">deepspeed_activation_checkpointing</span> .............. False</span>
<span id="cb7-325"><a href="#cb7-325"></a>  <span class="ex">deepspeed_config</span> ................................ ./examples_deepspeed/finetune_hf_llama/ds_config.json</span>
<span id="cb7-326"><a href="#cb7-326"></a>  <span class="ex">dino_bottleneck_size</span> ............................ 256</span>
<span id="cb7-327"><a href="#cb7-327"></a>  <span class="ex">dino_freeze_last_layer</span> .......................... 1</span>
<span id="cb7-328"><a href="#cb7-328"></a>  <span class="ex">dino_head_hidden_size</span> ........................... 2048</span>
<span id="cb7-329"><a href="#cb7-329"></a>  <span class="ex">dino_local_crops_number</span> ......................... 10</span>
<span id="cb7-330"><a href="#cb7-330"></a>  <span class="ex">dino_local_img_size</span> ............................. 96</span>
<span id="cb7-331"><a href="#cb7-331"></a>  <span class="ex">dino_norm_last_layer</span> ............................ False</span>
<span id="cb7-332"><a href="#cb7-332"></a>  <span class="ex">dino_teacher_temp</span> ............................... 0.07</span>
<span id="cb7-333"><a href="#cb7-333"></a>  <span class="ex">dino_warmup_teacher_temp</span> ........................ 0.04</span>
<span id="cb7-334"><a href="#cb7-334"></a>  <span class="ex">dino_warmup_teacher_temp_epochs</span> ................. 30</span>
<span id="cb7-335"><a href="#cb7-335"></a>  <span class="ex">distribute_checkpointed_activations</span> ............. False</span>
<span id="cb7-336"><a href="#cb7-336"></a>  <span class="ex">distribute_saved_activations</span> .................... False</span>
<span id="cb7-337"><a href="#cb7-337"></a>  <span class="ex">distributed_backend</span> ............................. ccl</span>
<span id="cb7-338"><a href="#cb7-338"></a>  <span class="ex">distributed_timeout_minutes</span> ..................... 10</span>
<span id="cb7-339"><a href="#cb7-339"></a>  <span class="ex">ds_fused_adam</span> ................................... False</span>
<span id="cb7-340"><a href="#cb7-340"></a>  <span class="ex">ds_inference</span> .................................... False</span>
<span id="cb7-341"><a href="#cb7-341"></a>  <span class="ex">ds_pipeline_enabled</span> ............................. True</span>
<span id="cb7-342"><a href="#cb7-342"></a>  <span class="ex">ds_sequence_parallel_size</span> ....................... 1</span>
<span id="cb7-343"><a href="#cb7-343"></a>  <span class="ex">embedding_path</span> .................................. None</span>
<span id="cb7-344"><a href="#cb7-344"></a>  <span class="ex">embedding_weights_in_fp32</span> ....................... False</span>
<span id="cb7-345"><a href="#cb7-345"></a>  <span class="ex">empty_unused_memory_level</span> ....................... 0</span>
<span id="cb7-346"><a href="#cb7-346"></a>  <span class="ex">enable_expert_tensor_parallelism</span> ................ False</span>
<span id="cb7-347"><a href="#cb7-347"></a>  <span class="ex">enable_zbh1_exact_semantics</span> ..................... False</span>
<span id="cb7-348"><a href="#cb7-348"></a>  <span class="ex">enable_zbh1_pipeline</span> ............................ False</span>
<span id="cb7-349"><a href="#cb7-349"></a>  <span class="ex">encoder_num_layers</span> .............................. 32</span>
<span id="cb7-350"><a href="#cb7-350"></a>  <span class="ex">encoder_seq_length</span> .............................. 2048</span>
<span id="cb7-351"><a href="#cb7-351"></a>  <span class="ex">end_weight_decay</span> ................................ 0.1</span>
<span id="cb7-352"><a href="#cb7-352"></a>  <span class="ex">eod_mask_loss</span> ................................... False</span>
<span id="cb7-353"><a href="#cb7-353"></a>  <span class="ex">eval_interval</span> ................................... 100</span>
<span id="cb7-354"><a href="#cb7-354"></a>  <span class="ex">eval_iters</span> ...................................... 100</span>
<span id="cb7-355"><a href="#cb7-355"></a>  <span class="ex">evidence_data_path</span> .............................. None</span>
<span id="cb7-356"><a href="#cb7-356"></a>  <span class="ex">exit_duration_in_mins</span> ........................... None</span>
<span id="cb7-357"><a href="#cb7-357"></a>  <span class="ex">exit_interval</span> ................................... None</span>
<span id="cb7-358"><a href="#cb7-358"></a>  <span class="ex">exit_on_missing_checkpoint</span> ...................... False</span>
<span id="cb7-359"><a href="#cb7-359"></a>  <span class="ex">exit_signal_handler</span> ............................. False</span>
<span id="cb7-360"><a href="#cb7-360"></a>  <span class="ex">expert_interval</span> ................................. 2</span>
<span id="cb7-361"><a href="#cb7-361"></a>  <span class="ex">ffn_hidden_size</span> ................................. 14336</span>
<span id="cb7-362"><a href="#cb7-362"></a>  <span class="ex">finetune</span> ........................................ False</span>
<span id="cb7-363"><a href="#cb7-363"></a>  <span class="ex">force_ds_sequence_parallel</span> ...................... False</span>
<span id="cb7-364"><a href="#cb7-364"></a>  <span class="ex">fp16</span> ............................................ False</span>
<span id="cb7-365"><a href="#cb7-365"></a>  <span class="ex">fp16_lm_cross_entropy</span> ........................... False</span>
<span id="cb7-366"><a href="#cb7-366"></a>  <span class="ex">fp32_residual_connection</span> ........................ False</span>
<span id="cb7-367"><a href="#cb7-367"></a>  <span class="ex">fp8_amax_compute_algo</span> ........................... most_recent</span>
<span id="cb7-368"><a href="#cb7-368"></a>  <span class="ex">fp8_amax_history_len</span> ............................ 1</span>
<span id="cb7-369"><a href="#cb7-369"></a>  <span class="ex">fp8_e4m3</span> ........................................ False</span>
<span id="cb7-370"><a href="#cb7-370"></a>  <span class="ex">fp8_hybrid</span> ...................................... False</span>
<span id="cb7-371"><a href="#cb7-371"></a>  <span class="ex">fp8_interval</span> .................................... 1</span>
<span id="cb7-372"><a href="#cb7-372"></a>  <span class="ex">fp8_margin</span> ...................................... 0</span>
<span id="cb7-373"><a href="#cb7-373"></a>  <span class="ex">fp8_wgrad</span> ....................................... True</span>
<span id="cb7-374"><a href="#cb7-374"></a>  <span class="ex">global_batch_size</span> ............................... 24</span>
<span id="cb7-375"><a href="#cb7-375"></a>  <span class="ex">gradient_accumulation_fusion</span> .................... False</span>
<span id="cb7-376"><a href="#cb7-376"></a>  <span class="ex">head_lr_mult</span> .................................... 1.0</span>
<span id="cb7-377"><a href="#cb7-377"></a>  <span class="ex">hf_ckpt_dir</span> ..................................... /flare/Aurora_deployment/foremans/Meta-Llama-3.1-8B-128512</span>
<span id="cb7-378"><a href="#cb7-378"></a>  <span class="ex">hf_ckpt_num_shards</span> .............................. 1</span>
<span id="cb7-379"><a href="#cb7-379"></a>  <span class="ex">hidden_dropout</span> .................................. 0.0</span>
<span id="cb7-380"><a href="#cb7-380"></a>  <span class="ex">hidden_size</span> ..................................... 4096</span>
<span id="cb7-381"><a href="#cb7-381"></a>  <span class="ex">hidden_size_teacher</span> ............................. None</span>
<span id="cb7-382"><a href="#cb7-382"></a>  <span class="ex">hysteresis</span> ...................................... 2</span>
<span id="cb7-383"><a href="#cb7-383"></a>  <span class="ex">ict_head_size</span> ................................... None</span>
<span id="cb7-384"><a href="#cb7-384"></a>  <span class="ex">ict_load</span> ........................................ None</span>
<span id="cb7-385"><a href="#cb7-385"></a>  <span class="ex">img_h</span> ........................................... 224</span>
<span id="cb7-386"><a href="#cb7-386"></a>  <span class="ex">img_w</span> ........................................... 224</span>
<span id="cb7-387"><a href="#cb7-387"></a>  <span class="ex">indexer_batch_size</span> .............................. 128</span>
<span id="cb7-388"><a href="#cb7-388"></a>  <span class="ex">indexer_log_interval</span> ............................ 1000</span>
<span id="cb7-389"><a href="#cb7-389"></a>  <span class="ex">inference</span> ....................................... False</span>
<span id="cb7-390"><a href="#cb7-390"></a>  <span class="ex">inference_batch_times_seqlen_threshold</span> .......... 512</span>
<span id="cb7-391"><a href="#cb7-391"></a>  <span class="ex">init_method_std</span> ................................. 0.02</span>
<span id="cb7-392"><a href="#cb7-392"></a>  <span class="ex">init_method_xavier_uniform</span> ...................... False</span>
<span id="cb7-393"><a href="#cb7-393"></a>  <span class="ex">initial_loss_scale</span> .............................. 4294967296</span>
<span id="cb7-394"><a href="#cb7-394"></a>  <span class="ex">iter_per_epoch</span> .................................. 1250</span>
<span id="cb7-395"><a href="#cb7-395"></a>  <span class="ex">kd</span> .............................................. False</span>
<span id="cb7-396"><a href="#cb7-396"></a>  <span class="ex">kd_alpha_ce</span> ..................................... 1</span>
<span id="cb7-397"><a href="#cb7-397"></a>  <span class="ex">kd_beta_ce</span> ...................................... 1</span>
<span id="cb7-398"><a href="#cb7-398"></a>  <span class="ex">kd_temp</span> ......................................... 1.0</span>
<span id="cb7-399"><a href="#cb7-399"></a>  <span class="ex">kill_switch_file</span> ................................ None</span>
<span id="cb7-400"><a href="#cb7-400"></a>  <span class="ex">kv_channels</span> ..................................... 128</span>
<span id="cb7-401"><a href="#cb7-401"></a>  <span class="ex">layernorm_epsilon</span> ............................... 1e-05</span>
<span id="cb7-402"><a href="#cb7-402"></a>  <span class="ex">lazy_mpu_init</span> ................................... None</span>
<span id="cb7-403"><a href="#cb7-403"></a>  <span class="ex">load</span> ............................................ None</span>
<span id="cb7-404"><a href="#cb7-404"></a>  <span class="ex">load_mode</span> ....................................... auto</span>
<span id="cb7-405"><a href="#cb7-405"></a>  <span class="ex">load_tag</span> ........................................ None</span>
<span id="cb7-406"><a href="#cb7-406"></a>  <span class="ex">load_teacher</span> .................................... None</span>
<span id="cb7-407"><a href="#cb7-407"></a>  <span class="ex">local_rank</span> ...................................... None</span>
<span id="cb7-408"><a href="#cb7-408"></a>  <span class="ex">log_batch_size_to_tensorboard</span> ................... False</span>
<span id="cb7-409"><a href="#cb7-409"></a>  <span class="ex">log_interval</span> .................................... 1</span>
<span id="cb7-410"><a href="#cb7-410"></a>  <span class="ex">log_learning_rate_to_tensorboard</span> ................ True</span>
<span id="cb7-411"><a href="#cb7-411"></a>  <span class="ex">log_loss_scale_to_tensorboard</span> ................... True</span>
<span id="cb7-412"><a href="#cb7-412"></a>  <span class="ex">log_memory_to_tensorboard</span> ....................... False</span>
<span id="cb7-413"><a href="#cb7-413"></a>  <span class="ex">log_num_zeros_in_grad</span> ........................... False</span>
<span id="cb7-414"><a href="#cb7-414"></a>  <span class="ex">log_optimizer_states_to_tensorboard</span> ............. False</span>
<span id="cb7-415"><a href="#cb7-415"></a>  <span class="ex">log_params_norm</span> ................................. False</span>
<span id="cb7-416"><a href="#cb7-416"></a>  <span class="ex">log_timers_to_tensorboard</span> ....................... False</span>
<span id="cb7-417"><a href="#cb7-417"></a>  <span class="ex">log_validation_ppl_to_tensorboard</span> ............... False</span>
<span id="cb7-418"><a href="#cb7-418"></a>  <span class="ex">log_world_size_to_tensorboard</span> ................... False</span>
<span id="cb7-419"><a href="#cb7-419"></a>  <span class="ex">loss_scale</span> ...................................... None</span>
<span id="cb7-420"><a href="#cb7-420"></a>  <span class="ex">loss_scale_window</span> ............................... 1000</span>
<span id="cb7-421"><a href="#cb7-421"></a>  <span class="ex">lr</span> .............................................. 2e-05</span>
<span id="cb7-422"><a href="#cb7-422"></a>  <span class="ex">lr_decay_iters</span> .................................. 320000</span>
<span id="cb7-423"><a href="#cb7-423"></a>  <span class="ex">lr_decay_samples</span> ................................ None</span>
<span id="cb7-424"><a href="#cb7-424"></a>  <span class="ex">lr_decay_style</span> .................................. cosine</span>
<span id="cb7-425"><a href="#cb7-425"></a>  <span class="ex">lr_decay_tokens</span> ................................. None</span>
<span id="cb7-426"><a href="#cb7-426"></a>  <span class="ex">lr_warmup_fraction</span> .............................. None</span>
<span id="cb7-427"><a href="#cb7-427"></a>  <span class="ex">lr_warmup_iters</span> ................................. 2000</span>
<span id="cb7-428"><a href="#cb7-428"></a>  <span class="ex">lr_warmup_samples</span> ............................... 0</span>
<span id="cb7-429"><a href="#cb7-429"></a>  <span class="ex">lr_warmup_tokens</span> ................................ None</span>
<span id="cb7-430"><a href="#cb7-430"></a>  <span class="ex">make_vocab_size_divisible_by</span> .................... 128512</span>
<span id="cb7-431"><a href="#cb7-431"></a>  <span class="ex">mask_factor</span> ..................................... 1.0</span>
<span id="cb7-432"><a href="#cb7-432"></a>  <span class="ex">mask_prob</span> ....................................... 0.15</span>
<span id="cb7-433"><a href="#cb7-433"></a>  <span class="ex">mask_type</span> ....................................... random</span>
<span id="cb7-434"><a href="#cb7-434"></a>  <span class="ex">masked_softmax_fusion</span> ........................... False</span>
<span id="cb7-435"><a href="#cb7-435"></a>  <span class="ex">max_position_embeddings</span> ......................... 2048</span>
<span id="cb7-436"><a href="#cb7-436"></a>  <span class="ex">max_tokens_to_oom</span> ............................... 12000</span>
<span id="cb7-437"><a href="#cb7-437"></a>  <span class="ex">mem_efficient_ln</span> ................................ True</span>
<span id="cb7-438"><a href="#cb7-438"></a>  <span class="ex">memory_centric_tiled_linear</span> ..................... False</span>
<span id="cb7-439"><a href="#cb7-439"></a>  <span class="ex">merge_file</span> ...................................... None</span>
<span id="cb7-440"><a href="#cb7-440"></a>  <span class="ex">micro_batch_size</span> ................................ 1</span>
<span id="cb7-441"><a href="#cb7-441"></a>  <span class="ex">min_loss_scale</span> .................................. 1.0</span>
<span id="cb7-442"><a href="#cb7-442"></a>  <span class="ex">min_lr</span> .......................................... 0.0</span>
<span id="cb7-443"><a href="#cb7-443"></a>  <span class="ex">mlp_type</span> ........................................ standard</span>
<span id="cb7-444"><a href="#cb7-444"></a>  <span class="ex">mmap_warmup</span> ..................................... False</span>
<span id="cb7-445"><a href="#cb7-445"></a>  <span class="ex">moe_eval_capacity_factor</span> ........................ 1.0</span>
<span id="cb7-446"><a href="#cb7-446"></a>  <span class="ex">moe_expert_parallel_size</span> ........................ 1</span>
<span id="cb7-447"><a href="#cb7-447"></a>  <span class="ex">moe_loss_coeff</span> .................................. 0.1</span>
<span id="cb7-448"><a href="#cb7-448"></a>  <span class="ex">moe_min_capacity</span> ................................ 4</span>
<span id="cb7-449"><a href="#cb7-449"></a>  <span class="ex">moe_token_dropping</span> .............................. True</span>
<span id="cb7-450"><a href="#cb7-450"></a>  <span class="ex">moe_top2_2nd_expert_sampling</span> .................... True</span>
<span id="cb7-451"><a href="#cb7-451"></a>  <span class="ex">moe_train_capacity_factor</span> ....................... 1.0</span>
<span id="cb7-452"><a href="#cb7-452"></a>  <span class="ex">mos</span> ............................................. False</span>
<span id="cb7-453"><a href="#cb7-453"></a>  <span class="ex">multiprocessing_context</span> ......................... fork</span>
<span id="cb7-454"><a href="#cb7-454"></a>  <span class="ex">no_load_lr_state</span> ................................ False</span>
<span id="cb7-455"><a href="#cb7-455"></a>  <span class="ex">no_load_optim</span> ................................... None</span>
<span id="cb7-456"><a href="#cb7-456"></a>  <span class="ex">no_load_rng</span> ..................................... None</span>
<span id="cb7-457"><a href="#cb7-457"></a>  <span class="ex">no_persist_layer_norm</span> ........................... False</span>
<span id="cb7-458"><a href="#cb7-458"></a>  <span class="ex">no_pipeline_parallel</span> ............................ False</span>
<span id="cb7-459"><a href="#cb7-459"></a>  <span class="ex">no_save_optim</span> ................................... None</span>
<span id="cb7-460"><a href="#cb7-460"></a>  <span class="ex">no_save_rng</span> ..................................... None</span>
<span id="cb7-461"><a href="#cb7-461"></a>  <span class="ex">normalization</span> ................................... rmsnorm</span>
<span id="cb7-462"><a href="#cb7-462"></a>  <span class="ex">num_attention_heads</span> ............................. 32</span>
<span id="cb7-463"><a href="#cb7-463"></a>  <span class="ex">num_attention_heads_teacher</span> ..................... None</span>
<span id="cb7-464"><a href="#cb7-464"></a>  <span class="ex">num_channels</span> .................................... 3</span>
<span id="cb7-465"><a href="#cb7-465"></a>  <span class="ex">num_classes</span> ..................................... 1000</span>
<span id="cb7-466"><a href="#cb7-466"></a>  <span class="ex">num_experts</span> ..................................... <span class="pp">[</span><span class="ss">1</span><span class="pp">]</span></span>
<span id="cb7-467"><a href="#cb7-467"></a>  <span class="ex">num_experts_switch</span> .............................. None</span>
<span id="cb7-468"><a href="#cb7-468"></a>  <span class="ex">num_experts_teacher</span> ............................. <span class="pp">[</span><span class="ss">1</span><span class="pp">]</span></span>
<span id="cb7-469"><a href="#cb7-469"></a>  <span class="ex">num_key_value_heads</span> ............................. 8</span>
<span id="cb7-470"><a href="#cb7-470"></a>  <span class="ex">num_layers</span> ...................................... 32</span>
<span id="cb7-471"><a href="#cb7-471"></a>  <span class="ex">num_layers_per_virtual_pipeline_stage</span> ........... None</span>
<span id="cb7-472"><a href="#cb7-472"></a>  <span class="ex">num_layers_teacher</span> .............................. None</span>
<span id="cb7-473"><a href="#cb7-473"></a>  <span class="ex">num_workers</span> ..................................... 2</span>
<span id="cb7-474"><a href="#cb7-474"></a>  <span class="ex">onnx_safe</span> ....................................... None</span>
<span id="cb7-475"><a href="#cb7-475"></a>  <span class="ex">openai_gelu</span> ..................................... False</span>
<span id="cb7-476"><a href="#cb7-476"></a>  <span class="ex">optimizer</span> ....................................... adam</span>
<span id="cb7-477"><a href="#cb7-477"></a>  <span class="ex">output_bert_embeddings</span> .......................... False</span>
<span id="cb7-478"><a href="#cb7-478"></a>  <span class="ex">overlap_p2p_comm</span> ................................ False</span>
<span id="cb7-479"><a href="#cb7-479"></a>  <span class="ex">override_opt_param_scheduler</span> .................... False</span>
<span id="cb7-480"><a href="#cb7-480"></a>  <span class="ex">params_dtype</span> .................................... torch.bfloat16</span>
<span id="cb7-481"><a href="#cb7-481"></a>  <span class="ex">partition_activations</span> ........................... False</span>
<span id="cb7-482"><a href="#cb7-482"></a>  <span class="ex">patch_dim</span> ....................................... 16</span>
<span id="cb7-483"><a href="#cb7-483"></a>  <span class="ex">perform_initialization</span> .......................... True</span>
<span id="cb7-484"><a href="#cb7-484"></a>  <span class="ex">pipeline_model_parallel_size</span> .................... 1</span>
<span id="cb7-485"><a href="#cb7-485"></a>  <span class="ex">pipeline_model_parallel_split_rank</span> .............. None</span>
<span id="cb7-486"><a href="#cb7-486"></a>  <span class="ex">profile</span> ......................................... None</span>
<span id="cb7-487"><a href="#cb7-487"></a>  <span class="ex">profile_backward</span> ................................ False</span>
<span id="cb7-488"><a href="#cb7-488"></a>  <span class="ex">profile_ranks</span> ................................... None</span>
<span id="cb7-489"><a href="#cb7-489"></a>  <span class="ex">profile_steps</span> ................................... 2,3</span>
<span id="cb7-490"><a href="#cb7-490"></a>  <span class="ex">query_in_block_prob</span> ............................. 0.1</span>
<span id="cb7-491"><a href="#cb7-491"></a>  <span class="ex">rampup_batch_size</span> ............................... None</span>
<span id="cb7-492"><a href="#cb7-492"></a>  <span class="ex">random_ltd</span> ...................................... False</span>
<span id="cb7-493"><a href="#cb7-493"></a>  <span class="ex">rank</span> ............................................ 0</span>
<span id="cb7-494"><a href="#cb7-494"></a>  <span class="ex">recompute_granularity</span> ........................... None</span>
<span id="cb7-495"><a href="#cb7-495"></a>  <span class="ex">recompute_method</span> ................................ None</span>
<span id="cb7-496"><a href="#cb7-496"></a>  <span class="ex">recompute_num_layers</span> ............................ 1</span>
<span id="cb7-497"><a href="#cb7-497"></a>  <span class="ex">remote_device</span> ................................... none</span>
<span id="cb7-498"><a href="#cb7-498"></a>  <span class="ex">repeated_dataloader</span> ............................. True</span>
<span id="cb7-499"><a href="#cb7-499"></a>  <span class="ex">reset_attention_mask</span> ............................ False</span>
<span id="cb7-500"><a href="#cb7-500"></a>  <span class="ex">reset_iteration</span> ................................. False</span>
<span id="cb7-501"><a href="#cb7-501"></a>  <span class="ex">reset_position_ids</span> .............................. False</span>
<span id="cb7-502"><a href="#cb7-502"></a>  <span class="ex">retriever_report_topk_accuracies</span> ................ []</span>
<span id="cb7-503"><a href="#cb7-503"></a>  <span class="ex">retriever_score_scaling</span> ......................... False</span>
<span id="cb7-504"><a href="#cb7-504"></a>  <span class="ex">retriever_seq_length</span> ............................ 256</span>
<span id="cb7-505"><a href="#cb7-505"></a>  <span class="ex">retro_add_retriever</span> ............................. False</span>
<span id="cb7-506"><a href="#cb7-506"></a>  <span class="ex">retro_cyclic_train_iters</span> ........................ None</span>
<span id="cb7-507"><a href="#cb7-507"></a>  <span class="ex">retro_encoder_attention_dropout</span> ................. 0.1</span>
<span id="cb7-508"><a href="#cb7-508"></a>  <span class="ex">retro_encoder_hidden_dropout</span> .................... 0.1</span>
<span id="cb7-509"><a href="#cb7-509"></a>  <span class="ex">retro_encoder_layers</span> ............................ 2</span>
<span id="cb7-510"><a href="#cb7-510"></a>  <span class="ex">retro_num_neighbors</span> ............................. 2</span>
<span id="cb7-511"><a href="#cb7-511"></a>  <span class="ex">retro_num_retrieved_chunks</span> ...................... 2</span>
<span id="cb7-512"><a href="#cb7-512"></a>  <span class="ex">retro_return_doc_ids</span> ............................ False</span>
<span id="cb7-513"><a href="#cb7-513"></a>  <span class="ex">retro_workdir</span> ................................... None</span>
<span id="cb7-514"><a href="#cb7-514"></a>  <span class="ex">return_data_index</span> ............................... False</span>
<span id="cb7-515"><a href="#cb7-515"></a>  <span class="ex">rope_theta</span> ...................................... 10000</span>
<span id="cb7-516"><a href="#cb7-516"></a>  <span class="ex">rotary_percent</span> .................................. 1.0</span>
<span id="cb7-517"><a href="#cb7-517"></a>  <span class="ex">sample_rate</span> ..................................... 1.0</span>
<span id="cb7-518"><a href="#cb7-518"></a>  <span class="ex">save</span> ............................................ ckpt-mds-llama-3/</span>
<span id="cb7-519"><a href="#cb7-519"></a>  <span class="ex">save_interval</span> ................................... 1500</span>
<span id="cb7-520"><a href="#cb7-520"></a>  <span class="ex">scatter_gather_tensors_in_pipeline</span> .............. True</span>
<span id="cb7-521"><a href="#cb7-521"></a>  <span class="ex">scattered_embeddings</span> ............................ False</span>
<span id="cb7-522"><a href="#cb7-522"></a>  <span class="ex">schedulefree_for_each</span> ........................... False</span>
<span id="cb7-523"><a href="#cb7-523"></a>  <span class="ex">seed</span> ............................................ 1234</span>
<span id="cb7-524"><a href="#cb7-524"></a>  <span class="ex">seq_length</span> ...................................... 2048</span>
<span id="cb7-525"><a href="#cb7-525"></a>  <span class="ex">sequence_parallel</span> ............................... False</span>
<span id="cb7-526"><a href="#cb7-526"></a>  <span class="ex">sgd_momentum</span> .................................... 0.9</span>
<span id="cb7-527"><a href="#cb7-527"></a>  <span class="ex">short_seq_prob</span> .................................. 0.1</span>
<span id="cb7-528"><a href="#cb7-528"></a>  <span class="ex">shuffle_sample</span> .................................. False</span>
<span id="cb7-529"><a href="#cb7-529"></a>  <span class="ex">skip_train</span> ...................................... False</span>
<span id="cb7-530"><a href="#cb7-530"></a>  <span class="ex">sophiag_beta1</span> ................................... 0.9</span>
<span id="cb7-531"><a href="#cb7-531"></a>  <span class="ex">sophiag_beta2</span> ................................... 0.95</span>
<span id="cb7-532"><a href="#cb7-532"></a>  <span class="ex">sophiag_rho</span> ..................................... 0.01</span>
<span id="cb7-533"><a href="#cb7-533"></a>  <span class="fu">split</span> ........................................... 100,0,0</span>
<span id="cb7-534"><a href="#cb7-534"></a>  <span class="ex">split_transformers</span> .............................. False</span>
<span id="cb7-535"><a href="#cb7-535"></a>  <span class="ex">squared_relu</span> .................................... False</span>
<span id="cb7-536"><a href="#cb7-536"></a>  <span class="ex">standalone_embedding_stage</span> ...................... False</span>
<span id="cb7-537"><a href="#cb7-537"></a>  <span class="ex">start_weight_decay</span> .............................. 0.1</span>
<span id="cb7-538"><a href="#cb7-538"></a>  <span class="ex">swiglu</span> .......................................... True</span>
<span id="cb7-539"><a href="#cb7-539"></a>  <span class="ex">swin_backbone_type</span> .............................. tiny</span>
<span id="cb7-540"><a href="#cb7-540"></a>  <span class="ex">synchronize_each_layer</span> .......................... False</span>
<span id="cb7-541"><a href="#cb7-541"></a>  <span class="ex">tensor_model_parallel_size</span> ...................... 1</span>
<span id="cb7-542"><a href="#cb7-542"></a>  <span class="ex">tensorboard_dir</span> ................................. tensorboard_output</span>
<span id="cb7-543"><a href="#cb7-543"></a>  <span class="ex">tensorboard_log_interval</span> ........................ 1</span>
<span id="cb7-544"><a href="#cb7-544"></a>  <span class="ex">tensorboard_queue_size</span> .......................... 1000</span>
<span id="cb7-545"><a href="#cb7-545"></a>  <span class="ex">test_data_path</span> .................................. None</span>
<span id="cb7-546"><a href="#cb7-546"></a>  <span class="ex">tile_factor</span> ..................................... 1</span>
<span id="cb7-547"><a href="#cb7-547"></a>  <span class="ex">timing_log_level</span> ................................ 0</span>
<span id="cb7-548"><a href="#cb7-548"></a>  <span class="ex">timing_log_option</span> ............................... minmax</span>
<span id="cb7-549"><a href="#cb7-549"></a>  <span class="ex">titles_data_path</span> ................................ None</span>
<span id="cb7-550"><a href="#cb7-550"></a>  <span class="ex">to_hf_ckpt</span> ...................................... False</span>
<span id="cb7-551"><a href="#cb7-551"></a>  <span class="ex">tokenizer_model</span> ................................. ALCF/custom_tokenizer.model</span>
<span id="cb7-552"><a href="#cb7-552"></a>  <span class="ex">tokenizer_type</span> .................................. HFTokenizer</span>
<span id="cb7-553"><a href="#cb7-553"></a>  <span class="ex">topk</span> ............................................ 1</span>
<span id="cb7-554"><a href="#cb7-554"></a>  <span class="ex">trace_dir</span> ....................................... ./trace/</span>
<span id="cb7-555"><a href="#cb7-555"></a>  <span class="ex">train_data_exact_num_epochs</span> ..................... None</span>
<span id="cb7-556"><a href="#cb7-556"></a>  <span class="ex">train_data_path</span> ................................. None</span>
<span id="cb7-557"><a href="#cb7-557"></a>  <span class="ex">train_desc_path</span> ................................. None</span>
<span id="cb7-558"><a href="#cb7-558"></a>  <span class="ex">train_doc_idx_path</span> .............................. None</span>
<span id="cb7-559"><a href="#cb7-559"></a>  <span class="ex">train_idx_path</span> .................................. None</span>
<span id="cb7-560"><a href="#cb7-560"></a>  <span class="ex">train_iters</span> ..................................... 3500</span>
<span id="cb7-561"><a href="#cb7-561"></a>  <span class="ex">train_iters_to_skip</span> ............................. None</span>
<span id="cb7-562"><a href="#cb7-562"></a>  <span class="ex">train_range_to_skip</span> ............................. None</span>
<span id="cb7-563"><a href="#cb7-563"></a>  <span class="ex">train_sample_idx_path</span> ........................... None</span>
<span id="cb7-564"><a href="#cb7-564"></a>  <span class="ex">train_samples</span> ................................... None</span>
<span id="cb7-565"><a href="#cb7-565"></a>  <span class="ex">train_shuffle_idx_path</span> .......................... None</span>
<span id="cb7-566"><a href="#cb7-566"></a>  <span class="ex">train_tokens</span> .................................... None</span>
<span id="cb7-567"><a href="#cb7-567"></a>  <span class="ex">transformer_impl</span> ................................ local</span>
<span id="cb7-568"><a href="#cb7-568"></a>  <span class="ex">transformer_pipeline_model_parallel_size</span> ........ 1</span>
<span id="cb7-569"><a href="#cb7-569"></a>  <span class="ex">trust_remote_code</span> ............................... False</span>
<span id="cb7-570"><a href="#cb7-570"></a>  <span class="ex">universal_checkpoint</span> ............................ False</span>
<span id="cb7-571"><a href="#cb7-571"></a>  <span class="ex">untie_embeddings_and_output_weights</span> ............. True</span>
<span id="cb7-572"><a href="#cb7-572"></a>  <span class="ex">use_checkpoint_args</span> ............................. False</span>
<span id="cb7-573"><a href="#cb7-573"></a>  <span class="ex">use_checkpoint_opt_param_scheduler</span> .............. False</span>
<span id="cb7-574"><a href="#cb7-574"></a>  <span class="ex">use_contiguous_buffers_in_local_ddp</span> ............. True</span>
<span id="cb7-575"><a href="#cb7-575"></a>  <span class="ex">use_cpu_initialization</span> .......................... None</span>
<span id="cb7-576"><a href="#cb7-576"></a>  <span class="ex">use_dataset_only</span> ................................ False</span>
<span id="cb7-577"><a href="#cb7-577"></a>  <span class="ex">use_distributed_optimizer</span> ....................... False</span>
<span id="cb7-578"><a href="#cb7-578"></a>  <span class="ex">use_flash_attn</span> .................................. False</span>
<span id="cb7-579"><a href="#cb7-579"></a>  <span class="ex">use_flash_attn_builder</span> .......................... False</span>
<span id="cb7-580"><a href="#cb7-580"></a>  <span class="ex">use_flash_attn_triton</span> ........................... False</span>
<span id="cb7-581"><a href="#cb7-581"></a>  <span class="ex">use_flash_attn_v1</span> ............................... False</span>
<span id="cb7-582"><a href="#cb7-582"></a>  <span class="ex">use_flash_attn_v2</span> ............................... False</span>
<span id="cb7-583"><a href="#cb7-583"></a>  <span class="ex">use_mics</span> ........................................ False</span>
<span id="cb7-584"><a href="#cb7-584"></a>  <span class="ex">use_one_sent_docs</span> ............................... False</span>
<span id="cb7-585"><a href="#cb7-585"></a>  <span class="ex">use_pin_memory</span> .................................. False</span>
<span id="cb7-586"><a href="#cb7-586"></a>  <span class="ex">use_ring_exchange_p2p</span> ........................... False</span>
<span id="cb7-587"><a href="#cb7-587"></a>  <span class="ex">use_rotary_position_embeddings</span> .................. True</span>
<span id="cb7-588"><a href="#cb7-588"></a>  <span class="ex">use_tutel</span> ....................................... False</span>
<span id="cb7-589"><a href="#cb7-589"></a>  <span class="ex">valid_data_path</span> ................................. None</span>
<span id="cb7-590"><a href="#cb7-590"></a>  <span class="ex">variable_seq_lengths</span> ............................ False</span>
<span id="cb7-591"><a href="#cb7-591"></a>  <span class="ex">virtual_pipeline_model_parallel_size</span> ............ None</span>
<span id="cb7-592"><a href="#cb7-592"></a>  <span class="ex">vision_backbone_type</span> ............................ vit</span>
<span id="cb7-593"><a href="#cb7-593"></a>  <span class="ex">vision_pretraining</span> .............................. False</span>
<span id="cb7-594"><a href="#cb7-594"></a>  <span class="ex">vision_pretraining_type</span> ......................... classify</span>
<span id="cb7-595"><a href="#cb7-595"></a>  <span class="ex">vocab_extra_ids</span> ................................. 0</span>
<span id="cb7-596"><a href="#cb7-596"></a>  <span class="ex">vocab_file</span> ...................................... None</span>
<span id="cb7-597"><a href="#cb7-597"></a>  <span class="ex">vocab_size</span> ...................................... 128512</span>
<span id="cb7-598"><a href="#cb7-598"></a>  <span class="ex">wandb_exp_name</span> ..................................</span>
<span id="cb7-599"><a href="#cb7-599"></a>  <span class="ex">wandb_project</span> ...................................</span>
<span id="cb7-600"><a href="#cb7-600"></a>  <span class="ex">wandb_save_dir</span> ..................................</span>
<span id="cb7-601"><a href="#cb7-601"></a>  <span class="ex">weight_decay</span> .................................... 0.1</span>
<span id="cb7-602"><a href="#cb7-602"></a>  <span class="ex">weight_decay_incr_style</span> ......................... constant</span>
<span id="cb7-603"><a href="#cb7-603"></a>  <span class="ex">world_size</span> ...................................... 24</span>
<span id="cb7-604"><a href="#cb7-604"></a>  <span class="ex">zero_allgather_bucket_size</span> ...................... 0.0</span>
<span id="cb7-605"><a href="#cb7-605"></a>  <span class="ex">zero_contigious_gradients</span> ....................... False</span>
<span id="cb7-606"><a href="#cb7-606"></a>  <span class="ex">zero_reduce_bucket_size</span> ......................... 0.0</span>
<span id="cb7-607"><a href="#cb7-607"></a>  <span class="ex">zero_reduce_scatter</span> ............................. False</span>
<span id="cb7-608"><a href="#cb7-608"></a>  <span class="ex">zero_stage</span> ...................................... 1.0</span>
<span id="cb7-609"><a href="#cb7-609"></a><span class="ex">--------------------</span> end of arguments <span class="at">---------------------</span></span>
<span id="cb7-610"><a href="#cb7-610"></a><span class="ex">setting</span> number of micro-batches to constant 1</span>
<span id="cb7-611"><a href="#cb7-611"></a><span class="op">&gt;</span> building <span class="ex">HFTokenizer</span> tokenizer ...</span>
<span id="cb7-612"><a href="#cb7-612"></a> <span class="op">&gt;</span> padded <span class="ex">vocab</span> <span class="er">(</span><span class="ex">size:</span> 128000<span class="kw">)</span> <span class="ex">with</span> 512 dummy tokens <span class="er">(</span><span class="ex">new</span> size: 128512<span class="kw">)</span></span>
<span id="cb7-613"><a href="#cb7-613"></a><span class="ex">torch</span> distributed is already initialized, skipping initialization ...</span>
<span id="cb7-614"><a href="#cb7-614"></a><span class="op">&gt;</span> initialized <span class="ex">tensor</span> model parallel with size 1</span>
<span id="cb7-615"><a href="#cb7-615"></a><span class="op">&gt;</span> initialized <span class="ex">pipeline</span> model parallel with size 1</span>
<span id="cb7-616"><a href="#cb7-616"></a><span class="op">&gt;</span> setting <span class="ex">random</span> seeds to 1234 ...</span>
<span id="cb7-617"><a href="#cb7-617"></a><span class="op">&gt;</span> initializing <span class="ex">model</span> parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234</span>
<span id="cb7-618"><a href="#cb7-618"></a><span class="op">&gt;</span>fused <span class="ex">kernel</span> is only supported in cuda, skip loading fused kernel</span>
<span id="cb7-619"><a href="#cb7-619"></a><span class="ex">[2024-10-16</span> 15:39:36,876] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb7-620"><a href="#cb7-620"></a><span class="op">&gt;</span>fused <span class="ex">kernel</span> is only supported in cuda, skip loading fused kernel</span>
<span id="cb7-621"><a href="#cb7-621"></a><span class="ex">[2024-10-16</span> 15:39:36,876] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb7-622"><a href="#cb7-622"></a><span class="op">&gt;</span>fused <span class="ex">kernel</span> is only supported in cuda, skip loading fused kernel</span>
<span id="cb7-623"><a href="#cb7-623"></a><span class="ex">[2024-10-16</span> 15:39:36,876] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb7-624"><a href="#cb7-624"></a><span class="op">&gt;</span>fused <span class="ex">kernel</span> is only supported in cuda, skip loading fused kernel</span>
<span id="cb7-625"><a href="#cb7-625"></a><span class="ex">[2024-10-16</span> 15:39:36,876] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb7-626"><a href="#cb7-626"></a><span class="op">&gt;</span>fused <span class="ex">kernel</span> is only supported in cuda, skip loading fused kernel</span>
<span id="cb7-627"><a href="#cb7-627"></a><span class="ex">[2024-10-16</span> 15:39:36,879] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb7-628"><a href="#cb7-628"></a><span class="op">&gt;</span>fused <span class="ex">kernel</span> is only supported in cuda, skip loading fused kernel</span>
<span id="cb7-629"><a href="#cb7-629"></a><span class="ex">[2024-10-16</span> 15:39:36,881] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb7-630"><a href="#cb7-630"></a><span class="op">&gt;</span>fused <span class="ex">kernel</span> is only supported in cuda, skip loading fused kernel</span>
<span id="cb7-631"><a href="#cb7-631"></a><span class="ex">[2024-10-16</span> 15:39:36,881] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb7-632"><a href="#cb7-632"></a><span class="op">&gt;</span>fused <span class="ex">kernel</span> is only supported in cuda, skip loading fused kernel</span>
<span id="cb7-633"><a href="#cb7-633"></a><span class="ex">[2024-10-16</span> 15:39:36,882] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb7-634"><a href="#cb7-634"></a><span class="op">&gt;</span>fused <span class="ex">kernel</span> is only supported in cuda, skip loading fused kernel</span>
<span id="cb7-635"><a href="#cb7-635"></a><span class="ex">[2024-10-16</span> 15:39:36,882] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb7-636"><a href="#cb7-636"></a><span class="ex">make:</span> Entering directory <span class="st">'/lus/flare/projects/Aurora_deployment/foremans/projects/argonne-lcf/Megatron-DeepSpeed/megatron/data'</span></span>
<span id="cb7-637"><a href="#cb7-637"></a><span class="op">&gt;</span>fused <span class="ex">kernel</span> is only supported in cuda, skip loading fused kernel</span>
<span id="cb7-638"><a href="#cb7-638"></a><span class="ex">[2024-10-16</span> 15:39:36,884] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb7-639"><a href="#cb7-639"></a><span class="op">&gt;</span>fused <span class="ex">kernel</span> is only supported in cuda, skip loading fused kernel</span>
<span id="cb7-640"><a href="#cb7-640"></a><span class="ex">[2024-10-16</span> 15:39:36,922] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb7-641"><a href="#cb7-641"></a><span class="op">&gt;</span>fused <span class="ex">kernel</span> is only supported in cuda, skip loading fused kernel</span>
<span id="cb7-642"><a href="#cb7-642"></a><span class="ex">[2024-10-16</span> 15:39:36,929] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb7-643"><a href="#cb7-643"></a><span class="op">&gt;</span>fused <span class="ex">kernel</span> is only supported in cuda, skip loading fused kernel</span>
<span id="cb7-644"><a href="#cb7-644"></a><span class="ex">[2024-10-16</span> 15:39:36,930] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb7-645"><a href="#cb7-645"></a><span class="op">&gt;</span>fused <span class="ex">kernel</span> is only supported in cuda, skip loading fused kernel</span>
<span id="cb7-646"><a href="#cb7-646"></a><span class="ex">[2024-10-16</span> 15:39:36,931] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb7-647"><a href="#cb7-647"></a><span class="op">&gt;</span>fused <span class="ex">kernel</span> is only supported in cuda, skip loading fused kernel</span>
<span id="cb7-648"><a href="#cb7-648"></a><span class="ex">[2024-10-16</span> 15:39:36,935] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb7-649"><a href="#cb7-649"></a><span class="op">&gt;</span>fused <span class="ex">kernel</span> is only supported in cuda, skip loading fused kernel</span>
<span id="cb7-650"><a href="#cb7-650"></a><span class="ex">[2024-10-16</span> 15:39:36,937] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb7-651"><a href="#cb7-651"></a><span class="op">&gt;</span>fused <span class="ex">kernel</span> is only supported in cuda, skip loading fused kernel</span>
<span id="cb7-652"><a href="#cb7-652"></a><span class="ex">[2024-10-16</span> 15:39:36,939] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb7-653"><a href="#cb7-653"></a><span class="op">&gt;</span>fused <span class="ex">kernel</span> is only supported in cuda, skip loading fused kernel</span>
<span id="cb7-654"><a href="#cb7-654"></a><span class="ex">[2024-10-16</span> 15:39:36,939] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb7-655"><a href="#cb7-655"></a><span class="op">&gt;</span>fused <span class="ex">kernel</span> is only supported in cuda, skip loading fused kernel</span>
<span id="cb7-656"><a href="#cb7-656"></a><span class="ex">[2024-10-16</span> 15:39:36,940] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb7-657"><a href="#cb7-657"></a><span class="op">&gt;</span>fused <span class="ex">kernel</span> is only supported in cuda, skip loading fused kernel</span>
<span id="cb7-658"><a href="#cb7-658"></a><span class="ex">[2024-10-16</span> 15:39:36,941] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb7-659"><a href="#cb7-659"></a><span class="op">&gt;</span>fused <span class="ex">kernel</span> is only supported in cuda, skip loading fused kernel</span>
<span id="cb7-660"><a href="#cb7-660"></a><span class="ex">[2024-10-16</span> 15:39:36,946] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb7-661"><a href="#cb7-661"></a><span class="op">&gt;</span>fused <span class="ex">kernel</span> is only supported in cuda, skip loading fused kernel</span>
<span id="cb7-662"><a href="#cb7-662"></a><span class="ex">[2024-10-16</span> 15:39:36,949] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb7-663"><a href="#cb7-663"></a><span class="ex">make:</span> Nothing to be done for <span class="st">'default'</span>.</span>
<span id="cb7-664"><a href="#cb7-664"></a><span class="ex">make:</span> Leaving directory <span class="st">'/lus/flare/projects/Aurora_deployment/foremans/projects/argonne-lcf/Megatron-DeepSpeed/megatron/data'</span></span>
<span id="cb7-665"><a href="#cb7-665"></a><span class="op">&gt;</span> compiling <span class="ex">dataset</span> index builder ...</span>
<span id="cb7-666"><a href="#cb7-666"></a><span class="op">&gt;&gt;&gt;</span> done <span class="ex">with</span> dataset index builder. Compilation time: 0.080 seconds</span>
<span id="cb7-667"><a href="#cb7-667"></a><span class="op">&gt;</span>fused <span class="ex">kernel</span> is only supported in cuda, skip loading fused kernel</span>
<span id="cb7-668"><a href="#cb7-668"></a><span class="ex">[2024-10-16</span> 15:39:36.956560]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:479</span><span class="pp">]</span> <span class="at">-</span> building model ...</span>
<span id="cb7-669"><a href="#cb7-669"></a><span class="ex">[2024-10-16</span> 15:39:37,133] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">utils.py:781:see_memory_usage</span><span class="pp">]</span> Before Building Model</span>
<span id="cb7-670"><a href="#cb7-670"></a><span class="ex">[2024-10-16</span> 15:39:37,133] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">utils.py:782:see_memory_usage</span><span class="pp">]</span> MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB</span>
<span id="cb7-671"><a href="#cb7-671"></a><span class="ex">[2024-10-16</span> 15:39:37,133] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">utils.py:789:see_memory_usage</span><span class="pp">]</span> CPU Virtual Memory:  used = 105.24 GB, percent = 9.3%</span>
<span id="cb7-672"><a href="#cb7-672"></a><span class="ex">[2024-10-16</span> 15:39:37,133] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb7-673"><a href="#cb7-673"></a><span class="va">SEED_LAYERS</span><span class="op">=</span>False <span class="va">BASE_SEED</span><span class="op">=</span>1234 <span class="va">SEED_FN</span><span class="op">=</span>None</span>
<span id="cb7-674"><a href="#cb7-674"></a><span class="ex">Using</span> topology: {ProcessCoord<span class="er">(</span><span class="va">pipe</span><span class="op">=</span>0, <span class="va">data</span><span class="op">=</span>0, <span class="va">model</span><span class="op">=</span>0<span class="kw">)</span><span class="bu">:</span> 0, ProcessCoord<span class="er">(</span><span class="va">pipe</span><span class="op">=</span>0, <span class="va">data</span><span class="op">=</span>1, <span class="va">model</span><span class="op">=</span>0<span class="kw">)</span><span class="bu">:</span> 1, ProcessCoord<span class="er">(</span><span class="va">pipe</span><span class="op">=</span>0, <span class="va">data</span><span class="op">=</span>2, <span class="va">model</span><span class="op">=</span>0<span class="kw">)</span><span class="bu">:</span> 2, ProcessCoord<span class="er">(</span><span class="va">pipe</span><span class="op">=</span>0, <span class="va">data</span><span class="op">=</span>3, <span class="va">model</span><span class="op">=</span>0<span class="kw">)</span><span class="bu">:</span> 3, ProcessCoord<span class="er">(</span><span class="va">pipe</span><span class="op">=</span>0, <span class="va">data</span><span class="op">=</span>4, <span class="va">model</span><span class="op">=</span>0<span class="kw">)</span><span class="bu">:</span> 4, ProcessCoord<span class="er">(</span><span class="va">pipe</span><span class="op">=</span>0, <span class="va">data</span><span class="op">=</span>5, <span class="va">model</span><span class="op">=</span>0<span class="kw">)</span><span class="bu">:</span> 5, ProcessCoord<span class="er">(</span><span class="va">pipe</span><span class="op">=</span>0, <span class="va">data</span><span class="op">=</span>6, <span class="va">model</span><span class="op">=</span>0<span class="kw">)</span><span class="bu">:</span> 6, ProcessCoord<span class="er">(</span><span class="va">pipe</span><span class="op">=</span>0, <span class="va">data</span><span class="op">=</span>7, <span class="va">model</span><span class="op">=</span>0<span class="kw">)</span><span class="bu">:</span> 7, ProcessCoord<span class="er">(</span><span class="va">pipe</span><span class="op">=</span>0, <span class="va">data</span><span class="op">=</span>8, <span class="va">model</span><span class="op">=</span>0<span class="kw">)</span><span class="bu">:</span> 8, ProcessCoord<span class="er">(</span><span class="va">pipe</span><span class="op">=</span>0, <span class="va">data</span><span class="op">=</span>9, <span class="va">model</span><span class="op">=</span>0<span class="kw">)</span><span class="bu">:</span> 9, ProcessCoord<span class="er">(</span><span class="va">pipe</span><span class="op">=</span>0, <span class="va">data</span><span class="op">=</span>10, <span class="va">model</span><span class="op">=</span>0<span class="kw">)</span><span class="bu">:</span> 10, ProcessCoord<span class="er">(</span><span class="va">pipe</span><span class="op">=</span>0, <span class="va">data</span><span class="op">=</span>11, <span class="va">model</span><span class="op">=</span>0<span class="kw">)</span><span class="bu">:</span> 11, ProcessCoord<span class="er">(</span><span class="va">pipe</span><span class="op">=</span>0, <span class="va">data</span><span class="op">=</span>12, <span class="va">model</span><span class="op">=</span>0<span class="kw">)</span><span class="bu">:</span> 12, ProcessCoord<span class="er">(</span><span class="va">pipe</span><span class="op">=</span>0, <span class="va">data</span><span class="op">=</span>13, <span class="va">model</span><span class="op">=</span>0<span class="kw">)</span><span class="bu">:</span> 13, ProcessCoord<span class="er">(</span><span class="va">pipe</span><span class="op">=</span>0, <span class="va">data</span><span class="op">=</span>14, <span class="va">model</span><span class="op">=</span>0<span class="kw">)</span><span class="bu">:</span> 14, ProcessCoord<span class="er">(</span><span class="va">pipe</span><span class="op">=</span>0, <span class="va">data</span><span class="op">=</span>15, <span class="va">model</span><span class="op">=</span>0<span class="kw">)</span><span class="bu">:</span> 15, ProcessCoord<span class="er">(</span><span class="va">pipe</span><span class="op">=</span>0, <span class="va">data</span><span class="op">=</span>16, <span class="va">model</span><span class="op">=</span>0<span class="kw">)</span><span class="bu">:</span> 16, ProcessCoord<span class="er">(</span><span class="va">pipe</span><span class="op">=</span>0, <span class="va">data</span><span class="op">=</span>17, <span class="va">model</span><span class="op">=</span>0<span class="kw">)</span><span class="bu">:</span> 17, ProcessCoord<span class="er">(</span><span class="va">pipe</span><span class="op">=</span>0, <span class="va">data</span><span class="op">=</span>18, <span class="va">model</span><span class="op">=</span>0<span class="kw">)</span><span class="bu">:</span> 18, ProcessCoord<span class="er">(</span><span class="va">pipe</span><span class="op">=</span>0, <span class="va">data</span><span class="op">=</span>19, <span class="va">model</span><span class="op">=</span>0<span class="kw">)</span><span class="bu">:</span> 19, ProcessCoord<span class="er">(</span><span class="va">pipe</span><span class="op">=</span>0, <span class="va">data</span><span class="op">=</span>20, <span class="va">model</span><span class="op">=</span>0<span class="kw">)</span><span class="bu">:</span> 20, ProcessCoord<span class="er">(</span><span class="va">pipe</span><span class="op">=</span>0, <span class="va">data</span><span class="op">=</span>21, <span class="va">model</span><span class="op">=</span>0<span class="kw">)</span><span class="bu">:</span> 21, ProcessCoord<span class="er">(</span><span class="va">pipe</span><span class="op">=</span>0, <span class="va">data</span><span class="op">=</span>22, <span class="va">model</span><span class="op">=</span>0<span class="kw">)</span><span class="bu">:</span> 22, ProcessCoord<span class="er">(</span><span class="va">pipe</span><span class="op">=</span>0, <span class="va">data</span><span class="op">=</span>23, <span class="va">model</span><span class="op">=</span>0<span class="kw">)</span><span class="bu">:</span> 23}</span>
<span id="cb7-675"><a href="#cb7-675"></a><span class="ex">[2024-10-16</span> 15:39:37,139] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">module.py:396:_partition_layers</span><span class="pp">]</span> Partitioning pipeline stages with method type:transformer</span>
<span id="cb7-676"><a href="#cb7-676"></a><span class="ex">2024-10-16</span> 15:39:37.212904: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered</span>
<span id="cb7-677"><a href="#cb7-677"></a><span class="ex">2024-10-16</span> 15:39:37.212925: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered</span>
<span id="cb7-678"><a href="#cb7-678"></a><span class="ex">2024-10-16</span> 15:39:37.213970: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered</span>
<span id="cb7-679"><a href="#cb7-679"></a><span class="ex">2024-10-16</span> 15:39:37.704522: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT</span>
<span id="cb7-680"><a href="#cb7-680"></a><span class="ex">Loading</span> checkpoint shards:   0%<span class="kw">|</span>          <span class="kw">|</span> <span class="ex">0/7</span> [00:00<span class="op">&lt;</span><span class="pp">?</span>, <span class="pp">?</span>it/s]1 GPTModelPipe<span class="er">(</span></span>
<span id="cb7-681"><a href="#cb7-681"></a>  <span class="kw">(</span><span class="ex">tied_modules</span><span class="kw">)</span><span class="bu">:</span> ModuleDict<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-682"><a href="#cb7-682"></a>  <span class="kw">(</span><span class="ex">1</span><span class="kw">)</span><span class="bu">:</span> EmbeddingPipe<span class="er">(</span></span>
<span id="cb7-683"><a href="#cb7-683"></a>    <span class="kw">(</span><span class="ex">word_embeddings</span><span class="kw">)</span><span class="bu">:</span> VocabParallelEmbedding<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-684"><a href="#cb7-684"></a>    <span class="kw">(</span><span class="ex">embedding_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-685"><a href="#cb7-685"></a>  <span class="kw">)</span></span>
<span id="cb7-686"><a href="#cb7-686"></a>  <span class="kw">(</span><span class="ex">2</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-687"><a href="#cb7-687"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-688"><a href="#cb7-688"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-689"><a href="#cb7-689"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-690"><a href="#cb7-690"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-691"><a href="#cb7-691"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-692"><a href="#cb7-692"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-693"><a href="#cb7-693"></a>      <span class="kw">)</span></span>
<span id="cb7-694"><a href="#cb7-694"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-695"><a href="#cb7-695"></a>    <span class="kw">)</span></span>
<span id="cb7-696"><a href="#cb7-696"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-697"><a href="#cb7-697"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-698"><a href="#cb7-698"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-699"><a href="#cb7-699"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-700"><a href="#cb7-700"></a>    <span class="kw">)</span></span>
<span id="cb7-701"><a href="#cb7-701"></a>  <span class="kw">)</span></span>
<span id="cb7-702"><a href="#cb7-702"></a>  <span class="kw">(</span><span class="ex">3</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-703"><a href="#cb7-703"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-704"><a href="#cb7-704"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-705"><a href="#cb7-705"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-706"><a href="#cb7-706"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-707"><a href="#cb7-707"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-708"><a href="#cb7-708"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-709"><a href="#cb7-709"></a>      <span class="kw">)</span></span>
<span id="cb7-710"><a href="#cb7-710"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-711"><a href="#cb7-711"></a>    <span class="kw">)</span></span>
<span id="cb7-712"><a href="#cb7-712"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-713"><a href="#cb7-713"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-714"><a href="#cb7-714"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-715"><a href="#cb7-715"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-716"><a href="#cb7-716"></a>    <span class="kw">)</span></span>
<span id="cb7-717"><a href="#cb7-717"></a>  <span class="kw">)</span></span>
<span id="cb7-718"><a href="#cb7-718"></a>  <span class="kw">(</span><span class="ex">4</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-719"><a href="#cb7-719"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-720"><a href="#cb7-720"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-721"><a href="#cb7-721"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-722"><a href="#cb7-722"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-723"><a href="#cb7-723"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-724"><a href="#cb7-724"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-725"><a href="#cb7-725"></a>      <span class="kw">)</span></span>
<span id="cb7-726"><a href="#cb7-726"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-727"><a href="#cb7-727"></a>    <span class="kw">)</span></span>
<span id="cb7-728"><a href="#cb7-728"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-729"><a href="#cb7-729"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-730"><a href="#cb7-730"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-731"><a href="#cb7-731"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-732"><a href="#cb7-732"></a>    <span class="kw">)</span></span>
<span id="cb7-733"><a href="#cb7-733"></a>  <span class="kw">)</span></span>
<span id="cb7-734"><a href="#cb7-734"></a>  <span class="kw">(</span><span class="ex">5</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-735"><a href="#cb7-735"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-736"><a href="#cb7-736"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-737"><a href="#cb7-737"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-738"><a href="#cb7-738"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-739"><a href="#cb7-739"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-740"><a href="#cb7-740"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-741"><a href="#cb7-741"></a>      <span class="kw">)</span></span>
<span id="cb7-742"><a href="#cb7-742"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-743"><a href="#cb7-743"></a>    <span class="kw">)</span></span>
<span id="cb7-744"><a href="#cb7-744"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-745"><a href="#cb7-745"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-746"><a href="#cb7-746"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-747"><a href="#cb7-747"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-748"><a href="#cb7-748"></a>    <span class="kw">)</span></span>
<span id="cb7-749"><a href="#cb7-749"></a>  <span class="kw">)</span></span>
<span id="cb7-750"><a href="#cb7-750"></a>  <span class="kw">(</span><span class="ex">6</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-751"><a href="#cb7-751"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-752"><a href="#cb7-752"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-753"><a href="#cb7-753"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-754"><a href="#cb7-754"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-755"><a href="#cb7-755"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-756"><a href="#cb7-756"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-757"><a href="#cb7-757"></a>      <span class="kw">)</span></span>
<span id="cb7-758"><a href="#cb7-758"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-759"><a href="#cb7-759"></a>    <span class="kw">)</span></span>
<span id="cb7-760"><a href="#cb7-760"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-761"><a href="#cb7-761"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-762"><a href="#cb7-762"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-763"><a href="#cb7-763"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-764"><a href="#cb7-764"></a>    <span class="kw">)</span></span>
<span id="cb7-765"><a href="#cb7-765"></a>  <span class="kw">)</span></span>
<span id="cb7-766"><a href="#cb7-766"></a>  <span class="kw">(</span><span class="ex">7</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-767"><a href="#cb7-767"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-768"><a href="#cb7-768"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-769"><a href="#cb7-769"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-770"><a href="#cb7-770"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-771"><a href="#cb7-771"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-772"><a href="#cb7-772"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-773"><a href="#cb7-773"></a>      <span class="kw">)</span></span>
<span id="cb7-774"><a href="#cb7-774"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-775"><a href="#cb7-775"></a>    <span class="kw">)</span></span>
<span id="cb7-776"><a href="#cb7-776"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-777"><a href="#cb7-777"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-778"><a href="#cb7-778"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-779"><a href="#cb7-779"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-780"><a href="#cb7-780"></a>    <span class="kw">)</span></span>
<span id="cb7-781"><a href="#cb7-781"></a>  <span class="kw">)</span></span>
<span id="cb7-782"><a href="#cb7-782"></a>  <span class="kw">(</span><span class="ex">8</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-783"><a href="#cb7-783"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-784"><a href="#cb7-784"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-785"><a href="#cb7-785"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-786"><a href="#cb7-786"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-787"><a href="#cb7-787"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-788"><a href="#cb7-788"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-789"><a href="#cb7-789"></a>      <span class="kw">)</span></span>
<span id="cb7-790"><a href="#cb7-790"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-791"><a href="#cb7-791"></a>    <span class="kw">)</span></span>
<span id="cb7-792"><a href="#cb7-792"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-793"><a href="#cb7-793"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-794"><a href="#cb7-794"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-795"><a href="#cb7-795"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-796"><a href="#cb7-796"></a>    <span class="kw">)</span></span>
<span id="cb7-797"><a href="#cb7-797"></a>  <span class="kw">)</span></span>
<span id="cb7-798"><a href="#cb7-798"></a>  <span class="kw">(</span><span class="ex">9</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-799"><a href="#cb7-799"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-800"><a href="#cb7-800"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-801"><a href="#cb7-801"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-802"><a href="#cb7-802"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-803"><a href="#cb7-803"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-804"><a href="#cb7-804"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-805"><a href="#cb7-805"></a>      <span class="kw">)</span></span>
<span id="cb7-806"><a href="#cb7-806"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-807"><a href="#cb7-807"></a>    <span class="kw">)</span></span>
<span id="cb7-808"><a href="#cb7-808"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-809"><a href="#cb7-809"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-810"><a href="#cb7-810"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-811"><a href="#cb7-811"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-812"><a href="#cb7-812"></a>    <span class="kw">)</span></span>
<span id="cb7-813"><a href="#cb7-813"></a>  <span class="kw">)</span></span>
<span id="cb7-814"><a href="#cb7-814"></a>  <span class="kw">(</span><span class="ex">10</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-815"><a href="#cb7-815"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-816"><a href="#cb7-816"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-817"><a href="#cb7-817"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-818"><a href="#cb7-818"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-819"><a href="#cb7-819"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-820"><a href="#cb7-820"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-821"><a href="#cb7-821"></a>      <span class="kw">)</span></span>
<span id="cb7-822"><a href="#cb7-822"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-823"><a href="#cb7-823"></a>    <span class="kw">)</span></span>
<span id="cb7-824"><a href="#cb7-824"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-825"><a href="#cb7-825"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-826"><a href="#cb7-826"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-827"><a href="#cb7-827"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-828"><a href="#cb7-828"></a>    <span class="kw">)</span></span>
<span id="cb7-829"><a href="#cb7-829"></a>  <span class="kw">)</span></span>
<span id="cb7-830"><a href="#cb7-830"></a>  <span class="kw">(</span><span class="ex">11</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-831"><a href="#cb7-831"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-832"><a href="#cb7-832"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-833"><a href="#cb7-833"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-834"><a href="#cb7-834"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-835"><a href="#cb7-835"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-836"><a href="#cb7-836"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-837"><a href="#cb7-837"></a>      <span class="kw">)</span></span>
<span id="cb7-838"><a href="#cb7-838"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-839"><a href="#cb7-839"></a>    <span class="kw">)</span></span>
<span id="cb7-840"><a href="#cb7-840"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-841"><a href="#cb7-841"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-842"><a href="#cb7-842"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-843"><a href="#cb7-843"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-844"><a href="#cb7-844"></a>    <span class="kw">)</span></span>
<span id="cb7-845"><a href="#cb7-845"></a>  <span class="kw">)</span></span>
<span id="cb7-846"><a href="#cb7-846"></a>  <span class="kw">(</span><span class="ex">12</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-847"><a href="#cb7-847"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-848"><a href="#cb7-848"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-849"><a href="#cb7-849"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-850"><a href="#cb7-850"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-851"><a href="#cb7-851"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-852"><a href="#cb7-852"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-853"><a href="#cb7-853"></a>      <span class="kw">)</span></span>
<span id="cb7-854"><a href="#cb7-854"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-855"><a href="#cb7-855"></a>    <span class="kw">)</span></span>
<span id="cb7-856"><a href="#cb7-856"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-857"><a href="#cb7-857"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-858"><a href="#cb7-858"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-859"><a href="#cb7-859"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-860"><a href="#cb7-860"></a>    <span class="kw">)</span></span>
<span id="cb7-861"><a href="#cb7-861"></a>  <span class="kw">)</span></span>
<span id="cb7-862"><a href="#cb7-862"></a>  <span class="kw">(</span><span class="ex">13</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-863"><a href="#cb7-863"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-864"><a href="#cb7-864"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-865"><a href="#cb7-865"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-866"><a href="#cb7-866"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-867"><a href="#cb7-867"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-868"><a href="#cb7-868"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-869"><a href="#cb7-869"></a>      <span class="kw">)</span></span>
<span id="cb7-870"><a href="#cb7-870"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-871"><a href="#cb7-871"></a>    <span class="kw">)</span></span>
<span id="cb7-872"><a href="#cb7-872"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-873"><a href="#cb7-873"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-874"><a href="#cb7-874"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-875"><a href="#cb7-875"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-876"><a href="#cb7-876"></a>    <span class="kw">)</span></span>
<span id="cb7-877"><a href="#cb7-877"></a>  <span class="kw">)</span></span>
<span id="cb7-878"><a href="#cb7-878"></a>  <span class="kw">(</span><span class="ex">14</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-879"><a href="#cb7-879"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-880"><a href="#cb7-880"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-881"><a href="#cb7-881"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-882"><a href="#cb7-882"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-883"><a href="#cb7-883"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-884"><a href="#cb7-884"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-885"><a href="#cb7-885"></a>      <span class="kw">)</span></span>
<span id="cb7-886"><a href="#cb7-886"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-887"><a href="#cb7-887"></a>    <span class="kw">)</span></span>
<span id="cb7-888"><a href="#cb7-888"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-889"><a href="#cb7-889"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-890"><a href="#cb7-890"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-891"><a href="#cb7-891"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-892"><a href="#cb7-892"></a>    <span class="kw">)</span></span>
<span id="cb7-893"><a href="#cb7-893"></a>  <span class="kw">)</span></span>
<span id="cb7-894"><a href="#cb7-894"></a>  <span class="kw">(</span><span class="ex">15</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-895"><a href="#cb7-895"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-896"><a href="#cb7-896"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-897"><a href="#cb7-897"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-898"><a href="#cb7-898"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-899"><a href="#cb7-899"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-900"><a href="#cb7-900"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-901"><a href="#cb7-901"></a>      <span class="kw">)</span></span>
<span id="cb7-902"><a href="#cb7-902"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-903"><a href="#cb7-903"></a>    <span class="kw">)</span></span>
<span id="cb7-904"><a href="#cb7-904"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-905"><a href="#cb7-905"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-906"><a href="#cb7-906"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-907"><a href="#cb7-907"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-908"><a href="#cb7-908"></a>    <span class="kw">)</span></span>
<span id="cb7-909"><a href="#cb7-909"></a>  <span class="kw">)</span></span>
<span id="cb7-910"><a href="#cb7-910"></a>  <span class="kw">(</span><span class="ex">16</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-911"><a href="#cb7-911"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-912"><a href="#cb7-912"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-913"><a href="#cb7-913"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-914"><a href="#cb7-914"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-915"><a href="#cb7-915"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-916"><a href="#cb7-916"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-917"><a href="#cb7-917"></a>      <span class="kw">)</span></span>
<span id="cb7-918"><a href="#cb7-918"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-919"><a href="#cb7-919"></a>    <span class="kw">)</span></span>
<span id="cb7-920"><a href="#cb7-920"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-921"><a href="#cb7-921"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-922"><a href="#cb7-922"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-923"><a href="#cb7-923"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-924"><a href="#cb7-924"></a>    <span class="kw">)</span></span>
<span id="cb7-925"><a href="#cb7-925"></a>  <span class="kw">)</span></span>
<span id="cb7-926"><a href="#cb7-926"></a>  <span class="kw">(</span><span class="ex">17</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-927"><a href="#cb7-927"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-928"><a href="#cb7-928"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-929"><a href="#cb7-929"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-930"><a href="#cb7-930"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-931"><a href="#cb7-931"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-932"><a href="#cb7-932"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-933"><a href="#cb7-933"></a>      <span class="kw">)</span></span>
<span id="cb7-934"><a href="#cb7-934"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-935"><a href="#cb7-935"></a>    <span class="kw">)</span></span>
<span id="cb7-936"><a href="#cb7-936"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-937"><a href="#cb7-937"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-938"><a href="#cb7-938"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-939"><a href="#cb7-939"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-940"><a href="#cb7-940"></a>    <span class="kw">)</span></span>
<span id="cb7-941"><a href="#cb7-941"></a>  <span class="kw">)</span></span>
<span id="cb7-942"><a href="#cb7-942"></a>  <span class="kw">(</span><span class="ex">18</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-943"><a href="#cb7-943"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-944"><a href="#cb7-944"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-945"><a href="#cb7-945"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-946"><a href="#cb7-946"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-947"><a href="#cb7-947"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-948"><a href="#cb7-948"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-949"><a href="#cb7-949"></a>      <span class="kw">)</span></span>
<span id="cb7-950"><a href="#cb7-950"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-951"><a href="#cb7-951"></a>    <span class="kw">)</span></span>
<span id="cb7-952"><a href="#cb7-952"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-953"><a href="#cb7-953"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-954"><a href="#cb7-954"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-955"><a href="#cb7-955"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-956"><a href="#cb7-956"></a>    <span class="kw">)</span></span>
<span id="cb7-957"><a href="#cb7-957"></a>  <span class="kw">)</span></span>
<span id="cb7-958"><a href="#cb7-958"></a>  <span class="kw">(</span><span class="ex">19</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-959"><a href="#cb7-959"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-960"><a href="#cb7-960"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-961"><a href="#cb7-961"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-962"><a href="#cb7-962"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-963"><a href="#cb7-963"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-964"><a href="#cb7-964"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-965"><a href="#cb7-965"></a>      <span class="kw">)</span></span>
<span id="cb7-966"><a href="#cb7-966"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-967"><a href="#cb7-967"></a>    <span class="kw">)</span></span>
<span id="cb7-968"><a href="#cb7-968"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-969"><a href="#cb7-969"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-970"><a href="#cb7-970"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-971"><a href="#cb7-971"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-972"><a href="#cb7-972"></a>    <span class="kw">)</span></span>
<span id="cb7-973"><a href="#cb7-973"></a>  <span class="kw">)</span></span>
<span id="cb7-974"><a href="#cb7-974"></a>  <span class="kw">(</span><span class="ex">20</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-975"><a href="#cb7-975"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-976"><a href="#cb7-976"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-977"><a href="#cb7-977"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-978"><a href="#cb7-978"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-979"><a href="#cb7-979"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-980"><a href="#cb7-980"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-981"><a href="#cb7-981"></a>      <span class="kw">)</span></span>
<span id="cb7-982"><a href="#cb7-982"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-983"><a href="#cb7-983"></a>    <span class="kw">)</span></span>
<span id="cb7-984"><a href="#cb7-984"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-985"><a href="#cb7-985"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-986"><a href="#cb7-986"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-987"><a href="#cb7-987"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-988"><a href="#cb7-988"></a>    <span class="kw">)</span></span>
<span id="cb7-989"><a href="#cb7-989"></a>  <span class="kw">)</span></span>
<span id="cb7-990"><a href="#cb7-990"></a>  <span class="kw">(</span><span class="ex">21</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-991"><a href="#cb7-991"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-992"><a href="#cb7-992"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-993"><a href="#cb7-993"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-994"><a href="#cb7-994"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-995"><a href="#cb7-995"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-996"><a href="#cb7-996"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-997"><a href="#cb7-997"></a>      <span class="kw">)</span></span>
<span id="cb7-998"><a href="#cb7-998"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-999"><a href="#cb7-999"></a>    <span class="kw">)</span></span>
<span id="cb7-1000"><a href="#cb7-1000"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1001"><a href="#cb7-1001"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-1002"><a href="#cb7-1002"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1003"><a href="#cb7-1003"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1004"><a href="#cb7-1004"></a>    <span class="kw">)</span></span>
<span id="cb7-1005"><a href="#cb7-1005"></a>  <span class="kw">)</span></span>
<span id="cb7-1006"><a href="#cb7-1006"></a>  <span class="kw">(</span><span class="ex">22</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-1007"><a href="#cb7-1007"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1008"><a href="#cb7-1008"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-1009"><a href="#cb7-1009"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1010"><a href="#cb7-1010"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-1011"><a href="#cb7-1011"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1012"><a href="#cb7-1012"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-1013"><a href="#cb7-1013"></a>      <span class="kw">)</span></span>
<span id="cb7-1014"><a href="#cb7-1014"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1015"><a href="#cb7-1015"></a>    <span class="kw">)</span></span>
<span id="cb7-1016"><a href="#cb7-1016"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1017"><a href="#cb7-1017"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-1018"><a href="#cb7-1018"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1019"><a href="#cb7-1019"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1020"><a href="#cb7-1020"></a>    <span class="kw">)</span></span>
<span id="cb7-1021"><a href="#cb7-1021"></a>  <span class="kw">)</span></span>
<span id="cb7-1022"><a href="#cb7-1022"></a>  <span class="kw">(</span><span class="ex">23</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-1023"><a href="#cb7-1023"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1024"><a href="#cb7-1024"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-1025"><a href="#cb7-1025"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1026"><a href="#cb7-1026"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-1027"><a href="#cb7-1027"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1028"><a href="#cb7-1028"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-1029"><a href="#cb7-1029"></a>      <span class="kw">)</span></span>
<span id="cb7-1030"><a href="#cb7-1030"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1031"><a href="#cb7-1031"></a>    <span class="kw">)</span></span>
<span id="cb7-1032"><a href="#cb7-1032"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1033"><a href="#cb7-1033"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-1034"><a href="#cb7-1034"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1035"><a href="#cb7-1035"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1036"><a href="#cb7-1036"></a>    <span class="kw">)</span></span>
<span id="cb7-1037"><a href="#cb7-1037"></a>  <span class="kw">)</span></span>
<span id="cb7-1038"><a href="#cb7-1038"></a>  <span class="kw">(</span><span class="ex">24</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-1039"><a href="#cb7-1039"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1040"><a href="#cb7-1040"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-1041"><a href="#cb7-1041"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1042"><a href="#cb7-1042"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-1043"><a href="#cb7-1043"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1044"><a href="#cb7-1044"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-1045"><a href="#cb7-1045"></a>      <span class="kw">)</span></span>
<span id="cb7-1046"><a href="#cb7-1046"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1047"><a href="#cb7-1047"></a>    <span class="kw">)</span></span>
<span id="cb7-1048"><a href="#cb7-1048"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1049"><a href="#cb7-1049"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-1050"><a href="#cb7-1050"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1051"><a href="#cb7-1051"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1052"><a href="#cb7-1052"></a>    <span class="kw">)</span></span>
<span id="cb7-1053"><a href="#cb7-1053"></a>  <span class="kw">)</span></span>
<span id="cb7-1054"><a href="#cb7-1054"></a>  <span class="kw">(</span><span class="ex">25</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-1055"><a href="#cb7-1055"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1056"><a href="#cb7-1056"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-1057"><a href="#cb7-1057"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1058"><a href="#cb7-1058"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-1059"><a href="#cb7-1059"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1060"><a href="#cb7-1060"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-1061"><a href="#cb7-1061"></a>      <span class="kw">)</span></span>
<span id="cb7-1062"><a href="#cb7-1062"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1063"><a href="#cb7-1063"></a>    <span class="kw">)</span></span>
<span id="cb7-1064"><a href="#cb7-1064"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1065"><a href="#cb7-1065"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-1066"><a href="#cb7-1066"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1067"><a href="#cb7-1067"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1068"><a href="#cb7-1068"></a>    <span class="kw">)</span></span>
<span id="cb7-1069"><a href="#cb7-1069"></a>  <span class="kw">)</span></span>
<span id="cb7-1070"><a href="#cb7-1070"></a>  <span class="kw">(</span><span class="ex">26</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-1071"><a href="#cb7-1071"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1072"><a href="#cb7-1072"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-1073"><a href="#cb7-1073"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1074"><a href="#cb7-1074"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-1075"><a href="#cb7-1075"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1076"><a href="#cb7-1076"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-1077"><a href="#cb7-1077"></a>      <span class="kw">)</span></span>
<span id="cb7-1078"><a href="#cb7-1078"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1079"><a href="#cb7-1079"></a>    <span class="kw">)</span></span>
<span id="cb7-1080"><a href="#cb7-1080"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1081"><a href="#cb7-1081"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-1082"><a href="#cb7-1082"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1083"><a href="#cb7-1083"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1084"><a href="#cb7-1084"></a>    <span class="kw">)</span></span>
<span id="cb7-1085"><a href="#cb7-1085"></a>  <span class="kw">)</span></span>
<span id="cb7-1086"><a href="#cb7-1086"></a>  <span class="kw">(</span><span class="ex">27</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-1087"><a href="#cb7-1087"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1088"><a href="#cb7-1088"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-1089"><a href="#cb7-1089"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1090"><a href="#cb7-1090"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-1091"><a href="#cb7-1091"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1092"><a href="#cb7-1092"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-1093"><a href="#cb7-1093"></a>      <span class="kw">)</span></span>
<span id="cb7-1094"><a href="#cb7-1094"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1095"><a href="#cb7-1095"></a>    <span class="kw">)</span></span>
<span id="cb7-1096"><a href="#cb7-1096"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1097"><a href="#cb7-1097"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-1098"><a href="#cb7-1098"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1099"><a href="#cb7-1099"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1100"><a href="#cb7-1100"></a>    <span class="kw">)</span></span>
<span id="cb7-1101"><a href="#cb7-1101"></a>  <span class="kw">)</span></span>
<span id="cb7-1102"><a href="#cb7-1102"></a>  <span class="kw">(</span><span class="ex">28</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-1103"><a href="#cb7-1103"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1104"><a href="#cb7-1104"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-1105"><a href="#cb7-1105"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1106"><a href="#cb7-1106"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-1107"><a href="#cb7-1107"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1108"><a href="#cb7-1108"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-1109"><a href="#cb7-1109"></a>      <span class="kw">)</span></span>
<span id="cb7-1110"><a href="#cb7-1110"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1111"><a href="#cb7-1111"></a>    <span class="kw">)</span></span>
<span id="cb7-1112"><a href="#cb7-1112"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1113"><a href="#cb7-1113"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-1114"><a href="#cb7-1114"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1115"><a href="#cb7-1115"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1116"><a href="#cb7-1116"></a>    <span class="kw">)</span></span>
<span id="cb7-1117"><a href="#cb7-1117"></a>  <span class="kw">)</span></span>
<span id="cb7-1118"><a href="#cb7-1118"></a>  <span class="kw">(</span><span class="ex">29</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-1119"><a href="#cb7-1119"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1120"><a href="#cb7-1120"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-1121"><a href="#cb7-1121"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1122"><a href="#cb7-1122"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-1123"><a href="#cb7-1123"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1124"><a href="#cb7-1124"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-1125"><a href="#cb7-1125"></a>      <span class="kw">)</span></span>
<span id="cb7-1126"><a href="#cb7-1126"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1127"><a href="#cb7-1127"></a>    <span class="kw">)</span></span>
<span id="cb7-1128"><a href="#cb7-1128"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1129"><a href="#cb7-1129"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-1130"><a href="#cb7-1130"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1131"><a href="#cb7-1131"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1132"><a href="#cb7-1132"></a>    <span class="kw">)</span></span>
<span id="cb7-1133"><a href="#cb7-1133"></a>  <span class="kw">)</span></span>
<span id="cb7-1134"><a href="#cb7-1134"></a>  <span class="kw">(</span><span class="ex">30</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-1135"><a href="#cb7-1135"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1136"><a href="#cb7-1136"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-1137"><a href="#cb7-1137"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1138"><a href="#cb7-1138"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-1139"><a href="#cb7-1139"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1140"><a href="#cb7-1140"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-1141"><a href="#cb7-1141"></a>      <span class="kw">)</span></span>
<span id="cb7-1142"><a href="#cb7-1142"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1143"><a href="#cb7-1143"></a>    <span class="kw">)</span></span>
<span id="cb7-1144"><a href="#cb7-1144"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1145"><a href="#cb7-1145"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-1146"><a href="#cb7-1146"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1147"><a href="#cb7-1147"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1148"><a href="#cb7-1148"></a>    <span class="kw">)</span></span>
<span id="cb7-1149"><a href="#cb7-1149"></a>  <span class="kw">)</span></span>
<span id="cb7-1150"><a href="#cb7-1150"></a>  <span class="kw">(</span><span class="ex">31</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-1151"><a href="#cb7-1151"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1152"><a href="#cb7-1152"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-1153"><a href="#cb7-1153"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1154"><a href="#cb7-1154"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-1155"><a href="#cb7-1155"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1156"><a href="#cb7-1156"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-1157"><a href="#cb7-1157"></a>      <span class="kw">)</span></span>
<span id="cb7-1158"><a href="#cb7-1158"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1159"><a href="#cb7-1159"></a>    <span class="kw">)</span></span>
<span id="cb7-1160"><a href="#cb7-1160"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1161"><a href="#cb7-1161"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-1162"><a href="#cb7-1162"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1163"><a href="#cb7-1163"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1164"><a href="#cb7-1164"></a>    <span class="kw">)</span></span>
<span id="cb7-1165"><a href="#cb7-1165"></a>  <span class="kw">)</span></span>
<span id="cb7-1166"><a href="#cb7-1166"></a>  <span class="kw">(</span><span class="ex">32</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-1167"><a href="#cb7-1167"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1168"><a href="#cb7-1168"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-1169"><a href="#cb7-1169"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1170"><a href="#cb7-1170"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-1171"><a href="#cb7-1171"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1172"><a href="#cb7-1172"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-1173"><a href="#cb7-1173"></a>      <span class="kw">)</span></span>
<span id="cb7-1174"><a href="#cb7-1174"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1175"><a href="#cb7-1175"></a>    <span class="kw">)</span></span>
<span id="cb7-1176"><a href="#cb7-1176"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1177"><a href="#cb7-1177"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-1178"><a href="#cb7-1178"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1179"><a href="#cb7-1179"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1180"><a href="#cb7-1180"></a>    <span class="kw">)</span></span>
<span id="cb7-1181"><a href="#cb7-1181"></a>  <span class="kw">)</span></span>
<span id="cb7-1182"><a href="#cb7-1182"></a>  <span class="kw">(</span><span class="ex">33</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-1183"><a href="#cb7-1183"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1184"><a href="#cb7-1184"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-1185"><a href="#cb7-1185"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1186"><a href="#cb7-1186"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-1187"><a href="#cb7-1187"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1188"><a href="#cb7-1188"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-1189"><a href="#cb7-1189"></a>      <span class="kw">)</span></span>
<span id="cb7-1190"><a href="#cb7-1190"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1191"><a href="#cb7-1191"></a>    <span class="kw">)</span></span>
<span id="cb7-1192"><a href="#cb7-1192"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1193"><a href="#cb7-1193"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-1194"><a href="#cb7-1194"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1195"><a href="#cb7-1195"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1196"><a href="#cb7-1196"></a>    <span class="kw">)</span></span>
<span id="cb7-1197"><a href="#cb7-1197"></a>  <span class="kw">)</span></span>
<span id="cb7-1198"><a href="#cb7-1198"></a>  <span class="kw">(</span><span class="ex">34</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1199"><a href="#cb7-1199"></a>  <span class="kw">(</span><span class="ex">35</span><span class="kw">)</span><span class="bu">:</span> LMHeadPipe<span class="er">(</span></span>
<span id="cb7-1200"><a href="#cb7-1200"></a>    <span class="kw">(</span><span class="ex">lm_head</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1201"><a href="#cb7-1201"></a>  <span class="kw">)</span></span>
<span id="cb7-1202"><a href="#cb7-1202"></a><span class="ex">Loading</span> checkpoint shards:   0%<span class="kw">|</span>          <span class="kw">|</span> <span class="ex">0/7</span> [00:00<span class="op">&lt;</span><span class="pp">?</span>, <span class="pp">?</span>it/s]</span>
<span id="cb7-1203"><a href="#cb7-1203"></a><span class="ex">Loading</span> checkpoint shards:   0%<span class="kw">|</span>          <span class="kw">|</span> <span class="ex">0/7</span> [00:00<span class="op">&lt;</span><span class="pp">?</span>, <span class="pp">?</span>it/s]stage=0 layers=37</span>
<span id="cb7-1204"><a href="#cb7-1204"></a>     <span class="ex">0:</span> _to_float16</span>
<span id="cb7-1205"><a href="#cb7-1205"></a>     <span class="ex">1:</span> EmbeddingPipe</span>
<span id="cb7-1206"><a href="#cb7-1206"></a>     <span class="ex">2:</span> ParallelTransformerLayerPipe</span>
<span id="cb7-1207"><a href="#cb7-1207"></a>     <span class="ex">3:</span> ParallelTransformerLayerPipe</span>
<span id="cb7-1208"><a href="#cb7-1208"></a>     <span class="ex">4:</span> ParallelTransformerLayerPipe</span>
<span id="cb7-1209"><a href="#cb7-1209"></a>     <span class="ex">5:</span> ParallelTransformerLayerPipe</span>
<span id="cb7-1210"><a href="#cb7-1210"></a>     <span class="ex">6:</span> ParallelTransformerLayerPipe</span>
<span id="cb7-1211"><a href="#cb7-1211"></a>     <span class="ex">7:</span> ParallelTransformerLayerPipe</span>
<span id="cb7-1212"><a href="#cb7-1212"></a>     <span class="ex">8:</span> ParallelTransformerLayerPipe</span>
<span id="cb7-1213"><a href="#cb7-1213"></a>     <span class="ex">9:</span> ParallelTransformerLayerPipe</span>
<span id="cb7-1214"><a href="#cb7-1214"></a>    <span class="ex">10:</span> ParallelTransformerLayerPipe</span>
<span id="cb7-1215"><a href="#cb7-1215"></a>    <span class="ex">11:</span> ParallelTransformerLayerPipe</span>
<span id="cb7-1216"><a href="#cb7-1216"></a>    <span class="ex">12:</span> ParallelTransformerLayerPipe</span>
<span id="cb7-1217"><a href="#cb7-1217"></a>    <span class="ex">13:</span> ParallelTransformerLayerPipe</span>
<span id="cb7-1218"><a href="#cb7-1218"></a>    <span class="ex">14:</span> ParallelTransformerLayerPipe</span>
<span id="cb7-1219"><a href="#cb7-1219"></a>    <span class="ex">15:</span> ParallelTransformerLayerPipe</span>
<span id="cb7-1220"><a href="#cb7-1220"></a>    <span class="ex">16:</span> ParallelTransformerLayerPipe</span>
<span id="cb7-1221"><a href="#cb7-1221"></a>    <span class="ex">17:</span> ParallelTransformerLayerPipe</span>
<span id="cb7-1222"><a href="#cb7-1222"></a>    <span class="ex">18:</span> ParallelTransformerLayerPipe</span>
<span id="cb7-1223"><a href="#cb7-1223"></a>    <span class="ex">19:</span> ParallelTransformerLayerPipe</span>
<span id="cb7-1224"><a href="#cb7-1224"></a>    <span class="ex">20:</span> ParallelTransformerLayerPipe</span>
<span id="cb7-1225"><a href="#cb7-1225"></a>    <span class="ex">21:</span> ParallelTransformerLayerPipe</span>
<span id="cb7-1226"><a href="#cb7-1226"></a>    <span class="ex">22:</span> ParallelTransformerLayerPipe</span>
<span id="cb7-1227"><a href="#cb7-1227"></a>    <span class="ex">23:</span> ParallelTransformerLayerPipe</span>
<span id="cb7-1228"><a href="#cb7-1228"></a>    <span class="ex">24:</span> ParallelTransformerLayerPipe</span>
<span id="cb7-1229"><a href="#cb7-1229"></a>    <span class="ex">25:</span> ParallelTransformerLayerPipe</span>
<span id="cb7-1230"><a href="#cb7-1230"></a>    <span class="ex">26:</span> ParallelTransformerLayerPipe</span>
<span id="cb7-1231"><a href="#cb7-1231"></a>    <span class="ex">27:</span> ParallelTransformerLayerPipe</span>
<span id="cb7-1232"><a href="#cb7-1232"></a>    <span class="ex">28:</span> ParallelTransformerLayerPipe</span>
<span id="cb7-1233"><a href="#cb7-1233"></a>    <span class="ex">29:</span> ParallelTransformerLayerPipe</span>
<span id="cb7-1234"><a href="#cb7-1234"></a>    <span class="ex">30:</span> ParallelTransformerLayerPipe</span>
<span id="cb7-1235"><a href="#cb7-1235"></a>    <span class="ex">31:</span> ParallelTransformerLayerPipe</span>
<span id="cb7-1236"><a href="#cb7-1236"></a>    <span class="ex">32:</span> ParallelTransformerLayerPipe</span>
<span id="cb7-1237"><a href="#cb7-1237"></a>    <span class="ex">33:</span> ParallelTransformerLayerPipe</span>
<span id="cb7-1238"><a href="#cb7-1238"></a>    <span class="ex">34:</span> RMSNorm</span>
<span id="cb7-1239"><a href="#cb7-1239"></a>    <span class="ex">35:</span> LMHeadPipe</span>
<span id="cb7-1240"><a href="#cb7-1240"></a>    <span class="ex">36:</span> float16_to_fp32</span>
<span id="cb7-1241"><a href="#cb7-1241"></a>  <span class="ex">loss:</span> loss_func</span>
<span id="cb7-1242"><a href="#cb7-1242"></a><span class="ex">[2024-10-16</span> 15:39:38,077] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">utils.py:781:see_memory_usage</span><span class="pp">]</span> After Building Model</span>
<span id="cb7-1243"><a href="#cb7-1243"></a><span class="ex">[2024-10-16</span> 15:39:38,077] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">utils.py:782:see_memory_usage</span><span class="pp">]</span> MA 14.96 GB         Max_MA 14.96 GB         CA 14.96 GB         Max_CA 15 GB</span>
<span id="cb7-1244"><a href="#cb7-1244"></a><span class="ex">[2024-10-16</span> 15:39:38,077] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">utils.py:789:see_memory_usage</span><span class="pp">]</span> CPU Virtual Memory:  used = 105.4 GB, percent = 9.3%</span>
<span id="cb7-1245"><a href="#cb7-1245"></a><span class="ex">0</span> GPTModelPipe<span class="er">(</span></span>
<span id="cb7-1246"><a href="#cb7-1246"></a>  <span class="kw">(</span><span class="ex">tied_modules</span><span class="kw">)</span><span class="bu">:</span> ModuleDict<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1247"><a href="#cb7-1247"></a>  <span class="kw">(</span><span class="ex">1</span><span class="kw">)</span><span class="bu">:</span> EmbeddingPipe<span class="er">(</span></span>
<span id="cb7-1248"><a href="#cb7-1248"></a>    <span class="kw">(</span><span class="ex">word_embeddings</span><span class="kw">)</span><span class="bu">:</span> VocabParallelEmbedding<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1249"><a href="#cb7-1249"></a>    <span class="kw">(</span><span class="ex">embedding_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-1250"><a href="#cb7-1250"></a>  <span class="kw">)</span></span>
<span id="cb7-1251"><a href="#cb7-1251"></a>  <span class="kw">(</span><span class="ex">2</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-1252"><a href="#cb7-1252"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1253"><a href="#cb7-1253"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-1254"><a href="#cb7-1254"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1255"><a href="#cb7-1255"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-1256"><a href="#cb7-1256"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1257"><a href="#cb7-1257"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-1258"><a href="#cb7-1258"></a>      <span class="kw">)</span></span>
<span id="cb7-1259"><a href="#cb7-1259"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1260"><a href="#cb7-1260"></a>    <span class="kw">)</span></span>
<span id="cb7-1261"><a href="#cb7-1261"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1262"><a href="#cb7-1262"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-1263"><a href="#cb7-1263"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1264"><a href="#cb7-1264"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1265"><a href="#cb7-1265"></a>    <span class="kw">)</span></span>
<span id="cb7-1266"><a href="#cb7-1266"></a>  <span class="kw">)</span></span>
<span id="cb7-1267"><a href="#cb7-1267"></a>  <span class="kw">(</span><span class="ex">3</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-1268"><a href="#cb7-1268"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1269"><a href="#cb7-1269"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-1270"><a href="#cb7-1270"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1271"><a href="#cb7-1271"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-1272"><a href="#cb7-1272"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1273"><a href="#cb7-1273"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-1274"><a href="#cb7-1274"></a>      <span class="kw">)</span></span>
<span id="cb7-1275"><a href="#cb7-1275"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1276"><a href="#cb7-1276"></a>    <span class="kw">)</span></span>
<span id="cb7-1277"><a href="#cb7-1277"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1278"><a href="#cb7-1278"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-1279"><a href="#cb7-1279"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1280"><a href="#cb7-1280"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1281"><a href="#cb7-1281"></a>    <span class="kw">)</span></span>
<span id="cb7-1282"><a href="#cb7-1282"></a>  <span class="kw">)</span></span>
<span id="cb7-1283"><a href="#cb7-1283"></a>  <span class="kw">(</span><span class="ex">4</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-1284"><a href="#cb7-1284"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1285"><a href="#cb7-1285"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-1286"><a href="#cb7-1286"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1287"><a href="#cb7-1287"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-1288"><a href="#cb7-1288"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1289"><a href="#cb7-1289"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-1290"><a href="#cb7-1290"></a>      <span class="kw">)</span></span>
<span id="cb7-1291"><a href="#cb7-1291"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1292"><a href="#cb7-1292"></a>    <span class="kw">)</span></span>
<span id="cb7-1293"><a href="#cb7-1293"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1294"><a href="#cb7-1294"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-1295"><a href="#cb7-1295"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1296"><a href="#cb7-1296"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1297"><a href="#cb7-1297"></a>    <span class="kw">)</span></span>
<span id="cb7-1298"><a href="#cb7-1298"></a>  <span class="kw">)</span></span>
<span id="cb7-1299"><a href="#cb7-1299"></a>  <span class="kw">(</span><span class="ex">5</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-1300"><a href="#cb7-1300"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1301"><a href="#cb7-1301"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-1302"><a href="#cb7-1302"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1303"><a href="#cb7-1303"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-1304"><a href="#cb7-1304"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1305"><a href="#cb7-1305"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-1306"><a href="#cb7-1306"></a>      <span class="kw">)</span></span>
<span id="cb7-1307"><a href="#cb7-1307"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1308"><a href="#cb7-1308"></a>    <span class="kw">)</span></span>
<span id="cb7-1309"><a href="#cb7-1309"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1310"><a href="#cb7-1310"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-1311"><a href="#cb7-1311"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1312"><a href="#cb7-1312"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1313"><a href="#cb7-1313"></a>    <span class="kw">)</span></span>
<span id="cb7-1314"><a href="#cb7-1314"></a>  <span class="kw">)</span></span>
<span id="cb7-1315"><a href="#cb7-1315"></a>  <span class="kw">(</span><span class="ex">6</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-1316"><a href="#cb7-1316"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1317"><a href="#cb7-1317"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-1318"><a href="#cb7-1318"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1319"><a href="#cb7-1319"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-1320"><a href="#cb7-1320"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1321"><a href="#cb7-1321"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-1322"><a href="#cb7-1322"></a>      <span class="kw">)</span></span>
<span id="cb7-1323"><a href="#cb7-1323"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1324"><a href="#cb7-1324"></a>    <span class="kw">)</span></span>
<span id="cb7-1325"><a href="#cb7-1325"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1326"><a href="#cb7-1326"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-1327"><a href="#cb7-1327"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1328"><a href="#cb7-1328"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1329"><a href="#cb7-1329"></a>    <span class="kw">)</span></span>
<span id="cb7-1330"><a href="#cb7-1330"></a>  <span class="kw">)</span></span>
<span id="cb7-1331"><a href="#cb7-1331"></a>  <span class="kw">(</span><span class="ex">7</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-1332"><a href="#cb7-1332"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1333"><a href="#cb7-1333"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-1334"><a href="#cb7-1334"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1335"><a href="#cb7-1335"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-1336"><a href="#cb7-1336"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1337"><a href="#cb7-1337"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-1338"><a href="#cb7-1338"></a>      <span class="kw">)</span></span>
<span id="cb7-1339"><a href="#cb7-1339"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1340"><a href="#cb7-1340"></a>    <span class="kw">)</span></span>
<span id="cb7-1341"><a href="#cb7-1341"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1342"><a href="#cb7-1342"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-1343"><a href="#cb7-1343"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1344"><a href="#cb7-1344"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1345"><a href="#cb7-1345"></a>    <span class="kw">)</span></span>
<span id="cb7-1346"><a href="#cb7-1346"></a>  <span class="kw">)</span></span>
<span id="cb7-1347"><a href="#cb7-1347"></a>  <span class="kw">(</span><span class="ex">8</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-1348"><a href="#cb7-1348"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1349"><a href="#cb7-1349"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-1350"><a href="#cb7-1350"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1351"><a href="#cb7-1351"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-1352"><a href="#cb7-1352"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1353"><a href="#cb7-1353"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-1354"><a href="#cb7-1354"></a>      <span class="kw">)</span></span>
<span id="cb7-1355"><a href="#cb7-1355"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1356"><a href="#cb7-1356"></a>    <span class="kw">)</span></span>
<span id="cb7-1357"><a href="#cb7-1357"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1358"><a href="#cb7-1358"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-1359"><a href="#cb7-1359"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1360"><a href="#cb7-1360"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1361"><a href="#cb7-1361"></a>    <span class="kw">)</span></span>
<span id="cb7-1362"><a href="#cb7-1362"></a>  <span class="kw">)</span></span>
<span id="cb7-1363"><a href="#cb7-1363"></a>  <span class="kw">(</span><span class="ex">9</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-1364"><a href="#cb7-1364"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1365"><a href="#cb7-1365"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-1366"><a href="#cb7-1366"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1367"><a href="#cb7-1367"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-1368"><a href="#cb7-1368"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1369"><a href="#cb7-1369"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-1370"><a href="#cb7-1370"></a>      <span class="kw">)</span></span>
<span id="cb7-1371"><a href="#cb7-1371"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1372"><a href="#cb7-1372"></a>    <span class="kw">)</span></span>
<span id="cb7-1373"><a href="#cb7-1373"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1374"><a href="#cb7-1374"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-1375"><a href="#cb7-1375"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1376"><a href="#cb7-1376"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1377"><a href="#cb7-1377"></a>    <span class="kw">)</span></span>
<span id="cb7-1378"><a href="#cb7-1378"></a>  <span class="kw">)</span></span>
<span id="cb7-1379"><a href="#cb7-1379"></a>  <span class="kw">(</span><span class="ex">10</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-1380"><a href="#cb7-1380"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1381"><a href="#cb7-1381"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-1382"><a href="#cb7-1382"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1383"><a href="#cb7-1383"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-1384"><a href="#cb7-1384"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1385"><a href="#cb7-1385"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-1386"><a href="#cb7-1386"></a>      <span class="kw">)</span></span>
<span id="cb7-1387"><a href="#cb7-1387"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1388"><a href="#cb7-1388"></a>    <span class="kw">)</span></span>
<span id="cb7-1389"><a href="#cb7-1389"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1390"><a href="#cb7-1390"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-1391"><a href="#cb7-1391"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1392"><a href="#cb7-1392"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1393"><a href="#cb7-1393"></a>    <span class="kw">)</span></span>
<span id="cb7-1394"><a href="#cb7-1394"></a>  <span class="kw">)</span></span>
<span id="cb7-1395"><a href="#cb7-1395"></a>  <span class="kw">(</span><span class="ex">11</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-1396"><a href="#cb7-1396"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1397"><a href="#cb7-1397"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-1398"><a href="#cb7-1398"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1399"><a href="#cb7-1399"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-1400"><a href="#cb7-1400"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1401"><a href="#cb7-1401"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-1402"><a href="#cb7-1402"></a>      <span class="kw">)</span></span>
<span id="cb7-1403"><a href="#cb7-1403"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1404"><a href="#cb7-1404"></a>    <span class="kw">)</span></span>
<span id="cb7-1405"><a href="#cb7-1405"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1406"><a href="#cb7-1406"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-1407"><a href="#cb7-1407"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1408"><a href="#cb7-1408"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1409"><a href="#cb7-1409"></a>    <span class="kw">)</span></span>
<span id="cb7-1410"><a href="#cb7-1410"></a>  <span class="kw">)</span></span>
<span id="cb7-1411"><a href="#cb7-1411"></a>  <span class="kw">(</span><span class="ex">12</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-1412"><a href="#cb7-1412"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1413"><a href="#cb7-1413"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-1414"><a href="#cb7-1414"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1415"><a href="#cb7-1415"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-1416"><a href="#cb7-1416"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1417"><a href="#cb7-1417"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-1418"><a href="#cb7-1418"></a>      <span class="kw">)</span></span>
<span id="cb7-1419"><a href="#cb7-1419"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1420"><a href="#cb7-1420"></a>    <span class="kw">)</span></span>
<span id="cb7-1421"><a href="#cb7-1421"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1422"><a href="#cb7-1422"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-1423"><a href="#cb7-1423"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1424"><a href="#cb7-1424"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1425"><a href="#cb7-1425"></a>    <span class="kw">)</span></span>
<span id="cb7-1426"><a href="#cb7-1426"></a>  <span class="kw">)</span></span>
<span id="cb7-1427"><a href="#cb7-1427"></a>  <span class="kw">(</span><span class="ex">13</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-1428"><a href="#cb7-1428"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1429"><a href="#cb7-1429"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-1430"><a href="#cb7-1430"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1431"><a href="#cb7-1431"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-1432"><a href="#cb7-1432"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1433"><a href="#cb7-1433"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-1434"><a href="#cb7-1434"></a>      <span class="kw">)</span></span>
<span id="cb7-1435"><a href="#cb7-1435"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1436"><a href="#cb7-1436"></a>    <span class="kw">)</span></span>
<span id="cb7-1437"><a href="#cb7-1437"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1438"><a href="#cb7-1438"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-1439"><a href="#cb7-1439"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1440"><a href="#cb7-1440"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1441"><a href="#cb7-1441"></a>    <span class="kw">)</span></span>
<span id="cb7-1442"><a href="#cb7-1442"></a>  <span class="kw">)</span></span>
<span id="cb7-1443"><a href="#cb7-1443"></a>  <span class="kw">(</span><span class="ex">14</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-1444"><a href="#cb7-1444"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1445"><a href="#cb7-1445"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-1446"><a href="#cb7-1446"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1447"><a href="#cb7-1447"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-1448"><a href="#cb7-1448"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1449"><a href="#cb7-1449"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-1450"><a href="#cb7-1450"></a>      <span class="kw">)</span></span>
<span id="cb7-1451"><a href="#cb7-1451"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1452"><a href="#cb7-1452"></a>    <span class="kw">)</span></span>
<span id="cb7-1453"><a href="#cb7-1453"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1454"><a href="#cb7-1454"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-1455"><a href="#cb7-1455"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1456"><a href="#cb7-1456"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1457"><a href="#cb7-1457"></a>    <span class="kw">)</span></span>
<span id="cb7-1458"><a href="#cb7-1458"></a>  <span class="kw">)</span></span>
<span id="cb7-1459"><a href="#cb7-1459"></a>  <span class="kw">(</span><span class="ex">15</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-1460"><a href="#cb7-1460"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1461"><a href="#cb7-1461"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-1462"><a href="#cb7-1462"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1463"><a href="#cb7-1463"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-1464"><a href="#cb7-1464"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1465"><a href="#cb7-1465"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-1466"><a href="#cb7-1466"></a>      <span class="kw">)</span></span>
<span id="cb7-1467"><a href="#cb7-1467"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1468"><a href="#cb7-1468"></a>    <span class="kw">)</span></span>
<span id="cb7-1469"><a href="#cb7-1469"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1470"><a href="#cb7-1470"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-1471"><a href="#cb7-1471"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1472"><a href="#cb7-1472"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1473"><a href="#cb7-1473"></a>    <span class="kw">)</span></span>
<span id="cb7-1474"><a href="#cb7-1474"></a>  <span class="kw">)</span></span>
<span id="cb7-1475"><a href="#cb7-1475"></a>  <span class="kw">(</span><span class="ex">16</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-1476"><a href="#cb7-1476"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1477"><a href="#cb7-1477"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-1478"><a href="#cb7-1478"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1479"><a href="#cb7-1479"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-1480"><a href="#cb7-1480"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1481"><a href="#cb7-1481"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-1482"><a href="#cb7-1482"></a>      <span class="kw">)</span></span>
<span id="cb7-1483"><a href="#cb7-1483"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1484"><a href="#cb7-1484"></a>    <span class="kw">)</span></span>
<span id="cb7-1485"><a href="#cb7-1485"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1486"><a href="#cb7-1486"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-1487"><a href="#cb7-1487"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1488"><a href="#cb7-1488"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1489"><a href="#cb7-1489"></a>    <span class="kw">)</span></span>
<span id="cb7-1490"><a href="#cb7-1490"></a>  <span class="kw">)</span></span>
<span id="cb7-1491"><a href="#cb7-1491"></a>  <span class="kw">(</span><span class="ex">17</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-1492"><a href="#cb7-1492"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1493"><a href="#cb7-1493"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-1494"><a href="#cb7-1494"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1495"><a href="#cb7-1495"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-1496"><a href="#cb7-1496"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1497"><a href="#cb7-1497"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-1498"><a href="#cb7-1498"></a>      <span class="kw">)</span></span>
<span id="cb7-1499"><a href="#cb7-1499"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1500"><a href="#cb7-1500"></a>    <span class="kw">)</span></span>
<span id="cb7-1501"><a href="#cb7-1501"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1502"><a href="#cb7-1502"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-1503"><a href="#cb7-1503"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1504"><a href="#cb7-1504"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1505"><a href="#cb7-1505"></a>    <span class="kw">)</span></span>
<span id="cb7-1506"><a href="#cb7-1506"></a>  <span class="kw">)</span></span>
<span id="cb7-1507"><a href="#cb7-1507"></a>  <span class="kw">(</span><span class="ex">18</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-1508"><a href="#cb7-1508"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1509"><a href="#cb7-1509"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-1510"><a href="#cb7-1510"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1511"><a href="#cb7-1511"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-1512"><a href="#cb7-1512"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1513"><a href="#cb7-1513"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-1514"><a href="#cb7-1514"></a>      <span class="kw">)</span></span>
<span id="cb7-1515"><a href="#cb7-1515"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1516"><a href="#cb7-1516"></a>    <span class="kw">)</span></span>
<span id="cb7-1517"><a href="#cb7-1517"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1518"><a href="#cb7-1518"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-1519"><a href="#cb7-1519"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1520"><a href="#cb7-1520"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1521"><a href="#cb7-1521"></a>    <span class="kw">)</span></span>
<span id="cb7-1522"><a href="#cb7-1522"></a>  <span class="kw">)</span></span>
<span id="cb7-1523"><a href="#cb7-1523"></a>  <span class="kw">(</span><span class="ex">19</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-1524"><a href="#cb7-1524"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1525"><a href="#cb7-1525"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-1526"><a href="#cb7-1526"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1527"><a href="#cb7-1527"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-1528"><a href="#cb7-1528"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1529"><a href="#cb7-1529"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-1530"><a href="#cb7-1530"></a>      <span class="kw">)</span></span>
<span id="cb7-1531"><a href="#cb7-1531"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1532"><a href="#cb7-1532"></a>    <span class="kw">)</span></span>
<span id="cb7-1533"><a href="#cb7-1533"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1534"><a href="#cb7-1534"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-1535"><a href="#cb7-1535"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1536"><a href="#cb7-1536"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1537"><a href="#cb7-1537"></a>    <span class="kw">)</span></span>
<span id="cb7-1538"><a href="#cb7-1538"></a>  <span class="kw">)</span></span>
<span id="cb7-1539"><a href="#cb7-1539"></a>  <span class="kw">(</span><span class="ex">20</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-1540"><a href="#cb7-1540"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1541"><a href="#cb7-1541"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-1542"><a href="#cb7-1542"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1543"><a href="#cb7-1543"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-1544"><a href="#cb7-1544"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1545"><a href="#cb7-1545"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-1546"><a href="#cb7-1546"></a>      <span class="kw">)</span></span>
<span id="cb7-1547"><a href="#cb7-1547"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1548"><a href="#cb7-1548"></a>    <span class="kw">)</span></span>
<span id="cb7-1549"><a href="#cb7-1549"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1550"><a href="#cb7-1550"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-1551"><a href="#cb7-1551"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1552"><a href="#cb7-1552"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1553"><a href="#cb7-1553"></a>    <span class="kw">)</span></span>
<span id="cb7-1554"><a href="#cb7-1554"></a>  <span class="kw">)</span></span>
<span id="cb7-1555"><a href="#cb7-1555"></a>  <span class="kw">(</span><span class="ex">21</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-1556"><a href="#cb7-1556"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1557"><a href="#cb7-1557"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-1558"><a href="#cb7-1558"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1559"><a href="#cb7-1559"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-1560"><a href="#cb7-1560"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1561"><a href="#cb7-1561"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-1562"><a href="#cb7-1562"></a>      <span class="kw">)</span></span>
<span id="cb7-1563"><a href="#cb7-1563"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1564"><a href="#cb7-1564"></a>    <span class="kw">)</span></span>
<span id="cb7-1565"><a href="#cb7-1565"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1566"><a href="#cb7-1566"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-1567"><a href="#cb7-1567"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1568"><a href="#cb7-1568"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1569"><a href="#cb7-1569"></a>    <span class="kw">)</span></span>
<span id="cb7-1570"><a href="#cb7-1570"></a>  <span class="kw">)</span></span>
<span id="cb7-1571"><a href="#cb7-1571"></a>  <span class="kw">(</span><span class="ex">22</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-1572"><a href="#cb7-1572"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1573"><a href="#cb7-1573"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-1574"><a href="#cb7-1574"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1575"><a href="#cb7-1575"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-1576"><a href="#cb7-1576"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1577"><a href="#cb7-1577"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-1578"><a href="#cb7-1578"></a>      <span class="kw">)</span></span>
<span id="cb7-1579"><a href="#cb7-1579"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1580"><a href="#cb7-1580"></a>    <span class="kw">)</span></span>
<span id="cb7-1581"><a href="#cb7-1581"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1582"><a href="#cb7-1582"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-1583"><a href="#cb7-1583"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1584"><a href="#cb7-1584"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1585"><a href="#cb7-1585"></a>    <span class="kw">)</span></span>
<span id="cb7-1586"><a href="#cb7-1586"></a>  <span class="kw">)</span></span>
<span id="cb7-1587"><a href="#cb7-1587"></a>  <span class="kw">(</span><span class="ex">23</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-1588"><a href="#cb7-1588"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1589"><a href="#cb7-1589"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-1590"><a href="#cb7-1590"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1591"><a href="#cb7-1591"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-1592"><a href="#cb7-1592"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1593"><a href="#cb7-1593"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-1594"><a href="#cb7-1594"></a>      <span class="kw">)</span></span>
<span id="cb7-1595"><a href="#cb7-1595"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1596"><a href="#cb7-1596"></a>    <span class="kw">)</span></span>
<span id="cb7-1597"><a href="#cb7-1597"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1598"><a href="#cb7-1598"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-1599"><a href="#cb7-1599"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1600"><a href="#cb7-1600"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1601"><a href="#cb7-1601"></a>    <span class="kw">)</span></span>
<span id="cb7-1602"><a href="#cb7-1602"></a>  <span class="kw">)</span></span>
<span id="cb7-1603"><a href="#cb7-1603"></a>  <span class="kw">(</span><span class="ex">24</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-1604"><a href="#cb7-1604"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1605"><a href="#cb7-1605"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-1606"><a href="#cb7-1606"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1607"><a href="#cb7-1607"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-1608"><a href="#cb7-1608"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1609"><a href="#cb7-1609"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-1610"><a href="#cb7-1610"></a>      <span class="kw">)</span></span>
<span id="cb7-1611"><a href="#cb7-1611"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1612"><a href="#cb7-1612"></a>    <span class="kw">)</span></span>
<span id="cb7-1613"><a href="#cb7-1613"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1614"><a href="#cb7-1614"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-1615"><a href="#cb7-1615"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1616"><a href="#cb7-1616"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1617"><a href="#cb7-1617"></a>    <span class="kw">)</span></span>
<span id="cb7-1618"><a href="#cb7-1618"></a>  <span class="kw">)</span></span>
<span id="cb7-1619"><a href="#cb7-1619"></a>  <span class="kw">(</span><span class="ex">25</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-1620"><a href="#cb7-1620"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1621"><a href="#cb7-1621"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-1622"><a href="#cb7-1622"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1623"><a href="#cb7-1623"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-1624"><a href="#cb7-1624"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1625"><a href="#cb7-1625"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-1626"><a href="#cb7-1626"></a>      <span class="kw">)</span></span>
<span id="cb7-1627"><a href="#cb7-1627"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1628"><a href="#cb7-1628"></a>    <span class="kw">)</span></span>
<span id="cb7-1629"><a href="#cb7-1629"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1630"><a href="#cb7-1630"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-1631"><a href="#cb7-1631"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1632"><a href="#cb7-1632"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1633"><a href="#cb7-1633"></a>    <span class="kw">)</span></span>
<span id="cb7-1634"><a href="#cb7-1634"></a>  <span class="kw">)</span></span>
<span id="cb7-1635"><a href="#cb7-1635"></a>  <span class="kw">(</span><span class="ex">26</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-1636"><a href="#cb7-1636"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1637"><a href="#cb7-1637"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-1638"><a href="#cb7-1638"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1639"><a href="#cb7-1639"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-1640"><a href="#cb7-1640"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1641"><a href="#cb7-1641"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-1642"><a href="#cb7-1642"></a>      <span class="kw">)</span></span>
<span id="cb7-1643"><a href="#cb7-1643"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1644"><a href="#cb7-1644"></a>    <span class="kw">)</span></span>
<span id="cb7-1645"><a href="#cb7-1645"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1646"><a href="#cb7-1646"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-1647"><a href="#cb7-1647"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1648"><a href="#cb7-1648"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1649"><a href="#cb7-1649"></a>    <span class="kw">)</span></span>
<span id="cb7-1650"><a href="#cb7-1650"></a>  <span class="kw">)</span></span>
<span id="cb7-1651"><a href="#cb7-1651"></a>  <span class="kw">(</span><span class="ex">27</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-1652"><a href="#cb7-1652"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1653"><a href="#cb7-1653"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-1654"><a href="#cb7-1654"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1655"><a href="#cb7-1655"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-1656"><a href="#cb7-1656"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1657"><a href="#cb7-1657"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-1658"><a href="#cb7-1658"></a>      <span class="kw">)</span></span>
<span id="cb7-1659"><a href="#cb7-1659"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1660"><a href="#cb7-1660"></a>    <span class="kw">)</span></span>
<span id="cb7-1661"><a href="#cb7-1661"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1662"><a href="#cb7-1662"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-1663"><a href="#cb7-1663"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1664"><a href="#cb7-1664"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1665"><a href="#cb7-1665"></a>    <span class="kw">)</span></span>
<span id="cb7-1666"><a href="#cb7-1666"></a>  <span class="kw">)</span></span>
<span id="cb7-1667"><a href="#cb7-1667"></a>  <span class="kw">(</span><span class="ex">28</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-1668"><a href="#cb7-1668"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1669"><a href="#cb7-1669"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-1670"><a href="#cb7-1670"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1671"><a href="#cb7-1671"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-1672"><a href="#cb7-1672"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1673"><a href="#cb7-1673"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-1674"><a href="#cb7-1674"></a>      <span class="kw">)</span></span>
<span id="cb7-1675"><a href="#cb7-1675"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1676"><a href="#cb7-1676"></a>    <span class="kw">)</span></span>
<span id="cb7-1677"><a href="#cb7-1677"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1678"><a href="#cb7-1678"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-1679"><a href="#cb7-1679"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1680"><a href="#cb7-1680"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1681"><a href="#cb7-1681"></a>    <span class="kw">)</span></span>
<span id="cb7-1682"><a href="#cb7-1682"></a>  <span class="kw">)</span></span>
<span id="cb7-1683"><a href="#cb7-1683"></a>  <span class="kw">(</span><span class="ex">29</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-1684"><a href="#cb7-1684"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1685"><a href="#cb7-1685"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-1686"><a href="#cb7-1686"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1687"><a href="#cb7-1687"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-1688"><a href="#cb7-1688"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1689"><a href="#cb7-1689"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-1690"><a href="#cb7-1690"></a>      <span class="kw">)</span></span>
<span id="cb7-1691"><a href="#cb7-1691"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1692"><a href="#cb7-1692"></a>    <span class="kw">)</span></span>
<span id="cb7-1693"><a href="#cb7-1693"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1694"><a href="#cb7-1694"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-1695"><a href="#cb7-1695"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1696"><a href="#cb7-1696"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1697"><a href="#cb7-1697"></a>    <span class="kw">)</span></span>
<span id="cb7-1698"><a href="#cb7-1698"></a>  <span class="kw">)</span></span>
<span id="cb7-1699"><a href="#cb7-1699"></a>  <span class="kw">(</span><span class="ex">30</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-1700"><a href="#cb7-1700"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1701"><a href="#cb7-1701"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-1702"><a href="#cb7-1702"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1703"><a href="#cb7-1703"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-1704"><a href="#cb7-1704"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1705"><a href="#cb7-1705"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-1706"><a href="#cb7-1706"></a>      <span class="kw">)</span></span>
<span id="cb7-1707"><a href="#cb7-1707"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1708"><a href="#cb7-1708"></a>    <span class="kw">)</span></span>
<span id="cb7-1709"><a href="#cb7-1709"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1710"><a href="#cb7-1710"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-1711"><a href="#cb7-1711"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1712"><a href="#cb7-1712"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1713"><a href="#cb7-1713"></a>    <span class="kw">)</span></span>
<span id="cb7-1714"><a href="#cb7-1714"></a>  <span class="kw">)</span></span>
<span id="cb7-1715"><a href="#cb7-1715"></a>  <span class="kw">(</span><span class="ex">31</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-1716"><a href="#cb7-1716"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1717"><a href="#cb7-1717"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-1718"><a href="#cb7-1718"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1719"><a href="#cb7-1719"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-1720"><a href="#cb7-1720"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1721"><a href="#cb7-1721"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-1722"><a href="#cb7-1722"></a>      <span class="kw">)</span></span>
<span id="cb7-1723"><a href="#cb7-1723"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1724"><a href="#cb7-1724"></a>    <span class="kw">)</span></span>
<span id="cb7-1725"><a href="#cb7-1725"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1726"><a href="#cb7-1726"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-1727"><a href="#cb7-1727"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1728"><a href="#cb7-1728"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1729"><a href="#cb7-1729"></a>    <span class="kw">)</span></span>
<span id="cb7-1730"><a href="#cb7-1730"></a>  <span class="kw">)</span></span>
<span id="cb7-1731"><a href="#cb7-1731"></a>  <span class="kw">(</span><span class="ex">32</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-1732"><a href="#cb7-1732"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1733"><a href="#cb7-1733"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-1734"><a href="#cb7-1734"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1735"><a href="#cb7-1735"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-1736"><a href="#cb7-1736"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1737"><a href="#cb7-1737"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-1738"><a href="#cb7-1738"></a>      <span class="kw">)</span></span>
<span id="cb7-1739"><a href="#cb7-1739"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1740"><a href="#cb7-1740"></a>    <span class="kw">)</span></span>
<span id="cb7-1741"><a href="#cb7-1741"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1742"><a href="#cb7-1742"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-1743"><a href="#cb7-1743"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1744"><a href="#cb7-1744"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1745"><a href="#cb7-1745"></a>    <span class="kw">)</span></span>
<span id="cb7-1746"><a href="#cb7-1746"></a>  <span class="kw">)</span></span>
<span id="cb7-1747"><a href="#cb7-1747"></a>  <span class="kw">(</span><span class="ex">33</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb7-1748"><a href="#cb7-1748"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1749"><a href="#cb7-1749"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb7-1750"><a href="#cb7-1750"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1751"><a href="#cb7-1751"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb7-1752"><a href="#cb7-1752"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1753"><a href="#cb7-1753"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb7-1754"><a href="#cb7-1754"></a>      <span class="kw">)</span></span>
<span id="cb7-1755"><a href="#cb7-1755"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1756"><a href="#cb7-1756"></a>    <span class="kw">)</span></span>
<span id="cb7-1757"><a href="#cb7-1757"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1758"><a href="#cb7-1758"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb7-1759"><a href="#cb7-1759"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1760"><a href="#cb7-1760"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1761"><a href="#cb7-1761"></a>    <span class="kw">)</span></span>
<span id="cb7-1762"><a href="#cb7-1762"></a>  <span class="kw">)</span></span>
<span id="cb7-1763"><a href="#cb7-1763"></a>  <span class="kw">(</span><span class="ex">34</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1764"><a href="#cb7-1764"></a>  <span class="kw">(</span><span class="ex">35</span><span class="kw">)</span><span class="bu">:</span> LMHeadPipe<span class="er">(</span></span>
<span id="cb7-1765"><a href="#cb7-1765"></a>    <span class="kw">(</span><span class="ex">lm_head</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-1766"><a href="#cb7-1766"></a>  <span class="kw">)</span></span>
<span id="cb7-1767"><a href="#cb7-1767"></a><span class="kw">)</span></span>
<span id="cb7-1768"><a href="#cb7-1768"></a><span class="ex">Loading</span> checkpoint shards:   0%<span class="kw">|</span>          <span class="kw">|</span> <span class="ex">0/7</span> [00:00<span class="op">&lt;</span><span class="pp">?</span>, <span class="pp">?</span>it/s]2024-10-16 15:39:39.334375: W external/local_tsl/tsl/lib/monitoring/collection_registry.cc:81] Trying to register 2 metrics with the same name: /tensorflow/core/bfc_allocator_delay. The old value will be erased inorder to register a new one. Please check if you link the metric more than once, or if the name is already used by other metrics.</span>
<span id="cb7-1769"><a href="#cb7-1769"></a><span class="ex">2024-10-16</span> 15:39:39.334561: W external/local_tsl/tsl/lib/monitoring/collection_registry.cc:81] Trying to register 2 metrics with the same name: /xla/service/gpu/compiled_programs_count. The old value will be erased in order to register a new one. Please check if you link the metric more than once, or if the name is already used by other metrics.</span>
<span id="cb7-1770"><a href="#cb7-1770"></a><span class="ex">2024-10-16</span> 15:39:39.335711: W external/local_tsl/tsl/lib/monitoring/collection_registry.cc:81] Trying to register 2 metrics with the same name: /jax/pjrt/pjrt_executable_executions. The old value will be erased in order to register a new one. Please check if you link the metric more than once, or if the name is already used by other metrics.</span>
<span id="cb7-1771"><a href="#cb7-1771"></a><span class="ex">2024-10-16</span> 15:39:39.335722: W external/local_tsl/tsl/lib/monitoring/collection_registry.cc:81] Trying to register 2 metrics with the same name: /jax/pjrt/pjrt_executable_execution_time_usecs. The old value will be erased in order to register a new one. Please check if you linkthe metric more than once, or if the name is already used by other metrics.</span>
<span id="cb7-1772"><a href="#cb7-1772"></a><span class="ex">2024-10-16</span> 15:39:39.587872: I itex/core/wrapper/itex_gpu_wrapper.cc:38] Intel Extension for Tensorflow<span class="pp">*</span> GPU backend is loaded.</span>
<span id="cb7-1773"><a href="#cb7-1773"></a><span class="ex">2024-10-16</span> 15:39:39.631109: I itex/core/devices/gpu/itex_gpu_runtime.cc:130] Selected platform: Intel<span class="er">(</span><span class="ex">R</span><span class="kw">)</span> <span class="ex">Level-Zero</span></span>
<span id="cb7-1774"><a href="#cb7-1774"></a><span class="ex">2024-10-16</span> 15:39:39.631594: I itex/core/devices/gpu/itex_gpu_runtime.cc:155] number of sub-devices is zero, expose root device.</span>
<span id="cb7-1775"><a href="#cb7-1775"></a><span class="ex">2024-10-16</span> 15:39:39.631598: I itex/core/devices/gpu/itex_gpu_runtime.cc:155] number of sub-devices is zero, expose root device.</span>
<span id="cb7-1776"><a href="#cb7-1776"></a><span class="ex">2024-10-16</span> 15:39:39.631600: I itex/core/devices/gpu/itex_gpu_runtime.cc:155] number of sub-devices is zero, expose root device.</span>
<span id="cb7-1777"><a href="#cb7-1777"></a><span class="ex">2024-10-16</span> 15:39:39.631602: I itex/core/devices/gpu/itex_gpu_runtime.cc:155] number of sub-devices is zero, expose root device.</span>
<span id="cb7-1778"><a href="#cb7-1778"></a><span class="ex">2024-10-16</span> 15:39:39.631604: I itex/core/devices/gpu/itex_gpu_runtime.cc:155] number of sub-devices is zero, expose root device.</span>
<span id="cb7-1779"><a href="#cb7-1779"></a><span class="ex">2024-10-16</span> 15:39:39.631607: I itex/core/devices/gpu/itex_gpu_runtime.cc:155] number of sub-devices is zero, expose root device.</span>
<span id="cb7-1780"><a href="#cb7-1780"></a><span class="ex">2024-10-16</span> 15:39:39.631609: I itex/core/devices/gpu/itex_gpu_runtime.cc:155] number of sub-devices is zero, expose root device.</span>
<span id="cb7-1781"><a href="#cb7-1781"></a><span class="ex">2024-10-16</span> 15:39:39.631611: I itex/core/devices/gpu/itex_gpu_runtime.cc:155] number of sub-devices is zero, expose root device.</span>
<span id="cb7-1782"><a href="#cb7-1782"></a><span class="ex">2024-10-16</span> 15:39:39.631613: I itex/core/devices/gpu/itex_gpu_runtime.cc:155] number of sub-devices is zero, expose root device.</span>
<span id="cb7-1783"><a href="#cb7-1783"></a><span class="ex">2024-10-16</span> 15:39:39.631615: I itex/core/devices/gpu/itex_gpu_runtime.cc:155] number of sub-devices is zero, expose root device.</span>
<span id="cb7-1784"><a href="#cb7-1784"></a><span class="ex">2024-10-16</span> 15:39:39.631617: I itex/core/devices/gpu/itex_gpu_runtime.cc:155] number of sub-devices is zero, expose root device.</span>
<span id="cb7-1785"><a href="#cb7-1785"></a><span class="ex">2024-10-16</span> 15:39:39.631619: I itex/core/devices/gpu/itex_gpu_runtime.cc:155] number of sub-devices is zero, expose root device.</span>
<span id="cb7-1786"><a href="#cb7-1786"></a><span class="op">&gt;</span> setting <span class="ex">tensorboard</span> ...</span>
<span id="cb7-1787"><a href="#cb7-1787"></a><span class="ex">WARNING:</span> WANDB writing requested but no legit wandb project or experiment name provided, therefore no WANDB logs will be written according to random generated project or experiment name.</span>
<span id="cb7-1788"><a href="#cb7-1788"></a><span class="op">&gt;</span>fused <span class="ex">kernel</span> is only supported in cuda, skip loading fused kernel</span>
<span id="cb7-1789"><a href="#cb7-1789"></a><span class="ex">[2024-10-16</span> 15:39:39,835] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb7-1790"><a href="#cb7-1790"></a><span class="ex">Loading</span> checkpoint shards: 100%<span class="kw">|</span><span class="ex">██████████</span><span class="kw">|</span> <span class="ex">7/7</span> [03:35<span class="op">&lt;</span>00:00, 30.78s/it]</span>
<span id="cb7-1791"><a href="#cb7-1791"></a><span class="ex">[2024-10-16</span> 15:43:13.654241]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:108</span><span class="pp">]</span> <span class="at">-</span> <span class="at">----------------------------hf</span> weight list----------------------------</span>
<span id="cb7-1792"><a href="#cb7-1792"></a><span class="ex">[2024-10-16</span> 15:43:13.705041]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.embed_tokens.weight</span>
<span id="cb7-1793"><a href="#cb7-1793"></a><span class="ex">[2024-10-16</span> 15:43:13.708051]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.0.self_attn.q_proj.weight</span>
<span id="cb7-1794"><a href="#cb7-1794"></a><span class="ex">[2024-10-16</span> 15:43:13.710201]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.0.self_attn.k_proj.weight</span>
<span id="cb7-1795"><a href="#cb7-1795"></a><span class="ex">[2024-10-16</span> 15:43:13.711855]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.0.self_attn.v_proj.weight</span>
<span id="cb7-1796"><a href="#cb7-1796"></a><span class="ex">[2024-10-16</span> 15:43:13.714676]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.0.self_attn.o_proj.weight</span>
<span id="cb7-1797"><a href="#cb7-1797"></a><span class="ex">[2024-10-16</span> 15:43:13.721608]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.0.mlp.gate_proj.weight</span>
<span id="cb7-1798"><a href="#cb7-1798"></a><span class="ex">[2024-10-16</span> 15:43:13.728527]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.0.mlp.up_proj.weight</span>
<span id="cb7-1799"><a href="#cb7-1799"></a><span class="ex">[2024-10-16</span> 15:43:13.735447]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.0.mlp.down_proj.weight</span>
<span id="cb7-1800"><a href="#cb7-1800"></a><span class="ex">[2024-10-16</span> 15:43:13.736224]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.0.input_layernorm.weight</span>
<span id="cb7-1801"><a href="#cb7-1801"></a><span class="ex">[2024-10-16</span> 15:43:13.736764]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.0.post_attention_layernorm.weight</span>
<span id="cb7-1802"><a href="#cb7-1802"></a><span class="ex">[2024-10-16</span> 15:43:13.739392]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.1.self_attn.q_proj.weight</span>
<span id="cb7-1803"><a href="#cb7-1803"></a><span class="ex">[2024-10-16</span> 15:43:13.741013]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.1.self_attn.k_proj.weight</span>
<span id="cb7-1804"><a href="#cb7-1804"></a><span class="ex">[2024-10-16</span> 15:43:13.742600]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.1.self_attn.v_proj.weight</span>
<span id="cb7-1805"><a href="#cb7-1805"></a><span class="ex">[2024-10-16</span> 15:43:13.745359]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.1.self_attn.o_proj.weight</span>
<span id="cb7-1806"><a href="#cb7-1806"></a><span class="ex">[2024-10-16</span> 15:43:13.752286]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.1.mlp.gate_proj.weight</span>
<span id="cb7-1807"><a href="#cb7-1807"></a><span class="ex">[2024-10-16</span> 15:43:13.759192]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.1.mlp.up_proj.weight</span>
<span id="cb7-1808"><a href="#cb7-1808"></a><span class="ex">[2024-10-16</span> 15:43:13.766054]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.1.mlp.down_proj.weight</span>
<span id="cb7-1809"><a href="#cb7-1809"></a><span class="ex">[2024-10-16</span> 15:43:13.766785]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.1.input_layernorm.weight</span>
<span id="cb7-1810"><a href="#cb7-1810"></a><span class="ex">[2024-10-16</span> 15:43:13.767321]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.1.post_attention_layernorm.weight</span>
<span id="cb7-1811"><a href="#cb7-1811"></a><span class="ex">[2024-10-16</span> 15:43:13.769938]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.2.self_attn.q_proj.weight</span>
<span id="cb7-1812"><a href="#cb7-1812"></a><span class="ex">[2024-10-16</span> 15:43:13.771536]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.2.self_attn.k_proj.weight</span>
<span id="cb7-1813"><a href="#cb7-1813"></a><span class="ex">[2024-10-16</span> 15:43:13.773107]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.2.self_attn.v_proj.weight</span>
<span id="cb7-1814"><a href="#cb7-1814"></a><span class="ex">[2024-10-16</span> 15:43:13.775861]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.2.self_attn.o_proj.weight</span>
<span id="cb7-1815"><a href="#cb7-1815"></a><span class="ex">[2024-10-16</span> 15:43:13.782733]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.2.mlp.gate_proj.weight</span>
<span id="cb7-1816"><a href="#cb7-1816"></a><span class="ex">[2024-10-16</span> 15:43:13.789559]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.2.mlp.up_proj.weight</span>
<span id="cb7-1817"><a href="#cb7-1817"></a><span class="ex">[2024-10-16</span> 15:43:13.796385]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.2.mlp.down_proj.weight</span>
<span id="cb7-1818"><a href="#cb7-1818"></a><span class="ex">[2024-10-16</span> 15:43:13.797080]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.2.input_layernorm.weight</span>
<span id="cb7-1819"><a href="#cb7-1819"></a><span class="ex">[2024-10-16</span> 15:43:13.797626]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.2.post_attention_layernorm.weight</span>
<span id="cb7-1820"><a href="#cb7-1820"></a><span class="ex">[2024-10-16</span> 15:43:13.800331]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.3.self_attn.q_proj.weight</span>
<span id="cb7-1821"><a href="#cb7-1821"></a><span class="ex">[2024-10-16</span> 15:43:13.801911]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.3.self_attn.k_proj.weight</span>
<span id="cb7-1822"><a href="#cb7-1822"></a><span class="ex">[2024-10-16</span> 15:43:13.803453]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.3.self_attn.v_proj.weight</span>
<span id="cb7-1823"><a href="#cb7-1823"></a><span class="ex">[2024-10-16</span> 15:43:13.806474]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.3.self_attn.o_proj.weight</span>
<span id="cb7-1824"><a href="#cb7-1824"></a><span class="ex">Loading</span> checkpoint shards: 100%<span class="kw">|</span><span class="ex">██████████</span><span class="kw">|</span> <span class="ex">7/7</span> [03:35<span class="op">&lt;</span>00:00, 30.84s/it]</span>
<span id="cb7-1825"><a href="#cb7-1825"></a><span class="ex">[2024-10-16</span> 15:43:13.813328]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.3.mlp.gate_proj.weight</span>
<span id="cb7-1826"><a href="#cb7-1826"></a><span class="ex">[2024-10-16</span> 15:43:13.820151]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.3.mlp.up_proj.weight</span>
<span id="cb7-1827"><a href="#cb7-1827"></a><span class="ex">[2024-10-16</span> 15:43:13.826971]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.3.mlp.down_proj.weight</span>
<span id="cb7-1828"><a href="#cb7-1828"></a><span class="ex">[2024-10-16</span> 15:43:13.827640]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.3.input_layernorm.weight</span>
<span id="cb7-1829"><a href="#cb7-1829"></a><span class="ex">[2024-10-16</span> 15:43:13.828185]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.3.post_attention_layernorm.weight</span>
<span id="cb7-1830"><a href="#cb7-1830"></a><span class="ex">[2024-10-16</span> 15:43:13.831062]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.4.self_attn.q_proj.weight</span>
<span id="cb7-1831"><a href="#cb7-1831"></a><span class="ex">[2024-10-16</span> 15:43:13.832590]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.4.self_attn.k_proj.weight</span>
<span id="cb7-1832"><a href="#cb7-1832"></a><span class="ex">[2024-10-16</span> 15:43:13.834171]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.4.self_attn.v_proj.weight</span>
<span id="cb7-1833"><a href="#cb7-1833"></a><span class="ex">[2024-10-16</span> 15:43:13.837147]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.4.self_attn.o_proj.weight</span>
<span id="cb7-1834"><a href="#cb7-1834"></a><span class="ex">[2024-10-16</span> 15:43:13.843970]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.4.mlp.gate_proj.weight</span>
<span id="cb7-1835"><a href="#cb7-1835"></a><span class="ex">[2024-10-16</span> 15:43:13.850793]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.4.mlp.up_proj.weight</span>
<span id="cb7-1836"><a href="#cb7-1836"></a><span class="ex">[2024-10-16</span> 15:43:13.857596]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.4.mlp.down_proj.weight</span>
<span id="cb7-1837"><a href="#cb7-1837"></a><span class="ex">[2024-10-16</span> 15:43:13.858284]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.4.input_layernorm.weight</span>
<span id="cb7-1838"><a href="#cb7-1838"></a><span class="ex">[2024-10-16</span> 15:43:13.858810]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.4.post_attention_layernorm.weight</span>
<span id="cb7-1839"><a href="#cb7-1839"></a><span class="ex">[2024-10-16</span> 15:43:13.861661]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.5.self_attn.q_proj.weight</span>
<span id="cb7-1840"><a href="#cb7-1840"></a><span class="ex">[2024-10-16</span> 15:43:13.863209]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.5.self_attn.k_proj.weight</span>
<span id="cb7-1841"><a href="#cb7-1841"></a><span class="ex">[2024-10-16</span> 15:43:13.864753]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.5.self_attn.v_proj.weight</span>
<span id="cb7-1842"><a href="#cb7-1842"></a><span class="ex">[2024-10-16</span> 15:43:13.867739]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.5.self_attn.o_proj.weight</span>
<span id="cb7-1843"><a href="#cb7-1843"></a><span class="ex">[2024-10-16</span> 15:43:13.874537]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.5.mlp.gate_proj.weight</span>
<span id="cb7-1844"><a href="#cb7-1844"></a><span class="ex">[2024-10-16</span> 15:43:13.881318]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.5.mlp.up_proj.weight</span>
<span id="cb7-1845"><a href="#cb7-1845"></a><span class="ex">[2024-10-16</span> 15:43:13.888097]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.5.mlp.down_proj.weight</span>
<span id="cb7-1846"><a href="#cb7-1846"></a><span class="ex">[2024-10-16</span> 15:43:13.888767]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.5.input_layernorm.weight</span>
<span id="cb7-1847"><a href="#cb7-1847"></a><span class="ex">[2024-10-16</span> 15:43:13.889295]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.5.post_attention_layernorm.weight</span>
<span id="cb7-1848"><a href="#cb7-1848"></a><span class="ex">[2024-10-16</span> 15:43:13.892151]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.6.self_attn.q_proj.weight</span>
<span id="cb7-1849"><a href="#cb7-1849"></a><span class="ex">[2024-10-16</span> 15:43:13.893641]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.6.self_attn.k_proj.weight</span>
<span id="cb7-1850"><a href="#cb7-1850"></a><span class="ex">[2024-10-16</span> 15:43:13.895211]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.6.self_attn.v_proj.weight</span>
<span id="cb7-1851"><a href="#cb7-1851"></a><span class="ex">[2024-10-16</span> 15:43:13.898154]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.6.self_attn.o_proj.weight</span>
<span id="cb7-1852"><a href="#cb7-1852"></a><span class="ex">[2024-10-16</span> 15:43:13.904976]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.6.mlp.gate_proj.weight</span>
<span id="cb7-1853"><a href="#cb7-1853"></a><span class="ex">[2024-10-16</span> 15:43:13.911767]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.6.mlp.up_proj.weight</span>
<span id="cb7-1854"><a href="#cb7-1854"></a><span class="ex">[2024-10-16</span> 15:43:13.918536]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.6.mlp.down_proj.weight</span>
<span id="cb7-1855"><a href="#cb7-1855"></a><span class="ex">[2024-10-16</span> 15:43:13.919201]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.6.input_layernorm.weight</span>
<span id="cb7-1856"><a href="#cb7-1856"></a><span class="ex">[2024-10-16</span> 15:43:13.919795]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.6.post_attention_layernorm.weight</span>
<span id="cb7-1857"><a href="#cb7-1857"></a><span class="ex">[2024-10-16</span> 15:43:13.922646]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.7.self_attn.q_proj.weight</span>
<span id="cb7-1858"><a href="#cb7-1858"></a><span class="ex">[2024-10-16</span> 15:43:13.924200]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.7.self_attn.k_proj.weight</span>
<span id="cb7-1859"><a href="#cb7-1859"></a><span class="ex">[2024-10-16</span> 15:43:13.925687]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.7.self_attn.v_proj.weight</span>
<span id="cb7-1860"><a href="#cb7-1860"></a><span class="ex">[2024-10-16</span> 15:43:13.928651]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.7.self_attn.o_proj.weight</span>
<span id="cb7-1861"><a href="#cb7-1861"></a><span class="ex">[2024-10-16</span> 15:43:13.935432]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.7.mlp.gate_proj.weight</span>
<span id="cb7-1862"><a href="#cb7-1862"></a><span class="ex">[2024-10-16</span> 15:43:13.942178]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.7.mlp.up_proj.weight</span>
<span id="cb7-1863"><a href="#cb7-1863"></a><span class="ex">[2024-10-16</span> 15:43:13.948916]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.7.mlp.down_proj.weight</span>
<span id="cb7-1864"><a href="#cb7-1864"></a><span class="ex">[2024-10-16</span> 15:43:13.949547]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.7.input_layernorm.weight</span>
<span id="cb7-1865"><a href="#cb7-1865"></a><span class="ex">[2024-10-16</span> 15:43:13.950059]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.7.post_attention_layernorm.weight</span>
<span id="cb7-1866"><a href="#cb7-1866"></a><span class="ex">[2024-10-16</span> 15:43:13.952902]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.8.self_attn.q_proj.weight</span>
<span id="cb7-1867"><a href="#cb7-1867"></a><span class="ex">[2024-10-16</span> 15:43:13.954461]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.8.self_attn.k_proj.weight</span>
<span id="cb7-1868"><a href="#cb7-1868"></a><span class="ex">[2024-10-16</span> 15:43:13.955985]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.8.self_attn.v_proj.weight</span>
<span id="cb7-1869"><a href="#cb7-1869"></a><span class="ex">[2024-10-16</span> 15:43:13.958931]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.8.self_attn.o_proj.weight</span>
<span id="cb7-1870"><a href="#cb7-1870"></a><span class="ex">[2024-10-16</span> 15:43:13.965709]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.8.mlp.gate_proj.weight</span>
<span id="cb7-1871"><a href="#cb7-1871"></a><span class="ex">[2024-10-16</span> 15:43:13.972481]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.8.mlp.up_proj.weight</span>
<span id="cb7-1872"><a href="#cb7-1872"></a><span class="ex">[2024-10-16</span> 15:43:13.979242]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.8.mlp.down_proj.weight</span>
<span id="cb7-1873"><a href="#cb7-1873"></a><span class="ex">[2024-10-16</span> 15:43:13.979876]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.8.input_layernorm.weight</span>
<span id="cb7-1874"><a href="#cb7-1874"></a><span class="ex">[2024-10-16</span> 15:43:13.980381]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.8.post_attention_layernorm.weight</span>
<span id="cb7-1875"><a href="#cb7-1875"></a><span class="ex">[2024-10-16</span> 15:43:13.983353]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.9.self_attn.q_proj.weight</span>
<span id="cb7-1876"><a href="#cb7-1876"></a><span class="ex">[2024-10-16</span> 15:43:13.984910]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.9.self_attn.k_proj.weight</span>
<span id="cb7-1877"><a href="#cb7-1877"></a><span class="ex">[2024-10-16</span> 15:43:13.986401]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.9.self_attn.v_proj.weight</span>
<span id="cb7-1878"><a href="#cb7-1878"></a><span class="ex">[2024-10-16</span> 15:43:13.989279]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.9.self_attn.o_proj.weight</span>
<span id="cb7-1879"><a href="#cb7-1879"></a><span class="ex">[2024-10-16</span> 15:43:13.996056]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.9.mlp.gate_proj.weight</span>
<span id="cb7-1880"><a href="#cb7-1880"></a><span class="ex">[2024-10-16</span> 15:43:14.002856]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.9.mlp.up_proj.weight</span>
<span id="cb7-1881"><a href="#cb7-1881"></a><span class="ex">[2024-10-16</span> 15:43:14.009601]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.9.mlp.down_proj.weight</span>
<span id="cb7-1882"><a href="#cb7-1882"></a><span class="ex">[2024-10-16</span> 15:43:14.010234]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.9.input_layernorm.weight</span>
<span id="cb7-1883"><a href="#cb7-1883"></a><span class="ex">[2024-10-16</span> 15:43:14.010742]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.9.post_attention_layernorm.weight</span>
<span id="cb7-1884"><a href="#cb7-1884"></a><span class="ex">[2024-10-16</span> 15:43:14.013552]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.10.self_attn.q_proj.weight</span>
<span id="cb7-1885"><a href="#cb7-1885"></a><span class="ex">[2024-10-16</span> 15:43:14.015117]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.10.self_attn.k_proj.weight</span>
<span id="cb7-1886"><a href="#cb7-1886"></a><span class="ex">[2024-10-16</span> 15:43:14.016607]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.10.self_attn.v_proj.weight</span>
<span id="cb7-1887"><a href="#cb7-1887"></a><span class="ex">[2024-10-16</span> 15:43:14.019542]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.10.self_attn.o_proj.weight</span>
<span id="cb7-1888"><a href="#cb7-1888"></a><span class="ex">[2024-10-16</span> 15:43:14.026297]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.10.mlp.gate_proj.weight</span>
<span id="cb7-1889"><a href="#cb7-1889"></a><span class="ex">[2024-10-16</span> 15:43:14.033038]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.10.mlp.up_proj.weight</span>
<span id="cb7-1890"><a href="#cb7-1890"></a><span class="ex">[2024-10-16</span> 15:43:14.039752]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.10.mlp.down_proj.weight</span>
<span id="cb7-1891"><a href="#cb7-1891"></a><span class="ex">[2024-10-16</span> 15:43:14.040455]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.10.input_layernorm.weight</span>
<span id="cb7-1892"><a href="#cb7-1892"></a><span class="ex">[2024-10-16</span> 15:43:14.040966]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.10.post_attention_layernorm.weight</span>
<span id="cb7-1893"><a href="#cb7-1893"></a><span class="ex">[2024-10-16</span> 15:43:14.043760]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.11.self_attn.q_proj.weight</span>
<span id="cb7-1894"><a href="#cb7-1894"></a><span class="ex">[2024-10-16</span> 15:43:14.045346]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.11.self_attn.k_proj.weight</span>
<span id="cb7-1895"><a href="#cb7-1895"></a><span class="ex">[2024-10-16</span> 15:43:14.046849]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.11.self_attn.v_proj.weight</span>
<span id="cb7-1896"><a href="#cb7-1896"></a><span class="ex">[2024-10-16</span> 15:43:14.049725]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.11.self_attn.o_proj.weight</span>
<span id="cb7-1897"><a href="#cb7-1897"></a><span class="ex">[2024-10-16</span> 15:43:14.056435]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.11.mlp.gate_proj.weight</span>
<span id="cb7-1898"><a href="#cb7-1898"></a><span class="ex">[2024-10-16</span> 15:43:14.063163]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.11.mlp.up_proj.weight</span>
<span id="cb7-1899"><a href="#cb7-1899"></a><span class="ex">[2024-10-16</span> 15:43:14.069898]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.11.mlp.down_proj.weight</span>
<span id="cb7-1900"><a href="#cb7-1900"></a><span class="ex">[2024-10-16</span> 15:43:14.070561]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.11.input_layernorm.weight</span>
<span id="cb7-1901"><a href="#cb7-1901"></a><span class="ex">[2024-10-16</span> 15:43:14.071084]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.11.post_attention_layernorm.weight</span>
<span id="cb7-1902"><a href="#cb7-1902"></a><span class="ex">[2024-10-16</span> 15:43:14.073869]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.12.self_attn.q_proj.weight</span>
<span id="cb7-1903"><a href="#cb7-1903"></a><span class="ex">[2024-10-16</span> 15:43:14.075363]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.12.self_attn.k_proj.weight</span>
<span id="cb7-1904"><a href="#cb7-1904"></a><span class="ex">[2024-10-16</span> 15:43:14.076918]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.12.self_attn.v_proj.weight</span>
<span id="cb7-1905"><a href="#cb7-1905"></a><span class="ex">[2024-10-16</span> 15:43:14.079777]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.12.self_attn.o_proj.weight</span>
<span id="cb7-1906"><a href="#cb7-1906"></a><span class="ex">[2024-10-16</span> 15:43:14.086497]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.12.mlp.gate_proj.weight</span>
<span id="cb7-1907"><a href="#cb7-1907"></a><span class="ex">[2024-10-16</span> 15:43:14.093183]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.12.mlp.up_proj.weight</span>
<span id="cb7-1908"><a href="#cb7-1908"></a><span class="ex">[2024-10-16</span> 15:43:14.099869]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.12.mlp.down_proj.weight</span>
<span id="cb7-1909"><a href="#cb7-1909"></a><span class="ex">[2024-10-16</span> 15:43:14.100523]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.12.input_layernorm.weight</span>
<span id="cb7-1910"><a href="#cb7-1910"></a><span class="ex">[2024-10-16</span> 15:43:14.101038]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.12.post_attention_layernorm.weight</span>
<span id="cb7-1911"><a href="#cb7-1911"></a><span class="ex">[2024-10-16</span> 15:43:14.103823]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.13.self_attn.q_proj.weight</span>
<span id="cb7-1912"><a href="#cb7-1912"></a><span class="ex">[2024-10-16</span> 15:43:14.105335]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.13.self_attn.k_proj.weight</span>
<span id="cb7-1913"><a href="#cb7-1913"></a><span class="ex">[2024-10-16</span> 15:43:14.106828]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.13.self_attn.v_proj.weight</span>
<span id="cb7-1914"><a href="#cb7-1914"></a><span class="ex">[2024-10-16</span> 15:43:14.109698]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.13.self_attn.o_proj.weight</span>
<span id="cb7-1915"><a href="#cb7-1915"></a><span class="ex">[2024-10-16</span> 15:43:14.116395]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.13.mlp.gate_proj.weight</span>
<span id="cb7-1916"><a href="#cb7-1916"></a><span class="ex">[2024-10-16</span> 15:43:14.123086]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.13.mlp.up_proj.weight</span>
<span id="cb7-1917"><a href="#cb7-1917"></a><span class="ex">[2024-10-16</span> 15:43:14.129807]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.13.mlp.down_proj.weight</span>
<span id="cb7-1918"><a href="#cb7-1918"></a><span class="ex">[2024-10-16</span> 15:43:14.130474]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.13.input_layernorm.weight</span>
<span id="cb7-1919"><a href="#cb7-1919"></a><span class="ex">[2024-10-16</span> 15:43:14.130997]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.13.post_attention_layernorm.weight</span>
<span id="cb7-1920"><a href="#cb7-1920"></a><span class="ex">[2024-10-16</span> 15:43:14.133762]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.14.self_attn.q_proj.weight</span>
<span id="cb7-1921"><a href="#cb7-1921"></a><span class="ex">[2024-10-16</span> 15:43:14.135290]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.14.self_attn.k_proj.weight</span>
<span id="cb7-1922"><a href="#cb7-1922"></a><span class="ex">[2024-10-16</span> 15:43:14.136791]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.14.self_attn.v_proj.weight</span>
<span id="cb7-1923"><a href="#cb7-1923"></a><span class="ex">[2024-10-16</span> 15:43:14.139860]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.14.self_attn.o_proj.weight</span>
<span id="cb7-1924"><a href="#cb7-1924"></a><span class="ex">[2024-10-16</span> 15:43:14.146560]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.14.mlp.gate_proj.weight</span>
<span id="cb7-1925"><a href="#cb7-1925"></a><span class="ex">[2024-10-16</span> 15:43:14.153229]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.14.mlp.up_proj.weight</span>
<span id="cb7-1926"><a href="#cb7-1926"></a><span class="ex">[2024-10-16</span> 15:43:14.160012]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.14.mlp.down_proj.weight</span>
<span id="cb7-1927"><a href="#cb7-1927"></a><span class="ex">[2024-10-16</span> 15:43:14.160681]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.14.input_layernorm.weight</span>
<span id="cb7-1928"><a href="#cb7-1928"></a><span class="ex">[2024-10-16</span> 15:43:14.161212]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.14.post_attention_layernorm.weight</span>
<span id="cb7-1929"><a href="#cb7-1929"></a><span class="ex">[2024-10-16</span> 15:43:14.164011]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.15.self_attn.q_proj.weight</span>
<span id="cb7-1930"><a href="#cb7-1930"></a><span class="ex">[2024-10-16</span> 15:43:14.165550]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.15.self_attn.k_proj.weight</span>
<span id="cb7-1931"><a href="#cb7-1931"></a><span class="ex">[2024-10-16</span> 15:43:14.167029]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.15.self_attn.v_proj.weight</span>
<span id="cb7-1932"><a href="#cb7-1932"></a><span class="ex">[2024-10-16</span> 15:43:14.169860]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.15.self_attn.o_proj.weight</span>
<span id="cb7-1933"><a href="#cb7-1933"></a><span class="ex">[2024-10-16</span> 15:43:14.176522]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.15.mlp.gate_proj.weight</span>
<span id="cb7-1934"><a href="#cb7-1934"></a><span class="ex">[2024-10-16</span> 15:43:14.183206]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.15.mlp.up_proj.weight</span>
<span id="cb7-1935"><a href="#cb7-1935"></a><span class="ex">[2024-10-16</span> 15:43:14.189866]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.15.mlp.down_proj.weight</span>
<span id="cb7-1936"><a href="#cb7-1936"></a><span class="ex">[2024-10-16</span> 15:43:14.190530]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.15.input_layernorm.weight</span>
<span id="cb7-1937"><a href="#cb7-1937"></a><span class="ex">[2024-10-16</span> 15:43:14.191065]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.15.post_attention_layernorm.weight</span>
<span id="cb7-1938"><a href="#cb7-1938"></a><span class="ex">[2024-10-16</span> 15:43:14.193838]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.16.self_attn.q_proj.weight</span>
<span id="cb7-1939"><a href="#cb7-1939"></a><span class="ex">[2024-10-16</span> 15:43:14.195331]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.16.self_attn.k_proj.weight</span>
<span id="cb7-1940"><a href="#cb7-1940"></a><span class="ex">[2024-10-16</span> 15:43:14.196892]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.16.self_attn.v_proj.weight</span>
<span id="cb7-1941"><a href="#cb7-1941"></a><span class="ex">[2024-10-16</span> 15:43:14.199748]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.16.self_attn.o_proj.weight</span>
<span id="cb7-1942"><a href="#cb7-1942"></a><span class="ex">[2024-10-16</span> 15:43:14.206446]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.16.mlp.gate_proj.weight</span>
<span id="cb7-1943"><a href="#cb7-1943"></a><span class="ex">[2024-10-16</span> 15:43:14.213109]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.16.mlp.up_proj.weight</span>
<span id="cb7-1944"><a href="#cb7-1944"></a><span class="ex">[2024-10-16</span> 15:43:14.219783]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.16.mlp.down_proj.weight</span>
<span id="cb7-1945"><a href="#cb7-1945"></a><span class="ex">[2024-10-16</span> 15:43:14.220402]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.16.input_layernorm.weight</span>
<span id="cb7-1946"><a href="#cb7-1946"></a><span class="ex">[2024-10-16</span> 15:43:14.220932]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.16.post_attention_layernorm.weight</span>
<span id="cb7-1947"><a href="#cb7-1947"></a><span class="ex">[2024-10-16</span> 15:43:14.223676]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.17.self_attn.q_proj.weight</span>
<span id="cb7-1948"><a href="#cb7-1948"></a><span class="ex">[2024-10-16</span> 15:43:14.225232]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.17.self_attn.k_proj.weight</span>
<span id="cb7-1949"><a href="#cb7-1949"></a><span class="ex">[2024-10-16</span> 15:43:14.226737]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.17.self_attn.v_proj.weight</span>
<span id="cb7-1950"><a href="#cb7-1950"></a><span class="ex">[2024-10-16</span> 15:43:14.229543]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.17.self_attn.o_proj.weight</span>
<span id="cb7-1951"><a href="#cb7-1951"></a><span class="ex">[2024-10-16</span> 15:43:14.236240]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.17.mlp.gate_proj.weight</span>
<span id="cb7-1952"><a href="#cb7-1952"></a><span class="ex">[2024-10-16</span> 15:43:14.242898]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.17.mlp.up_proj.weight</span>
<span id="cb7-1953"><a href="#cb7-1953"></a><span class="ex">[2024-10-16</span> 15:43:14.249590]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.17.mlp.down_proj.weight</span>
<span id="cb7-1954"><a href="#cb7-1954"></a><span class="ex">[2024-10-16</span> 15:43:14.250235]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.17.input_layernorm.weight</span>
<span id="cb7-1955"><a href="#cb7-1955"></a><span class="ex">[2024-10-16</span> 15:43:14.250747]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.17.post_attention_layernorm.weight</span>
<span id="cb7-1956"><a href="#cb7-1956"></a><span class="ex">[2024-10-16</span> 15:43:14.253465]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.18.self_attn.q_proj.weight</span>
<span id="cb7-1957"><a href="#cb7-1957"></a><span class="ex">[2024-10-16</span> 15:43:14.255062]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.18.self_attn.k_proj.weight</span>
<span id="cb7-1958"><a href="#cb7-1958"></a><span class="ex">[2024-10-16</span> 15:43:14.256546]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.18.self_attn.v_proj.weight</span>
<span id="cb7-1959"><a href="#cb7-1959"></a><span class="ex">[2024-10-16</span> 15:43:14.259362]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.18.self_attn.o_proj.weight</span>
<span id="cb7-1960"><a href="#cb7-1960"></a><span class="ex">[2024-10-16</span> 15:43:14.266006]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.18.mlp.gate_proj.weight</span>
<span id="cb7-1961"><a href="#cb7-1961"></a><span class="ex">[2024-10-16</span> 15:43:14.272677]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.18.mlp.up_proj.weight</span>
<span id="cb7-1962"><a href="#cb7-1962"></a><span class="ex">[2024-10-16</span> 15:43:14.279406]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.18.mlp.down_proj.weight</span>
<span id="cb7-1963"><a href="#cb7-1963"></a><span class="ex">[2024-10-16</span> 15:43:14.280055]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.18.input_layernorm.weight</span>
<span id="cb7-1964"><a href="#cb7-1964"></a><span class="ex">[2024-10-16</span> 15:43:14.280566]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.18.post_attention_layernorm.weight</span>
<span id="cb7-1965"><a href="#cb7-1965"></a><span class="ex">[2024-10-16</span> 15:43:14.283295]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.19.self_attn.q_proj.weight</span>
<span id="cb7-1966"><a href="#cb7-1966"></a><span class="ex">[2024-10-16</span> 15:43:14.284803]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.19.self_attn.k_proj.weight</span>
<span id="cb7-1967"><a href="#cb7-1967"></a><span class="ex">[2024-10-16</span> 15:43:14.286350]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.19.self_attn.v_proj.weight</span>
<span id="cb7-1968"><a href="#cb7-1968"></a><span class="ex">[2024-10-16</span> 15:43:14.289142]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.19.self_attn.o_proj.weight</span>
<span id="cb7-1969"><a href="#cb7-1969"></a><span class="ex">[2024-10-16</span> 15:43:14.295818]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.19.mlp.gate_proj.weight</span>
<span id="cb7-1970"><a href="#cb7-1970"></a><span class="ex">[2024-10-16</span> 15:43:14.302488]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.19.mlp.up_proj.weight</span>
<span id="cb7-1971"><a href="#cb7-1971"></a><span class="ex">[2024-10-16</span> 15:43:14.309098]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.19.mlp.down_proj.weight</span>
<span id="cb7-1972"><a href="#cb7-1972"></a><span class="ex">[2024-10-16</span> 15:43:14.309731]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.19.input_layernorm.weight</span>
<span id="cb7-1973"><a href="#cb7-1973"></a><span class="ex">[2024-10-16</span> 15:43:14.310234]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.19.post_attention_layernorm.weight</span>
<span id="cb7-1974"><a href="#cb7-1974"></a><span class="ex">[2024-10-16</span> 15:43:14.312927]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.20.self_attn.q_proj.weight</span>
<span id="cb7-1975"><a href="#cb7-1975"></a><span class="ex">[2024-10-16</span> 15:43:14.314505]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.20.self_attn.k_proj.weight</span>
<span id="cb7-1976"><a href="#cb7-1976"></a><span class="ex">[2024-10-16</span> 15:43:14.315992]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.20.self_attn.v_proj.weight</span>
<span id="cb7-1977"><a href="#cb7-1977"></a><span class="ex">[2024-10-16</span> 15:43:14.318788]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.20.self_attn.o_proj.weight</span>
<span id="cb7-1978"><a href="#cb7-1978"></a><span class="ex">[2024-10-16</span> 15:43:14.325390]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.20.mlp.gate_proj.weight</span>
<span id="cb7-1979"><a href="#cb7-1979"></a><span class="ex">[2024-10-16</span> 15:43:14.332020]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.20.mlp.up_proj.weight</span>
<span id="cb7-1980"><a href="#cb7-1980"></a><span class="ex">[2024-10-16</span> 15:43:14.338682]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.20.mlp.down_proj.weight</span>
<span id="cb7-1981"><a href="#cb7-1981"></a><span class="ex">[2024-10-16</span> 15:43:14.339334]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.20.input_layernorm.weight</span>
<span id="cb7-1982"><a href="#cb7-1982"></a><span class="ex">[2024-10-16</span> 15:43:14.339845]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.20.post_attention_layernorm.weight</span>
<span id="cb7-1983"><a href="#cb7-1983"></a><span class="ex">[2024-10-16</span> 15:43:14.342562]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.21.self_attn.q_proj.weight</span>
<span id="cb7-1984"><a href="#cb7-1984"></a><span class="ex">[2024-10-16</span> 15:43:14.344113]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.21.self_attn.k_proj.weight</span>
<span id="cb7-1985"><a href="#cb7-1985"></a><span class="ex">[2024-10-16</span> 15:43:14.345593]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.21.self_attn.v_proj.weight</span>
<span id="cb7-1986"><a href="#cb7-1986"></a><span class="ex">[2024-10-16</span> 15:43:14.348370]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.21.self_attn.o_proj.weight</span>
<span id="cb7-1987"><a href="#cb7-1987"></a><span class="ex">[2024-10-16</span> 15:43:14.355167]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.21.mlp.gate_proj.weight</span>
<span id="cb7-1988"><a href="#cb7-1988"></a><span class="ex">[2024-10-16</span> 15:43:14.361823]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.21.mlp.up_proj.weight</span>
<span id="cb7-1989"><a href="#cb7-1989"></a><span class="ex">[2024-10-16</span> 15:43:14.368428]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.21.mlp.down_proj.weight</span>
<span id="cb7-1990"><a href="#cb7-1990"></a><span class="ex">[2024-10-16</span> 15:43:14.369055]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.21.input_layernorm.weight</span>
<span id="cb7-1991"><a href="#cb7-1991"></a><span class="ex">[2024-10-16</span> 15:43:14.369558]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.21.post_attention_layernorm.weight</span>
<span id="cb7-1992"><a href="#cb7-1992"></a><span class="ex">[2024-10-16</span> 15:43:14.372269]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.22.self_attn.q_proj.weight</span>
<span id="cb7-1993"><a href="#cb7-1993"></a><span class="ex">[2024-10-16</span> 15:43:14.373830]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.22.self_attn.k_proj.weight</span>
<span id="cb7-1994"><a href="#cb7-1994"></a><span class="ex">[2024-10-16</span> 15:43:14.375316]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.22.self_attn.v_proj.weight</span>
<span id="cb7-1995"><a href="#cb7-1995"></a><span class="ex">[2024-10-16</span> 15:43:14.378084]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.22.self_attn.o_proj.weight</span>
<span id="cb7-1996"><a href="#cb7-1996"></a><span class="ex">[2024-10-16</span> 15:43:14.384700]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.22.mlp.gate_proj.weight</span>
<span id="cb7-1997"><a href="#cb7-1997"></a><span class="ex">[2024-10-16</span> 15:43:14.391366]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.22.mlp.up_proj.weight</span>
<span id="cb7-1998"><a href="#cb7-1998"></a><span class="ex">[2024-10-16</span> 15:43:14.398053]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.22.mlp.down_proj.weight</span>
<span id="cb7-1999"><a href="#cb7-1999"></a><span class="ex">[2024-10-16</span> 15:43:14.398695]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.22.input_layernorm.weight</span>
<span id="cb7-2000"><a href="#cb7-2000"></a><span class="ex">[2024-10-16</span> 15:43:14.399206]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.22.post_attention_layernorm.weight</span>
<span id="cb7-2001"><a href="#cb7-2001"></a><span class="ex">[2024-10-16</span> 15:43:14.401929]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.23.self_attn.q_proj.weight</span>
<span id="cb7-2002"><a href="#cb7-2002"></a><span class="ex">[2024-10-16</span> 15:43:14.403487]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.23.self_attn.k_proj.weight</span>
<span id="cb7-2003"><a href="#cb7-2003"></a><span class="ex">[2024-10-16</span> 15:43:14.404961]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.23.self_attn.v_proj.weight</span>
<span id="cb7-2004"><a href="#cb7-2004"></a><span class="ex">[2024-10-16</span> 15:43:14.407720]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.23.self_attn.o_proj.weight</span>
<span id="cb7-2005"><a href="#cb7-2005"></a><span class="ex">[2024-10-16</span> 15:43:14.414356]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.23.mlp.gate_proj.weight</span>
<span id="cb7-2006"><a href="#cb7-2006"></a><span class="ex">[2024-10-16</span> 15:43:14.421000]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.23.mlp.up_proj.weight</span>
<span id="cb7-2007"><a href="#cb7-2007"></a><span class="ex">[2024-10-16</span> 15:43:14.427610]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.23.mlp.down_proj.weight</span>
<span id="cb7-2008"><a href="#cb7-2008"></a><span class="ex">[2024-10-16</span> 15:43:14.428269]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.23.input_layernorm.weight</span>
<span id="cb7-2009"><a href="#cb7-2009"></a><span class="ex">[2024-10-16</span> 15:43:14.428783]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.23.post_attention_layernorm.weight</span>
<span id="cb7-2010"><a href="#cb7-2010"></a><span class="ex">[2024-10-16</span> 15:43:14.431461]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.24.self_attn.q_proj.weight</span>
<span id="cb7-2011"><a href="#cb7-2011"></a><span class="ex">[2024-10-16</span> 15:43:14.432995]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.24.self_attn.k_proj.weight</span>
<span id="cb7-2012"><a href="#cb7-2012"></a><span class="ex">[2024-10-16</span> 15:43:14.434507]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.24.self_attn.v_proj.weight</span>
<span id="cb7-2013"><a href="#cb7-2013"></a><span class="ex">[2024-10-16</span> 15:43:14.437268]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.24.self_attn.o_proj.weight</span>
<span id="cb7-2014"><a href="#cb7-2014"></a><span class="ex">[2024-10-16</span> 15:43:14.443850]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.24.mlp.gate_proj.weight</span>
<span id="cb7-2015"><a href="#cb7-2015"></a><span class="ex">[2024-10-16</span> 15:43:14.450632]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.24.mlp.up_proj.weight</span>
<span id="cb7-2016"><a href="#cb7-2016"></a><span class="ex">[2024-10-16</span> 15:43:14.457242]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.24.mlp.down_proj.weight</span>
<span id="cb7-2017"><a href="#cb7-2017"></a><span class="ex">[2024-10-16</span> 15:43:14.457890]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.24.input_layernorm.weight</span>
<span id="cb7-2018"><a href="#cb7-2018"></a><span class="ex">[2024-10-16</span> 15:43:14.458409]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.24.post_attention_layernorm.weight</span>
<span id="cb7-2019"><a href="#cb7-2019"></a><span class="ex">[2024-10-16</span> 15:43:14.461063]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.25.self_attn.q_proj.weight</span>
<span id="cb7-2020"><a href="#cb7-2020"></a><span class="ex">[2024-10-16</span> 15:43:14.462620]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.25.self_attn.k_proj.weight</span>
<span id="cb7-2021"><a href="#cb7-2021"></a><span class="ex">[2024-10-16</span> 15:43:14.464102]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.25.self_attn.v_proj.weight</span>
<span id="cb7-2022"><a href="#cb7-2022"></a><span class="ex">[2024-10-16</span> 15:43:14.466871]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.25.self_attn.o_proj.weight</span>
<span id="cb7-2023"><a href="#cb7-2023"></a><span class="ex">[2024-10-16</span> 15:43:14.473435]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.25.mlp.gate_proj.weight</span>
<span id="cb7-2024"><a href="#cb7-2024"></a><span class="ex">[2024-10-16</span> 15:43:14.480017]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.25.mlp.up_proj.weight</span>
<span id="cb7-2025"><a href="#cb7-2025"></a><span class="ex">[2024-10-16</span> 15:43:14.486605]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.25.mlp.down_proj.weight</span>
<span id="cb7-2026"><a href="#cb7-2026"></a><span class="ex">[2024-10-16</span> 15:43:14.487227]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.25.input_layernorm.weight</span>
<span id="cb7-2027"><a href="#cb7-2027"></a><span class="ex">[2024-10-16</span> 15:43:14.487743]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.25.post_attention_layernorm.weight</span>
<span id="cb7-2028"><a href="#cb7-2028"></a><span class="ex">[2024-10-16</span> 15:43:14.490427]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.26.self_attn.q_proj.weight</span>
<span id="cb7-2029"><a href="#cb7-2029"></a><span class="ex">[2024-10-16</span> 15:43:14.491946]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.26.self_attn.k_proj.weight</span>
<span id="cb7-2030"><a href="#cb7-2030"></a><span class="ex">[2024-10-16</span> 15:43:14.493433]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.26.self_attn.v_proj.weight</span>
<span id="cb7-2031"><a href="#cb7-2031"></a><span class="ex">[2024-10-16</span> 15:43:14.496192]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.26.self_attn.o_proj.weight</span>
<span id="cb7-2032"><a href="#cb7-2032"></a><span class="ex">[2024-10-16</span> 15:43:14.502792]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.26.mlp.gate_proj.weight</span>
<span id="cb7-2033"><a href="#cb7-2033"></a><span class="ex">[2024-10-16</span> 15:43:14.509329]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.26.mlp.up_proj.weight</span>
<span id="cb7-2034"><a href="#cb7-2034"></a><span class="ex">[2024-10-16</span> 15:43:14.515980]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.26.mlp.down_proj.weight</span>
<span id="cb7-2035"><a href="#cb7-2035"></a><span class="ex">[2024-10-16</span> 15:43:14.516659]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.26.input_layernorm.weight</span>
<span id="cb7-2036"><a href="#cb7-2036"></a><span class="ex">[2024-10-16</span> 15:43:14.517200]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.26.post_attention_layernorm.weight</span>
<span id="cb7-2037"><a href="#cb7-2037"></a><span class="ex">[2024-10-16</span> 15:43:14.519874]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.27.self_attn.q_proj.weight</span>
<span id="cb7-2038"><a href="#cb7-2038"></a><span class="ex">[2024-10-16</span> 15:43:14.521415]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.27.self_attn.k_proj.weight</span>
<span id="cb7-2039"><a href="#cb7-2039"></a><span class="ex">[2024-10-16</span> 15:43:14.522879]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.27.self_attn.v_proj.weight</span>
<span id="cb7-2040"><a href="#cb7-2040"></a><span class="ex">[2024-10-16</span> 15:43:14.525620]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.27.self_attn.o_proj.weight</span>
<span id="cb7-2041"><a href="#cb7-2041"></a><span class="ex">[2024-10-16</span> 15:43:14.532202]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.27.mlp.gate_proj.weight</span>
<span id="cb7-2042"><a href="#cb7-2042"></a><span class="ex">[2024-10-16</span> 15:43:14.538768]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.27.mlp.up_proj.weight</span>
<span id="cb7-2043"><a href="#cb7-2043"></a><span class="ex">[2024-10-16</span> 15:43:14.545303]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.27.mlp.down_proj.weight</span>
<span id="cb7-2044"><a href="#cb7-2044"></a><span class="ex">[2024-10-16</span> 15:43:14.545921]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.27.input_layernorm.weight</span>
<span id="cb7-2045"><a href="#cb7-2045"></a><span class="ex">[2024-10-16</span> 15:43:14.546440]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.27.post_attention_layernorm.weight</span>
<span id="cb7-2046"><a href="#cb7-2046"></a><span class="ex">[2024-10-16</span> 15:43:14.549101]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.28.self_attn.q_proj.weight</span>
<span id="cb7-2047"><a href="#cb7-2047"></a><span class="ex">[2024-10-16</span> 15:43:14.550596]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.28.self_attn.k_proj.weight</span>
<span id="cb7-2048"><a href="#cb7-2048"></a><span class="ex">[2024-10-16</span> 15:43:14.552114]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.28.self_attn.v_proj.weight</span>
<span id="cb7-2049"><a href="#cb7-2049"></a><span class="ex">[2024-10-16</span> 15:43:14.554821]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.28.self_attn.o_proj.weight</span>
<span id="cb7-2050"><a href="#cb7-2050"></a><span class="ex">[2024-10-16</span> 15:43:14.561373]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.28.mlp.gate_proj.weight</span>
<span id="cb7-2051"><a href="#cb7-2051"></a><span class="ex">[2024-10-16</span> 15:43:14.567945]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.28.mlp.up_proj.weight</span>
<span id="cb7-2052"><a href="#cb7-2052"></a><span class="ex">[2024-10-16</span> 15:43:14.574713]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.28.mlp.down_proj.weight</span>
<span id="cb7-2053"><a href="#cb7-2053"></a><span class="ex">[2024-10-16</span> 15:43:14.575333]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.28.input_layernorm.weight</span>
<span id="cb7-2054"><a href="#cb7-2054"></a><span class="ex">[2024-10-16</span> 15:43:14.575833]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.28.post_attention_layernorm.weight</span>
<span id="cb7-2055"><a href="#cb7-2055"></a><span class="ex">[2024-10-16</span> 15:43:14.578469]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.29.self_attn.q_proj.weight</span>
<span id="cb7-2056"><a href="#cb7-2056"></a><span class="ex">[2024-10-16</span> 15:43:14.580020]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.29.self_attn.k_proj.weight</span>
<span id="cb7-2057"><a href="#cb7-2057"></a><span class="ex">[2024-10-16</span> 15:43:14.581498]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.29.self_attn.v_proj.weight</span>
<span id="cb7-2058"><a href="#cb7-2058"></a><span class="ex">[2024-10-16</span> 15:43:14.584217]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.29.self_attn.o_proj.weight</span>
<span id="cb7-2059"><a href="#cb7-2059"></a><span class="ex">[2024-10-16</span> 15:43:14.590850]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.29.mlp.gate_proj.weight</span>
<span id="cb7-2060"><a href="#cb7-2060"></a><span class="ex">[2024-10-16</span> 15:43:14.597375]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.29.mlp.up_proj.weight</span>
<span id="cb7-2061"><a href="#cb7-2061"></a><span class="ex">[2024-10-16</span> 15:43:14.603899]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.29.mlp.down_proj.weight</span>
<span id="cb7-2062"><a href="#cb7-2062"></a><span class="ex">[2024-10-16</span> 15:43:14.604548]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.29.input_layernorm.weight</span>
<span id="cb7-2063"><a href="#cb7-2063"></a><span class="ex">[2024-10-16</span> 15:43:14.605067]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.29.post_attention_layernorm.weight</span>
<span id="cb7-2064"><a href="#cb7-2064"></a><span class="ex">[2024-10-16</span> 15:43:14.607694]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.30.self_attn.q_proj.weight</span>
<span id="cb7-2065"><a href="#cb7-2065"></a><span class="ex">[2024-10-16</span> 15:43:14.609232]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.30.self_attn.k_proj.weight</span>
<span id="cb7-2066"><a href="#cb7-2066"></a><span class="ex">[2024-10-16</span> 15:43:14.610733]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.30.self_attn.v_proj.weight</span>
<span id="cb7-2067"><a href="#cb7-2067"></a><span class="ex">[2024-10-16</span> 15:43:14.613448]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.30.self_attn.o_proj.weight</span>
<span id="cb7-2068"><a href="#cb7-2068"></a><span class="ex">[2024-10-16</span> 15:43:14.619991]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.30.mlp.gate_proj.weight</span>
<span id="cb7-2069"><a href="#cb7-2069"></a><span class="ex">[2024-10-16</span> 15:43:14.626559]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.30.mlp.up_proj.weight</span>
<span id="cb7-2070"><a href="#cb7-2070"></a><span class="ex">[2024-10-16</span> 15:43:14.633070]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.30.mlp.down_proj.weight</span>
<span id="cb7-2071"><a href="#cb7-2071"></a><span class="ex">[2024-10-16</span> 15:43:14.633733]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.30.input_layernorm.weight</span>
<span id="cb7-2072"><a href="#cb7-2072"></a><span class="ex">[2024-10-16</span> 15:43:14.634259]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.30.post_attention_layernorm.weight</span>
<span id="cb7-2073"><a href="#cb7-2073"></a><span class="ex">[2024-10-16</span> 15:43:14.636877]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.31.self_attn.q_proj.weight</span>
<span id="cb7-2074"><a href="#cb7-2074"></a><span class="ex">[2024-10-16</span> 15:43:14.638487]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.31.self_attn.k_proj.weight</span>
<span id="cb7-2075"><a href="#cb7-2075"></a><span class="ex">[2024-10-16</span> 15:43:14.639954]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.31.self_attn.v_proj.weight</span>
<span id="cb7-2076"><a href="#cb7-2076"></a><span class="ex">[2024-10-16</span> 15:43:14.642672]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.31.self_attn.o_proj.weight</span>
<span id="cb7-2077"><a href="#cb7-2077"></a><span class="ex">[2024-10-16</span> 15:43:14.649190]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.31.mlp.gate_proj.weight</span>
<span id="cb7-2078"><a href="#cb7-2078"></a><span class="ex">[2024-10-16</span> 15:43:14.655715]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.31.mlp.up_proj.weight</span>
<span id="cb7-2079"><a href="#cb7-2079"></a><span class="ex">[2024-10-16</span> 15:43:14.662234]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.31.mlp.down_proj.weight</span>
<span id="cb7-2080"><a href="#cb7-2080"></a><span class="ex">[2024-10-16</span> 15:43:14.662858]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.31.input_layernorm.weight</span>
<span id="cb7-2081"><a href="#cb7-2081"></a><span class="ex">[2024-10-16</span> 15:43:14.663389]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.31.post_attention_layernorm.weight</span>
<span id="cb7-2082"><a href="#cb7-2082"></a><span class="ex">[2024-10-16</span> 15:43:14.663915]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.norm.weight</span>
<span id="cb7-2083"><a href="#cb7-2083"></a><span class="ex">[2024-10-16</span> 15:43:14,693] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb7-2084"><a href="#cb7-2084"></a><span class="ex">[2024-10-16</span> 15:43:14.713565]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> lm_head.weight</span>
<span id="cb7-2085"><a href="#cb7-2085"></a><span class="ex">[2024-10-16</span> 15:43:14.714574]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:504</span><span class="pp">]</span> <span class="at">-</span> before deepspeed init</span>
<span id="cb7-2086"><a href="#cb7-2086"></a><span class="ex">[2024-10-16</span> 15:43:14,715] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">logging.py:129:log_dist</span><span class="pp">]</span> [Rank 0] DeepSpeed info: version=0.15.3+unknown, git-hash=unknown, git-branch=unknown</span>
<span id="cb7-2087"><a href="#cb7-2087"></a><span class="ex">[2024-10-16</span> 15:43:14,715] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb7-2088"><a href="#cb7-2088"></a><span class="ex">Loading</span> checkpoint shards: 100%<span class="kw">|</span><span class="ex">██████████</span><span class="kw">|</span> <span class="ex">7/7</span> [03:57<span class="op">&lt;</span>00:00, 33.97s/it]</span>
<span id="cb7-2089"><a href="#cb7-2089"></a><span class="ex">[2024-10-16</span> 15:43:36,676] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb7-2090"><a href="#cb7-2090"></a><span class="ex">Loading</span> checkpoint shards: 100%<span class="kw">|</span><span class="ex">██████████</span><span class="kw">|</span> <span class="ex">7/7</span> [04:00<span class="op">&lt;</span>00:00, 34.36s/it]</span>
<span id="cb7-2091"><a href="#cb7-2091"></a><span class="ex">Loading</span> checkpoint shards: 100%<span class="kw">|</span><span class="ex">██████████</span><span class="kw">|</span> <span class="ex">7/7</span> [04:00<span class="op">&lt;</span>00:00, 34.38s/it]</span>
<span id="cb7-2092"><a href="#cb7-2092"></a><span class="ex">Loading</span> checkpoint shards: 100%<span class="kw">|</span><span class="ex">██████████</span><span class="kw">|</span> <span class="ex">7/7</span> [04:00<span class="op">&lt;</span>00:00, 34.40s/it]</span>
<span id="cb7-2093"><a href="#cb7-2093"></a><span class="ex">Loading</span> checkpoint shards: 100%<span class="kw">|</span><span class="ex">██████████</span><span class="kw">|</span> <span class="ex">7/7</span> [04:00<span class="op">&lt;</span>00:00, 34.39s/it]</span>
<span id="cb7-2094"><a href="#cb7-2094"></a><span class="ex">Loading</span> checkpoint shards: 100%<span class="kw">|</span><span class="ex">██████████</span><span class="kw">|</span> <span class="ex">7/7</span> [04:00<span class="op">&lt;</span>00:00, 34.41s/it]</span>
<span id="cb7-2095"><a href="#cb7-2095"></a><span class="ex">Loading</span> checkpoint shards: 100%<span class="kw">|</span><span class="ex">██████████</span><span class="kw">|</span> <span class="ex">7/7</span> [04:01<span class="op">&lt;</span>00:00, 34.43s/it]</span>
<span id="cb7-2096"><a href="#cb7-2096"></a><span class="ex">Loading</span> checkpoint shards: 100%<span class="kw">|</span><span class="ex">██████████</span><span class="kw">|</span> <span class="ex">7/7</span> [04:00<span class="op">&lt;</span>00:00, 34.40s/it]</span>
<span id="cb7-2097"><a href="#cb7-2097"></a><span class="ex">Loading</span> checkpoint shards: 100%<span class="kw">|</span><span class="ex">██████████</span><span class="kw">|</span> <span class="ex">7/7</span> [04:00<span class="op">&lt;</span>00:00, 34.41s/it]</span>
<span id="cb7-2098"><a href="#cb7-2098"></a><span class="ex">Loading</span> checkpoint shards: 100%<span class="kw">|</span><span class="ex">██████████</span><span class="kw">|</span> <span class="ex">7/7</span> [04:00<span class="op">&lt;</span>00:00, 34.41s/it]</span>
<span id="cb7-2099"><a href="#cb7-2099"></a><span class="ex">Loading</span> checkpoint shards: 100%<span class="kw">|</span><span class="ex">██████████</span><span class="kw">|</span> <span class="ex">7/7</span> [04:01<span class="op">&lt;</span>00:00, 34.44s/it]</span>
<span id="cb7-2100"><a href="#cb7-2100"></a><span class="ex">Loading</span> checkpoint shards: 100%<span class="kw">|</span><span class="ex">██████████</span><span class="kw">|</span> <span class="ex">7/7</span> [04:01<span class="op">&lt;</span>00:00, 34.44s/it]</span>
<span id="cb7-2101"><a href="#cb7-2101"></a><span class="ex">Loading</span> checkpoint shards: 100%<span class="kw">|</span><span class="ex">██████████</span><span class="kw">|</span> <span class="ex">7/7</span> [04:01<span class="op">&lt;</span>00:00, 34.46s/it]</span>
<span id="cb7-2102"><a href="#cb7-2102"></a><span class="ex">Loading</span> checkpoint shards: 100%<span class="kw">|</span><span class="ex">██████████</span><span class="kw">|</span> <span class="ex">7/7</span> [04:01<span class="op">&lt;</span>00:00, 34.44s/it]</span>
<span id="cb7-2103"><a href="#cb7-2103"></a><span class="ex">Loading</span> checkpoint shards: 100%<span class="kw">|</span><span class="ex">██████████</span><span class="kw">|</span> <span class="ex">7/7</span> [04:01<span class="op">&lt;</span>00:00, 34.43s/it]</span>
<span id="cb7-2104"><a href="#cb7-2104"></a><span class="ex">Loading</span> checkpoint shards: 100%<span class="kw">|</span><span class="ex">██████████</span><span class="kw">|</span> <span class="ex">7/7</span> [04:01<span class="op">&lt;</span>00:00, 34.47s/it]</span>
<span id="cb7-2105"><a href="#cb7-2105"></a><span class="ex">Loading</span> checkpoint shards: 100%<span class="kw">|</span><span class="ex">██████████</span><span class="kw">|</span> <span class="ex">7/7</span> [04:01<span class="op">&lt;</span>00:00, 34.44s/it]</span>
<span id="cb7-2106"><a href="#cb7-2106"></a><span class="ex">Loading</span> checkpoint shards: 100%<span class="kw">|</span><span class="ex">██████████</span><span class="kw">|</span> <span class="ex">7/7</span> [04:01<span class="op">&lt;</span>00:00, 34.47s/it]</span>
<span id="cb7-2107"><a href="#cb7-2107"></a><span class="ex">Loading</span> checkpoint shards: 100%<span class="kw">|</span><span class="ex">██████████</span><span class="kw">|</span> <span class="ex">7/7</span> [04:01<span class="op">&lt;</span>00:00, 34.44s/it]</span>
<span id="cb7-2108"><a href="#cb7-2108"></a><span class="ex">Loading</span> checkpoint shards: 100%<span class="kw">|</span><span class="ex">██████████</span><span class="kw">|</span> <span class="ex">7/7</span> [03:58<span class="op">&lt;</span>00:00, 34.12s/it]</span>
<span id="cb7-2109"><a href="#cb7-2109"></a><span class="ex">Loading</span> checkpoint shards: 100%<span class="kw">|</span><span class="ex">██████████</span><span class="kw">|</span> <span class="ex">7/7</span> [04:01<span class="op">&lt;</span>00:00, 34.47s/it]</span>
<span id="cb7-2110"><a href="#cb7-2110"></a><span class="ex">Loading</span> checkpoint shards: 100%<span class="kw">|</span><span class="ex">██████████</span><span class="kw">|</span> <span class="ex">7/7</span> [04:01<span class="op">&lt;</span>00:00, 34.51s/it]</span>
<span id="cb7-2111"><a href="#cb7-2111"></a><span class="ex">[2024-10-16</span> 15:43:39,455] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb7-2112"><a href="#cb7-2112"></a><span class="ex">[2024-10-16</span> 15:43:39,666] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb7-2113"><a href="#cb7-2113"></a><span class="ex">[2024-10-16</span> 15:43:39,739] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb7-2114"><a href="#cb7-2114"></a><span class="ex">[2024-10-16</span> 15:43:39,801] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb7-2115"><a href="#cb7-2115"></a><span class="ex">[2024-10-16</span> 15:43:39,842] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb7-2116"><a href="#cb7-2116"></a><span class="ex">[2024-10-16</span> 15:43:39,860] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb7-2117"><a href="#cb7-2117"></a><span class="ex">[2024-10-16</span> 15:43:39,862] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb7-2118"><a href="#cb7-2118"></a><span class="ex">[2024-10-16</span> 15:43:39,881] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb7-2119"><a href="#cb7-2119"></a><span class="ex">[2024-10-16</span> 15:43:39,920] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb7-2120"><a href="#cb7-2120"></a><span class="ex">[2024-10-16</span> 15:43:40,138] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb7-2121"><a href="#cb7-2121"></a><span class="ex">[2024-10-16</span> 15:43:40,153] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb7-2122"><a href="#cb7-2122"></a><span class="ex">[2024-10-16</span> 15:43:40,174] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb7-2123"><a href="#cb7-2123"></a><span class="ex">[2024-10-16</span> 15:43:40,175] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb7-2124"><a href="#cb7-2124"></a><span class="ex">[2024-10-16</span> 15:43:40,205] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb7-2125"><a href="#cb7-2125"></a><span class="ex">[2024-10-16</span> 15:43:40,212] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb7-2126"><a href="#cb7-2126"></a><span class="ex">[2024-10-16</span> 15:43:40,224] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb7-2127"><a href="#cb7-2127"></a><span class="ex">[2024-10-16</span> 15:43:40,251] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb7-2128"><a href="#cb7-2128"></a><span class="ex">[2024-10-16</span> 15:43:40,255] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb7-2129"><a href="#cb7-2129"></a><span class="ex">[2024-10-16</span> 15:43:40,256] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb7-2130"><a href="#cb7-2130"></a><span class="ex">[2024-10-16</span> 15:43:40,417] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb7-2131"><a href="#cb7-2131"></a><span class="ex">[2024-10-16</span> 15:43:40,538] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb7-2132"><a href="#cb7-2132"></a><span class="ex">[2024-10-16</span> 15:43:56,920] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">logging.py:129:log_dist</span><span class="pp">]</span> [Rank 0] DeepSpeed Flops Profiler Enabled: False</span>
<span id="cb7-2133"><a href="#cb7-2133"></a><span class="ex">[2024-10-16</span> 15:43:56,921] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">logging.py:129:log_dist</span><span class="pp">]</span> [Rank 0] Creating BF16 optimizer</span>
<span id="cb7-2134"><a href="#cb7-2134"></a><span class="ex">[2024-10-16</span> 15:43:57,118] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">utils.py:781:see_memory_usage</span><span class="pp">]</span> begin bf16_optimizer</span>
<span id="cb7-2135"><a href="#cb7-2135"></a><span class="ex">[2024-10-16</span> 15:43:57,118] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">utils.py:782:see_memory_usage</span><span class="pp">]</span> MA 14.96 GB         Max_MA 14.96 GB         CA 14.96 GB         Max_CA 15 GB</span>
<span id="cb7-2136"><a href="#cb7-2136"></a><span class="ex">[2024-10-16</span> 15:43:57,118] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">utils.py:789:see_memory_usage</span><span class="pp">]</span> CPU Virtual Memory:  used = 490.74 GB, percent = 43.3%</span>
<span id="cb7-2137"><a href="#cb7-2137"></a><span class="ex">[2024-10-16</span> 15:43:57,290] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">utils.py:781:see_memory_usage</span><span class="pp">]</span> end bf16_ optimizer</span>
<span id="cb7-2138"><a href="#cb7-2138"></a><span class="ex">[2024-10-16</span> 15:43:57,291] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">utils.py:782:see_memory_usage</span><span class="pp">]</span> MA 14.96 GB         Max_MA 14.96 GB         CA 14.96 GB         Max_CA 15 GB</span>
<span id="cb7-2139"><a href="#cb7-2139"></a><span class="ex">[2024-10-16</span> 15:43:57,291] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">utils.py:789:see_memory_usage</span><span class="pp">]</span> CPU Virtual Memory:  used = 490.74 GB, percent = 43.3%</span>
<span id="cb7-2140"><a href="#cb7-2140"></a><span class="ex">[2024-10-16</span> 15:43:57,291] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:999:print</span><span class="pp">]</span> DeepSpeedEngine configuration:</span>
<span id="cb7-2141"><a href="#cb7-2141"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   activation_checkpointing_config  {</span>
<span id="cb7-2142"><a href="#cb7-2142"></a>    <span class="st">"partition_activations"</span><span class="ex">:</span> false,</span>
<span id="cb7-2143"><a href="#cb7-2143"></a>    <span class="st">"contiguous_memory_optimization"</span><span class="ex">:</span> false,</span>
<span id="cb7-2144"><a href="#cb7-2144"></a>    <span class="st">"cpu_checkpointing"</span><span class="ex">:</span> false,</span>
<span id="cb7-2145"><a href="#cb7-2145"></a>    <span class="st">"number_checkpoints"</span><span class="ex">:</span> null,</span>
<span id="cb7-2146"><a href="#cb7-2146"></a>    <span class="st">"synchronize_checkpoint_boundary"</span><span class="ex">:</span> false,</span>
<span id="cb7-2147"><a href="#cb7-2147"></a>    <span class="st">"profile"</span><span class="ex">:</span> false</span>
<span id="cb7-2148"><a href="#cb7-2148"></a><span class="er">}</span></span>
<span id="cb7-2149"><a href="#cb7-2149"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   aio_config ................... {<span class="st">'block_size'</span>: 1048576, <span class="st">'queue_depth'</span>: 8, <span class="st">'thread_count'</span>: 1, <span class="st">'single_submit'</span>: False, <span class="st">'overlap_events'</span>: True, <span class="st">'use_gds'</span>: False}</span>
<span id="cb7-2150"><a href="#cb7-2150"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   amp_enabled .................. False</span>
<span id="cb7-2151"><a href="#cb7-2151"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   amp_params ................... False</span>
<span id="cb7-2152"><a href="#cb7-2152"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   autotuning_config ............ {</span>
<span id="cb7-2153"><a href="#cb7-2153"></a>    <span class="st">"enabled"</span><span class="ex">:</span> false,</span>
<span id="cb7-2154"><a href="#cb7-2154"></a>    <span class="st">"start_step"</span><span class="ex">:</span> null,</span>
<span id="cb7-2155"><a href="#cb7-2155"></a>    <span class="st">"end_step"</span><span class="ex">:</span> null,</span>
<span id="cb7-2156"><a href="#cb7-2156"></a>    <span class="st">"metric_path"</span><span class="ex">:</span> null,</span>
<span id="cb7-2157"><a href="#cb7-2157"></a>    <span class="st">"arg_mappings"</span><span class="ex">:</span> null,</span>
<span id="cb7-2158"><a href="#cb7-2158"></a>    <span class="st">"metric"</span><span class="ex">:</span> <span class="st">"throughput"</span>,</span>
<span id="cb7-2159"><a href="#cb7-2159"></a>    <span class="st">"model_info"</span><span class="ex">:</span> null,</span>
<span id="cb7-2160"><a href="#cb7-2160"></a>    <span class="st">"results_dir"</span><span class="ex">:</span> <span class="st">"autotuning_results"</span>,</span>
<span id="cb7-2161"><a href="#cb7-2161"></a>    <span class="st">"exps_dir"</span><span class="ex">:</span> <span class="st">"autotuning_exps"</span>,</span>
<span id="cb7-2162"><a href="#cb7-2162"></a>    <span class="st">"overwrite"</span><span class="ex">:</span> true,</span>
<span id="cb7-2163"><a href="#cb7-2163"></a>    <span class="st">"fast"</span><span class="ex">:</span> true,</span>
<span id="cb7-2164"><a href="#cb7-2164"></a>    <span class="st">"start_profile_step"</span><span class="ex">:</span> 3,</span>
<span id="cb7-2165"><a href="#cb7-2165"></a>    <span class="st">"end_profile_step"</span><span class="ex">:</span> 5,</span>
<span id="cb7-2166"><a href="#cb7-2166"></a>    <span class="st">"tuner_type"</span><span class="ex">:</span> <span class="st">"gridsearch"</span>,</span>
<span id="cb7-2167"><a href="#cb7-2167"></a>    <span class="st">"tuner_early_stopping"</span><span class="ex">:</span> 5,</span>
<span id="cb7-2168"><a href="#cb7-2168"></a>    <span class="st">"tuner_num_trials"</span><span class="ex">:</span> 50,</span>
<span id="cb7-2169"><a href="#cb7-2169"></a>    <span class="st">"model_info_path"</span><span class="ex">:</span> null,</span>
<span id="cb7-2170"><a href="#cb7-2170"></a>    <span class="st">"mp_size"</span><span class="ex">:</span> 1,</span>
<span id="cb7-2171"><a href="#cb7-2171"></a>    <span class="st">"max_train_batch_size"</span><span class="ex">:</span> null,</span>
<span id="cb7-2172"><a href="#cb7-2172"></a>    <span class="st">"min_train_batch_size"</span><span class="ex">:</span> 1,</span>
<span id="cb7-2173"><a href="#cb7-2173"></a>    <span class="st">"max_train_micro_batch_size_per_gpu"</span><span class="ex">:</span> 1.024000e+03,</span>
<span id="cb7-2174"><a href="#cb7-2174"></a>    <span class="st">"min_train_micro_batch_size_per_gpu"</span><span class="ex">:</span> 1,</span>
<span id="cb7-2175"><a href="#cb7-2175"></a>    <span class="st">"num_tuning_micro_batch_sizes"</span><span class="ex">:</span> 3</span>
<span id="cb7-2176"><a href="#cb7-2176"></a><span class="er">}</span></span>
<span id="cb7-2177"><a href="#cb7-2177"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   bfloat16_enabled ............. True</span>
<span id="cb7-2178"><a href="#cb7-2178"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   bfloat16_immediate_grad_update  False</span>
<span id="cb7-2179"><a href="#cb7-2179"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   checkpoint_parallel_write_pipeline  False</span>
<span id="cb7-2180"><a href="#cb7-2180"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   checkpoint_tag_validation_enabled  True</span>
<span id="cb7-2181"><a href="#cb7-2181"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   checkpoint_tag_validation_fail  False</span>
<span id="cb7-2182"><a href="#cb7-2182"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   comms_config ................. <span class="op">&lt;</span>deepspeed.comm.config.DeepSpeedCommsConfig object at 0x145ea0815900<span class="op">&gt;</span></span>
<span id="cb7-2183"><a href="#cb7-2183"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   communication_data_type ...... None</span>
<span id="cb7-2184"><a href="#cb7-2184"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   compression_config ........... {<span class="st">'weight_quantization'</span>: {<span class="st">'shared_parameters'</span>: {<span class="st">'enabled'</span>: False, <span class="st">'quantizer_kernel'</span>: False, <span class="st">'schedule_offset'</span>: 0, <span class="st">'quantize_groups'</span>: 1, <span class="st">'quantize_verbose'</span>: False, <span class="st">'quantization_type'</span>: <span class="st">'symmetric'</span>, <span class="st">'quantize_weight_in_forward'</span>: False, <span class="st">'rounding'</span>: <span class="st">'nearest'</span>, <span class="st">'fp16_mixed_quantize'</span>: False, <span class="st">'quantize_change_ratio'</span>: 0.001}, <span class="st">'different_groups'</span>: {}}, <span class="st">'activation_quantization'</span>: {<span class="st">'shared_parameters'</span>: {<span class="st">'enabled'</span>: False, <span class="st">'quantization_type'</span>: <span class="st">'symmetric'</span>, <span class="st">'range_calibration'</span>: <span class="st">'dynamic'</span>, <span class="st">'schedule_offset'</span>: 1000}, <span class="st">'different_groups'</span>: {}}, <span class="st">'sparse_pruning'</span>: {<span class="st">'shared_parameters'</span>: {<span class="st">'enabled'</span>: False, <span class="st">'method'</span>: <span class="st">'l1'</span>, <span class="st">'schedule_offset'</span>: 1000}, <span class="st">'different_groups'</span>: {}}, <span class="st">'row_pruning'</span>: {<span class="st">'shared_parameters'</span>: {<span class="st">'enabled'</span>: False, <span class="st">'method'</span>: <span class="st">'l1'</span>, <span class="st">'schedule_offset'</span>: 1000}, <span class="st">'different_groups'</span>: {}}, <span class="st">'head_pruning'</span>: {<span class="st">'shared_parameters'</span>: {<span class="st">'enabled'</span>: False, <span class="st">'method'</span>: <span class="st">'topk'</span>, <span class="st">'schedule_offset'</span>: 1000}, <span class="st">'different_groups'</span>: {}}, <span class="st">'channel_pruning'</span>: {<span class="st">'shared_parameters'</span>: {<span class="st">'enabled'</span>: False, <span class="st">'method'</span>: <span class="st">'l1'</span>, <span class="st">'schedule_offset'</span>: 1000}, <span class="st">'different_groups'</span>: {}}, <span class="st">'layer_reduction'</span>: {<span class="st">'enabled'</span>: False}}</span>
<span id="cb7-2185"><a href="#cb7-2185"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   curriculum_enabled_legacy .... False</span>
<span id="cb7-2186"><a href="#cb7-2186"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   curriculum_params_legacy ..... False</span>
<span id="cb7-2187"><a href="#cb7-2187"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   data_efficiency_config ....... {<span class="st">'enabled'</span>: False, <span class="st">'seed'</span>: 1234, <span class="st">'data_sampling'</span>: {<span class="st">'enabled'</span>: False, <span class="st">'num_epochs'</span>: 1000, <span class="st">'num_workers'</span>: 0, <span class="st">'curriculum_learning'</span>: {<span class="st">'enabled'</span>: False}}, <span class="st">'data_routing'</span>: {<span class="st">'enabled'</span>: False, <span class="st">'random_ltd'</span>: {<span class="st">'enabled'</span>: False, <span class="st">'layer_token_lr_schedule'</span>: {<span class="st">'enabled'</span>: False}}}}</span>
<span id="cb7-2188"><a href="#cb7-2188"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   data_efficiency_enabled ...... False</span>
<span id="cb7-2189"><a href="#cb7-2189"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   dataloader_drop_last ......... False</span>
<span id="cb7-2190"><a href="#cb7-2190"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   disable_allgather ............ False</span>
<span id="cb7-2191"><a href="#cb7-2191"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   dump_state ................... False</span>
<span id="cb7-2192"><a href="#cb7-2192"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   dynamic_loss_scale_args ...... None</span>
<span id="cb7-2193"><a href="#cb7-2193"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   eigenvalue_enabled ........... False</span>
<span id="cb7-2194"><a href="#cb7-2194"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   eigenvalue_gas_boundary_resolution  1</span>
<span id="cb7-2195"><a href="#cb7-2195"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   eigenvalue_layer_name ........ bert.encoder.layer</span>
<span id="cb7-2196"><a href="#cb7-2196"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   eigenvalue_layer_num ......... 0</span>
<span id="cb7-2197"><a href="#cb7-2197"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   eigenvalue_max_iter .......... 100</span>
<span id="cb7-2198"><a href="#cb7-2198"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   eigenvalue_stability ......... 1e-06</span>
<span id="cb7-2199"><a href="#cb7-2199"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   eigenvalue_tol ............... 0.01</span>
<span id="cb7-2200"><a href="#cb7-2200"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   eigenvalue_verbose ........... False</span>
<span id="cb7-2201"><a href="#cb7-2201"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   elasticity_enabled ........... False</span>
<span id="cb7-2202"><a href="#cb7-2202"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   flops_profiler_config ........ {</span>
<span id="cb7-2203"><a href="#cb7-2203"></a>    <span class="st">"enabled"</span><span class="ex">:</span> false,</span>
<span id="cb7-2204"><a href="#cb7-2204"></a>    <span class="st">"recompute_fwd_factor"</span><span class="ex">:</span> 0.0,</span>
<span id="cb7-2205"><a href="#cb7-2205"></a>    <span class="st">"profile_step"</span><span class="ex">:</span> 1,</span>
<span id="cb7-2206"><a href="#cb7-2206"></a>    <span class="st">"module_depth"</span><span class="ex">:</span> <span class="at">-1,</span></span>
<span id="cb7-2207"><a href="#cb7-2207"></a>    <span class="st">"top_modules"</span><span class="ex">:</span> 1,</span>
<span id="cb7-2208"><a href="#cb7-2208"></a>    <span class="st">"detailed"</span><span class="ex">:</span> true,</span>
<span id="cb7-2209"><a href="#cb7-2209"></a>    <span class="st">"output_file"</span><span class="ex">:</span> null</span>
<span id="cb7-2210"><a href="#cb7-2210"></a><span class="er">}</span></span>
<span id="cb7-2211"><a href="#cb7-2211"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   fp16_auto_cast ............... None</span>
<span id="cb7-2212"><a href="#cb7-2212"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   fp16_enabled ................. False</span>
<span id="cb7-2213"><a href="#cb7-2213"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   fp16_master_weights_and_gradients  False</span>
<span id="cb7-2214"><a href="#cb7-2214"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   global_rank .................. 0</span>
<span id="cb7-2215"><a href="#cb7-2215"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   grad_accum_dtype ............. None</span>
<span id="cb7-2216"><a href="#cb7-2216"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   gradient_accumulation_steps .. 1</span>
<span id="cb7-2217"><a href="#cb7-2217"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   gradient_clipping ............ 0.0</span>
<span id="cb7-2218"><a href="#cb7-2218"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   gradient_predivide_factor .... 1.0</span>
<span id="cb7-2219"><a href="#cb7-2219"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   graph_harvesting ............. False</span>
<span id="cb7-2220"><a href="#cb7-2220"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8</span>
<span id="cb7-2221"><a href="#cb7-2221"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   initial_dynamic_scale ........ 1</span>
<span id="cb7-2222"><a href="#cb7-2222"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   load_universal_checkpoint .... False</span>
<span id="cb7-2223"><a href="#cb7-2223"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   loss_scale ................... 1.0</span>
<span id="cb7-2224"><a href="#cb7-2224"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   memory_breakdown ............. False</span>
<span id="cb7-2225"><a href="#cb7-2225"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   mics_hierarchial_params_gather  False</span>
<span id="cb7-2226"><a href="#cb7-2226"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   mics_shard_size .............. <span class="at">-1</span></span>
<span id="cb7-2227"><a href="#cb7-2227"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   monitor_config ............... tensorboard=TensorBoardConfig<span class="er">(</span><span class="va">enabled</span><span class="op">=</span>False, <span class="va">output_path</span><span class="op">=</span><span class="st">''</span>, <span class="va">job_name</span><span class="op">=</span><span class="st">'DeepSpeedJobName'</span><span class="kw">)</span> <span class="va">comet</span><span class="op">=</span>CometConfig<span class="kw">(</span><span class="va">enabled</span><span class="op">=</span>False, <span class="va">samples_log_interval</span><span class="op">=</span>100, <span class="va">project</span><span class="op">=</span>None, <span class="va">workspace</span><span class="op">=</span>None, <span class="va">api_key</span><span class="op">=</span>None, <span class="va">experiment_name</span><span class="op">=</span>None, <span class="va">experiment_key</span><span class="op">=</span>None, <span class="va">online</span><span class="op">=</span>None, <span class="va">mode</span><span class="op">=</span>None<span class="kw">)</span> <span class="va">wandb</span><span class="op">=</span>WandbConfig<span class="kw">(</span><span class="va">enabled</span><span class="op">=</span>False, <span class="va">group</span><span class="op">=</span>None, <span class="va">team</span><span class="op">=</span>None, <span class="va">project</span><span class="op">=</span><span class="st">'deepspeed'</span><span class="kw">)</span> <span class="va">csv_monitor</span><span class="op">=</span>CSVConfig<span class="kw">(</span><span class="va">enabled</span><span class="op">=</span>False, <span class="va">output_path</span><span class="op">=</span><span class="st">''</span>, <span class="va">job_name</span><span class="op">=</span><span class="st">'DeepSpeedJobName'</span><span class="kw">)</span></span>
<span id="cb7-2228"><a href="#cb7-2228"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   nebula_config ................ {</span>
<span id="cb7-2229"><a href="#cb7-2229"></a>    <span class="st">"enabled"</span><span class="ex">:</span> false,</span>
<span id="cb7-2230"><a href="#cb7-2230"></a>    <span class="st">"persistent_storage_path"</span><span class="ex">:</span> null,</span>
<span id="cb7-2231"><a href="#cb7-2231"></a>    <span class="st">"persistent_time_interval"</span><span class="ex">:</span> 100,</span>
<span id="cb7-2232"><a href="#cb7-2232"></a>    <span class="st">"num_of_version_in_retention"</span><span class="ex">:</span> 2,</span>
<span id="cb7-2233"><a href="#cb7-2233"></a>    <span class="st">"enable_nebula_load"</span><span class="ex">:</span> true,</span>
<span id="cb7-2234"><a href="#cb7-2234"></a>    <span class="st">"load_path"</span><span class="ex">:</span> null</span>
<span id="cb7-2235"><a href="#cb7-2235"></a><span class="er">}</span></span>
<span id="cb7-2236"><a href="#cb7-2236"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   optimizer_legacy_fusion ...... False</span>
<span id="cb7-2237"><a href="#cb7-2237"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   optimizer_name ............... None</span>
<span id="cb7-2238"><a href="#cb7-2238"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   optimizer_params ............. None</span>
<span id="cb7-2239"><a href="#cb7-2239"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   pipeline ..................... {<span class="st">'stages'</span>: <span class="st">'auto'</span>, <span class="st">'partition'</span>: <span class="st">'best'</span>, <span class="st">'seed_layers'</span>: False, <span class="st">'activation_checkpoint_interval'</span>: 0, <span class="st">'pipe_partitioned'</span>: True, <span class="st">'grad_partitioned'</span>: True}</span>
<span id="cb7-2240"><a href="#cb7-2240"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   pld_enabled .................. False</span>
<span id="cb7-2241"><a href="#cb7-2241"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   pld_params ................... False</span>
<span id="cb7-2242"><a href="#cb7-2242"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   prescale_gradients ........... False</span>
<span id="cb7-2243"><a href="#cb7-2243"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   scheduler_name ............... None</span>
<span id="cb7-2244"><a href="#cb7-2244"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   scheduler_params ............. None</span>
<span id="cb7-2245"><a href="#cb7-2245"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   seq_parallel_communication_data_type  torch.float32</span>
<span id="cb7-2246"><a href="#cb7-2246"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   sparse_attention ............. None</span>
<span id="cb7-2247"><a href="#cb7-2247"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   sparse_gradients_enabled ..... False</span>
<span id="cb7-2248"><a href="#cb7-2248"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   steps_per_print .............. 100</span>
<span id="cb7-2249"><a href="#cb7-2249"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   timers_config ................ enabled=True synchronized=True</span>
<span id="cb7-2250"><a href="#cb7-2250"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   train_batch_size ............. 24</span>
<span id="cb7-2251"><a href="#cb7-2251"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   train_micro_batch_size_per_gpu  1</span>
<span id="cb7-2252"><a href="#cb7-2252"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   use_data_before_expert_parallel_  False</span>
<span id="cb7-2253"><a href="#cb7-2253"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   use_node_local_storage ....... False</span>
<span id="cb7-2254"><a href="#cb7-2254"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   wall_clock_breakdown ......... False</span>
<span id="cb7-2255"><a href="#cb7-2255"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   weight_quantization_config ... None</span>
<span id="cb7-2256"><a href="#cb7-2256"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   world_size ................... 24</span>
<span id="cb7-2257"><a href="#cb7-2257"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   zero_allow_untested_optimizer  False</span>
<span id="cb7-2258"><a href="#cb7-2258"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True</span>
<span id="cb7-2259"><a href="#cb7-2259"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   zero_enabled ................. False</span>
<span id="cb7-2260"><a href="#cb7-2260"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   zero_force_ds_cpu_optimizer .. True</span>
<span id="cb7-2261"><a href="#cb7-2261"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   zero_optimization_stage ...... 0</span>
<span id="cb7-2262"><a href="#cb7-2262"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:989:print_user_config</span><span class="pp">]</span>   json = {</span>
<span id="cb7-2263"><a href="#cb7-2263"></a>    <span class="st">"train_batch_size"</span><span class="ex">:</span> 24,</span>
<span id="cb7-2264"><a href="#cb7-2264"></a>    <span class="st">"train_micro_batch_size_per_gpu"</span><span class="ex">:</span> 1,</span>
<span id="cb7-2265"><a href="#cb7-2265"></a>    <span class="st">"steps_per_print"</span><span class="ex">:</span> 100,</span>
<span id="cb7-2266"><a href="#cb7-2266"></a>    <span class="st">"zero_optimization"</span><span class="ex">:</span> {</span>
<span id="cb7-2267"><a href="#cb7-2267"></a>        <span class="st">"stage"</span><span class="ex">:</span> 0</span>
<span id="cb7-2268"><a href="#cb7-2268"></a>    <span class="er">}</span><span class="ex">,</span></span>
<span id="cb7-2269"><a href="#cb7-2269"></a>    <span class="st">"bf16"</span><span class="ex">:</span> {</span>
<span id="cb7-2270"><a href="#cb7-2270"></a>        <span class="st">"enabled"</span><span class="ex">:</span> true</span>
<span id="cb7-2271"><a href="#cb7-2271"></a>    <span class="er">}</span></span>
<span id="cb7-2272"><a href="#cb7-2272"></a><span class="er">}</span></span>
<span id="cb7-2273"><a href="#cb7-2273"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:105:__init__</span><span class="pp">]</span> CONFIG: micro_batches=1 micro_batch_size=1</span>
<span id="cb7-2274"><a href="#cb7-2274"></a><span class="ex">[2024-10-16</span> 15:43:57,294] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:146:__init__</span><span class="pp">]</span> is_pipe_partitioned= False is_grad_partitioned= False</span>
<span id="cb7-2275"><a href="#cb7-2275"></a><span class="ex">[2024-10-16</span> 15:43:57,298] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:165:__init__</span><span class="pp">]</span> RANK=0 STAGE=0 LAYERS=37 [0, 37<span class="kw">)</span> <span class="va">STAGE_PARAMS</span><span class="op">=</span>8032358400 <span class="kw">(</span><span class="ex">8032.358M</span><span class="kw">)</span> <span class="va">TOTAL_PARAMS</span><span class="op">=</span>8032358400 <span class="kw">(</span><span class="ex">8032.358M</span><span class="kw">)</span> <span class="va">UNIQUE_PARAMS</span><span class="op">=</span>8032358400 <span class="kw">(</span><span class="ex">8032.358M</span><span class="kw">)</span></span>
<span id="cb7-2276"><a href="#cb7-2276"></a><span class="ex">[2024-10-16</span> 15:43:57.298751]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:511</span><span class="pp">]</span> <span class="at">-</span> after deepspeed init</span>
<span id="cb7-2277"><a href="#cb7-2277"></a><span class="ex">[2024-10-16</span> 15:43:57.299527]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:162</span><span class="pp">]</span> <span class="at">-</span> hf_w.shape<span class="pp">[</span><span class="ss">0</span><span class="pp">]</span>=128512</span>
<span id="cb7-2278"><a href="#cb7-2278"></a><span class="ex">[2024-10-16</span> 15:43:57.299951]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:163</span><span class="pp">]</span> <span class="at">-</span> self.token_vocab=128000</span>
<span id="cb7-2279"><a href="#cb7-2279"></a><span class="ex">[2024-10-16</span> 15:43:57,373] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:146:__init__</span><span class="pp">]</span> is_pipe_partitioned= False is_grad_partitioned= False</span>
<span id="cb7-2280"><a href="#cb7-2280"></a><span class="ex">[2024-10-16</span> 15:43:57,410] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:146:__init__</span><span class="pp">]</span> is_pipe_partitioned= False is_grad_partitioned= False</span>
<span id="cb7-2281"><a href="#cb7-2281"></a><span class="ex">[2024-10-16</span> 15:43:57,525] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:146:__init__</span><span class="pp">]</span> is_pipe_partitioned= False is_grad_partitioned= False</span>
<span id="cb7-2282"><a href="#cb7-2282"></a>                                     <span class="ex">[--hidden-size</span> HIDDEN_SIZE]</span>
<span id="cb7-2283"><a href="#cb7-2283"></a><span class="ex">!!!</span> ATTENTION !!!</span>
<span id="cb7-2284"><a href="#cb7-2284"></a>                                     <span class="ex">[--num-attention-heads</span> NUM_ATTENTION_HEADS]</span>
<span id="cb7-2285"><a href="#cb7-2285"></a><span class="ex">Type</span> <span class="st">'up'</span> to get to the frame that called dist.breakpoint<span class="er">(</span><span class="va">rank</span><span class="op">=</span>0<span class="kw">)</span></span>
<span id="cb7-2286"><a href="#cb7-2286"></a>                                     <span class="ex">[--kv-channels</span> KV_CHANNELS]</span>
<span id="cb7-2287"><a href="#cb7-2287"></a><span class="op">&gt;</span> /opt/aurora/24.180.0/frameworks/aurora_nre_models_frameworks-2024.2.1_u1/lib/python3.10/site-packages/torch/distributed/__init__.py<span class="kw">(</span><span class="ex">89</span><span class="kw">)</span><span class="fu">breakpoint()</span></span>
<span id="cb7-2288"><a href="#cb7-2288"></a><span class="ex">-</span><span class="op">&gt;</span> barrier<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-2289"><a href="#cb7-2289"></a><span class="kw">(</span><span class="ex">Pdb</span><span class="kw">)</span> <span class="ex">[2024-10-16</span> 15:43:57,531] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:146:__init__</span><span class="pp">]</span> is_pipe_partitioned= False is_grad_partitioned= False</span>
<span id="cb7-2290"><a href="#cb7-2290"></a><span class="ex">[2024-10-16</span> 15:43:57,539] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:146:__init__</span><span class="pp">]</span> is_pipe_partitioned= False is_grad_partitioned= False</span>
<span id="cb7-2291"><a href="#cb7-2291"></a><span class="ex">[2024-10-16</span> 15:43:57,613] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:146:__init__</span><span class="pp">]</span> is_pipe_partitioned= False is_grad_partitioned= False</span>
<span id="cb7-2292"><a href="#cb7-2292"></a><span class="ex">[2024-10-16</span> 15:43:57,677] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:146:__init__</span><span class="pp">]</span> is_pipe_partitioned= False is_grad_partitioned= False</span>
<span id="cb7-2293"><a href="#cb7-2293"></a><span class="ex">[2024-10-16</span> 15:43:57,689] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:146:__init__</span><span class="pp">]</span> is_pipe_partitioned= False is_grad_partitioned= False</span>
<span id="cb7-2294"><a href="#cb7-2294"></a><span class="ex">[2024-10-16</span> 15:43:57,689] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:146:__init__</span><span class="pp">]</span> is_pipe_partitioned= False is_grad_partitioned= False</span>
<span id="cb7-2295"><a href="#cb7-2295"></a><span class="ex">[2024-10-16</span> 15:43:57,694] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:146:__init__</span><span class="pp">]</span> is_pipe_partitioned= False is_grad_partitioned= False</span>
<span id="cb7-2296"><a href="#cb7-2296"></a><span class="ex">[2024-10-16</span> 15:43:57,694] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:146:__init__</span><span class="pp">]</span> is_pipe_partitioned= False is_grad_partitioned= False</span>
<span id="cb7-2297"><a href="#cb7-2297"></a><span class="ex">[2024-10-16</span> 15:43:57,733] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:146:__init__</span><span class="pp">]</span> is_pipe_partitioned= False is_grad_partitioned= False</span>
<span id="cb7-2298"><a href="#cb7-2298"></a><span class="ex">[2024-10-16</span> 15:43:57,768] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:146:__init__</span><span class="pp">]</span> is_pipe_partitioned= False is_grad_partitioned= False</span>
<span id="cb7-2299"><a href="#cb7-2299"></a><span class="ex">[2024-10-16</span> 15:43:57,769] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:146:__init__</span><span class="pp">]</span> is_pipe_partitioned= False is_grad_partitioned= False</span>
<span id="cb7-2300"><a href="#cb7-2300"></a><span class="ex">[2024-10-16</span> 15:43:57,823] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:146:__init__</span><span class="pp">]</span> is_pipe_partitioned= False is_grad_partitioned= False</span>
<span id="cb7-2301"><a href="#cb7-2301"></a><span class="ex">[2024-10-16</span> 15:43:57,857] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:146:__init__</span><span class="pp">]</span> is_pipe_partitioned= False is_grad_partitioned= False</span>
<span id="cb7-2302"><a href="#cb7-2302"></a><span class="ex">[2024-10-16</span> 15:43:57,869] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:146:__init__</span><span class="pp">]</span> is_pipe_partitioned= False is_grad_partitioned= False</span>
<span id="cb7-2303"><a href="#cb7-2303"></a><span class="ex">[2024-10-16</span> 15:43:57,876] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:146:__init__</span><span class="pp">]</span> is_pipe_partitioned= False is_grad_partitioned= False</span>
<span id="cb7-2304"><a href="#cb7-2304"></a><span class="ex">[2024-10-16</span> 15:43:57,876] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:146:__init__</span><span class="pp">]</span> is_pipe_partitioned= False is_grad_partitioned= False</span>
<span id="cb7-2305"><a href="#cb7-2305"></a><span class="ex">[2024-10-16</span> 15:43:58,057] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:146:__init__</span><span class="pp">]</span> is_pipe_partitioned= False is_grad_partitioned= False</span>
<span id="cb7-2306"><a href="#cb7-2306"></a><span class="ex">[2024-10-16</span> 15:43:58,058] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:146:__init__</span><span class="pp">]</span> is_pipe_partitioned= False is_grad_partitioned= False</span>
<span id="cb7-2307"><a href="#cb7-2307"></a><span class="ex">[2024-10-16</span> 15:43:58,102] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:146:__init__</span><span class="pp">]</span> is_pipe_partitioned= False is_grad_partitioned= False</span>
<span id="cb7-2308"><a href="#cb7-2308"></a><span class="ex">[2024-10-16</span> 15:43:58,102] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:146:__init__</span><span class="pp">]</span> is_pipe_partitioned= False is_grad_partitioned= False</span>
<span id="cb7-2309"><a href="#cb7-2309"></a>                                     <span class="ex">[--ffn-hidden-size</span> FFN_HIDDEN_SIZE]</span>
<span id="cb7-2310"><a href="#cb7-2310"></a><span class="kw">(</span><span class="ex">Pdb</span><span class="kw">)</span> <span class="ex">l</span></span>
<span id="cb7-2311"><a href="#cb7-2311"></a> <span class="ex">84</span>                pdb.message<span class="er">(</span></span>
<span id="cb7-2312"><a href="#cb7-2312"></a> <span class="ex">85</span>                    <span class="st">"\n!!! ATTENTION !!!\n\n"</span></span>
<span id="cb7-2313"><a href="#cb7-2313"></a> <span class="ex">86</span>                    f<span class="st">"Type 'up' to get to the frame that called dist.breakpoint(rank={rank})\n"</span></span>
<span id="cb7-2314"><a href="#cb7-2314"></a> <span class="ex">87</span>                <span class="kw">)</span></span>
<span id="cb7-2315"><a href="#cb7-2315"></a> <span class="ex">88</span>                pdb.set_trace<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-2316"><a href="#cb7-2316"></a> <span class="ex">89</span>  <span class="at">-</span><span class="op">&gt;</span>        barrier<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-2317"><a href="#cb7-2317"></a> <span class="ex">90</span>     </span>
<span id="cb7-2318"><a href="#cb7-2318"></a> <span class="ex">91</span>        if sys.platform != <span class="st">"win32"</span>:</span>
<span id="cb7-2319"><a href="#cb7-2319"></a> <span class="ex">92</span>            from torch._C._distributed_c10d import <span class="er">(</span></span>
<span id="cb7-2320"><a href="#cb7-2320"></a> <span class="ex">93</span>                HashStore,</span>
<span id="cb7-2321"><a href="#cb7-2321"></a> <span class="ex">94</span>                _round_robin_process_groups,</span>
<span id="cb7-2322"><a href="#cb7-2322"></a><span class="kw">(</span><span class="ex">Pdb</span><span class="kw">)</span> <span class="ex">ll</span></span>
<span id="cb7-2323"><a href="#cb7-2323"></a> <span class="ex">74</span>        def breakpoint<span class="er">(</span><span class="ex">rank:</span> int = 0<span class="kw">)</span><span class="bu">:</span></span>
<span id="cb7-2324"><a href="#cb7-2324"></a> <span class="ex">75</span>            <span class="st">"""</span></span>
<span id="cb7-2325"><a href="#cb7-2325"></a><span class="st"> 76            Set a breakpoint, but only on a single rank.  All other ranks will wait for you to be</span></span>
<span id="cb7-2326"><a href="#cb7-2326"></a><span class="st"> 77            done with the breakpoint before continuing.</span></span>
<span id="cb7-2327"><a href="#cb7-2327"></a><span class="st"> 78     </span></span>
<span id="cb7-2328"><a href="#cb7-2328"></a><span class="st"> 79            Args:</span></span>
<span id="cb7-2329"><a href="#cb7-2329"></a><span class="st"> 80                rank (int): Which rank to break on.  Default: </span><span class="kw">``</span><span class="st">0</span><span class="kw">``</span></span>
<span id="cb7-2330"><a href="#cb7-2330"></a><span class="st"> 81            """</span></span>
<span id="cb7-2331"><a href="#cb7-2331"></a> <span class="ex">82</span>            if get_rank<span class="er">(</span><span class="kw">)</span> <span class="ex">==</span> rank:</span>
<span id="cb7-2332"><a href="#cb7-2332"></a> <span class="ex">83</span>                pdb = _DistributedPdb<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-2333"><a href="#cb7-2333"></a> <span class="ex">84</span>                pdb.message<span class="er">(</span></span>
<span id="cb7-2334"><a href="#cb7-2334"></a> <span class="ex">85</span>                    <span class="st">"\n!!! ATTENTION !!!\n\n"</span></span>
<span id="cb7-2335"><a href="#cb7-2335"></a> <span class="ex">86</span>                    f<span class="st">"Type 'up' to get to the frame that called dist.breakpoint(rank={rank})\n"</span></span>
<span id="cb7-2336"><a href="#cb7-2336"></a> <span class="ex">87</span>                <span class="kw">)</span></span>
<span id="cb7-2337"><a href="#cb7-2337"></a> <span class="ex">88</span>                pdb.set_trace<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-2338"><a href="#cb7-2338"></a> <span class="ex">89</span>  <span class="at">-</span><span class="op">&gt;</span>        barrier<span class="er">(</span><span class="kw">)</span></span>
<span id="cb7-2339"><a href="#cb7-2339"></a><span class="kw">(</span><span class="ex">Pdb</span><span class="kw">)</span> <span class="ex">up</span></span>
<span id="cb7-2340"><a href="#cb7-2340"></a><span class="op">&gt;</span> /lus/flare/projects/Aurora_deployment/foremans/projects/argonne-lcf/Megatron-DeepSpeed/tools/hf2megads_weight_converter.py<span class="kw">(</span><span class="ex">224</span><span class="kw">)</span><span class="fu">_qkv_refactor()</span></span>
<span id="cb7-2341"><a href="#cb7-2341"></a><span class="ex">-</span><span class="op">&gt;</span> torch.distributed.breakpoint<span class="er">(</span><span class="ex">0</span><span class="kw">)</span></span>
<span id="cb7-2342"><a href="#cb7-2342"></a><span class="kw">(</span><span class="ex">Pdb</span><span class="kw">)</span> <span class="ex">ll</span></span>
<span id="cb7-2343"><a href="#cb7-2343"></a><span class="ex">194</span>        def _qkv_refactor<span class="er">(</span><span class="ex">self,</span> pname, p, hf_layer<span class="kw">)</span><span class="bu">:</span></span>
<span id="cb7-2344"><a href="#cb7-2344"></a><span class="ex">195</span>            hf_wq_name = f<span class="st">"model.layers.{hf_layer}.self_attn.q_proj.weight"</span></span>
<span id="cb7-2345"><a href="#cb7-2345"></a><span class="ex">196</span>            hf_wk_name = f<span class="st">"model.layers.{hf_layer}.self_attn.k_proj.weight"</span></span>
<span id="cb7-2346"><a href="#cb7-2346"></a><span class="ex">197</span>            hf_wv_name = f<span class="st">"model.layers.{hf_layer}.self_attn.v_proj.weight"</span></span>
<span id="cb7-2347"><a href="#cb7-2347"></a><span class="ex">198</span>            wq = self.hf_model<span class="pp">[</span><span class="ss">hf_wq_name</span><span class="pp">]</span></span>
<span id="cb7-2348"><a href="#cb7-2348"></a><span class="ex">199</span>            wk = self.hf_model<span class="pp">[</span><span class="ss">hf_wk_name</span><span class="pp">]</span></span>
<span id="cb7-2349"><a href="#cb7-2349"></a><span class="ex">200</span>            wv = self.hf_model<span class="pp">[</span><span class="ss">hf_wv_name</span><span class="pp">]</span></span>
<span id="cb7-2350"><a href="#cb7-2350"></a><span class="ex">201</span>     </span>
<span id="cb7-2351"><a href="#cb7-2351"></a><span class="ex">202</span>            hidden_size = wq.shape<span class="pp">[</span><span class="ss">0</span><span class="pp">]</span></span>
<span id="cb7-2352"><a href="#cb7-2352"></a><span class="ex">203</span>            per_partition_size, start_index, end_index = compute_partition_range<span class="er">(</span></span>
<span id="cb7-2353"><a href="#cb7-2353"></a><span class="ex">204</span>                hidden_size, self.tp_rank, self.tp_size<span class="kw">)</span></span>
<span id="cb7-2354"><a href="#cb7-2354"></a><span class="ex">205</span>            hidden_size_per_attention_head = divide<span class="er">(</span><span class="ex">hidden_size,</span></span>
<span id="cb7-2355"><a href="#cb7-2355"></a><span class="ex">206</span>                                                    self.config.num_attention_heads<span class="kw">)</span></span>
<span id="cb7-2356"><a href="#cb7-2356"></a><span class="ex">207</span>            num_attention_heads_per_partition = divide<span class="er">(</span><span class="ex">self.config.num_attention_heads,</span></span>
<span id="cb7-2357"><a href="#cb7-2357"></a><span class="ex">208</span>                                                       self.tp_size<span class="kw">)</span></span>
<span id="cb7-2358"><a href="#cb7-2358"></a><span class="ex">209</span>     </span>
<span id="cb7-2359"><a href="#cb7-2359"></a><span class="ex">210</span>            new_w = torch.zeros<span class="er">(</span><span class="kw">(</span><span class="ex">per_partition_size</span> <span class="pp">*</span> 3, wq.shape<span class="pp">[</span><span class="ss">1</span><span class="pp">]</span><span class="kw">)</span><span class="ex">,</span> dtype=wq.dtype<span class="kw">)</span></span>
<span id="cb7-2360"><a href="#cb7-2360"></a><span class="ex">211</span>     </span>
<span id="cb7-2361"><a href="#cb7-2361"></a><span class="ex">212</span>            for i in range<span class="er">(</span><span class="ex">num_attention_heads_per_partition</span><span class="kw">)</span><span class="bu">:</span></span>
<span id="cb7-2362"><a href="#cb7-2362"></a><span class="ex">213</span>                try:</span>
<span id="cb7-2363"><a href="#cb7-2363"></a><span class="ex">214</span>                    current_index = start_index + i <span class="pp">*</span> hidden_size_per_attention_head</span>
<span id="cb7-2364"><a href="#cb7-2364"></a><span class="ex">215</span>                    next_index = current_index + hidden_size_per_attention_head</span>
<span id="cb7-2365"><a href="#cb7-2365"></a><span class="ex">216</span>                    new_w_index = i <span class="pp">*</span> <span class="er">(</span><span class="ex">3</span> <span class="pp">*</span> hidden_size_per_attention_head<span class="kw">)</span></span>
<span id="cb7-2366"><a href="#cb7-2366"></a><span class="ex">217</span>                    new_w[new_w_index: new_w_index + <span class="er">(</span><span class="ex">3</span> <span class="pp">*</span> hidden_size_per_attention_head<span class="kw">)</span><span class="ex">,</span> :] = <span class="dt">\</span></span>
<span id="cb7-2367"><a href="#cb7-2367"></a>218                        torch.cat<span class="er">(</span><span class="bu">[</span></span>
<span id="cb7-2368"><a href="#cb7-2368"></a>219                            wq[current_index: next_index, <span class="er">:],</span></span>
<span id="cb7-2369"><a href="#cb7-2369"></a><span class="ex">220</span>                            wk[current_index: next_index, :],</span>
<span id="cb7-2370"><a href="#cb7-2370"></a><span class="ex">221</span>                            wv[current_index: next_index, :]</span>
<span id="cb7-2371"><a href="#cb7-2371"></a><span class="ex">222</span>                        ], dim=0<span class="kw">)</span></span>
<span id="cb7-2372"><a href="#cb7-2372"></a><span class="ex">223</span>                except Exception:</span>
<span id="cb7-2373"><a href="#cb7-2373"></a><span class="ex">224</span>  <span class="at">-</span><span class="op">&gt;</span>                torch.distributed.breakpoint<span class="er">(</span><span class="ex">0</span><span class="kw">)</span></span>
<span id="cb7-2374"><a href="#cb7-2374"></a><span class="ex">225</span>            self.record_mapping_info<span class="er">(</span></span>
<span id="cb7-2375"><a href="#cb7-2375"></a><span class="ex">226</span>                f<span class="st">"mega-ds:{pname,p.data.shape}&lt;--hf{hf_wq_name,hf_wk_name,hf_wv_name,}  cat q,k,v [{current_index}:{next_index},:]  of q,k,v{wq.shape}"</span></span>
<span id="cb7-2376"><a href="#cb7-2376"></a><span class="ex">227</span>            <span class="kw">)</span></span>
<span id="cb7-2377"><a href="#cb7-2377"></a><span class="ex">228</span>            return new_w</span>
<span id="cb7-2378"><a href="#cb7-2378"></a><span class="kw">(</span><span class="ex">Pdb</span><span class="kw">)</span> <span class="ex">current_index</span></span>
<span id="cb7-2379"><a href="#cb7-2379"></a><span class="ex">1024</span></span>
<span id="cb7-2380"><a href="#cb7-2380"></a><span class="kw">(</span><span class="ex">Pdb</span><span class="kw">)</span> <span class="ex">next_index</span></span>
<span id="cb7-2381"><a href="#cb7-2381"></a><span class="ex">1152</span></span>
<span id="cb7-2382"><a href="#cb7-2382"></a><span class="kw">(</span><span class="ex">Pdb</span><span class="kw">)</span> <span class="ex">new_w_index</span></span>
<span id="cb7-2383"><a href="#cb7-2383"></a><span class="ex">3072</span></span>
<span id="cb7-2384"><a href="#cb7-2384"></a><span class="kw">(</span><span class="ex">Pdb</span><span class="kw">)</span> <span class="ex">new_w.shape</span></span>
<span id="cb7-2385"><a href="#cb7-2385"></a><span class="ex">torch.Size</span><span class="er">(</span><span class="ex">[12288,</span> 4096]<span class="kw">)</span></span>
<span id="cb7-2386"><a href="#cb7-2386"></a><span class="kw">(</span><span class="ex">Pdb</span><span class="kw">)</span> <span class="ex">wq</span></span>
<span id="cb7-2387"><a href="#cb7-2387"></a><span class="ex">tensor</span><span class="er">(</span><span class="kw">[[</span> 0.0053, -0.0291, -0.0058,  <span class="er">...,</span>  <span class="ex">0.0095,</span> <span class="at">-0.0420,</span> <span class="at">-0.0272],</span></span>
<span id="cb7-2388"><a href="#cb7-2388"></a>        <span class="ex">[-0.0142,</span> <span class="at">-0.0679,</span> <span class="at">-0.0049,</span>  ..., <span class="at">-0.0142,</span> <span class="at">-0.0498,</span>  0.0192],</span>
<span id="cb7-2389"><a href="#cb7-2389"></a>        <span class="ex">[-0.0162,</span> <span class="at">-0.0393,</span> <span class="at">-0.0026,</span>  ...,  0.0115, <span class="at">-0.0126,</span>  0.0071],</span>
<span id="cb7-2390"><a href="#cb7-2390"></a>        <span class="ex">...,</span></span>
<span id="cb7-2391"><a href="#cb7-2391"></a>        <span class="ex">[-0.0039,</span> <span class="at">-0.0393,</span>  0.0806,  ...,  0.0061, <span class="at">-0.0013,</span>  0.0023],</span>
<span id="cb7-2392"><a href="#cb7-2392"></a>        <span class="ex">[-0.0035,</span> <span class="at">-0.0101,</span>  0.0459,  ...,  0.0049, <span class="at">-0.0011,</span>  0.0011],</span>
<span id="cb7-2393"><a href="#cb7-2393"></a>        <span class="ex">[-0.0018,</span> <span class="at">-0.0153,</span>  0.0347,  ...,  0.0110,  0.0004,  0.0044]],</span>
<span id="cb7-2394"><a href="#cb7-2394"></a>       <span class="va">dtype</span><span class="op">=</span>torch.bfloat16, <span class="va">grad_fn</span><span class="op">=&lt;</span>CloneBackward0<span class="op">&gt;</span><span class="kw">)</span></span>
<span id="cb7-2395"><a href="#cb7-2395"></a><span class="kw">(</span><span class="ex">Pdb</span><span class="kw">)</span> <span class="ex">wq.shape</span></span>
<span id="cb7-2396"><a href="#cb7-2396"></a><span class="ex">torch.Size</span><span class="er">(</span><span class="ex">[4096,</span> 4096]<span class="kw">)</span></span>
<span id="cb7-2397"><a href="#cb7-2397"></a><span class="kw">(</span><span class="ex">Pdb</span><span class="kw">)</span> <span class="ex">wk.shape</span></span>
<span id="cb7-2398"><a href="#cb7-2398"></a><span class="ex">torch.Size</span><span class="er">(</span><span class="ex">[1024,</span> 4096]<span class="kw">)</span></span>
<span id="cb7-2399"><a href="#cb7-2399"></a><span class="kw">(</span><span class="ex">Pdb</span><span class="kw">)</span> <span class="ex">wv.shape</span></span>
<span id="cb7-2400"><a href="#cb7-2400"></a><span class="ex">torch.Size</span><span class="er">(</span><span class="ex">[1024,</span> 4096]<span class="kw">)</span></span>
<span id="cb7-2401"><a href="#cb7-2401"></a><span class="kw">(</span><span class="ex">Pdb</span><span class="kw">)</span> <span class="ex">hidden_size</span></span>
<span id="cb7-2402"><a href="#cb7-2402"></a><span class="ex">4096</span></span>
<span id="cb7-2403"><a href="#cb7-2403"></a><span class="kw">(</span><span class="ex">Pdb</span><span class="kw">)</span> <span class="ex">per_partition_size</span></span>
<span id="cb7-2404"><a href="#cb7-2404"></a><span class="ex">4096</span></span>
<span id="cb7-2405"><a href="#cb7-2405"></a><span class="kw">(</span><span class="ex">Pdb</span><span class="kw">)</span> <span class="ex">num_attention_heads_per_partition</span></span>
<span id="cb7-2406"><a href="#cb7-2406"></a><span class="ex">32</span></span>
<span id="cb7-2407"><a href="#cb7-2407"></a><span class="kw">(</span><span class="ex">Pdb</span><span class="kw">)</span> <span class="ex">new_w.shape</span></span>
<span id="cb7-2408"><a href="#cb7-2408"></a><span class="ex">torch.Size</span><span class="er">(</span><span class="ex">[12288,</span> 4096]<span class="kw">)</span></span>
<span id="cb7-2409"><a href="#cb7-2409"></a><span class="kw">(</span><span class="ex">Pdb</span><span class="kw">)</span> <span class="ex">new_w.shape</span></span>
<span id="cb7-2410"><a href="#cb7-2410"></a><span class="ex">torch.Size</span><span class="er">(</span><span class="ex">[12288,</span> 4096]<span class="kw">)</span></span>
<span id="cb7-2411"><a href="#cb7-2411"></a><span class="kw">(</span><span class="ex">Pdb</span><span class="kw">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>



</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{foreman2024,
  author = {Foreman, Sam},
  title = {Converting {Checkpoints}},
  date = {2024-10-17},
  url = {https://samforeman.me/posts/AuroraGPT/checkpoints},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-foreman2024" class="csl-entry quarto-appendix-citeas" role="listitem">
Foreman, Sam. 2024. <span>“Converting Checkpoints.”</span> October 17,
2024. <a href="https://samforeman.me/posts/AuroraGPT/checkpoints">https://samforeman.me/posts/AuroraGPT/checkpoints</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = true;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/samforeman\.me");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb8" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb8-1"><a href="#cb8-1"></a><span class="co">---</span></span>
<span id="cb8-2"><a href="#cb8-2"></a><span class="an">title:</span><span class="co"> "💾 Converting Checkpoints"</span></span>
<span id="cb8-3"><a href="#cb8-3"></a><span class="an">toc:</span><span class="co"> true</span></span>
<span id="cb8-4"><a href="#cb8-4"></a><span class="an">date:</span><span class="co"> 2024-10-17</span></span>
<span id="cb8-5"><a href="#cb8-5"></a><span class="an">date-modified:</span><span class="co"> last-modified</span></span>
<span id="cb8-6"><a href="#cb8-6"></a><span class="an">lightbox:</span><span class="co"> auto</span></span>
<span id="cb8-7"><a href="#cb8-7"></a><span class="an">tbl-cap-location:</span><span class="co"> bottom</span></span>
<span id="cb8-8"><a href="#cb8-8"></a><span class="an">citation:</span></span>
<span id="cb8-9"><a href="#cb8-9"></a><span class="co">   author: Sam Foreman</span></span>
<span id="cb8-10"><a href="#cb8-10"></a><span class="co">   type: webpage</span></span>
<span id="cb8-11"><a href="#cb8-11"></a><span class="co">   title: "Converting Checkpoints"</span></span>
<span id="cb8-12"><a href="#cb8-12"></a><span class="co">   url: https://samforeman.me/posts/AuroraGPT/checkpoints</span></span>
<span id="cb8-13"><a href="#cb8-13"></a><span class="an">open-graph:</span></span>
<span id="cb8-14"><a href="#cb8-14"></a><span class="co">  title: "Converting Checkpoints"</span></span>
<span id="cb8-15"><a href="#cb8-15"></a><span class="co">  # description: "Details of how I create my slides, with Quarto + Reveal.js"</span></span>
<span id="cb8-16"><a href="#cb8-16"></a><span class="co">  # image: "./assets/spike-skipper.png"</span></span>
<span id="cb8-17"><a href="#cb8-17"></a><span class="an">twitter-card:</span></span>
<span id="cb8-18"><a href="#cb8-18"></a><span class="co">  site: "saforem2"</span></span>
<span id="cb8-19"><a href="#cb8-19"></a><span class="co">  creator: "saforem2"</span></span>
<span id="cb8-20"><a href="#cb8-20"></a><span class="co">  title: "Converting Checkpoints"</span></span>
<span id="cb8-21"><a href="#cb8-21"></a><span class="co">  # image: "./assets/spike-skipper.png"</span></span>
<span id="cb8-22"><a href="#cb8-22"></a><span class="co">  # description: "Details of how I create my slides, with Quarto + Reveal.js"</span></span>
<span id="cb8-23"><a href="#cb8-23"></a><span class="an">format:</span></span>
<span id="cb8-24"><a href="#cb8-24"></a><span class="co">  html: default</span></span>
<span id="cb8-25"><a href="#cb8-25"></a><span class="co">  gfm:</span></span>
<span id="cb8-26"><a href="#cb8-26"></a><span class="co">    output_file: "checkpoints.md"</span></span>
<span id="cb8-27"><a href="#cb8-27"></a><span class="an">categories:</span></span>
<span id="cb8-28"><a href="#cb8-28"></a><span class="co">  - AuroraGPT</span></span>
<span id="cb8-29"><a href="#cb8-29"></a><span class="co"># twitter-card:                      </span></span>
<span id="cb8-30"><a href="#cb8-30"></a><span class="co">#     image: "./assets/thumbnail.png"</span></span>
<span id="cb8-31"><a href="#cb8-31"></a><span class="co">#     creator: "@saforem2"           </span></span>
<span id="cb8-32"><a href="#cb8-32"></a><span class="co">#     site: "@saforem2"              </span></span>
<span id="cb8-33"><a href="#cb8-33"></a><span class="co"># open-graph:                        </span></span>
<span id="cb8-34"><a href="#cb8-34"></a><span class="co">#     image: "./assets/thumbnail.png"</span></span>
<span id="cb8-35"><a href="#cb8-35"></a><span class="co">---</span></span>
<span id="cb8-36"><a href="#cb8-36"></a></span>
<span id="cb8-37"><a href="#cb8-37"></a><span class="fu">## MDS --&gt; HF</span></span>
<span id="cb8-38"><a href="#cb8-38"></a></span>
<span id="cb8-39"><a href="#cb8-39"></a><span class="in">```bash</span></span>
<span id="cb8-40"><a href="#cb8-40"></a><span class="fu">convert_mds_to_hf()</span> <span class="kw">{</span></span>
<span id="cb8-41"><a href="#cb8-41"></a> <span class="co"># GLOBAL_STEP=$1</span></span>
<span id="cb8-42"><a href="#cb8-42"></a> <span class="va">CKPT_ROOT</span><span class="op">=</span><span class="va">$2</span></span>
<span id="cb8-43"><a href="#cb8-43"></a></span>
<span id="cb8-44"><a href="#cb8-44"></a> <span class="va">CKPT_ROOT</span><span class="op">=</span><span class="st">"/flare/Aurora_deployment/AuroraGPT-Testing/foremans/rollback-41k8/Megatron-DeepSpeed-41800/checkpoints/ws768_ds_stage1_nl32_hs4096_mb4_seq4096_gb3072_sp1_pp1_tp1_bf16_optadamw_lr0.00020_lwf0.05/"</span><span class="kw">;</span></span>
<span id="cb8-45"><a href="#cb8-45"></a> <span class="va">SRC</span><span class="op">=</span><span class="st">"</span><span class="va">${CKPT_ROOT}</span><span class="st">/global_step</span><span class="va">${GLOBAL_STEP}</span><span class="st">"</span></span>
<span id="cb8-46"><a href="#cb8-46"></a> <span class="cf">if</span> <span class="kw">[[</span> <span class="ot">-d</span> <span class="st">"</span><span class="va">${SRC}</span><span class="st">"</span> <span class="kw">]];</span> <span class="cf">then</span></span>
<span id="cb8-47"><a href="#cb8-47"></a>        <span class="bu">echo</span> <span class="st">"Converting checkpoint @ global step </span><span class="va">${GLOBAL_STEP}</span><span class="st">"</span></span>
<span id="cb8-48"><a href="#cb8-48"></a>        <span class="bu">echo</span> <span class="st">"\tsrc=</span><span class="va">${SRC}</span><span class="st">"</span></span>
<span id="cb8-49"><a href="#cb8-49"></a>        <span class="va">DST</span><span class="op">=</span><span class="st">"/flare/Aurora_deployment/AuroraGPT-Checkpoints/Megatron-DeepSpeed/checkpoints-to-convert/ws768_ds_stage1_nl32_hs4096_mb4_seq4096_gb3072_sp1_pp1_tp1_bf16_optadamw_lr0.00020_lwf0.05/global_step</span><span class="va">${GLOBAL_STEP}</span><span class="st">_hf"</span></span>
<span id="cb8-50"><a href="#cb8-50"></a>        <span class="bu">echo</span> <span class="st">"\tdst=</span><span class="va">${DST}</span><span class="st">"</span></span>
<span id="cb8-51"><a href="#cb8-51"></a>        <span class="ex">python3</span> mds_to_hf.py <span class="at">--mds_checkpoint</span> <span class="st">"</span><span class="va">${SRC}</span><span class="st">/mp_rank_00_model_states.pt"</span> <span class="at">--output_dir</span> <span class="st">"</span><span class="va">${DST}</span><span class="st">"</span> <span class="at">--cache_dir</span> <span class="st">"./.cache"</span></span>
<span id="cb8-52"><a href="#cb8-52"></a> <span class="cf">else</span></span>
<span id="cb8-53"><a href="#cb8-53"></a>        <span class="bu">echo</span> <span class="st">"Unable to locate directory </span><span class="va">${SRC}</span><span class="st">. Exiting"</span></span>
<span id="cb8-54"><a href="#cb8-54"></a>        <span class="bu">exit</span> 1</span>
<span id="cb8-55"><a href="#cb8-55"></a> <span class="cf">fi</span></span>
<span id="cb8-56"><a href="#cb8-56"></a><span class="kw">}</span></span>
<span id="cb8-57"><a href="#cb8-57"></a><span class="kw">```</span></span>
<span id="cb8-58"><a href="#cb8-58"></a></span>
<span id="cb8-59"><a href="#cb8-59"></a></span>
<span id="cb8-60"><a href="#cb8-60"></a><span class="co"># 🚧 HF to Meg-DS</span></span>
<span id="cb8-61"><a href="#cb8-61"></a></span>
<span id="cb8-62"><a href="#cb8-62"></a><span class="co">## 2024-10-17</span></span>
<span id="cb8-63"><a href="#cb8-63"></a></span>
<span id="cb8-64"><a href="#cb8-64"></a><span class="kw">```</span>python</span>
<span id="cb8-65"><a href="#cb8-65"></a><span class="ex">import</span> os</span>
<span id="cb8-66"><a href="#cb8-66"></a><span class="ex">import</span> ezpz as ez</span>
<span id="cb8-67"><a href="#cb8-67"></a><span class="ex">import</span> torch</span>
<span id="cb8-68"><a href="#cb8-68"></a><span class="ex">import</span> deepspeed</span>
<span id="cb8-69"><a href="#cb8-69"></a><span class="ex">from</span> datasets import load_dataset</span>
<span id="cb8-70"><a href="#cb8-70"></a><span class="ex">from</span> transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM</span>
<span id="cb8-71"><a href="#cb8-71"></a><span class="ex">from</span> transformers.integrations import HfDeepSpeedConfig</span>
<span id="cb8-72"><a href="#cb8-72"></a><span class="co"># distributed setup</span></span>
<span id="cb8-73"><a href="#cb8-73"></a></span>
<span id="cb8-74"><a href="#cb8-74"></a><span class="ex">os.environ[</span><span class="st">'WORLD_SIZE'</span><span class="ex">]</span> = <span class="st">'12'</span></span>
<span id="cb8-75"><a href="#cb8-75"></a><span class="ex">rank</span> = ez.setup_torch<span class="er">(</span><span class="va">backend</span><span class="op">=</span><span class="st">'deepspeed'</span><span class="kw">)</span></span>
<span id="cb8-76"><a href="#cb8-76"></a><span class="fu">deepspeed.init_distributed()</span></span>
<span id="cb8-77"><a href="#cb8-77"></a><span class="ex">model_name</span> = <span class="st">"meta-llama/Llama-3.2-1B"</span></span>
<span id="cb8-78"><a href="#cb8-78"></a><span class="ex">config</span> = AutoConfig.from_pretrained<span class="er">(</span><span class="ex">model_name</span><span class="kw">)</span></span>
<span id="cb8-79"><a href="#cb8-79"></a><span class="ex">ds_config</span> = {</span>
<span id="cb8-80"><a href="#cb8-80"></a>    <span class="st">"steps_per_print"</span><span class="ex">:</span> 1,</span>
<span id="cb8-81"><a href="#cb8-81"></a>    <span class="st">"train_batch_size"</span><span class="ex">:</span> 1,</span>
<span id="cb8-82"><a href="#cb8-82"></a>    <span class="st">"train_micro_batch_size_per_gpu"</span><span class="ex">:</span> 1,</span>
<span id="cb8-83"><a href="#cb8-83"></a>    <span class="st">"bf16"</span><span class="ex">:</span> {</span>
<span id="cb8-84"><a href="#cb8-84"></a>        <span class="st">"enabled"</span><span class="ex">:</span> True</span>
<span id="cb8-85"><a href="#cb8-85"></a>    <span class="er">}</span><span class="ex">,</span></span>
<span id="cb8-86"><a href="#cb8-86"></a>    <span class="st">"optimizer"</span><span class="ex">:</span> {</span>
<span id="cb8-87"><a href="#cb8-87"></a>        <span class="st">"type"</span><span class="ex">:</span> <span class="st">"Adam"</span>,</span>
<span id="cb8-88"><a href="#cb8-88"></a>    <span class="er">}</span><span class="ex">,</span></span>
<span id="cb8-89"><a href="#cb8-89"></a>    <span class="st">"zero_optimization"</span><span class="ex">:</span> {</span>
<span id="cb8-90"><a href="#cb8-90"></a>        <span class="st">"stage"</span><span class="ex">:</span> 3,</span>
<span id="cb8-91"><a href="#cb8-91"></a>    <span class="er">}</span><span class="ex">,</span></span>
<span id="cb8-92"><a href="#cb8-92"></a><span class="er">}</span></span>
<span id="cb8-93"><a href="#cb8-93"></a></span>
<span id="cb8-94"><a href="#cb8-94"></a><span class="ex">dschf</span> = HfDeepSpeedConfig<span class="er">(</span><span class="ex">ds_config</span><span class="kw">)</span>  <span class="co"># keep this object alive</span></span>
<span id="cb8-95"><a href="#cb8-95"></a><span class="co"># now a model can be loaded.</span></span>
<span id="cb8-96"><a href="#cb8-96"></a><span class="ex">model</span> = AutoModelForCausalLM.from_pretrained<span class="er">(</span><span class="ex">model_name</span><span class="kw">)</span><span class="ex">.to</span><span class="er">(</span><span class="fu">ez.get_torch_device()</span><span class="kw">)</span><span class="ex">.to</span><span class="er">(</span><span class="ex">torch.bfloat16</span><span class="kw">)</span></span>
<span id="cb8-97"><a href="#cb8-97"></a><span class="co"># initialise Deepspeed ZeRO and store only the engine object</span></span>
<span id="cb8-98"><a href="#cb8-98"></a><span class="ex">ds_engine</span> = deepspeed.initialize<span class="er">(</span><span class="va">model</span><span class="op">=</span>model, <span class="va">config_params</span><span class="op">=</span>ds_config<span class="kw">)</span><span class="ex">[0]</span></span>
<span id="cb8-99"><a href="#cb8-99"></a><span class="ex">tokenizer</span> = AutoTokenizer.from_pretrained<span class="er">(</span><span class="ex">model_name</span><span class="kw">)</span></span>
<span id="cb8-100"><a href="#cb8-100"></a></span>
<span id="cb8-101"><a href="#cb8-101"></a><span class="kw">```</span></span>
<span id="cb8-102"><a href="#cb8-102"></a><span class="kw">```</span>python</span>
<span id="cb8-103"><a href="#cb8-103"></a><span class="ex">def</span> tokenize_function<span class="er">(</span><span class="ex">examples</span><span class="kw">)</span><span class="bu">:</span></span>
<span id="cb8-104"><a href="#cb8-104"></a>    <span class="cf">return</span> <span class="ex">tokenizer</span><span class="er">(</span><span class="va">examples</span><span class="op">[</span><span class="st">"text"</span><span class="op">]</span><span class="ex">,</span> truncation=True<span class="kw">)</span></span>
<span id="cb8-105"><a href="#cb8-105"></a></span>
<span id="cb8-106"><a href="#cb8-106"></a><span class="ex">dataset</span> = load_dataset<span class="er">(</span><span class="st">"yelp_review_full"</span><span class="kw">)</span></span>
<span id="cb8-107"><a href="#cb8-107"></a><span class="ex">tokenized_datasets</span> = dataset.map<span class="er">(</span><span class="ex">tokenize_function,</span> batched=True<span class="kw">)</span></span>
<span id="cb8-108"><a href="#cb8-108"></a><span class="ex">small_train_dataset</span> = tokenized_datasets<span class="pp">[</span><span class="st">"train"</span><span class="pp">]</span>.shuffle<span class="er">(</span><span class="va">seed</span><span class="op">=</span>42<span class="kw">)</span><span class="ex">.select</span><span class="er">(</span><span class="ex">range</span><span class="er">(</span><span class="ex">1000</span><span class="kw">))</span></span>
<span id="cb8-109"><a href="#cb8-109"></a><span class="ex">small_eval_dataset</span> = tokenized_datasets<span class="pp">[</span><span class="st">"test"</span><span class="pp">]</span>.shuffle<span class="er">(</span><span class="va">seed</span><span class="op">=</span>42<span class="kw">)</span><span class="ex">.select</span><span class="er">(</span><span class="ex">range</span><span class="er">(</span><span class="ex">1000</span><span class="kw">))</span></span>
<span id="cb8-110"><a href="#cb8-110"></a><span class="kw">```</span></span>
<span id="cb8-111"><a href="#cb8-111"></a></span>
<span id="cb8-112"><a href="#cb8-112"></a><span class="kw">```</span>python</span>
<span id="cb8-113"><a href="#cb8-113"></a><span class="ex">from</span> transformers import Trainer</span>
<span id="cb8-114"><a href="#cb8-114"></a><span class="ex">training_args</span> = TrainingArguments<span class="er">(</span><span class="va">output_dir</span><span class="op">=</span><span class="st">"llama-3.2-1B"</span>, <span class="va">deepspeed</span><span class="op">=</span>ds_config<span class="kw">)</span></span>
<span id="cb8-115"><a href="#cb8-115"></a><span class="ex">trainer</span> = Trainer<span class="er">(</span><span class="ex">model,</span> training_args, train_dataset=small_train_dataset, eval_dataset=small_eval_dataset, tokenizer=tokenizer<span class="kw">)</span></span>
<span id="cb8-116"><a href="#cb8-116"></a><span class="kw">```</span></span>
<span id="cb8-117"><a href="#cb8-117"></a></span>
<span id="cb8-118"><a href="#cb8-118"></a></span>
<span id="cb8-119"><a href="#cb8-119"></a><span class="co">## Older</span></span>
<span id="cb8-120"><a href="#cb8-120"></a></span>
<span id="cb8-121"><a href="#cb8-121"></a><span class="ex">Current</span> status:</span>
<span id="cb8-122"><a href="#cb8-122"></a></span>
<span id="cb8-123"><a href="#cb8-123"></a><span class="kw">```</span>bash</span>
<span id="cb8-124"><a href="#cb8-124"></a><span class="co">#[🐍 aurora_nre_models_frameworks-2024.2.1_u1][👻 aurora_nre_models_frameworks-2024.2.1_u1]</span></span>
<span id="cb8-125"><a href="#cb8-125"></a><span class="co">#[🌌][03:39:06 PM][foremans]@[x4407c6s7b0n0][/f/A/f/p/a/Megatron-DeepSpeed][🌱hzheng-data-fix][📝🤷🏎💨]</span></span>
<span id="cb8-126"><a href="#cb8-126"></a><span class="ex">$</span> _conversion_args=<span class="er">(</span><span class="st">"--hf-ckpt-num-shards 1"</span></span>
<span id="cb8-127"><a href="#cb8-127"></a> <span class="st">"--hf-ckpt-dir /flare/Aurora_deployment/foremans/Meta-Llama-3.1-8B-128512"</span></span>
<span id="cb8-128"><a href="#cb8-128"></a> <span class="st">"--load-mode auto"</span></span>
<span id="cb8-129"><a href="#cb8-129"></a> <span class="st">"--save ckpt-mds-llama-3/"</span></span>
<span id="cb8-130"><a href="#cb8-130"></a> <span class="st">"--tensor-model-parallel-size 1"</span></span>
<span id="cb8-131"><a href="#cb8-131"></a> <span class="st">"--pipeline-model-parallel-size 1"</span></span>
<span id="cb8-132"><a href="#cb8-132"></a> <span class="st">"--lr-warmup-iters 2000"</span></span>
<span id="cb8-133"><a href="#cb8-133"></a> <span class="st">"--weight-decay 0.1"</span></span>
<span id="cb8-134"><a href="#cb8-134"></a> <span class="st">"--clip-grad 1"</span></span>
<span id="cb8-135"><a href="#cb8-135"></a> <span class="st">"--num-layers 32"</span></span>
<span id="cb8-136"><a href="#cb8-136"></a> <span class="st">"--hidden-size 4096"</span></span>
<span id="cb8-137"><a href="#cb8-137"></a> <span class="st">"--num-attention-heads 32"</span></span>
<span id="cb8-138"><a href="#cb8-138"></a> <span class="st">"--ffn-hidden-size 14336"</span></span>
<span id="cb8-139"><a href="#cb8-139"></a> <span class="st">"--attention-dropout 0"</span></span>
<span id="cb8-140"><a href="#cb8-140"></a> <span class="st">"--hidden-dropout 0"</span></span>
<span id="cb8-141"><a href="#cb8-141"></a> <span class="st">"--no-query-key-layer-scaling"</span></span>
<span id="cb8-142"><a href="#cb8-142"></a> <span class="st">"--num-key-value-heads 8"</span></span>
<span id="cb8-143"><a href="#cb8-143"></a> <span class="st">"--disable-bias-linear"</span></span>
<span id="cb8-144"><a href="#cb8-144"></a> <span class="st">"--normalization rmsnorm"</span></span>
<span id="cb8-145"><a href="#cb8-145"></a> <span class="st">"--use-rotary-position-embeddings"</span></span>
<span id="cb8-146"><a href="#cb8-146"></a> <span class="st">"--untie-embeddings-and-output-weights"</span></span>
<span id="cb8-147"><a href="#cb8-147"></a> <span class="st">"--swiglu"</span></span>
<span id="cb8-148"><a href="#cb8-148"></a> <span class="st">"--seq-length 2048"</span></span>
<span id="cb8-149"><a href="#cb8-149"></a> <span class="st">"--max-position-embeddings 2048"</span></span>
<span id="cb8-150"><a href="#cb8-150"></a> <span class="st">"--micro-batch-size 1"</span></span>
<span id="cb8-151"><a href="#cb8-151"></a> <span class="st">"--global-batch-size 24"</span></span>
<span id="cb8-152"><a href="#cb8-152"></a> <span class="st">"--train-iters 3500"</span></span>
<span id="cb8-153"><a href="#cb8-153"></a> <span class="st">"--lr 2e-5"</span></span>
<span id="cb8-154"><a href="#cb8-154"></a> <span class="st">"--tensorboard-dir tensorboard_output"</span></span>
<span id="cb8-155"><a href="#cb8-155"></a> <span class="st">"--lr-decay-iters 320000"</span></span>
<span id="cb8-156"><a href="#cb8-156"></a> <span class="st">"--lr-decay-style cosine"</span></span>
<span id="cb8-157"><a href="#cb8-157"></a> <span class="st">"--log-interval 1"</span></span>
<span id="cb8-158"><a href="#cb8-158"></a> <span class="st">"--eval-iters 100"</span></span>
<span id="cb8-159"><a href="#cb8-159"></a> <span class="st">"--eval-interval 100"</span></span>
<span id="cb8-160"><a href="#cb8-160"></a> <span class="st">"--data-path /lus/flare/projects/candle_aesp_CNDA/azton/data/v2/megatron/dataset_v2_wtextbooks_text_document"</span></span>
<span id="cb8-161"><a href="#cb8-161"></a> <span class="st">"--save-interval 1500"</span></span>
<span id="cb8-162"><a href="#cb8-162"></a> <span class="st">"--split 100,0,0"</span></span>
<span id="cb8-163"><a href="#cb8-163"></a> <span class="st">"--bf16"</span></span>
<span id="cb8-164"><a href="#cb8-164"></a> <span class="st">"--tokenizer-type HFTokenizer"</span></span>
<span id="cb8-165"><a href="#cb8-165"></a> <span class="st">"--tokenizer-model ALCF/custom_tokenizer.model"</span></span>
<span id="cb8-166"><a href="#cb8-166"></a> <span class="st">"--deepspeed_config ./examples_deepspeed/finetune_hf_llama/ds_config.json"</span></span>
<span id="cb8-167"><a href="#cb8-167"></a> <span class="st">"--deepspeed"</span></span>
<span id="cb8-168"><a href="#cb8-168"></a> <span class="st">"--distributed-backend ccl"</span></span>
<span id="cb8-169"><a href="#cb8-169"></a> <span class="st">"--no-masked-softmax-fusion"</span></span>
<span id="cb8-170"><a href="#cb8-170"></a> <span class="st">"--no-bias-gelu-fusion"</span></span>
<span id="cb8-171"><a href="#cb8-171"></a> <span class="st">"--no-bias-dropout-fusion"</span></span>
<span id="cb8-172"><a href="#cb8-172"></a> <span class="st">"--no-gradient-accumulation-fusion"</span></span>
<span id="cb8-173"><a href="#cb8-173"></a> <span class="st">"--repeated-dataloader"</span></span>
<span id="cb8-174"><a href="#cb8-174"></a> <span class="st">"--data-cache-path ./.cache"</span></span>
<span id="cb8-175"><a href="#cb8-175"></a> <span class="st">"--make-vocab-size-divisible-by 128512"</span></span>
<span id="cb8-176"><a href="#cb8-176"></a> <span class="st">"--vocab-size 128512"</span></span>
<span id="cb8-177"><a href="#cb8-177"></a><span class="kw">)</span></span>
<span id="cb8-178"><a href="#cb8-178"></a></span>
<span id="cb8-179"><a href="#cb8-179"></a><span class="va">conversion_flags</span><span class="op">=</span><span class="va">($(</span><span class="bu">printf</span> <span class="st">'%s\n'</span> <span class="st">"</span><span class="va">${_conversion_args</span><span class="op">[@]</span><span class="va">}</span><span class="st">"</span> <span class="kw">|</span> <span class="fu">sort</span><span class="va">))</span></span>
<span id="cb8-180"><a href="#cb8-180"></a><span class="bu">echo</span> <span class="st">"</span><span class="va">${conversion_flags</span><span class="op">[@]</span><span class="va">}</span><span class="st">"</span></span>
<span id="cb8-181"><a href="#cb8-181"></a><span class="ex">--attention-dropout</span> 0 <span class="at">--bf16</span> <span class="at">--clip-grad</span> 1 <span class="at">--data-cache-path</span> ./.cache <span class="at">--data-path</span> /lus/flare/projects/candle_aesp_CNDA/azton/data/v2/megatron/dataset_v2_wtextbooks_text_document <span class="at">--deepspeed</span> <span class="at">--deepspeed_config</span> ./examples_deepspeed/finetune_hf_llama/ds_config.json <span class="at">--disable-bias-linear</span> <span class="at">--distributed-backend</span> ccl <span class="at">--eval-interval</span> 100 <span class="at">--eval-iters</span> 100 <span class="at">--ffn-hidden-size</span> 14336 <span class="at">--global-batch-size</span> 24 <span class="at">--hf-ckpt-dir</span> /flare/Aurora_deployment/foremans/Meta-Llama-3.1-8B-128512 <span class="at">--hf-ckpt-num-shards</span> 1 <span class="at">--hidden-dropout</span> 0 <span class="at">--hidden-size</span> 4096 <span class="at">--load-mode</span> auto <span class="at">--log-interval</span> 1 <span class="at">--lr</span> 2e-5 <span class="at">--lr-decay-iters</span> 320000 <span class="at">--lr-decay-style</span> cosine <span class="at">--lr-warmup-iters</span> 2000 <span class="at">--make-vocab-size-divisible-by</span> 128512 <span class="at">--max-position-embeddings</span> 2048 <span class="at">--micro-batch-size</span> 1 <span class="at">--no-bias-dropout-fusion</span> <span class="at">--no-bias-gelu-fusion</span> <span class="at">--no-gradient-accumulation-fusion</span> <span class="at">--no-masked-softmax-fusion</span> <span class="at">--no-query-key-layer-scaling</span> <span class="at">--normalization</span> rmsnorm <span class="at">--num-attention-heads</span> 32 <span class="at">--num-key-value-heads</span> 8 <span class="at">--num-layers</span> 32 <span class="at">--pipeline-model-parallel-size</span> 1 <span class="at">--repeated-dataloader</span> <span class="at">--save</span> ckpt-mds-llama-3/ <span class="at">--save-interval</span> 1500 <span class="at">--seq-length</span> 2048 <span class="at">--split</span> 100,0,0 <span class="at">--swiglu</span> <span class="at">--tensorboard-dir</span> tensorboard_output <span class="at">--tensor-model-parallel-size</span> 1 <span class="at">--tokenizer-model</span> ALCF/custom_tokenizer.model <span class="at">--tokenizer-type</span> HFTokenizer <span class="at">--train-iters</span> 3500 <span class="at">--untie-embeddings-and-output-weights</span> <span class="at">--use-rotary-position-embeddings</span> <span class="at">--vocab-size</span> 128512 <span class="at">--weight-decay</span> 0.1</span>
<span id="cb8-182"><a href="#cb8-182"></a><span class="kw">```</span></span>
<span id="cb8-183"><a href="#cb8-183"></a></span>
<span id="cb8-184"><a href="#cb8-184"></a><span class="kw">```</span>bash</span>
<span id="cb8-185"><a href="#cb8-185"></a><span class="co">#[🐍 aurora_nre_models_frameworks-2024.2.1_u1][👻 aurora_nre_models_frameworks-2024.2.1_u1]</span></span>
<span id="cb8-186"><a href="#cb8-186"></a><span class="co">#[🌌][03:39:18 PM][foremans]@[x4407c6s7b0n0][/f/A/f/p/a/Megatron-DeepSpeed][🌱hzheng-data-fix][📝🤷🏎💨]</span></span>
<span id="cb8-187"><a href="#cb8-187"></a><span class="ex">$</span> launch python3 tools/hf2megads_weight_converter.py <span class="st">"</span><span class="va">${conversion_flags</span><span class="op">[@]</span><span class="va">}</span><span class="st">"</span></span>
<span id="cb8-188"><a href="#cb8-188"></a><span class="kw">```</span></span>
<span id="cb8-189"><a href="#cb8-189"></a></span>
<span id="cb8-190"><a href="#cb8-190"></a></span>
<span id="cb8-191"><a href="#cb8-191"></a><span class="op">&lt;</span>details <span class="ex">closed</span><span class="op">&gt;&lt;</span>summary<span class="op">&gt;</span>output:<span class="op">&lt;</span>/summary<span class="op">&gt;</span></span>
<span id="cb8-192"><a href="#cb8-192"></a></span>
<span id="cb8-193"><a href="#cb8-193"></a><span class="kw">```</span>bash</span>
<span id="cb8-194"><a href="#cb8-194"></a><span class="ex">Disabling</span> local launch: multi-node application</span>
<span id="cb8-195"><a href="#cb8-195"></a><span class="ex">Connected</span> to tcp://x4407c6s7b0n0.hostmgmt2407.cm.aurora.alcf.anl.gov:7919</span>
<span id="cb8-196"><a href="#cb8-196"></a><span class="ex">Found</span> executable /flare/Aurora_deployment/foremans/projects/argonne-lcf/Megatron-DeepSpeed/venvs/aurora_nre_models_frameworks-2024.2.1_u1/bin/python3</span>
<span id="cb8-197"><a href="#cb8-197"></a><span class="ex">Launching</span> application bdbd987f-b27b-4922-928b-5d1a166e800b</span>
<span id="cb8-198"><a href="#cb8-198"></a><span class="ex">[2024-10-16</span> 15:39:30,424] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb8-199"><a href="#cb8-199"></a><span class="ex">[2024-10-16</span> 15:39:30,456] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb8-200"><a href="#cb8-200"></a><span class="ex">[2024-10-16</span> 15:39:30,456] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb8-201"><a href="#cb8-201"></a><span class="ex">[2024-10-16</span> 15:39:30,456] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb8-202"><a href="#cb8-202"></a><span class="ex">[2024-10-16</span> 15:39:30,457] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb8-203"><a href="#cb8-203"></a><span class="ex">[2024-10-16</span> 15:39:30,568] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb8-204"><a href="#cb8-204"></a><span class="ex">[2024-10-16</span> 15:39:30,571] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb8-205"><a href="#cb8-205"></a><span class="ex">[2024-10-16</span> 15:39:30,575] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb8-206"><a href="#cb8-206"></a><span class="ex">[2024-10-16</span> 15:39:30,578] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb8-207"><a href="#cb8-207"></a><span class="ex">[2024-10-16</span> 15:39:30,584] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb8-208"><a href="#cb8-208"></a><span class="ex">[2024-10-16</span> 15:39:30,608] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb8-209"><a href="#cb8-209"></a><span class="ex">[2024-10-16</span> 15:39:30,613] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb8-210"><a href="#cb8-210"></a><span class="ex">[2024-10-16</span> 15:39:30,613] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb8-211"><a href="#cb8-211"></a><span class="ex">[2024-10-16</span> 15:39:30,614] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb8-212"><a href="#cb8-212"></a><span class="ex">[2024-10-16</span> 15:39:30,615] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb8-213"><a href="#cb8-213"></a><span class="ex">[2024-10-16</span> 15:39:30,630] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb8-214"><a href="#cb8-214"></a><span class="ex">[2024-10-16</span> 15:39:30,686] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb8-215"><a href="#cb8-215"></a><span class="ex">[2024-10-16</span> 15:39:30,698] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb8-216"><a href="#cb8-216"></a><span class="ex">[2024-10-16</span> 15:39:30,711] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb8-217"><a href="#cb8-217"></a><span class="ex">[2024-10-16</span> 15:39:30,715] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb8-218"><a href="#cb8-218"></a><span class="ex">[2024-10-16</span> 15:39:30,716] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb8-219"><a href="#cb8-219"></a><span class="ex">[2024-10-16</span> 15:39:30,723] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb8-220"><a href="#cb8-220"></a><span class="ex">[2024-10-16</span> 15:39:30,724] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb8-221"><a href="#cb8-221"></a><span class="ex">[2024-10-16</span> 15:39:30,726] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb8-222"><a href="#cb8-222"></a><span class="ex">[2024-10-16</span> 15:39:30,731] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb8-223"><a href="#cb8-223"></a><span class="ex">[2024-10-16</span> 15:39:30,737] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb8-224"><a href="#cb8-224"></a><span class="ex">[2024-10-16</span> 15:39:30,755] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb8-225"><a href="#cb8-225"></a><span class="ex">[2024-10-16</span> 15:39:30,766] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb8-226"><a href="#cb8-226"></a><span class="ex">[2024-10-16</span> 15:39:30,768] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb8-227"><a href="#cb8-227"></a><span class="ex">[2024-10-16</span> 15:39:30,780] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb8-228"><a href="#cb8-228"></a><span class="ex">[2024-10-16</span> 15:39:30,798] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb8-229"><a href="#cb8-229"></a><span class="ex">[2024-10-16</span> 15:39:30,840] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb8-230"><a href="#cb8-230"></a><span class="ex">[2024-10-16</span> 15:39:30,870] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb8-231"><a href="#cb8-231"></a><span class="ex">[2024-10-16</span> 15:39:30,870] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb8-232"><a href="#cb8-232"></a><span class="ex">[2024-10-16</span> 15:39:30,872] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb8-233"><a href="#cb8-233"></a><span class="ex">[2024-10-16</span> 15:39:30,873] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb8-234"><a href="#cb8-234"></a><span class="ex">[2024-10-16</span> 15:39:30,877] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb8-235"><a href="#cb8-235"></a><span class="ex">[2024-10-16</span> 15:39:30,888] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb8-236"><a href="#cb8-236"></a><span class="ex">[2024-10-16</span> 15:39:30,896] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb8-237"><a href="#cb8-237"></a><span class="ex">[2024-10-16</span> 15:39:30,902] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb8-238"><a href="#cb8-238"></a><span class="ex">[2024-10-16</span> 15:39:30,932] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb8-239"><a href="#cb8-239"></a><span class="ex">[2024-10-16</span> 15:39:30,934] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb8-240"><a href="#cb8-240"></a><span class="ex">[2024-10-16</span> 15:39:30,957] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb8-241"><a href="#cb8-241"></a><span class="ex">[2024-10-16</span> 15:39:30,984] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb8-242"><a href="#cb8-242"></a><span class="ex">[2024-10-16</span> 15:39:31,028] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb8-243"><a href="#cb8-243"></a><span class="ex">[2024-10-16</span> 15:39:31,029] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb8-244"><a href="#cb8-244"></a><span class="ex">[2024-10-16</span> 15:39:31,062] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb8-245"><a href="#cb8-245"></a><span class="ex">[2024-10-16</span> 15:39:31,150] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:219:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to xpu <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb8-246"><a href="#cb8-246"></a><span class="ex">[2024-10-16</span> 15:39:33,079] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:161:init_deepspeed_backend</span><span class="pp">]</span> Initialize ccl backend</span>
<span id="cb8-247"><a href="#cb8-247"></a><span class="ex">[2024-10-16</span> 15:39:33,079] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:652:init_distributed</span><span class="pp">]</span> cdb=None</span>
<span id="cb8-248"><a href="#cb8-248"></a><span class="ex">[2024-10-16</span> 15:39:33,079] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:667:init_distributed</span><span class="pp">]</span> Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...</span>
<span id="cb8-249"><a href="#cb8-249"></a><span class="ex">[2024-10-16</span> 15:39:33,481] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:161:init_deepspeed_backend</span><span class="pp">]</span> Initialize ccl backend</span>
<span id="cb8-250"><a href="#cb8-250"></a><span class="ex">[2024-10-16</span> 15:39:33,481] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:652:init_distributed</span><span class="pp">]</span> cdb=None</span>
<span id="cb8-251"><a href="#cb8-251"></a><span class="ex">[2024-10-16</span> 15:39:33,481] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:667:init_distributed</span><span class="pp">]</span> Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...</span>
<span id="cb8-252"><a href="#cb8-252"></a><span class="ex">[2024-10-16</span> 15:39:33,982] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:161:init_deepspeed_backend</span><span class="pp">]</span> Initialize ccl backend</span>
<span id="cb8-253"><a href="#cb8-253"></a><span class="ex">[2024-10-16</span> 15:39:33,982] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:652:init_distributed</span><span class="pp">]</span> cdb=None</span>
<span id="cb8-254"><a href="#cb8-254"></a><span class="ex">[2024-10-16</span> 15:39:33,982] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:667:init_distributed</span><span class="pp">]</span> Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...</span>
<span id="cb8-255"><a href="#cb8-255"></a><span class="ex">[2024-10-16</span> 15:39:33,983] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:161:init_deepspeed_backend</span><span class="pp">]</span> Initialize ccl backend</span>
<span id="cb8-256"><a href="#cb8-256"></a><span class="ex">[2024-10-16</span> 15:39:33,983] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:652:init_distributed</span><span class="pp">]</span> cdb=None</span>
<span id="cb8-257"><a href="#cb8-257"></a><span class="ex">[2024-10-16</span> 15:39:33,983] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:667:init_distributed</span><span class="pp">]</span> Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...</span>
<span id="cb8-258"><a href="#cb8-258"></a><span class="ex">[2024-10-16</span> 15:39:33,984] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:161:init_deepspeed_backend</span><span class="pp">]</span> Initialize ccl backend</span>
<span id="cb8-259"><a href="#cb8-259"></a><span class="ex">[2024-10-16</span> 15:39:33,984] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:652:init_distributed</span><span class="pp">]</span> cdb=None</span>
<span id="cb8-260"><a href="#cb8-260"></a><span class="ex">[2024-10-16</span> 15:39:33,984] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:667:init_distributed</span><span class="pp">]</span> Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...</span>
<span id="cb8-261"><a href="#cb8-261"></a><span class="ex">[2024-10-16</span> 15:39:34,133] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:161:init_deepspeed_backend</span><span class="pp">]</span> Initialize ccl backend</span>
<span id="cb8-262"><a href="#cb8-262"></a><span class="ex">[2024-10-16</span> 15:39:34,133] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:652:init_distributed</span><span class="pp">]</span> cdb=None</span>
<span id="cb8-263"><a href="#cb8-263"></a><span class="ex">[2024-10-16</span> 15:39:34,133] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:667:init_distributed</span><span class="pp">]</span> Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...</span>
<span id="cb8-264"><a href="#cb8-264"></a><span class="ex">[2024-10-16</span> 15:39:34,165] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:161:init_deepspeed_backend</span><span class="pp">]</span> Initialize ccl backend</span>
<span id="cb8-265"><a href="#cb8-265"></a><span class="ex">[2024-10-16</span> 15:39:34,165] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:652:init_distributed</span><span class="pp">]</span> cdb=None</span>
<span id="cb8-266"><a href="#cb8-266"></a><span class="ex">[2024-10-16</span> 15:39:34,165] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:667:init_distributed</span><span class="pp">]</span> Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...</span>
<span id="cb8-267"><a href="#cb8-267"></a><span class="ex">[2024-10-16</span> 15:39:34,207] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:161:init_deepspeed_backend</span><span class="pp">]</span> Initialize ccl backend</span>
<span id="cb8-268"><a href="#cb8-268"></a><span class="ex">[2024-10-16</span> 15:39:34,207] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:652:init_distributed</span><span class="pp">]</span> cdb=None</span>
<span id="cb8-269"><a href="#cb8-269"></a><span class="ex">[2024-10-16</span> 15:39:34,207] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:667:init_distributed</span><span class="pp">]</span> Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...</span>
<span id="cb8-270"><a href="#cb8-270"></a><span class="ex">[2024-10-16</span> 15:39:34,210] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:161:init_deepspeed_backend</span><span class="pp">]</span> Initialize ccl backend</span>
<span id="cb8-271"><a href="#cb8-271"></a><span class="ex">[2024-10-16</span> 15:39:34,210] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:652:init_distributed</span><span class="pp">]</span> cdb=None</span>
<span id="cb8-272"><a href="#cb8-272"></a><span class="ex">[2024-10-16</span> 15:39:34,210] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:667:init_distributed</span><span class="pp">]</span> Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...</span>
<span id="cb8-273"><a href="#cb8-273"></a><span class="ex">[2024-10-16</span> 15:39:34,216] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:161:init_deepspeed_backend</span><span class="pp">]</span> Initialize ccl backend</span>
<span id="cb8-274"><a href="#cb8-274"></a><span class="ex">[2024-10-16</span> 15:39:34,216] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:652:init_distributed</span><span class="pp">]</span> cdb=None</span>
<span id="cb8-275"><a href="#cb8-275"></a><span class="ex">[2024-10-16</span> 15:39:34,216] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:667:init_distributed</span><span class="pp">]</span> Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...</span>
<span id="cb8-276"><a href="#cb8-276"></a><span class="ex">[2024-10-16</span> 15:39:34,218] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:161:init_deepspeed_backend</span><span class="pp">]</span> Initialize ccl backend</span>
<span id="cb8-277"><a href="#cb8-277"></a><span class="ex">[2024-10-16</span> 15:39:34,218] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:652:init_distributed</span><span class="pp">]</span> cdb=None</span>
<span id="cb8-278"><a href="#cb8-278"></a><span class="ex">[2024-10-16</span> 15:39:34,219] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:667:init_distributed</span><span class="pp">]</span> Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...</span>
<span id="cb8-279"><a href="#cb8-279"></a><span class="ex">[2024-10-16</span> 15:39:34,271] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:161:init_deepspeed_backend</span><span class="pp">]</span> Initialize ccl backend</span>
<span id="cb8-280"><a href="#cb8-280"></a><span class="ex">[2024-10-16</span> 15:39:34,271] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:652:init_distributed</span><span class="pp">]</span> cdb=None</span>
<span id="cb8-281"><a href="#cb8-281"></a><span class="ex">[2024-10-16</span> 15:39:34,271] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:667:init_distributed</span><span class="pp">]</span> Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...</span>
<span id="cb8-282"><a href="#cb8-282"></a><span class="ex">[2024-10-16</span> 15:39:34,768] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:161:init_deepspeed_backend</span><span class="pp">]</span> Initialize ccl backend</span>
<span id="cb8-283"><a href="#cb8-283"></a><span class="ex">[2024-10-16</span> 15:39:34,768] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:652:init_distributed</span><span class="pp">]</span> cdb=None</span>
<span id="cb8-284"><a href="#cb8-284"></a><span class="ex">[2024-10-16</span> 15:39:34,768] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:667:init_distributed</span><span class="pp">]</span> Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...</span>
<span id="cb8-285"><a href="#cb8-285"></a><span class="ex">[2024-10-16</span> 15:39:34,772] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:161:init_deepspeed_backend</span><span class="pp">]</span> Initialize ccl backend</span>
<span id="cb8-286"><a href="#cb8-286"></a><span class="ex">[2024-10-16</span> 15:39:34,772] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:652:init_distributed</span><span class="pp">]</span> cdb=None</span>
<span id="cb8-287"><a href="#cb8-287"></a><span class="ex">[2024-10-16</span> 15:39:34,772] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:667:init_distributed</span><span class="pp">]</span> Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...</span>
<span id="cb8-288"><a href="#cb8-288"></a><span class="ex">[2024-10-16</span> 15:39:34,784] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:161:init_deepspeed_backend</span><span class="pp">]</span> Initialize ccl backend</span>
<span id="cb8-289"><a href="#cb8-289"></a><span class="ex">[2024-10-16</span> 15:39:34,784] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:652:init_distributed</span><span class="pp">]</span> cdb=None</span>
<span id="cb8-290"><a href="#cb8-290"></a><span class="ex">[2024-10-16</span> 15:39:34,784] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:667:init_distributed</span><span class="pp">]</span> Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...</span>
<span id="cb8-291"><a href="#cb8-291"></a><span class="ex">[2024-10-16</span> 15:39:34,795] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:161:init_deepspeed_backend</span><span class="pp">]</span> Initialize ccl backend</span>
<span id="cb8-292"><a href="#cb8-292"></a><span class="ex">[2024-10-16</span> 15:39:34,795] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:652:init_distributed</span><span class="pp">]</span> cdb=None</span>
<span id="cb8-293"><a href="#cb8-293"></a><span class="ex">[2024-10-16</span> 15:39:34,795] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:667:init_distributed</span><span class="pp">]</span> Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...</span>
<span id="cb8-294"><a href="#cb8-294"></a><span class="ex">[2024-10-16</span> 15:39:35,038] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:161:init_deepspeed_backend</span><span class="pp">]</span> Initialize ccl backend</span>
<span id="cb8-295"><a href="#cb8-295"></a><span class="ex">[2024-10-16</span> 15:39:35,038] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:652:init_distributed</span><span class="pp">]</span> cdb=None</span>
<span id="cb8-296"><a href="#cb8-296"></a><span class="ex">[2024-10-16</span> 15:39:35,038] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:667:init_distributed</span><span class="pp">]</span> Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...</span>
<span id="cb8-297"><a href="#cb8-297"></a><span class="ex">[2024-10-16</span> 15:39:35,048] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:161:init_deepspeed_backend</span><span class="pp">]</span> Initialize ccl backend</span>
<span id="cb8-298"><a href="#cb8-298"></a><span class="ex">[2024-10-16</span> 15:39:35,048] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:652:init_distributed</span><span class="pp">]</span> cdb=None</span>
<span id="cb8-299"><a href="#cb8-299"></a><span class="ex">[2024-10-16</span> 15:39:35,048] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:667:init_distributed</span><span class="pp">]</span> Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...</span>
<span id="cb8-300"><a href="#cb8-300"></a><span class="ex">[2024-10-16</span> 15:39:35,062] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:161:init_deepspeed_backend</span><span class="pp">]</span> Initialize ccl backend</span>
<span id="cb8-301"><a href="#cb8-301"></a><span class="ex">[2024-10-16</span> 15:39:35,062] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:652:init_distributed</span><span class="pp">]</span> cdb=None</span>
<span id="cb8-302"><a href="#cb8-302"></a><span class="ex">[2024-10-16</span> 15:39:35,062] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:667:init_distributed</span><span class="pp">]</span> Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...</span>
<span id="cb8-303"><a href="#cb8-303"></a><span class="ex">[2024-10-16</span> 15:39:35,070] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:161:init_deepspeed_backend</span><span class="pp">]</span> Initialize ccl backend</span>
<span id="cb8-304"><a href="#cb8-304"></a><span class="ex">[2024-10-16</span> 15:39:35,070] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:652:init_distributed</span><span class="pp">]</span> cdb=None</span>
<span id="cb8-305"><a href="#cb8-305"></a><span class="ex">[2024-10-16</span> 15:39:35,070] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:667:init_distributed</span><span class="pp">]</span> Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...</span>
<span id="cb8-306"><a href="#cb8-306"></a><span class="ex">[2024-10-16</span> 15:39:35,073] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:161:init_deepspeed_backend</span><span class="pp">]</span> Initialize ccl backend</span>
<span id="cb8-307"><a href="#cb8-307"></a><span class="ex">[2024-10-16</span> 15:39:35,073] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:652:init_distributed</span><span class="pp">]</span> cdb=None</span>
<span id="cb8-308"><a href="#cb8-308"></a><span class="ex">[2024-10-16</span> 15:39:35,073] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:667:init_distributed</span><span class="pp">]</span> Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...</span>
<span id="cb8-309"><a href="#cb8-309"></a><span class="ex">[2024-10-16</span> 15:39:35,077] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:161:init_deepspeed_backend</span><span class="pp">]</span> Initialize ccl backend</span>
<span id="cb8-310"><a href="#cb8-310"></a><span class="ex">[2024-10-16</span> 15:39:35,078] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:652:init_distributed</span><span class="pp">]</span> cdb=None</span>
<span id="cb8-311"><a href="#cb8-311"></a><span class="ex">[2024-10-16</span> 15:39:35,078] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:667:init_distributed</span><span class="pp">]</span> Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...</span>
<span id="cb8-312"><a href="#cb8-312"></a><span class="ex">[2024-10-16</span> 15:39:35,103] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:161:init_deepspeed_backend</span><span class="pp">]</span> Initialize ccl backend</span>
<span id="cb8-313"><a href="#cb8-313"></a><span class="ex">[2024-10-16</span> 15:39:35,104] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:652:init_distributed</span><span class="pp">]</span> cdb=None</span>
<span id="cb8-314"><a href="#cb8-314"></a><span class="ex">[2024-10-16</span> 15:39:35,104] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:667:init_distributed</span><span class="pp">]</span> Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...</span>
<span id="cb8-315"><a href="#cb8-315"></a><span class="ex">[2024-10-16</span> 15:39:35,106] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:161:init_deepspeed_backend</span><span class="pp">]</span> Initialize ccl backend</span>
<span id="cb8-316"><a href="#cb8-316"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:652:init_distributed</span><span class="pp">]</span> cdb=None</span>
<span id="cb8-317"><a href="#cb8-317"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:667:init_distributed</span><span class="pp">]</span> Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...</span>
<span id="cb8-318"><a href="#cb8-318"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:718:mpi_discovery</span><span class="pp">]</span> Discovered MPI settings of world_rank=13, local_rank=1, world_size=24, master_addr=10.115.45.184, master_port=29500</span>
<span id="cb8-319"><a href="#cb8-319"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:718:mpi_discovery</span><span class="pp">]</span> Discovered MPI settings of world_rank=15, local_rank=3, world_size=24, master_addr=10.115.45.184, master_port=29500</span>
<span id="cb8-320"><a href="#cb8-320"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:718:mpi_discovery</span><span class="pp">]</span> Discovered MPI settings of world_rank=16, local_rank=4, world_size=24, master_addr=10.115.45.184, master_port=29500</span>
<span id="cb8-321"><a href="#cb8-321"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:718:mpi_discovery</span><span class="pp">]</span> Discovered MPI settings of world_rank=17, local_rank=5, world_size=24, master_addr=10.115.45.184, master_port=29500</span>
<span id="cb8-322"><a href="#cb8-322"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:718:mpi_discovery</span><span class="pp">]</span> Discovered MPI settings of world_rank=12, local_rank=0, world_size=24, master_addr=10.115.45.184, master_port=29500</span>
<span id="cb8-323"><a href="#cb8-323"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:718:mpi_discovery</span><span class="pp">]</span> Discovered MPI settings of world_rank=14, local_rank=2, world_size=24, master_addr=10.115.45.184, master_port=29500</span>
<span id="cb8-324"><a href="#cb8-324"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:718:mpi_discovery</span><span class="pp">]</span> Discovered MPI settings of world_rank=0, local_rank=0, world_size=24, master_addr=10.115.45.184, master_port=29500</span>
<span id="cb8-325"><a href="#cb8-325"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:683:init_distributed</span><span class="pp">]</span> Initializing TorchBackend in DeepSpeed with backend ccl</span>
<span id="cb8-326"><a href="#cb8-326"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:718:mpi_discovery</span><span class="pp">]</span> Discovered MPI settings of world_rank=18, local_rank=6, world_size=24, master_addr=10.115.45.184, master_port=29500</span>
<span id="cb8-327"><a href="#cb8-327"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:718:mpi_discovery</span><span class="pp">]</span> Discovered MPI settings of world_rank=19, local_rank=7, world_size=24, master_addr=10.115.45.184, master_port=29500</span>
<span id="cb8-328"><a href="#cb8-328"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:718:mpi_discovery</span><span class="pp">]</span> Discovered MPI settings of world_rank=20, local_rank=8, world_size=24, master_addr=10.115.45.184, master_port=29500</span>
<span id="cb8-329"><a href="#cb8-329"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:718:mpi_discovery</span><span class="pp">]</span> Discovered MPI settings of world_rank=21, local_rank=9, world_size=24, master_addr=10.115.45.184, master_port=29500</span>
<span id="cb8-330"><a href="#cb8-330"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:718:mpi_discovery</span><span class="pp">]</span> Discovered MPI settings of world_rank=22, local_rank=10, world_size=24, master_addr=10.115.45.184, master_port=29500</span>
<span id="cb8-331"><a href="#cb8-331"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:718:mpi_discovery</span><span class="pp">]</span> Discovered MPI settings of world_rank=23, local_rank=11, world_size=24, master_addr=10.115.45.184, master_port=29500</span>
<span id="cb8-332"><a href="#cb8-332"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:718:mpi_discovery</span><span class="pp">]</span> Discovered MPI settings of world_rank=1, local_rank=1, world_size=24, master_addr=10.115.45.184, master_port=29500</span>
<span id="cb8-333"><a href="#cb8-333"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:718:mpi_discovery</span><span class="pp">]</span> Discovered MPI settings of world_rank=2, local_rank=2, world_size=24, master_addr=10.115.45.184, master_port=29500</span>
<span id="cb8-334"><a href="#cb8-334"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:718:mpi_discovery</span><span class="pp">]</span> Discovered MPI settings of world_rank=3, local_rank=3, world_size=24, master_addr=10.115.45.184, master_port=29500</span>
<span id="cb8-335"><a href="#cb8-335"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:718:mpi_discovery</span><span class="pp">]</span> Discovered MPI settings of world_rank=4, local_rank=4, world_size=24, master_addr=10.115.45.184, master_port=29500</span>
<span id="cb8-336"><a href="#cb8-336"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:718:mpi_discovery</span><span class="pp">]</span> Discovered MPI settings of world_rank=5, local_rank=5, world_size=24, master_addr=10.115.45.184, master_port=29500</span>
<span id="cb8-337"><a href="#cb8-337"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:718:mpi_discovery</span><span class="pp">]</span> Discovered MPI settings of world_rank=6, local_rank=6, world_size=24, master_addr=10.115.45.184, master_port=29500</span>
<span id="cb8-338"><a href="#cb8-338"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:718:mpi_discovery</span><span class="pp">]</span> Discovered MPI settings of world_rank=7, local_rank=7, world_size=24, master_addr=10.115.45.184, master_port=29500</span>
<span id="cb8-339"><a href="#cb8-339"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:718:mpi_discovery</span><span class="pp">]</span> Discovered MPI settings of world_rank=8, local_rank=8, world_size=24, master_addr=10.115.45.184, master_port=29500</span>
<span id="cb8-340"><a href="#cb8-340"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:718:mpi_discovery</span><span class="pp">]</span> Discovered MPI settings of world_rank=9, local_rank=9, world_size=24, master_addr=10.115.45.184, master_port=29500</span>
<span id="cb8-341"><a href="#cb8-341"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:718:mpi_discovery</span><span class="pp">]</span> Discovered MPI settings of world_rank=10, local_rank=10, world_size=24, master_addr=10.115.45.184, master_port=29500</span>
<span id="cb8-342"><a href="#cb8-342"></a><span class="ex">[2024-10-16</span> 15:39:35,107] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">comm.py:718:mpi_discovery</span><span class="pp">]</span> Discovered MPI settings of world_rank=11, local_rank=11, world_size=24, master_addr=10.115.45.184, master_port=29500</span>
<span id="cb8-343"><a href="#cb8-343"></a><span class="ex">[2024-10-16</span> 15:39:35.116490]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:348</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'xpu'</span><span class="pp">][</span><span class="ss">rank=15/23</span><span class="pp">][</span><span class="ss">local_rank=3/11</span><span class="pp">][</span><span class="ss">node=1/1</span><span class="pp">]</span></span>
<span id="cb8-344"><a href="#cb8-344"></a><span class="ex">[2024-10-16</span> 15:39:35.117455]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:348</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'xpu'</span><span class="pp">][</span><span class="ss">rank=1/23</span><span class="pp">][</span><span class="ss">local_rank=1/11</span><span class="pp">][</span><span class="ss">node=1/1</span><span class="pp">]</span></span>
<span id="cb8-345"><a href="#cb8-345"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">34466</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">MPI</span> was initialized externally, CCL-MPI specific environment is ignored</span>
<span id="cb8-346"><a href="#cb8-346"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">169066</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">MPI</span> was initialized externally, CCL-MPI specific environment is ignored</span>
<span id="cb8-347"><a href="#cb8-347"></a><span class="ex">[2024-10-16</span> 15:39:35.120281]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:348</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'xpu'</span><span class="pp">][</span><span class="ss">rank=10/23</span><span class="pp">][</span><span class="ss">local_rank=10/11</span><span class="pp">][</span><span class="ss">node=0/1</span><span class="pp">]</span></span>
<span id="cb8-348"><a href="#cb8-348"></a><span class="ex">[2024-10-16</span> 15:39:35.120280]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:348</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'xpu'</span><span class="pp">][</span><span class="ss">rank=11/23</span><span class="pp">][</span><span class="ss">local_rank=11/11</span><span class="pp">][</span><span class="ss">node=1/1</span><span class="pp">]</span></span>
<span id="cb8-349"><a href="#cb8-349"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">169075</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">MPI</span> was initialized externally, CCL-MPI specific environment is ignored</span>
<span id="cb8-350"><a href="#cb8-350"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">169076</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">MPI</span> was initialized externally, CCL-MPI specific environment is ignored</span>
<span id="cb8-351"><a href="#cb8-351"></a><span class="ex">[2024-10-16</span> 15:39:35.124730]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:348</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'xpu'</span><span class="pp">][</span><span class="ss">rank=4/23</span><span class="pp">][</span><span class="ss">local_rank=4/11</span><span class="pp">][</span><span class="ss">node=0/1</span><span class="pp">]</span></span>
<span id="cb8-352"><a href="#cb8-352"></a><span class="ex">[2024-10-16</span> 15:39:35.125260]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:348</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'xpu'</span><span class="pp">][</span><span class="ss">rank=6/23</span><span class="pp">][</span><span class="ss">local_rank=6/11</span><span class="pp">][</span><span class="ss">node=0/1</span><span class="pp">]</span></span>
<span id="cb8-353"><a href="#cb8-353"></a><span class="ex">[2024-10-16</span> 15:39:35.125427]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:348</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'xpu'</span><span class="pp">][</span><span class="ss">rank=3/23</span><span class="pp">][</span><span class="ss">local_rank=3/11</span><span class="pp">][</span><span class="ss">node=1/1</span><span class="pp">]</span></span>
<span id="cb8-354"><a href="#cb8-354"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">169069</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">MPI</span> was initialized externally, CCL-MPI specific environment is ignored</span>
<span id="cb8-355"><a href="#cb8-355"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">169068</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">MPI</span> was initialized externally, CCL-MPI specific environment is ignored</span>
<span id="cb8-356"><a href="#cb8-356"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">169071</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">MPI</span> was initialized externally, CCL-MPI specific environment is ignored</span>
<span id="cb8-357"><a href="#cb8-357"></a><span class="ex">[2024-10-16</span> 15:39:35.127167]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:348</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'xpu'</span><span class="pp">][</span><span class="ss">rank=8/23</span><span class="pp">][</span><span class="ss">local_rank=8/11</span><span class="pp">][</span><span class="ss">node=0/1</span><span class="pp">]</span></span>
<span id="cb8-358"><a href="#cb8-358"></a><span class="ex">[2024-10-16</span> 15:39:35.127199]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:348</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'xpu'</span><span class="pp">][</span><span class="ss">rank=21/23</span><span class="pp">][</span><span class="ss">local_rank=9/11</span><span class="pp">][</span><span class="ss">node=1/1</span><span class="pp">]</span></span>
<span id="cb8-359"><a href="#cb8-359"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">169073</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">MPI</span> was initialized externally, CCL-MPI specific environment is ignored</span>
<span id="cb8-360"><a href="#cb8-360"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">34472</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">MPI</span> was initialized externally, CCL-MPI specific environment is ignored</span>
<span id="cb8-361"><a href="#cb8-361"></a><span class="ex">[2024-10-16</span> 15:39:35.129097]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:348</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'xpu'</span><span class="pp">][</span><span class="ss">rank=14/23</span><span class="pp">][</span><span class="ss">local_rank=2/11</span><span class="pp">][</span><span class="ss">node=0/1</span><span class="pp">]</span></span>
<span id="cb8-362"><a href="#cb8-362"></a><span class="ex">[2024-10-16</span> 15:39:35.129281]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:348</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'xpu'</span><span class="pp">][</span><span class="ss">rank=9/23</span><span class="pp">][</span><span class="ss">local_rank=9/11</span><span class="pp">][</span><span class="ss">node=1/1</span><span class="pp">]</span></span>
<span id="cb8-363"><a href="#cb8-363"></a><span class="ex">[2024-10-16</span> 15:39:35.129345]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:348</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'xpu'</span><span class="pp">][</span><span class="ss">rank=22/23</span><span class="pp">][</span><span class="ss">local_rank=10/11</span><span class="pp">][</span><span class="ss">node=0/1</span><span class="pp">]</span></span>
<span id="cb8-364"><a href="#cb8-364"></a><span class="ex">[2024-10-16</span> 15:39:35.129461]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:348</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'xpu'</span><span class="pp">][</span><span class="ss">rank=20/23</span><span class="pp">][</span><span class="ss">local_rank=8/11</span><span class="pp">][</span><span class="ss">node=0/1</span><span class="pp">]</span></span>
<span id="cb8-365"><a href="#cb8-365"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">34465</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">MPI</span> was initialized externally, CCL-MPI specific environment is ignored</span>
<span id="cb8-366"><a href="#cb8-366"></a><span class="ex">[2024-10-16</span> 15:39:35.130518]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:348</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'xpu'</span><span class="pp">][</span><span class="ss">rank=13/23</span><span class="pp">][</span><span class="ss">local_rank=1/11</span><span class="pp">][</span><span class="ss">node=1/1</span><span class="pp">]</span></span>
<span id="cb8-367"><a href="#cb8-367"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">34473</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">MPI</span> was initialized externally, CCL-MPI specific environment is ignored</span>
<span id="cb8-368"><a href="#cb8-368"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">169074</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">MPI</span> was initialized externally, CCL-MPI specific environment is ignored</span>
<span id="cb8-369"><a href="#cb8-369"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">34471</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">MPI</span> was initialized externally, CCL-MPI specific environment is ignored</span>
<span id="cb8-370"><a href="#cb8-370"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">34464</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">MPI</span> was initialized externally, CCL-MPI specific environment is ignored</span>
<span id="cb8-371"><a href="#cb8-371"></a><span class="ex">[2024-10-16</span> 15:39:35.131822]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:348</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'xpu'</span><span class="pp">][</span><span class="ss">rank=18/23</span><span class="pp">][</span><span class="ss">local_rank=6/11</span><span class="pp">][</span><span class="ss">node=0/1</span><span class="pp">]</span></span>
<span id="cb8-372"><a href="#cb8-372"></a><span class="ex">[2024-10-16</span> 15:39:35.131816]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:348</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'xpu'</span><span class="pp">][</span><span class="ss">rank=19/23</span><span class="pp">][</span><span class="ss">local_rank=7/11</span><span class="pp">][</span><span class="ss">node=1/1</span><span class="pp">]</span></span>
<span id="cb8-373"><a href="#cb8-373"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">34469</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">MPI</span> was initialized externally, CCL-MPI specific environment is ignored</span>
<span id="cb8-374"><a href="#cb8-374"></a><span class="ex">[2024-10-16</span> 15:39:35.133183]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:348</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'xpu'</span><span class="pp">][</span><span class="ss">rank=7/23</span><span class="pp">][</span><span class="ss">local_rank=7/11</span><span class="pp">][</span><span class="ss">node=1/1</span><span class="pp">]</span></span>
<span id="cb8-375"><a href="#cb8-375"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">34470</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">MPI</span> was initialized externally, CCL-MPI specific environment is ignored</span>
<span id="cb8-376"><a href="#cb8-376"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">169072</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">MPI</span> was initialized externally, CCL-MPI specific environment is ignored</span>
<span id="cb8-377"><a href="#cb8-377"></a><span class="ex">[2024-10-16</span> 15:39:35.192746]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:348</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'xpu'</span><span class="pp">][</span><span class="ss">rank=2/23</span><span class="pp">][</span><span class="ss">local_rank=2/11</span><span class="pp">][</span><span class="ss">node=0/1</span><span class="pp">]</span></span>
<span id="cb8-378"><a href="#cb8-378"></a><span class="ex">[2024-10-16</span> 15:39:35.192682]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:348</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'xpu'</span><span class="pp">][</span><span class="ss">rank=5/23</span><span class="pp">][</span><span class="ss">local_rank=5/11</span><span class="pp">][</span><span class="ss">node=1/1</span><span class="pp">]</span></span>
<span id="cb8-379"><a href="#cb8-379"></a><span class="ex">[2024-10-16</span> 15:39:35.193446]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:348</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'xpu'</span><span class="pp">][</span><span class="ss">rank=17/23</span><span class="pp">][</span><span class="ss">local_rank=5/11</span><span class="pp">][</span><span class="ss">node=1/1</span><span class="pp">]</span></span>
<span id="cb8-380"><a href="#cb8-380"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">169067</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">MPI</span> was initialized externally, CCL-MPI specific environment is ignored</span>
<span id="cb8-381"><a href="#cb8-381"></a><span class="ex">[2024-10-16</span> 15:39:35.193506]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:348</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'xpu'</span><span class="pp">][</span><span class="ss">rank=12/23</span><span class="pp">][</span><span class="ss">local_rank=0/11</span><span class="pp">][</span><span class="ss">node=0/1</span><span class="pp">]</span></span>
<span id="cb8-382"><a href="#cb8-382"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">169070</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">MPI</span> was initialized externally, CCL-MPI specific environment is ignored</span>
<span id="cb8-383"><a href="#cb8-383"></a><span class="ex">[2024-10-16</span> 15:39:35.194435]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:348</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'xpu'</span><span class="pp">][</span><span class="ss">rank=23/23</span><span class="pp">][</span><span class="ss">local_rank=11/11</span><span class="pp">][</span><span class="ss">node=1/1</span><span class="pp">]</span></span>
<span id="cb8-384"><a href="#cb8-384"></a><span class="ex">[2024-10-16</span> 15:39:35.194529]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:348</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'xpu'</span><span class="pp">][</span><span class="ss">rank=16/23</span><span class="pp">][</span><span class="ss">local_rank=4/11</span><span class="pp">][</span><span class="ss">node=0/1</span><span class="pp">]</span></span>
<span id="cb8-385"><a href="#cb8-385"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">34468</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">MPI</span> was initialized externally, CCL-MPI specific environment is ignored</span>
<span id="cb8-386"><a href="#cb8-386"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">34463</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">MPI</span> was initialized externally, CCL-MPI specific environment is ignored</span>
<span id="cb8-387"><a href="#cb8-387"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">34467</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">MPI</span> was initialized externally, CCL-MPI specific environment is ignored</span>
<span id="cb8-388"><a href="#cb8-388"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">34474</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">MPI</span> was initialized externally, CCL-MPI specific environment is ignored</span>
<span id="cb8-389"><a href="#cb8-389"></a><span class="ex">[2024-10-16</span> 15:39:35.198383]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:92</span><span class="pp">]</span> <span class="at">-</span></span>
<span id="cb8-390"><a href="#cb8-390"></a></span>
<span id="cb8-391"><a href="#cb8-391"></a><span class="ex">[dist_info]:</span></span>
<span id="cb8-392"><a href="#cb8-392"></a>  <span class="ex">•</span> DEVICE=xpu</span>
<span id="cb8-393"><a href="#cb8-393"></a>  <span class="ex">•</span> DEVICE_ID=xpu:0</span>
<span id="cb8-394"><a href="#cb8-394"></a>  <span class="ex">•</span> DISTRIBUTED_BACKEND=ccl</span>
<span id="cb8-395"><a href="#cb8-395"></a>  <span class="ex">•</span> GPUS_PER_NODE=12</span>
<span id="cb8-396"><a href="#cb8-396"></a>  <span class="ex">•</span> HOSTS=[<span class="st">'x4407c6s7b0n0.hostmgmt2407.cm.aurora.alcf.anl.gov'</span>, <span class="st">'x4308c2s5b0n0.hostmgmt2308.cm.aurora.alcf.anl.gov'</span>]</span>
<span id="cb8-397"><a href="#cb8-397"></a>  <span class="ex">•</span> HOSTFILE=/var/spool/pbs/aux/886439.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov</span>
<span id="cb8-398"><a href="#cb8-398"></a>  <span class="ex">•</span> HOSTNAME=x4407c6s7b0n0.hostmgmt2407.cm.aurora.alcf.anl.gov</span>
<span id="cb8-399"><a href="#cb8-399"></a>  <span class="ex">•</span> LOCAL_RANK=0</span>
<span id="cb8-400"><a href="#cb8-400"></a>  <span class="ex">•</span> MACHINE=Aurora</span>
<span id="cb8-401"><a href="#cb8-401"></a>  <span class="ex">•</span> NUM_NODES=2</span>
<span id="cb8-402"><a href="#cb8-402"></a>  <span class="ex">•</span> NGPUS=24</span>
<span id="cb8-403"><a href="#cb8-403"></a>  <span class="ex">•</span> NGPUS_AVAILABLE=24</span>
<span id="cb8-404"><a href="#cb8-404"></a>  <span class="ex">•</span> NODE_ID=0</span>
<span id="cb8-405"><a href="#cb8-405"></a>  <span class="ex">•</span> RANK=0</span>
<span id="cb8-406"><a href="#cb8-406"></a>  <span class="ex">•</span> SCHEDULER=PBS</span>
<span id="cb8-407"><a href="#cb8-407"></a>  <span class="ex">•</span> WORLD_SIZE_TOTAL=24</span>
<span id="cb8-408"><a href="#cb8-408"></a>  <span class="ex">•</span> WORLD_SIZE_IN_USE=24</span>
<span id="cb8-409"><a href="#cb8-409"></a>  <span class="ex">•</span> LAUNCH_CMD=mpiexec <span class="at">--verbose</span> <span class="at">--envall</span> <span class="at">-n</span> 24 <span class="at">-ppn</span> 12 <span class="at">--hostfile</span> /var/spool/pbs/aux/886439.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov <span class="at">--cpu-bind</span> depth <span class="at">-d</span> 16</span>
<span id="cb8-410"><a href="#cb8-410"></a></span>
<span id="cb8-411"><a href="#cb8-411"></a></span>
<span id="cb8-412"><a href="#cb8-412"></a><span class="ex">--------------------------------------------------</span></span>
<span id="cb8-413"><a href="#cb8-413"></a><span class="ex">DeepSpeed</span> C++/CUDA extension op report</span>
<span id="cb8-414"><a href="#cb8-414"></a><span class="ex">--------------------------------------------------</span></span>
<span id="cb8-415"><a href="#cb8-415"></a><span class="ex">NOTE:</span> Ops not installed will be just-in-time <span class="er">(</span><span class="ex">JIT</span><span class="kw">)</span> <span class="ex">compiled</span> at</span>
<span id="cb8-416"><a href="#cb8-416"></a>      <span class="ex">runtime</span> if needed. Op compatibility means that your system</span>
<span id="cb8-417"><a href="#cb8-417"></a>      <span class="ex">meet</span> the required dependencies to JIT install the op.</span>
<span id="cb8-418"><a href="#cb8-418"></a><span class="ex">--------------------------------------------------</span></span>
<span id="cb8-419"><a href="#cb8-419"></a><span class="ex">JIT</span> compiled ops requires ninja</span>
<span id="cb8-420"><a href="#cb8-420"></a><span class="ex">ninja</span> .................. <span class="pp">[</span><span class="ss">OKAY</span><span class="pp">]</span></span>
<span id="cb8-421"><a href="#cb8-421"></a><span class="ex">--------------------------------------------------</span></span>
<span id="cb8-422"><a href="#cb8-422"></a><span class="ex">op</span> name ................ installed .. compatible</span>
<span id="cb8-423"><a href="#cb8-423"></a><span class="ex">--------------------------------------------------</span></span>
<span id="cb8-424"><a href="#cb8-424"></a><span class="ex">deepspeed_not_implemented</span>  <span class="pp">[</span><span class="ss">NO</span><span class="pp">]</span> ....... <span class="pp">[</span><span class="ss">OKAY</span><span class="pp">]</span></span>
<span id="cb8-425"><a href="#cb8-425"></a> <span class="ex">[WARNING]</span>  async_io requires the dev libaio .so object and headers but these were not found.</span>
<span id="cb8-426"><a href="#cb8-426"></a> <span class="ex">[WARNING]</span>  If libaio is already installed <span class="er">(</span><span class="ex">perhaps</span> from source<span class="kw">)</span><span class="ex">,</span> try setting the CFLAGS and LDFLAGS environment variables to where it can be found.</span>
<span id="cb8-427"><a href="#cb8-427"></a><span class="ex">async_io</span> ............... <span class="pp">[</span><span class="ss">NO</span><span class="pp">]</span> ....... <span class="pp">[</span><span class="ss">NO</span><span class="pp">]</span></span>
<span id="cb8-428"><a href="#cb8-428"></a><span class="ex">cpu_adagrad</span> ............ <span class="pp">[</span><span class="ss">NO</span><span class="pp">]</span> ....... <span class="pp">[</span><span class="ss">OKAY</span><span class="pp">]</span></span>
<span id="cb8-429"><a href="#cb8-429"></a><span class="ex">cpu_adam</span> ............... <span class="pp">[</span><span class="ss">NO</span><span class="pp">]</span> ....... <span class="pp">[</span><span class="ss">OKAY</span><span class="pp">]</span></span>
<span id="cb8-430"><a href="#cb8-430"></a><span class="ex">flash_attn</span> ............. <span class="pp">[</span><span class="ss">NO</span><span class="pp">]</span> ....... <span class="pp">[</span><span class="ss">OKAY</span><span class="pp">]</span></span>
<span id="cb8-431"><a href="#cb8-431"></a><span class="ex">fused_adam</span> ............. <span class="pp">[</span><span class="ss">NO</span><span class="pp">]</span> ....... <span class="pp">[</span><span class="ss">OKAY</span><span class="pp">]</span></span>
<span id="cb8-432"><a href="#cb8-432"></a><span class="ex">transformer_inference</span> .. <span class="pp">[</span><span class="ss">NO</span><span class="pp">]</span> ....... <span class="pp">[</span><span class="ss">OKAY</span><span class="pp">]</span></span>
<span id="cb8-433"><a href="#cb8-433"></a><span class="ex">pack_bits</span> .............. <span class="pp">[</span><span class="ss">NO</span><span class="pp">]</span> ....... <span class="pp">[</span><span class="ss">OKAY</span><span class="pp">]</span></span>
<span id="cb8-434"><a href="#cb8-434"></a><span class="ex">--------------------------------------------------</span></span>
<span id="cb8-435"><a href="#cb8-435"></a><span class="ex">DeepSpeed</span> general environment info:</span>
<span id="cb8-436"><a href="#cb8-436"></a><span class="ex">torch</span> install path ............... <span class="pp">[</span><span class="st">'/opt/aurora/24.180.0/frameworks/aurora_nre_models_frameworks-2024.2.1_u1/lib/python3.10/site-packages/torch'</span><span class="pp">]</span></span>
<span id="cb8-437"><a href="#cb8-437"></a><span class="ex">torch</span> version .................... 2.3.1+cxx11.abi</span>
<span id="cb8-438"><a href="#cb8-438"></a><span class="ex">deepspeed</span> install path ........... <span class="pp">[</span><span class="st">'/lus/flare/projects/Aurora_deployment/foremans/projects/argonne-lcf/Megatron-DeepSpeed/deps/DeepSpeed/deepspeed'</span><span class="pp">]</span></span>
<span id="cb8-439"><a href="#cb8-439"></a><span class="ex">deepspeed</span> info ................... 0.15.3+unknown, unknown, unknown</span>
<span id="cb8-440"><a href="#cb8-440"></a><span class="ex">deepspeed</span> wheel compiled w. ...... torch 2.3</span>
<span id="cb8-441"><a href="#cb8-441"></a><span class="ex">shared</span> memory <span class="er">(</span><span class="ex">/dev/shm</span><span class="kw">)</span> <span class="fu">size</span> .... 503.18 GB</span>
<span id="cb8-442"><a href="#cb8-442"></a><span class="ex">[2024-10-16</span> 15:39:35.319255]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">configs.py:272</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">****</span> Git info for DeepSpeed: git_hash=7ef26bf git_branch=hzheng-data-fix <span class="pp">****</span></span>
<span id="cb8-443"><a href="#cb8-443"></a><span class="ex">[2024-10-16</span> 15:39:35.319927]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:725</span><span class="pp">]</span> <span class="at">-</span> Using oneccl_bindings from: /opt/aurora/24.180.0/frameworks/aurora_nre_models_frameworks-2024.2.1_u1/lib/python3.10/site-packages/oneccl_bindings_for_pytorch/__init__.py</span>
<span id="cb8-444"><a href="#cb8-444"></a><span class="ex">[2024-10-16</span> 15:39:35.320347]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:727</span><span class="pp">]</span> <span class="at">-</span> Using ipex from: /opt/aurora/24.180.0/frameworks/aurora_nre_models_frameworks-2024.2.1_u1/lib/python3.10/site-packages/intel_extension_for_pytorch/__init__.py</span>
<span id="cb8-445"><a href="#cb8-445"></a><span class="ex">[2024-10-16</span> 15:39:35.320734]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:728</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">0/24</span><span class="pp">]</span> Using device=<span class="st">'xpu'</span> with backend=<span class="st">'deepspeed'</span> + <span class="st">'ccl'</span> for distributed training.</span>
<span id="cb8-446"><a href="#cb8-446"></a><span class="ex">[2024-10-16</span> 15:39:35.325748]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">dist.py:348</span><span class="pp">]</span> <span class="at">-</span> <span class="pp">[</span><span class="ss">device=</span><span class="st">'xpu'</span><span class="pp">][</span><span class="ss">rank=0/23</span><span class="pp">][</span><span class="ss">local_rank=0/11</span><span class="pp">][</span><span class="ss">node=0/1</span><span class="pp">]</span></span>
<span id="cb8-447"><a href="#cb8-447"></a><span class="ex">[2024-10-16</span> 15:39:35.326291]<span class="pp">[</span><span class="ss">WARNING</span><span class="pp">][</span><span class="ss">_logger.py:68</span><span class="pp">]</span> <span class="at">-</span> Using [24 / 24] available <span class="st">"xpu"</span> devices !!</span>
<span id="cb8-448"><a href="#cb8-448"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">169065</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">value</span> of CCL_KVS_MODE changed to be mpi <span class="er">(</span><span class="ex">default:pmi</span><span class="kw">)</span></span>
<span id="cb8-449"><a href="#cb8-449"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">169065</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">value</span> of CCL_KVS_CONNECTION_TIMEOUT changed to be 3600 <span class="er">(</span><span class="ex">default:120</span><span class="kw">)</span></span>
<span id="cb8-450"><a href="#cb8-450"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">169065</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">value</span> of CCL_BCAST changed to be double_tree <span class="er">(</span><span class="ex">default:</span><span class="kw">)</span></span>
<span id="cb8-451"><a href="#cb8-451"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">169065</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">value</span> of CCL_ENABLE_SYCL_KERNELS changed to be 1 <span class="er">(</span><span class="ex">default:0</span><span class="kw">)</span></span>
<span id="cb8-452"><a href="#cb8-452"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">169065</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">value</span> of CCL_SYCL_ESIMD changed to be 1 <span class="er">(</span><span class="ex">default:0</span><span class="kw">)</span></span>
<span id="cb8-453"><a href="#cb8-453"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">169065</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">value</span> of CCL_PROCESS_LAUNCHER changed to be pmix <span class="er">(</span><span class="ex">default:hydra</span><span class="kw">)</span></span>
<span id="cb8-454"><a href="#cb8-454"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">169065</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">value</span> of CCL_ZE_CACHE_OPEN_IPC_HANDLES_THRESHOLD changed to be 32768 <span class="er">(</span><span class="ex">default:1000</span><span class="kw">)</span></span>
<span id="cb8-455"><a href="#cb8-455"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">169065</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="va">CCL_ALLGATHERV_MEDIUM_SIZE_THRESHOLD</span><span class="op">=</span>0 <span class="ex">is</span> unknown to and unused by oneCCL code but is present in the environment, check if it is not mistyped.</span>
<span id="cb8-456"><a href="#cb8-456"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">169065</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="va">CCL_SKIP_SCHEDULER</span><span class="op">=</span>1 <span class="ex">is</span> unknown to and unused by oneCCL code but is present in the environment, check if it is not mistyped.</span>
<span id="cb8-457"><a href="#cb8-457"></a><span class="ex">2024:10:16-15:39:35:</span><span class="er">(</span><span class="ex">169065</span><span class="kw">)</span> <span class="kw">|</span><span class="ex">CCL_WARN</span><span class="kw">|</span> <span class="ex">MPI</span> was initialized externally, CCL-MPI specific environment is ignored</span>
<span id="cb8-458"><a href="#cb8-458"></a><span class="ex">using</span> world size: 24, data-parallel-size: 24, sequence-parallel size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1</span>
<span id="cb8-459"><a href="#cb8-459"></a><span class="ex">accumulate</span> and all-reduce gradients in fp32 for bfloat16 data type.</span>
<span id="cb8-460"><a href="#cb8-460"></a><span class="ex">using</span> torch.bfloat16 for parameters ...</span>
<span id="cb8-461"><a href="#cb8-461"></a><span class="ex">------------------------</span> arguments <span class="at">------------------------</span></span>
<span id="cb8-462"><a href="#cb8-462"></a>  <span class="ex">accumulate_allreduce_grads_in_fp32</span> .............. True</span>
<span id="cb8-463"><a href="#cb8-463"></a>  <span class="ex">adam_beta1</span> ...................................... 0.9</span>
<span id="cb8-464"><a href="#cb8-464"></a>  <span class="ex">adam_beta2</span> ...................................... 0.999</span>
<span id="cb8-465"><a href="#cb8-465"></a>  <span class="ex">adam_eps</span> ........................................ 1e-08</span>
<span id="cb8-466"><a href="#cb8-466"></a>  <span class="ex">add_bias_linear</span> ................................. False</span>
<span id="cb8-467"><a href="#cb8-467"></a>  <span class="ex">add_position_embedding</span> .......................... False</span>
<span id="cb8-468"><a href="#cb8-468"></a>  <span class="ex">adlr_autoresume</span> ................................. False</span>
<span id="cb8-469"><a href="#cb8-469"></a>  <span class="ex">adlr_autoresume_interval</span> ........................ 1000</span>
<span id="cb8-470"><a href="#cb8-470"></a>  <span class="ex">aml_data_download_path</span> .......................... None</span>
<span id="cb8-471"><a href="#cb8-471"></a>  <span class="ex">apply_layernorm_1p</span> .............................. False</span>
<span id="cb8-472"><a href="#cb8-472"></a>  <span class="ex">apply_query_key_layer_scaling</span> ................... False</span>
<span id="cb8-473"><a href="#cb8-473"></a>  <span class="ex">apply_residual_connection_post_layernorm</span> ........ False</span>
<span id="cb8-474"><a href="#cb8-474"></a>  <span class="ex">async_tensor_model_parallel_allreduce</span> ........... False</span>
<span id="cb8-475"><a href="#cb8-475"></a>  <span class="ex">attention_dropout</span> ............................... 0.0</span>
<span id="cb8-476"><a href="#cb8-476"></a>  <span class="ex">attention_softmax_in_fp32</span> ....................... False</span>
<span id="cb8-477"><a href="#cb8-477"></a>  <span class="ex">barrier_with_L1_time</span> ............................ True</span>
<span id="cb8-478"><a href="#cb8-478"></a>  <span class="ex">bert_binary_head</span> ................................ True</span>
<span id="cb8-479"><a href="#cb8-479"></a>  <span class="ex">bert_embedder_type</span> .............................. megatron</span>
<span id="cb8-480"><a href="#cb8-480"></a>  <span class="ex">bert_load</span> ....................................... None</span>
<span id="cb8-481"><a href="#cb8-481"></a>  <span class="ex">bf16</span> ............................................ True</span>
<span id="cb8-482"><a href="#cb8-482"></a>  <span class="ex">bias_dropout_fusion</span> ............................. False</span>
<span id="cb8-483"><a href="#cb8-483"></a>  <span class="ex">bias_gelu_fusion</span> ................................ False</span>
<span id="cb8-484"><a href="#cb8-484"></a>  <span class="ex">biencoder_projection_dim</span> ........................ 0</span>
<span id="cb8-485"><a href="#cb8-485"></a>  <span class="ex">biencoder_shared_query_context_model</span> ............ False</span>
<span id="cb8-486"><a href="#cb8-486"></a>  <span class="ex">block_data_path</span> ................................. None</span>
<span id="cb8-487"><a href="#cb8-487"></a>  <span class="ex">checkpoint_activations</span> .......................... False</span>
<span id="cb8-488"><a href="#cb8-488"></a>  <span class="ex">checkpoint_in_cpu</span> ............................... False</span>
<span id="cb8-489"><a href="#cb8-489"></a>  <span class="ex">checkpoint_num_layers</span> ........................... 1</span>
<span id="cb8-490"><a href="#cb8-490"></a>  <span class="ex">classes_fraction</span> ................................ 1.0</span>
<span id="cb8-491"><a href="#cb8-491"></a>  <span class="ex">clip_grad</span> ....................................... 1.0</span>
<span id="cb8-492"><a href="#cb8-492"></a>  <span class="ex">compression_training</span> ............................ False</span>
<span id="cb8-493"><a href="#cb8-493"></a>  <span class="ex">consumed_train_samples</span> .......................... 0</span>
<span id="cb8-494"><a href="#cb8-494"></a>  <span class="ex">consumed_train_tokens</span> ........................... 0</span>
<span id="cb8-495"><a href="#cb8-495"></a>  <span class="ex">consumed_valid_samples</span> .......................... 0</span>
<span id="cb8-496"><a href="#cb8-496"></a>  <span class="ex">contigious_checkpointing</span> ........................ False</span>
<span id="cb8-497"><a href="#cb8-497"></a>  <span class="ex">cpu_optimizer</span> ................................... False</span>
<span id="cb8-498"><a href="#cb8-498"></a>  <span class="ex">cpu_torch_adam</span> .................................. False</span>
<span id="cb8-499"><a href="#cb8-499"></a>  <span class="ex">create_moe_param_group</span> .......................... False</span>
<span id="cb8-500"><a href="#cb8-500"></a>  <span class="ex">curriculum_learning_legacy</span> ...................... False</span>
<span id="cb8-501"><a href="#cb8-501"></a>  <span class="ex">data_cache_path</span> ................................. ./.cache</span>
<span id="cb8-502"><a href="#cb8-502"></a>  <span class="ex">data_efficiency_curriculum_learning</span> ............. False</span>
<span id="cb8-503"><a href="#cb8-503"></a>  <span class="ex">data_file_list</span> .................................. None</span>
<span id="cb8-504"><a href="#cb8-504"></a>  <span class="ex">data_impl</span> ....................................... infer</span>
<span id="cb8-505"><a href="#cb8-505"></a>  <span class="ex">data_parallel_random_init</span> ....................... False</span>
<span id="cb8-506"><a href="#cb8-506"></a>  <span class="ex">data_parallel_size</span> .............................. 24</span>
<span id="cb8-507"><a href="#cb8-507"></a>  <span class="ex">data_path</span> ....................................... <span class="pp">[</span><span class="st">'/lus/flare/projects/candle_aesp_CNDA/azton/data/v2/megatron/dataset_v2_wtextbooks_text_document'</span><span class="pp">]</span></span>
<span id="cb8-508"><a href="#cb8-508"></a>  <span class="ex">data_per_class_fraction</span> ......................... 1.0</span>
<span id="cb8-509"><a href="#cb8-509"></a>  <span class="ex">data_sharding</span> ................................... True</span>
<span id="cb8-510"><a href="#cb8-510"></a>  <span class="ex">dataloader_type</span> ................................. single</span>
<span id="cb8-511"><a href="#cb8-511"></a>  <span class="ex">DDP_impl</span> ........................................ local</span>
<span id="cb8-512"><a href="#cb8-512"></a>  <span class="ex">decoder_num_layers</span> .............................. None</span>
<span id="cb8-513"><a href="#cb8-513"></a>  <span class="ex">decoder_seq_length</span> .............................. None</span>
<span id="cb8-514"><a href="#cb8-514"></a>  <span class="ex">deepscale</span> ....................................... False</span>
<span id="cb8-515"><a href="#cb8-515"></a>  <span class="ex">deepscale_config</span> ................................ None</span>
<span id="cb8-516"><a href="#cb8-516"></a>  <span class="ex">deepspeed</span> ....................................... True</span>
<span id="cb8-517"><a href="#cb8-517"></a>  <span class="ex">deepspeed_activation_checkpointing</span> .............. False</span>
<span id="cb8-518"><a href="#cb8-518"></a>  <span class="ex">deepspeed_config</span> ................................ ./examples_deepspeed/finetune_hf_llama/ds_config.json</span>
<span id="cb8-519"><a href="#cb8-519"></a>  <span class="ex">dino_bottleneck_size</span> ............................ 256</span>
<span id="cb8-520"><a href="#cb8-520"></a>  <span class="ex">dino_freeze_last_layer</span> .......................... 1</span>
<span id="cb8-521"><a href="#cb8-521"></a>  <span class="ex">dino_head_hidden_size</span> ........................... 2048</span>
<span id="cb8-522"><a href="#cb8-522"></a>  <span class="ex">dino_local_crops_number</span> ......................... 10</span>
<span id="cb8-523"><a href="#cb8-523"></a>  <span class="ex">dino_local_img_size</span> ............................. 96</span>
<span id="cb8-524"><a href="#cb8-524"></a>  <span class="ex">dino_norm_last_layer</span> ............................ False</span>
<span id="cb8-525"><a href="#cb8-525"></a>  <span class="ex">dino_teacher_temp</span> ............................... 0.07</span>
<span id="cb8-526"><a href="#cb8-526"></a>  <span class="ex">dino_warmup_teacher_temp</span> ........................ 0.04</span>
<span id="cb8-527"><a href="#cb8-527"></a>  <span class="ex">dino_warmup_teacher_temp_epochs</span> ................. 30</span>
<span id="cb8-528"><a href="#cb8-528"></a>  <span class="ex">distribute_checkpointed_activations</span> ............. False</span>
<span id="cb8-529"><a href="#cb8-529"></a>  <span class="ex">distribute_saved_activations</span> .................... False</span>
<span id="cb8-530"><a href="#cb8-530"></a>  <span class="ex">distributed_backend</span> ............................. ccl</span>
<span id="cb8-531"><a href="#cb8-531"></a>  <span class="ex">distributed_timeout_minutes</span> ..................... 10</span>
<span id="cb8-532"><a href="#cb8-532"></a>  <span class="ex">ds_fused_adam</span> ................................... False</span>
<span id="cb8-533"><a href="#cb8-533"></a>  <span class="ex">ds_inference</span> .................................... False</span>
<span id="cb8-534"><a href="#cb8-534"></a>  <span class="ex">ds_pipeline_enabled</span> ............................. True</span>
<span id="cb8-535"><a href="#cb8-535"></a>  <span class="ex">ds_sequence_parallel_size</span> ....................... 1</span>
<span id="cb8-536"><a href="#cb8-536"></a>  <span class="ex">embedding_path</span> .................................. None</span>
<span id="cb8-537"><a href="#cb8-537"></a>  <span class="ex">embedding_weights_in_fp32</span> ....................... False</span>
<span id="cb8-538"><a href="#cb8-538"></a>  <span class="ex">empty_unused_memory_level</span> ....................... 0</span>
<span id="cb8-539"><a href="#cb8-539"></a>  <span class="ex">enable_expert_tensor_parallelism</span> ................ False</span>
<span id="cb8-540"><a href="#cb8-540"></a>  <span class="ex">enable_zbh1_exact_semantics</span> ..................... False</span>
<span id="cb8-541"><a href="#cb8-541"></a>  <span class="ex">enable_zbh1_pipeline</span> ............................ False</span>
<span id="cb8-542"><a href="#cb8-542"></a>  <span class="ex">encoder_num_layers</span> .............................. 32</span>
<span id="cb8-543"><a href="#cb8-543"></a>  <span class="ex">encoder_seq_length</span> .............................. 2048</span>
<span id="cb8-544"><a href="#cb8-544"></a>  <span class="ex">end_weight_decay</span> ................................ 0.1</span>
<span id="cb8-545"><a href="#cb8-545"></a>  <span class="ex">eod_mask_loss</span> ................................... False</span>
<span id="cb8-546"><a href="#cb8-546"></a>  <span class="ex">eval_interval</span> ................................... 100</span>
<span id="cb8-547"><a href="#cb8-547"></a>  <span class="ex">eval_iters</span> ...................................... 100</span>
<span id="cb8-548"><a href="#cb8-548"></a>  <span class="ex">evidence_data_path</span> .............................. None</span>
<span id="cb8-549"><a href="#cb8-549"></a>  <span class="ex">exit_duration_in_mins</span> ........................... None</span>
<span id="cb8-550"><a href="#cb8-550"></a>  <span class="ex">exit_interval</span> ................................... None</span>
<span id="cb8-551"><a href="#cb8-551"></a>  <span class="ex">exit_on_missing_checkpoint</span> ...................... False</span>
<span id="cb8-552"><a href="#cb8-552"></a>  <span class="ex">exit_signal_handler</span> ............................. False</span>
<span id="cb8-553"><a href="#cb8-553"></a>  <span class="ex">expert_interval</span> ................................. 2</span>
<span id="cb8-554"><a href="#cb8-554"></a>  <span class="ex">ffn_hidden_size</span> ................................. 14336</span>
<span id="cb8-555"><a href="#cb8-555"></a>  <span class="ex">finetune</span> ........................................ False</span>
<span id="cb8-556"><a href="#cb8-556"></a>  <span class="ex">force_ds_sequence_parallel</span> ...................... False</span>
<span id="cb8-557"><a href="#cb8-557"></a>  <span class="ex">fp16</span> ............................................ False</span>
<span id="cb8-558"><a href="#cb8-558"></a>  <span class="ex">fp16_lm_cross_entropy</span> ........................... False</span>
<span id="cb8-559"><a href="#cb8-559"></a>  <span class="ex">fp32_residual_connection</span> ........................ False</span>
<span id="cb8-560"><a href="#cb8-560"></a>  <span class="ex">fp8_amax_compute_algo</span> ........................... most_recent</span>
<span id="cb8-561"><a href="#cb8-561"></a>  <span class="ex">fp8_amax_history_len</span> ............................ 1</span>
<span id="cb8-562"><a href="#cb8-562"></a>  <span class="ex">fp8_e4m3</span> ........................................ False</span>
<span id="cb8-563"><a href="#cb8-563"></a>  <span class="ex">fp8_hybrid</span> ...................................... False</span>
<span id="cb8-564"><a href="#cb8-564"></a>  <span class="ex">fp8_interval</span> .................................... 1</span>
<span id="cb8-565"><a href="#cb8-565"></a>  <span class="ex">fp8_margin</span> ...................................... 0</span>
<span id="cb8-566"><a href="#cb8-566"></a>  <span class="ex">fp8_wgrad</span> ....................................... True</span>
<span id="cb8-567"><a href="#cb8-567"></a>  <span class="ex">global_batch_size</span> ............................... 24</span>
<span id="cb8-568"><a href="#cb8-568"></a>  <span class="ex">gradient_accumulation_fusion</span> .................... False</span>
<span id="cb8-569"><a href="#cb8-569"></a>  <span class="ex">head_lr_mult</span> .................................... 1.0</span>
<span id="cb8-570"><a href="#cb8-570"></a>  <span class="ex">hf_ckpt_dir</span> ..................................... /flare/Aurora_deployment/foremans/Meta-Llama-3.1-8B-128512</span>
<span id="cb8-571"><a href="#cb8-571"></a>  <span class="ex">hf_ckpt_num_shards</span> .............................. 1</span>
<span id="cb8-572"><a href="#cb8-572"></a>  <span class="ex">hidden_dropout</span> .................................. 0.0</span>
<span id="cb8-573"><a href="#cb8-573"></a>  <span class="ex">hidden_size</span> ..................................... 4096</span>
<span id="cb8-574"><a href="#cb8-574"></a>  <span class="ex">hidden_size_teacher</span> ............................. None</span>
<span id="cb8-575"><a href="#cb8-575"></a>  <span class="ex">hysteresis</span> ...................................... 2</span>
<span id="cb8-576"><a href="#cb8-576"></a>  <span class="ex">ict_head_size</span> ................................... None</span>
<span id="cb8-577"><a href="#cb8-577"></a>  <span class="ex">ict_load</span> ........................................ None</span>
<span id="cb8-578"><a href="#cb8-578"></a>  <span class="ex">img_h</span> ........................................... 224</span>
<span id="cb8-579"><a href="#cb8-579"></a>  <span class="ex">img_w</span> ........................................... 224</span>
<span id="cb8-580"><a href="#cb8-580"></a>  <span class="ex">indexer_batch_size</span> .............................. 128</span>
<span id="cb8-581"><a href="#cb8-581"></a>  <span class="ex">indexer_log_interval</span> ............................ 1000</span>
<span id="cb8-582"><a href="#cb8-582"></a>  <span class="ex">inference</span> ....................................... False</span>
<span id="cb8-583"><a href="#cb8-583"></a>  <span class="ex">inference_batch_times_seqlen_threshold</span> .......... 512</span>
<span id="cb8-584"><a href="#cb8-584"></a>  <span class="ex">init_method_std</span> ................................. 0.02</span>
<span id="cb8-585"><a href="#cb8-585"></a>  <span class="ex">init_method_xavier_uniform</span> ...................... False</span>
<span id="cb8-586"><a href="#cb8-586"></a>  <span class="ex">initial_loss_scale</span> .............................. 4294967296</span>
<span id="cb8-587"><a href="#cb8-587"></a>  <span class="ex">iter_per_epoch</span> .................................. 1250</span>
<span id="cb8-588"><a href="#cb8-588"></a>  <span class="ex">kd</span> .............................................. False</span>
<span id="cb8-589"><a href="#cb8-589"></a>  <span class="ex">kd_alpha_ce</span> ..................................... 1</span>
<span id="cb8-590"><a href="#cb8-590"></a>  <span class="ex">kd_beta_ce</span> ...................................... 1</span>
<span id="cb8-591"><a href="#cb8-591"></a>  <span class="ex">kd_temp</span> ......................................... 1.0</span>
<span id="cb8-592"><a href="#cb8-592"></a>  <span class="ex">kill_switch_file</span> ................................ None</span>
<span id="cb8-593"><a href="#cb8-593"></a>  <span class="ex">kv_channels</span> ..................................... 128</span>
<span id="cb8-594"><a href="#cb8-594"></a>  <span class="ex">layernorm_epsilon</span> ............................... 1e-05</span>
<span id="cb8-595"><a href="#cb8-595"></a>  <span class="ex">lazy_mpu_init</span> ................................... None</span>
<span id="cb8-596"><a href="#cb8-596"></a>  <span class="ex">load</span> ............................................ None</span>
<span id="cb8-597"><a href="#cb8-597"></a>  <span class="ex">load_mode</span> ....................................... auto</span>
<span id="cb8-598"><a href="#cb8-598"></a>  <span class="ex">load_tag</span> ........................................ None</span>
<span id="cb8-599"><a href="#cb8-599"></a>  <span class="ex">load_teacher</span> .................................... None</span>
<span id="cb8-600"><a href="#cb8-600"></a>  <span class="ex">local_rank</span> ...................................... None</span>
<span id="cb8-601"><a href="#cb8-601"></a>  <span class="ex">log_batch_size_to_tensorboard</span> ................... False</span>
<span id="cb8-602"><a href="#cb8-602"></a>  <span class="ex">log_interval</span> .................................... 1</span>
<span id="cb8-603"><a href="#cb8-603"></a>  <span class="ex">log_learning_rate_to_tensorboard</span> ................ True</span>
<span id="cb8-604"><a href="#cb8-604"></a>  <span class="ex">log_loss_scale_to_tensorboard</span> ................... True</span>
<span id="cb8-605"><a href="#cb8-605"></a>  <span class="ex">log_memory_to_tensorboard</span> ....................... False</span>
<span id="cb8-606"><a href="#cb8-606"></a>  <span class="ex">log_num_zeros_in_grad</span> ........................... False</span>
<span id="cb8-607"><a href="#cb8-607"></a>  <span class="ex">log_optimizer_states_to_tensorboard</span> ............. False</span>
<span id="cb8-608"><a href="#cb8-608"></a>  <span class="ex">log_params_norm</span> ................................. False</span>
<span id="cb8-609"><a href="#cb8-609"></a>  <span class="ex">log_timers_to_tensorboard</span> ....................... False</span>
<span id="cb8-610"><a href="#cb8-610"></a>  <span class="ex">log_validation_ppl_to_tensorboard</span> ............... False</span>
<span id="cb8-611"><a href="#cb8-611"></a>  <span class="ex">log_world_size_to_tensorboard</span> ................... False</span>
<span id="cb8-612"><a href="#cb8-612"></a>  <span class="ex">loss_scale</span> ...................................... None</span>
<span id="cb8-613"><a href="#cb8-613"></a>  <span class="ex">loss_scale_window</span> ............................... 1000</span>
<span id="cb8-614"><a href="#cb8-614"></a>  <span class="ex">lr</span> .............................................. 2e-05</span>
<span id="cb8-615"><a href="#cb8-615"></a>  <span class="ex">lr_decay_iters</span> .................................. 320000</span>
<span id="cb8-616"><a href="#cb8-616"></a>  <span class="ex">lr_decay_samples</span> ................................ None</span>
<span id="cb8-617"><a href="#cb8-617"></a>  <span class="ex">lr_decay_style</span> .................................. cosine</span>
<span id="cb8-618"><a href="#cb8-618"></a>  <span class="ex">lr_decay_tokens</span> ................................. None</span>
<span id="cb8-619"><a href="#cb8-619"></a>  <span class="ex">lr_warmup_fraction</span> .............................. None</span>
<span id="cb8-620"><a href="#cb8-620"></a>  <span class="ex">lr_warmup_iters</span> ................................. 2000</span>
<span id="cb8-621"><a href="#cb8-621"></a>  <span class="ex">lr_warmup_samples</span> ............................... 0</span>
<span id="cb8-622"><a href="#cb8-622"></a>  <span class="ex">lr_warmup_tokens</span> ................................ None</span>
<span id="cb8-623"><a href="#cb8-623"></a>  <span class="ex">make_vocab_size_divisible_by</span> .................... 128512</span>
<span id="cb8-624"><a href="#cb8-624"></a>  <span class="ex">mask_factor</span> ..................................... 1.0</span>
<span id="cb8-625"><a href="#cb8-625"></a>  <span class="ex">mask_prob</span> ....................................... 0.15</span>
<span id="cb8-626"><a href="#cb8-626"></a>  <span class="ex">mask_type</span> ....................................... random</span>
<span id="cb8-627"><a href="#cb8-627"></a>  <span class="ex">masked_softmax_fusion</span> ........................... False</span>
<span id="cb8-628"><a href="#cb8-628"></a>  <span class="ex">max_position_embeddings</span> ......................... 2048</span>
<span id="cb8-629"><a href="#cb8-629"></a>  <span class="ex">max_tokens_to_oom</span> ............................... 12000</span>
<span id="cb8-630"><a href="#cb8-630"></a>  <span class="ex">mem_efficient_ln</span> ................................ True</span>
<span id="cb8-631"><a href="#cb8-631"></a>  <span class="ex">memory_centric_tiled_linear</span> ..................... False</span>
<span id="cb8-632"><a href="#cb8-632"></a>  <span class="ex">merge_file</span> ...................................... None</span>
<span id="cb8-633"><a href="#cb8-633"></a>  <span class="ex">micro_batch_size</span> ................................ 1</span>
<span id="cb8-634"><a href="#cb8-634"></a>  <span class="ex">min_loss_scale</span> .................................. 1.0</span>
<span id="cb8-635"><a href="#cb8-635"></a>  <span class="ex">min_lr</span> .......................................... 0.0</span>
<span id="cb8-636"><a href="#cb8-636"></a>  <span class="ex">mlp_type</span> ........................................ standard</span>
<span id="cb8-637"><a href="#cb8-637"></a>  <span class="ex">mmap_warmup</span> ..................................... False</span>
<span id="cb8-638"><a href="#cb8-638"></a>  <span class="ex">moe_eval_capacity_factor</span> ........................ 1.0</span>
<span id="cb8-639"><a href="#cb8-639"></a>  <span class="ex">moe_expert_parallel_size</span> ........................ 1</span>
<span id="cb8-640"><a href="#cb8-640"></a>  <span class="ex">moe_loss_coeff</span> .................................. 0.1</span>
<span id="cb8-641"><a href="#cb8-641"></a>  <span class="ex">moe_min_capacity</span> ................................ 4</span>
<span id="cb8-642"><a href="#cb8-642"></a>  <span class="ex">moe_token_dropping</span> .............................. True</span>
<span id="cb8-643"><a href="#cb8-643"></a>  <span class="ex">moe_top2_2nd_expert_sampling</span> .................... True</span>
<span id="cb8-644"><a href="#cb8-644"></a>  <span class="ex">moe_train_capacity_factor</span> ....................... 1.0</span>
<span id="cb8-645"><a href="#cb8-645"></a>  <span class="ex">mos</span> ............................................. False</span>
<span id="cb8-646"><a href="#cb8-646"></a>  <span class="ex">multiprocessing_context</span> ......................... fork</span>
<span id="cb8-647"><a href="#cb8-647"></a>  <span class="ex">no_load_lr_state</span> ................................ False</span>
<span id="cb8-648"><a href="#cb8-648"></a>  <span class="ex">no_load_optim</span> ................................... None</span>
<span id="cb8-649"><a href="#cb8-649"></a>  <span class="ex">no_load_rng</span> ..................................... None</span>
<span id="cb8-650"><a href="#cb8-650"></a>  <span class="ex">no_persist_layer_norm</span> ........................... False</span>
<span id="cb8-651"><a href="#cb8-651"></a>  <span class="ex">no_pipeline_parallel</span> ............................ False</span>
<span id="cb8-652"><a href="#cb8-652"></a>  <span class="ex">no_save_optim</span> ................................... None</span>
<span id="cb8-653"><a href="#cb8-653"></a>  <span class="ex">no_save_rng</span> ..................................... None</span>
<span id="cb8-654"><a href="#cb8-654"></a>  <span class="ex">normalization</span> ................................... rmsnorm</span>
<span id="cb8-655"><a href="#cb8-655"></a>  <span class="ex">num_attention_heads</span> ............................. 32</span>
<span id="cb8-656"><a href="#cb8-656"></a>  <span class="ex">num_attention_heads_teacher</span> ..................... None</span>
<span id="cb8-657"><a href="#cb8-657"></a>  <span class="ex">num_channels</span> .................................... 3</span>
<span id="cb8-658"><a href="#cb8-658"></a>  <span class="ex">num_classes</span> ..................................... 1000</span>
<span id="cb8-659"><a href="#cb8-659"></a>  <span class="ex">num_experts</span> ..................................... <span class="pp">[</span><span class="ss">1</span><span class="pp">]</span></span>
<span id="cb8-660"><a href="#cb8-660"></a>  <span class="ex">num_experts_switch</span> .............................. None</span>
<span id="cb8-661"><a href="#cb8-661"></a>  <span class="ex">num_experts_teacher</span> ............................. <span class="pp">[</span><span class="ss">1</span><span class="pp">]</span></span>
<span id="cb8-662"><a href="#cb8-662"></a>  <span class="ex">num_key_value_heads</span> ............................. 8</span>
<span id="cb8-663"><a href="#cb8-663"></a>  <span class="ex">num_layers</span> ...................................... 32</span>
<span id="cb8-664"><a href="#cb8-664"></a>  <span class="ex">num_layers_per_virtual_pipeline_stage</span> ........... None</span>
<span id="cb8-665"><a href="#cb8-665"></a>  <span class="ex">num_layers_teacher</span> .............................. None</span>
<span id="cb8-666"><a href="#cb8-666"></a>  <span class="ex">num_workers</span> ..................................... 2</span>
<span id="cb8-667"><a href="#cb8-667"></a>  <span class="ex">onnx_safe</span> ....................................... None</span>
<span id="cb8-668"><a href="#cb8-668"></a>  <span class="ex">openai_gelu</span> ..................................... False</span>
<span id="cb8-669"><a href="#cb8-669"></a>  <span class="ex">optimizer</span> ....................................... adam</span>
<span id="cb8-670"><a href="#cb8-670"></a>  <span class="ex">output_bert_embeddings</span> .......................... False</span>
<span id="cb8-671"><a href="#cb8-671"></a>  <span class="ex">overlap_p2p_comm</span> ................................ False</span>
<span id="cb8-672"><a href="#cb8-672"></a>  <span class="ex">override_opt_param_scheduler</span> .................... False</span>
<span id="cb8-673"><a href="#cb8-673"></a>  <span class="ex">params_dtype</span> .................................... torch.bfloat16</span>
<span id="cb8-674"><a href="#cb8-674"></a>  <span class="ex">partition_activations</span> ........................... False</span>
<span id="cb8-675"><a href="#cb8-675"></a>  <span class="ex">patch_dim</span> ....................................... 16</span>
<span id="cb8-676"><a href="#cb8-676"></a>  <span class="ex">perform_initialization</span> .......................... True</span>
<span id="cb8-677"><a href="#cb8-677"></a>  <span class="ex">pipeline_model_parallel_size</span> .................... 1</span>
<span id="cb8-678"><a href="#cb8-678"></a>  <span class="ex">pipeline_model_parallel_split_rank</span> .............. None</span>
<span id="cb8-679"><a href="#cb8-679"></a>  <span class="ex">profile</span> ......................................... None</span>
<span id="cb8-680"><a href="#cb8-680"></a>  <span class="ex">profile_backward</span> ................................ False</span>
<span id="cb8-681"><a href="#cb8-681"></a>  <span class="ex">profile_ranks</span> ................................... None</span>
<span id="cb8-682"><a href="#cb8-682"></a>  <span class="ex">profile_steps</span> ................................... 2,3</span>
<span id="cb8-683"><a href="#cb8-683"></a>  <span class="ex">query_in_block_prob</span> ............................. 0.1</span>
<span id="cb8-684"><a href="#cb8-684"></a>  <span class="ex">rampup_batch_size</span> ............................... None</span>
<span id="cb8-685"><a href="#cb8-685"></a>  <span class="ex">random_ltd</span> ...................................... False</span>
<span id="cb8-686"><a href="#cb8-686"></a>  <span class="ex">rank</span> ............................................ 0</span>
<span id="cb8-687"><a href="#cb8-687"></a>  <span class="ex">recompute_granularity</span> ........................... None</span>
<span id="cb8-688"><a href="#cb8-688"></a>  <span class="ex">recompute_method</span> ................................ None</span>
<span id="cb8-689"><a href="#cb8-689"></a>  <span class="ex">recompute_num_layers</span> ............................ 1</span>
<span id="cb8-690"><a href="#cb8-690"></a>  <span class="ex">remote_device</span> ................................... none</span>
<span id="cb8-691"><a href="#cb8-691"></a>  <span class="ex">repeated_dataloader</span> ............................. True</span>
<span id="cb8-692"><a href="#cb8-692"></a>  <span class="ex">reset_attention_mask</span> ............................ False</span>
<span id="cb8-693"><a href="#cb8-693"></a>  <span class="ex">reset_iteration</span> ................................. False</span>
<span id="cb8-694"><a href="#cb8-694"></a>  <span class="ex">reset_position_ids</span> .............................. False</span>
<span id="cb8-695"><a href="#cb8-695"></a>  <span class="ex">retriever_report_topk_accuracies</span> ................ []</span>
<span id="cb8-696"><a href="#cb8-696"></a>  <span class="ex">retriever_score_scaling</span> ......................... False</span>
<span id="cb8-697"><a href="#cb8-697"></a>  <span class="ex">retriever_seq_length</span> ............................ 256</span>
<span id="cb8-698"><a href="#cb8-698"></a>  <span class="ex">retro_add_retriever</span> ............................. False</span>
<span id="cb8-699"><a href="#cb8-699"></a>  <span class="ex">retro_cyclic_train_iters</span> ........................ None</span>
<span id="cb8-700"><a href="#cb8-700"></a>  <span class="ex">retro_encoder_attention_dropout</span> ................. 0.1</span>
<span id="cb8-701"><a href="#cb8-701"></a>  <span class="ex">retro_encoder_hidden_dropout</span> .................... 0.1</span>
<span id="cb8-702"><a href="#cb8-702"></a>  <span class="ex">retro_encoder_layers</span> ............................ 2</span>
<span id="cb8-703"><a href="#cb8-703"></a>  <span class="ex">retro_num_neighbors</span> ............................. 2</span>
<span id="cb8-704"><a href="#cb8-704"></a>  <span class="ex">retro_num_retrieved_chunks</span> ...................... 2</span>
<span id="cb8-705"><a href="#cb8-705"></a>  <span class="ex">retro_return_doc_ids</span> ............................ False</span>
<span id="cb8-706"><a href="#cb8-706"></a>  <span class="ex">retro_workdir</span> ................................... None</span>
<span id="cb8-707"><a href="#cb8-707"></a>  <span class="ex">return_data_index</span> ............................... False</span>
<span id="cb8-708"><a href="#cb8-708"></a>  <span class="ex">rope_theta</span> ...................................... 10000</span>
<span id="cb8-709"><a href="#cb8-709"></a>  <span class="ex">rotary_percent</span> .................................. 1.0</span>
<span id="cb8-710"><a href="#cb8-710"></a>  <span class="ex">sample_rate</span> ..................................... 1.0</span>
<span id="cb8-711"><a href="#cb8-711"></a>  <span class="ex">save</span> ............................................ ckpt-mds-llama-3/</span>
<span id="cb8-712"><a href="#cb8-712"></a>  <span class="ex">save_interval</span> ................................... 1500</span>
<span id="cb8-713"><a href="#cb8-713"></a>  <span class="ex">scatter_gather_tensors_in_pipeline</span> .............. True</span>
<span id="cb8-714"><a href="#cb8-714"></a>  <span class="ex">scattered_embeddings</span> ............................ False</span>
<span id="cb8-715"><a href="#cb8-715"></a>  <span class="ex">schedulefree_for_each</span> ........................... False</span>
<span id="cb8-716"><a href="#cb8-716"></a>  <span class="ex">seed</span> ............................................ 1234</span>
<span id="cb8-717"><a href="#cb8-717"></a>  <span class="ex">seq_length</span> ...................................... 2048</span>
<span id="cb8-718"><a href="#cb8-718"></a>  <span class="ex">sequence_parallel</span> ............................... False</span>
<span id="cb8-719"><a href="#cb8-719"></a>  <span class="ex">sgd_momentum</span> .................................... 0.9</span>
<span id="cb8-720"><a href="#cb8-720"></a>  <span class="ex">short_seq_prob</span> .................................. 0.1</span>
<span id="cb8-721"><a href="#cb8-721"></a>  <span class="ex">shuffle_sample</span> .................................. False</span>
<span id="cb8-722"><a href="#cb8-722"></a>  <span class="ex">skip_train</span> ...................................... False</span>
<span id="cb8-723"><a href="#cb8-723"></a>  <span class="ex">sophiag_beta1</span> ................................... 0.9</span>
<span id="cb8-724"><a href="#cb8-724"></a>  <span class="ex">sophiag_beta2</span> ................................... 0.95</span>
<span id="cb8-725"><a href="#cb8-725"></a>  <span class="ex">sophiag_rho</span> ..................................... 0.01</span>
<span id="cb8-726"><a href="#cb8-726"></a>  <span class="fu">split</span> ........................................... 100,0,0</span>
<span id="cb8-727"><a href="#cb8-727"></a>  <span class="ex">split_transformers</span> .............................. False</span>
<span id="cb8-728"><a href="#cb8-728"></a>  <span class="ex">squared_relu</span> .................................... False</span>
<span id="cb8-729"><a href="#cb8-729"></a>  <span class="ex">standalone_embedding_stage</span> ...................... False</span>
<span id="cb8-730"><a href="#cb8-730"></a>  <span class="ex">start_weight_decay</span> .............................. 0.1</span>
<span id="cb8-731"><a href="#cb8-731"></a>  <span class="ex">swiglu</span> .......................................... True</span>
<span id="cb8-732"><a href="#cb8-732"></a>  <span class="ex">swin_backbone_type</span> .............................. tiny</span>
<span id="cb8-733"><a href="#cb8-733"></a>  <span class="ex">synchronize_each_layer</span> .......................... False</span>
<span id="cb8-734"><a href="#cb8-734"></a>  <span class="ex">tensor_model_parallel_size</span> ...................... 1</span>
<span id="cb8-735"><a href="#cb8-735"></a>  <span class="ex">tensorboard_dir</span> ................................. tensorboard_output</span>
<span id="cb8-736"><a href="#cb8-736"></a>  <span class="ex">tensorboard_log_interval</span> ........................ 1</span>
<span id="cb8-737"><a href="#cb8-737"></a>  <span class="ex">tensorboard_queue_size</span> .......................... 1000</span>
<span id="cb8-738"><a href="#cb8-738"></a>  <span class="ex">test_data_path</span> .................................. None</span>
<span id="cb8-739"><a href="#cb8-739"></a>  <span class="ex">tile_factor</span> ..................................... 1</span>
<span id="cb8-740"><a href="#cb8-740"></a>  <span class="ex">timing_log_level</span> ................................ 0</span>
<span id="cb8-741"><a href="#cb8-741"></a>  <span class="ex">timing_log_option</span> ............................... minmax</span>
<span id="cb8-742"><a href="#cb8-742"></a>  <span class="ex">titles_data_path</span> ................................ None</span>
<span id="cb8-743"><a href="#cb8-743"></a>  <span class="ex">to_hf_ckpt</span> ...................................... False</span>
<span id="cb8-744"><a href="#cb8-744"></a>  <span class="ex">tokenizer_model</span> ................................. ALCF/custom_tokenizer.model</span>
<span id="cb8-745"><a href="#cb8-745"></a>  <span class="ex">tokenizer_type</span> .................................. HFTokenizer</span>
<span id="cb8-746"><a href="#cb8-746"></a>  <span class="ex">topk</span> ............................................ 1</span>
<span id="cb8-747"><a href="#cb8-747"></a>  <span class="ex">trace_dir</span> ....................................... ./trace/</span>
<span id="cb8-748"><a href="#cb8-748"></a>  <span class="ex">train_data_exact_num_epochs</span> ..................... None</span>
<span id="cb8-749"><a href="#cb8-749"></a>  <span class="ex">train_data_path</span> ................................. None</span>
<span id="cb8-750"><a href="#cb8-750"></a>  <span class="ex">train_desc_path</span> ................................. None</span>
<span id="cb8-751"><a href="#cb8-751"></a>  <span class="ex">train_doc_idx_path</span> .............................. None</span>
<span id="cb8-752"><a href="#cb8-752"></a>  <span class="ex">train_idx_path</span> .................................. None</span>
<span id="cb8-753"><a href="#cb8-753"></a>  <span class="ex">train_iters</span> ..................................... 3500</span>
<span id="cb8-754"><a href="#cb8-754"></a>  <span class="ex">train_iters_to_skip</span> ............................. None</span>
<span id="cb8-755"><a href="#cb8-755"></a>  <span class="ex">train_range_to_skip</span> ............................. None</span>
<span id="cb8-756"><a href="#cb8-756"></a>  <span class="ex">train_sample_idx_path</span> ........................... None</span>
<span id="cb8-757"><a href="#cb8-757"></a>  <span class="ex">train_samples</span> ................................... None</span>
<span id="cb8-758"><a href="#cb8-758"></a>  <span class="ex">train_shuffle_idx_path</span> .......................... None</span>
<span id="cb8-759"><a href="#cb8-759"></a>  <span class="ex">train_tokens</span> .................................... None</span>
<span id="cb8-760"><a href="#cb8-760"></a>  <span class="ex">transformer_impl</span> ................................ local</span>
<span id="cb8-761"><a href="#cb8-761"></a>  <span class="ex">transformer_pipeline_model_parallel_size</span> ........ 1</span>
<span id="cb8-762"><a href="#cb8-762"></a>  <span class="ex">trust_remote_code</span> ............................... False</span>
<span id="cb8-763"><a href="#cb8-763"></a>  <span class="ex">universal_checkpoint</span> ............................ False</span>
<span id="cb8-764"><a href="#cb8-764"></a>  <span class="ex">untie_embeddings_and_output_weights</span> ............. True</span>
<span id="cb8-765"><a href="#cb8-765"></a>  <span class="ex">use_checkpoint_args</span> ............................. False</span>
<span id="cb8-766"><a href="#cb8-766"></a>  <span class="ex">use_checkpoint_opt_param_scheduler</span> .............. False</span>
<span id="cb8-767"><a href="#cb8-767"></a>  <span class="ex">use_contiguous_buffers_in_local_ddp</span> ............. True</span>
<span id="cb8-768"><a href="#cb8-768"></a>  <span class="ex">use_cpu_initialization</span> .......................... None</span>
<span id="cb8-769"><a href="#cb8-769"></a>  <span class="ex">use_dataset_only</span> ................................ False</span>
<span id="cb8-770"><a href="#cb8-770"></a>  <span class="ex">use_distributed_optimizer</span> ....................... False</span>
<span id="cb8-771"><a href="#cb8-771"></a>  <span class="ex">use_flash_attn</span> .................................. False</span>
<span id="cb8-772"><a href="#cb8-772"></a>  <span class="ex">use_flash_attn_builder</span> .......................... False</span>
<span id="cb8-773"><a href="#cb8-773"></a>  <span class="ex">use_flash_attn_triton</span> ........................... False</span>
<span id="cb8-774"><a href="#cb8-774"></a>  <span class="ex">use_flash_attn_v1</span> ............................... False</span>
<span id="cb8-775"><a href="#cb8-775"></a>  <span class="ex">use_flash_attn_v2</span> ............................... False</span>
<span id="cb8-776"><a href="#cb8-776"></a>  <span class="ex">use_mics</span> ........................................ False</span>
<span id="cb8-777"><a href="#cb8-777"></a>  <span class="ex">use_one_sent_docs</span> ............................... False</span>
<span id="cb8-778"><a href="#cb8-778"></a>  <span class="ex">use_pin_memory</span> .................................. False</span>
<span id="cb8-779"><a href="#cb8-779"></a>  <span class="ex">use_ring_exchange_p2p</span> ........................... False</span>
<span id="cb8-780"><a href="#cb8-780"></a>  <span class="ex">use_rotary_position_embeddings</span> .................. True</span>
<span id="cb8-781"><a href="#cb8-781"></a>  <span class="ex">use_tutel</span> ....................................... False</span>
<span id="cb8-782"><a href="#cb8-782"></a>  <span class="ex">valid_data_path</span> ................................. None</span>
<span id="cb8-783"><a href="#cb8-783"></a>  <span class="ex">variable_seq_lengths</span> ............................ False</span>
<span id="cb8-784"><a href="#cb8-784"></a>  <span class="ex">virtual_pipeline_model_parallel_size</span> ............ None</span>
<span id="cb8-785"><a href="#cb8-785"></a>  <span class="ex">vision_backbone_type</span> ............................ vit</span>
<span id="cb8-786"><a href="#cb8-786"></a>  <span class="ex">vision_pretraining</span> .............................. False</span>
<span id="cb8-787"><a href="#cb8-787"></a>  <span class="ex">vision_pretraining_type</span> ......................... classify</span>
<span id="cb8-788"><a href="#cb8-788"></a>  <span class="ex">vocab_extra_ids</span> ................................. 0</span>
<span id="cb8-789"><a href="#cb8-789"></a>  <span class="ex">vocab_file</span> ...................................... None</span>
<span id="cb8-790"><a href="#cb8-790"></a>  <span class="ex">vocab_size</span> ...................................... 128512</span>
<span id="cb8-791"><a href="#cb8-791"></a>  <span class="ex">wandb_exp_name</span> ..................................</span>
<span id="cb8-792"><a href="#cb8-792"></a>  <span class="ex">wandb_project</span> ...................................</span>
<span id="cb8-793"><a href="#cb8-793"></a>  <span class="ex">wandb_save_dir</span> ..................................</span>
<span id="cb8-794"><a href="#cb8-794"></a>  <span class="ex">weight_decay</span> .................................... 0.1</span>
<span id="cb8-795"><a href="#cb8-795"></a>  <span class="ex">weight_decay_incr_style</span> ......................... constant</span>
<span id="cb8-796"><a href="#cb8-796"></a>  <span class="ex">world_size</span> ...................................... 24</span>
<span id="cb8-797"><a href="#cb8-797"></a>  <span class="ex">zero_allgather_bucket_size</span> ...................... 0.0</span>
<span id="cb8-798"><a href="#cb8-798"></a>  <span class="ex">zero_contigious_gradients</span> ....................... False</span>
<span id="cb8-799"><a href="#cb8-799"></a>  <span class="ex">zero_reduce_bucket_size</span> ......................... 0.0</span>
<span id="cb8-800"><a href="#cb8-800"></a>  <span class="ex">zero_reduce_scatter</span> ............................. False</span>
<span id="cb8-801"><a href="#cb8-801"></a>  <span class="ex">zero_stage</span> ...................................... 1.0</span>
<span id="cb8-802"><a href="#cb8-802"></a><span class="ex">--------------------</span> end of arguments <span class="at">---------------------</span></span>
<span id="cb8-803"><a href="#cb8-803"></a><span class="ex">setting</span> number of micro-batches to constant 1</span>
<span id="cb8-804"><a href="#cb8-804"></a><span class="op">&gt;</span> building <span class="ex">HFTokenizer</span> tokenizer ...</span>
<span id="cb8-805"><a href="#cb8-805"></a> <span class="op">&gt;</span> padded <span class="ex">vocab</span> <span class="er">(</span><span class="ex">size:</span> 128000<span class="kw">)</span> <span class="ex">with</span> 512 dummy tokens <span class="er">(</span><span class="ex">new</span> size: 128512<span class="kw">)</span></span>
<span id="cb8-806"><a href="#cb8-806"></a><span class="ex">torch</span> distributed is already initialized, skipping initialization ...</span>
<span id="cb8-807"><a href="#cb8-807"></a><span class="op">&gt;</span> initialized <span class="ex">tensor</span> model parallel with size 1</span>
<span id="cb8-808"><a href="#cb8-808"></a><span class="op">&gt;</span> initialized <span class="ex">pipeline</span> model parallel with size 1</span>
<span id="cb8-809"><a href="#cb8-809"></a><span class="op">&gt;</span> setting <span class="ex">random</span> seeds to 1234 ...</span>
<span id="cb8-810"><a href="#cb8-810"></a><span class="op">&gt;</span> initializing <span class="ex">model</span> parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234</span>
<span id="cb8-811"><a href="#cb8-811"></a><span class="op">&gt;</span>fused <span class="ex">kernel</span> is only supported in cuda, skip loading fused kernel</span>
<span id="cb8-812"><a href="#cb8-812"></a><span class="ex">[2024-10-16</span> 15:39:36,876] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb8-813"><a href="#cb8-813"></a><span class="op">&gt;</span>fused <span class="ex">kernel</span> is only supported in cuda, skip loading fused kernel</span>
<span id="cb8-814"><a href="#cb8-814"></a><span class="ex">[2024-10-16</span> 15:39:36,876] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb8-815"><a href="#cb8-815"></a><span class="op">&gt;</span>fused <span class="ex">kernel</span> is only supported in cuda, skip loading fused kernel</span>
<span id="cb8-816"><a href="#cb8-816"></a><span class="ex">[2024-10-16</span> 15:39:36,876] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb8-817"><a href="#cb8-817"></a><span class="op">&gt;</span>fused <span class="ex">kernel</span> is only supported in cuda, skip loading fused kernel</span>
<span id="cb8-818"><a href="#cb8-818"></a><span class="ex">[2024-10-16</span> 15:39:36,876] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb8-819"><a href="#cb8-819"></a><span class="op">&gt;</span>fused <span class="ex">kernel</span> is only supported in cuda, skip loading fused kernel</span>
<span id="cb8-820"><a href="#cb8-820"></a><span class="ex">[2024-10-16</span> 15:39:36,879] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb8-821"><a href="#cb8-821"></a><span class="op">&gt;</span>fused <span class="ex">kernel</span> is only supported in cuda, skip loading fused kernel</span>
<span id="cb8-822"><a href="#cb8-822"></a><span class="ex">[2024-10-16</span> 15:39:36,881] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb8-823"><a href="#cb8-823"></a><span class="op">&gt;</span>fused <span class="ex">kernel</span> is only supported in cuda, skip loading fused kernel</span>
<span id="cb8-824"><a href="#cb8-824"></a><span class="ex">[2024-10-16</span> 15:39:36,881] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb8-825"><a href="#cb8-825"></a><span class="op">&gt;</span>fused <span class="ex">kernel</span> is only supported in cuda, skip loading fused kernel</span>
<span id="cb8-826"><a href="#cb8-826"></a><span class="ex">[2024-10-16</span> 15:39:36,882] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb8-827"><a href="#cb8-827"></a><span class="op">&gt;</span>fused <span class="ex">kernel</span> is only supported in cuda, skip loading fused kernel</span>
<span id="cb8-828"><a href="#cb8-828"></a><span class="ex">[2024-10-16</span> 15:39:36,882] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb8-829"><a href="#cb8-829"></a><span class="ex">make:</span> Entering directory <span class="st">'/lus/flare/projects/Aurora_deployment/foremans/projects/argonne-lcf/Megatron-DeepSpeed/megatron/data'</span></span>
<span id="cb8-830"><a href="#cb8-830"></a><span class="op">&gt;</span>fused <span class="ex">kernel</span> is only supported in cuda, skip loading fused kernel</span>
<span id="cb8-831"><a href="#cb8-831"></a><span class="ex">[2024-10-16</span> 15:39:36,884] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb8-832"><a href="#cb8-832"></a><span class="op">&gt;</span>fused <span class="ex">kernel</span> is only supported in cuda, skip loading fused kernel</span>
<span id="cb8-833"><a href="#cb8-833"></a><span class="ex">[2024-10-16</span> 15:39:36,922] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb8-834"><a href="#cb8-834"></a><span class="op">&gt;</span>fused <span class="ex">kernel</span> is only supported in cuda, skip loading fused kernel</span>
<span id="cb8-835"><a href="#cb8-835"></a><span class="ex">[2024-10-16</span> 15:39:36,929] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb8-836"><a href="#cb8-836"></a><span class="op">&gt;</span>fused <span class="ex">kernel</span> is only supported in cuda, skip loading fused kernel</span>
<span id="cb8-837"><a href="#cb8-837"></a><span class="ex">[2024-10-16</span> 15:39:36,930] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb8-838"><a href="#cb8-838"></a><span class="op">&gt;</span>fused <span class="ex">kernel</span> is only supported in cuda, skip loading fused kernel</span>
<span id="cb8-839"><a href="#cb8-839"></a><span class="ex">[2024-10-16</span> 15:39:36,931] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb8-840"><a href="#cb8-840"></a><span class="op">&gt;</span>fused <span class="ex">kernel</span> is only supported in cuda, skip loading fused kernel</span>
<span id="cb8-841"><a href="#cb8-841"></a><span class="ex">[2024-10-16</span> 15:39:36,935] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb8-842"><a href="#cb8-842"></a><span class="op">&gt;</span>fused <span class="ex">kernel</span> is only supported in cuda, skip loading fused kernel</span>
<span id="cb8-843"><a href="#cb8-843"></a><span class="ex">[2024-10-16</span> 15:39:36,937] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb8-844"><a href="#cb8-844"></a><span class="op">&gt;</span>fused <span class="ex">kernel</span> is only supported in cuda, skip loading fused kernel</span>
<span id="cb8-845"><a href="#cb8-845"></a><span class="ex">[2024-10-16</span> 15:39:36,939] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb8-846"><a href="#cb8-846"></a><span class="op">&gt;</span>fused <span class="ex">kernel</span> is only supported in cuda, skip loading fused kernel</span>
<span id="cb8-847"><a href="#cb8-847"></a><span class="ex">[2024-10-16</span> 15:39:36,939] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb8-848"><a href="#cb8-848"></a><span class="op">&gt;</span>fused <span class="ex">kernel</span> is only supported in cuda, skip loading fused kernel</span>
<span id="cb8-849"><a href="#cb8-849"></a><span class="ex">[2024-10-16</span> 15:39:36,940] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb8-850"><a href="#cb8-850"></a><span class="op">&gt;</span>fused <span class="ex">kernel</span> is only supported in cuda, skip loading fused kernel</span>
<span id="cb8-851"><a href="#cb8-851"></a><span class="ex">[2024-10-16</span> 15:39:36,941] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb8-852"><a href="#cb8-852"></a><span class="op">&gt;</span>fused <span class="ex">kernel</span> is only supported in cuda, skip loading fused kernel</span>
<span id="cb8-853"><a href="#cb8-853"></a><span class="ex">[2024-10-16</span> 15:39:36,946] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb8-854"><a href="#cb8-854"></a><span class="op">&gt;</span>fused <span class="ex">kernel</span> is only supported in cuda, skip loading fused kernel</span>
<span id="cb8-855"><a href="#cb8-855"></a><span class="ex">[2024-10-16</span> 15:39:36,949] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb8-856"><a href="#cb8-856"></a><span class="ex">make:</span> Nothing to be done for <span class="st">'default'</span>.</span>
<span id="cb8-857"><a href="#cb8-857"></a><span class="ex">make:</span> Leaving directory <span class="st">'/lus/flare/projects/Aurora_deployment/foremans/projects/argonne-lcf/Megatron-DeepSpeed/megatron/data'</span></span>
<span id="cb8-858"><a href="#cb8-858"></a><span class="op">&gt;</span> compiling <span class="ex">dataset</span> index builder ...</span>
<span id="cb8-859"><a href="#cb8-859"></a><span class="op">&gt;&gt;&gt;</span> done <span class="ex">with</span> dataset index builder. Compilation time: 0.080 seconds</span>
<span id="cb8-860"><a href="#cb8-860"></a><span class="op">&gt;</span>fused <span class="ex">kernel</span> is only supported in cuda, skip loading fused kernel</span>
<span id="cb8-861"><a href="#cb8-861"></a><span class="ex">[2024-10-16</span> 15:39:36.956560]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:479</span><span class="pp">]</span> <span class="at">-</span> building model ...</span>
<span id="cb8-862"><a href="#cb8-862"></a><span class="ex">[2024-10-16</span> 15:39:37,133] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">utils.py:781:see_memory_usage</span><span class="pp">]</span> Before Building Model</span>
<span id="cb8-863"><a href="#cb8-863"></a><span class="ex">[2024-10-16</span> 15:39:37,133] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">utils.py:782:see_memory_usage</span><span class="pp">]</span> MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB</span>
<span id="cb8-864"><a href="#cb8-864"></a><span class="ex">[2024-10-16</span> 15:39:37,133] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">utils.py:789:see_memory_usage</span><span class="pp">]</span> CPU Virtual Memory:  used = 105.24 GB, percent = 9.3%</span>
<span id="cb8-865"><a href="#cb8-865"></a><span class="ex">[2024-10-16</span> 15:39:37,133] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb8-866"><a href="#cb8-866"></a><span class="va">SEED_LAYERS</span><span class="op">=</span>False <span class="va">BASE_SEED</span><span class="op">=</span>1234 <span class="va">SEED_FN</span><span class="op">=</span>None</span>
<span id="cb8-867"><a href="#cb8-867"></a><span class="ex">Using</span> topology: {ProcessCoord<span class="er">(</span><span class="va">pipe</span><span class="op">=</span>0, <span class="va">data</span><span class="op">=</span>0, <span class="va">model</span><span class="op">=</span>0<span class="kw">)</span><span class="bu">:</span> 0, ProcessCoord<span class="er">(</span><span class="va">pipe</span><span class="op">=</span>0, <span class="va">data</span><span class="op">=</span>1, <span class="va">model</span><span class="op">=</span>0<span class="kw">)</span><span class="bu">:</span> 1, ProcessCoord<span class="er">(</span><span class="va">pipe</span><span class="op">=</span>0, <span class="va">data</span><span class="op">=</span>2, <span class="va">model</span><span class="op">=</span>0<span class="kw">)</span><span class="bu">:</span> 2, ProcessCoord<span class="er">(</span><span class="va">pipe</span><span class="op">=</span>0, <span class="va">data</span><span class="op">=</span>3, <span class="va">model</span><span class="op">=</span>0<span class="kw">)</span><span class="bu">:</span> 3, ProcessCoord<span class="er">(</span><span class="va">pipe</span><span class="op">=</span>0, <span class="va">data</span><span class="op">=</span>4, <span class="va">model</span><span class="op">=</span>0<span class="kw">)</span><span class="bu">:</span> 4, ProcessCoord<span class="er">(</span><span class="va">pipe</span><span class="op">=</span>0, <span class="va">data</span><span class="op">=</span>5, <span class="va">model</span><span class="op">=</span>0<span class="kw">)</span><span class="bu">:</span> 5, ProcessCoord<span class="er">(</span><span class="va">pipe</span><span class="op">=</span>0, <span class="va">data</span><span class="op">=</span>6, <span class="va">model</span><span class="op">=</span>0<span class="kw">)</span><span class="bu">:</span> 6, ProcessCoord<span class="er">(</span><span class="va">pipe</span><span class="op">=</span>0, <span class="va">data</span><span class="op">=</span>7, <span class="va">model</span><span class="op">=</span>0<span class="kw">)</span><span class="bu">:</span> 7, ProcessCoord<span class="er">(</span><span class="va">pipe</span><span class="op">=</span>0, <span class="va">data</span><span class="op">=</span>8, <span class="va">model</span><span class="op">=</span>0<span class="kw">)</span><span class="bu">:</span> 8, ProcessCoord<span class="er">(</span><span class="va">pipe</span><span class="op">=</span>0, <span class="va">data</span><span class="op">=</span>9, <span class="va">model</span><span class="op">=</span>0<span class="kw">)</span><span class="bu">:</span> 9, ProcessCoord<span class="er">(</span><span class="va">pipe</span><span class="op">=</span>0, <span class="va">data</span><span class="op">=</span>10, <span class="va">model</span><span class="op">=</span>0<span class="kw">)</span><span class="bu">:</span> 10, ProcessCoord<span class="er">(</span><span class="va">pipe</span><span class="op">=</span>0, <span class="va">data</span><span class="op">=</span>11, <span class="va">model</span><span class="op">=</span>0<span class="kw">)</span><span class="bu">:</span> 11, ProcessCoord<span class="er">(</span><span class="va">pipe</span><span class="op">=</span>0, <span class="va">data</span><span class="op">=</span>12, <span class="va">model</span><span class="op">=</span>0<span class="kw">)</span><span class="bu">:</span> 12, ProcessCoord<span class="er">(</span><span class="va">pipe</span><span class="op">=</span>0, <span class="va">data</span><span class="op">=</span>13, <span class="va">model</span><span class="op">=</span>0<span class="kw">)</span><span class="bu">:</span> 13, ProcessCoord<span class="er">(</span><span class="va">pipe</span><span class="op">=</span>0, <span class="va">data</span><span class="op">=</span>14, <span class="va">model</span><span class="op">=</span>0<span class="kw">)</span><span class="bu">:</span> 14, ProcessCoord<span class="er">(</span><span class="va">pipe</span><span class="op">=</span>0, <span class="va">data</span><span class="op">=</span>15, <span class="va">model</span><span class="op">=</span>0<span class="kw">)</span><span class="bu">:</span> 15, ProcessCoord<span class="er">(</span><span class="va">pipe</span><span class="op">=</span>0, <span class="va">data</span><span class="op">=</span>16, <span class="va">model</span><span class="op">=</span>0<span class="kw">)</span><span class="bu">:</span> 16, ProcessCoord<span class="er">(</span><span class="va">pipe</span><span class="op">=</span>0, <span class="va">data</span><span class="op">=</span>17, <span class="va">model</span><span class="op">=</span>0<span class="kw">)</span><span class="bu">:</span> 17, ProcessCoord<span class="er">(</span><span class="va">pipe</span><span class="op">=</span>0, <span class="va">data</span><span class="op">=</span>18, <span class="va">model</span><span class="op">=</span>0<span class="kw">)</span><span class="bu">:</span> 18, ProcessCoord<span class="er">(</span><span class="va">pipe</span><span class="op">=</span>0, <span class="va">data</span><span class="op">=</span>19, <span class="va">model</span><span class="op">=</span>0<span class="kw">)</span><span class="bu">:</span> 19, ProcessCoord<span class="er">(</span><span class="va">pipe</span><span class="op">=</span>0, <span class="va">data</span><span class="op">=</span>20, <span class="va">model</span><span class="op">=</span>0<span class="kw">)</span><span class="bu">:</span> 20, ProcessCoord<span class="er">(</span><span class="va">pipe</span><span class="op">=</span>0, <span class="va">data</span><span class="op">=</span>21, <span class="va">model</span><span class="op">=</span>0<span class="kw">)</span><span class="bu">:</span> 21, ProcessCoord<span class="er">(</span><span class="va">pipe</span><span class="op">=</span>0, <span class="va">data</span><span class="op">=</span>22, <span class="va">model</span><span class="op">=</span>0<span class="kw">)</span><span class="bu">:</span> 22, ProcessCoord<span class="er">(</span><span class="va">pipe</span><span class="op">=</span>0, <span class="va">data</span><span class="op">=</span>23, <span class="va">model</span><span class="op">=</span>0<span class="kw">)</span><span class="bu">:</span> 23}</span>
<span id="cb8-868"><a href="#cb8-868"></a><span class="ex">[2024-10-16</span> 15:39:37,139] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">module.py:396:_partition_layers</span><span class="pp">]</span> Partitioning pipeline stages with method type:transformer</span>
<span id="cb8-869"><a href="#cb8-869"></a><span class="ex">2024-10-16</span> 15:39:37.212904: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered</span>
<span id="cb8-870"><a href="#cb8-870"></a><span class="ex">2024-10-16</span> 15:39:37.212925: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered</span>
<span id="cb8-871"><a href="#cb8-871"></a><span class="ex">2024-10-16</span> 15:39:37.213970: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered</span>
<span id="cb8-872"><a href="#cb8-872"></a><span class="ex">2024-10-16</span> 15:39:37.704522: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT</span>
<span id="cb8-873"><a href="#cb8-873"></a><span class="ex">Loading</span> checkpoint shards:   0%<span class="kw">|</span>          <span class="kw">|</span> <span class="ex">0/7</span> [00:00<span class="op">&lt;</span><span class="pp">?</span>, <span class="pp">?</span>it/s]1 GPTModelPipe<span class="er">(</span></span>
<span id="cb8-874"><a href="#cb8-874"></a>  <span class="kw">(</span><span class="ex">tied_modules</span><span class="kw">)</span><span class="bu">:</span> ModuleDict<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-875"><a href="#cb8-875"></a>  <span class="kw">(</span><span class="ex">1</span><span class="kw">)</span><span class="bu">:</span> EmbeddingPipe<span class="er">(</span></span>
<span id="cb8-876"><a href="#cb8-876"></a>    <span class="kw">(</span><span class="ex">word_embeddings</span><span class="kw">)</span><span class="bu">:</span> VocabParallelEmbedding<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-877"><a href="#cb8-877"></a>    <span class="kw">(</span><span class="ex">embedding_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-878"><a href="#cb8-878"></a>  <span class="kw">)</span></span>
<span id="cb8-879"><a href="#cb8-879"></a>  <span class="kw">(</span><span class="ex">2</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-880"><a href="#cb8-880"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-881"><a href="#cb8-881"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-882"><a href="#cb8-882"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-883"><a href="#cb8-883"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-884"><a href="#cb8-884"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-885"><a href="#cb8-885"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-886"><a href="#cb8-886"></a>      <span class="kw">)</span></span>
<span id="cb8-887"><a href="#cb8-887"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-888"><a href="#cb8-888"></a>    <span class="kw">)</span></span>
<span id="cb8-889"><a href="#cb8-889"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-890"><a href="#cb8-890"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-891"><a href="#cb8-891"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-892"><a href="#cb8-892"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-893"><a href="#cb8-893"></a>    <span class="kw">)</span></span>
<span id="cb8-894"><a href="#cb8-894"></a>  <span class="kw">)</span></span>
<span id="cb8-895"><a href="#cb8-895"></a>  <span class="kw">(</span><span class="ex">3</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-896"><a href="#cb8-896"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-897"><a href="#cb8-897"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-898"><a href="#cb8-898"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-899"><a href="#cb8-899"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-900"><a href="#cb8-900"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-901"><a href="#cb8-901"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-902"><a href="#cb8-902"></a>      <span class="kw">)</span></span>
<span id="cb8-903"><a href="#cb8-903"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-904"><a href="#cb8-904"></a>    <span class="kw">)</span></span>
<span id="cb8-905"><a href="#cb8-905"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-906"><a href="#cb8-906"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-907"><a href="#cb8-907"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-908"><a href="#cb8-908"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-909"><a href="#cb8-909"></a>    <span class="kw">)</span></span>
<span id="cb8-910"><a href="#cb8-910"></a>  <span class="kw">)</span></span>
<span id="cb8-911"><a href="#cb8-911"></a>  <span class="kw">(</span><span class="ex">4</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-912"><a href="#cb8-912"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-913"><a href="#cb8-913"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-914"><a href="#cb8-914"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-915"><a href="#cb8-915"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-916"><a href="#cb8-916"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-917"><a href="#cb8-917"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-918"><a href="#cb8-918"></a>      <span class="kw">)</span></span>
<span id="cb8-919"><a href="#cb8-919"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-920"><a href="#cb8-920"></a>    <span class="kw">)</span></span>
<span id="cb8-921"><a href="#cb8-921"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-922"><a href="#cb8-922"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-923"><a href="#cb8-923"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-924"><a href="#cb8-924"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-925"><a href="#cb8-925"></a>    <span class="kw">)</span></span>
<span id="cb8-926"><a href="#cb8-926"></a>  <span class="kw">)</span></span>
<span id="cb8-927"><a href="#cb8-927"></a>  <span class="kw">(</span><span class="ex">5</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-928"><a href="#cb8-928"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-929"><a href="#cb8-929"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-930"><a href="#cb8-930"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-931"><a href="#cb8-931"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-932"><a href="#cb8-932"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-933"><a href="#cb8-933"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-934"><a href="#cb8-934"></a>      <span class="kw">)</span></span>
<span id="cb8-935"><a href="#cb8-935"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-936"><a href="#cb8-936"></a>    <span class="kw">)</span></span>
<span id="cb8-937"><a href="#cb8-937"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-938"><a href="#cb8-938"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-939"><a href="#cb8-939"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-940"><a href="#cb8-940"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-941"><a href="#cb8-941"></a>    <span class="kw">)</span></span>
<span id="cb8-942"><a href="#cb8-942"></a>  <span class="kw">)</span></span>
<span id="cb8-943"><a href="#cb8-943"></a>  <span class="kw">(</span><span class="ex">6</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-944"><a href="#cb8-944"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-945"><a href="#cb8-945"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-946"><a href="#cb8-946"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-947"><a href="#cb8-947"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-948"><a href="#cb8-948"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-949"><a href="#cb8-949"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-950"><a href="#cb8-950"></a>      <span class="kw">)</span></span>
<span id="cb8-951"><a href="#cb8-951"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-952"><a href="#cb8-952"></a>    <span class="kw">)</span></span>
<span id="cb8-953"><a href="#cb8-953"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-954"><a href="#cb8-954"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-955"><a href="#cb8-955"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-956"><a href="#cb8-956"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-957"><a href="#cb8-957"></a>    <span class="kw">)</span></span>
<span id="cb8-958"><a href="#cb8-958"></a>  <span class="kw">)</span></span>
<span id="cb8-959"><a href="#cb8-959"></a>  <span class="kw">(</span><span class="ex">7</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-960"><a href="#cb8-960"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-961"><a href="#cb8-961"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-962"><a href="#cb8-962"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-963"><a href="#cb8-963"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-964"><a href="#cb8-964"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-965"><a href="#cb8-965"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-966"><a href="#cb8-966"></a>      <span class="kw">)</span></span>
<span id="cb8-967"><a href="#cb8-967"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-968"><a href="#cb8-968"></a>    <span class="kw">)</span></span>
<span id="cb8-969"><a href="#cb8-969"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-970"><a href="#cb8-970"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-971"><a href="#cb8-971"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-972"><a href="#cb8-972"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-973"><a href="#cb8-973"></a>    <span class="kw">)</span></span>
<span id="cb8-974"><a href="#cb8-974"></a>  <span class="kw">)</span></span>
<span id="cb8-975"><a href="#cb8-975"></a>  <span class="kw">(</span><span class="ex">8</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-976"><a href="#cb8-976"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-977"><a href="#cb8-977"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-978"><a href="#cb8-978"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-979"><a href="#cb8-979"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-980"><a href="#cb8-980"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-981"><a href="#cb8-981"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-982"><a href="#cb8-982"></a>      <span class="kw">)</span></span>
<span id="cb8-983"><a href="#cb8-983"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-984"><a href="#cb8-984"></a>    <span class="kw">)</span></span>
<span id="cb8-985"><a href="#cb8-985"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-986"><a href="#cb8-986"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-987"><a href="#cb8-987"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-988"><a href="#cb8-988"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-989"><a href="#cb8-989"></a>    <span class="kw">)</span></span>
<span id="cb8-990"><a href="#cb8-990"></a>  <span class="kw">)</span></span>
<span id="cb8-991"><a href="#cb8-991"></a>  <span class="kw">(</span><span class="ex">9</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-992"><a href="#cb8-992"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-993"><a href="#cb8-993"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-994"><a href="#cb8-994"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-995"><a href="#cb8-995"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-996"><a href="#cb8-996"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-997"><a href="#cb8-997"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-998"><a href="#cb8-998"></a>      <span class="kw">)</span></span>
<span id="cb8-999"><a href="#cb8-999"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1000"><a href="#cb8-1000"></a>    <span class="kw">)</span></span>
<span id="cb8-1001"><a href="#cb8-1001"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1002"><a href="#cb8-1002"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1003"><a href="#cb8-1003"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1004"><a href="#cb8-1004"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1005"><a href="#cb8-1005"></a>    <span class="kw">)</span></span>
<span id="cb8-1006"><a href="#cb8-1006"></a>  <span class="kw">)</span></span>
<span id="cb8-1007"><a href="#cb8-1007"></a>  <span class="kw">(</span><span class="ex">10</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1008"><a href="#cb8-1008"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1009"><a href="#cb8-1009"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1010"><a href="#cb8-1010"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1011"><a href="#cb8-1011"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1012"><a href="#cb8-1012"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1013"><a href="#cb8-1013"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1014"><a href="#cb8-1014"></a>      <span class="kw">)</span></span>
<span id="cb8-1015"><a href="#cb8-1015"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1016"><a href="#cb8-1016"></a>    <span class="kw">)</span></span>
<span id="cb8-1017"><a href="#cb8-1017"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1018"><a href="#cb8-1018"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1019"><a href="#cb8-1019"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1020"><a href="#cb8-1020"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1021"><a href="#cb8-1021"></a>    <span class="kw">)</span></span>
<span id="cb8-1022"><a href="#cb8-1022"></a>  <span class="kw">)</span></span>
<span id="cb8-1023"><a href="#cb8-1023"></a>  <span class="kw">(</span><span class="ex">11</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1024"><a href="#cb8-1024"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1025"><a href="#cb8-1025"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1026"><a href="#cb8-1026"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1027"><a href="#cb8-1027"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1028"><a href="#cb8-1028"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1029"><a href="#cb8-1029"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1030"><a href="#cb8-1030"></a>      <span class="kw">)</span></span>
<span id="cb8-1031"><a href="#cb8-1031"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1032"><a href="#cb8-1032"></a>    <span class="kw">)</span></span>
<span id="cb8-1033"><a href="#cb8-1033"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1034"><a href="#cb8-1034"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1035"><a href="#cb8-1035"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1036"><a href="#cb8-1036"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1037"><a href="#cb8-1037"></a>    <span class="kw">)</span></span>
<span id="cb8-1038"><a href="#cb8-1038"></a>  <span class="kw">)</span></span>
<span id="cb8-1039"><a href="#cb8-1039"></a>  <span class="kw">(</span><span class="ex">12</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1040"><a href="#cb8-1040"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1041"><a href="#cb8-1041"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1042"><a href="#cb8-1042"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1043"><a href="#cb8-1043"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1044"><a href="#cb8-1044"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1045"><a href="#cb8-1045"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1046"><a href="#cb8-1046"></a>      <span class="kw">)</span></span>
<span id="cb8-1047"><a href="#cb8-1047"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1048"><a href="#cb8-1048"></a>    <span class="kw">)</span></span>
<span id="cb8-1049"><a href="#cb8-1049"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1050"><a href="#cb8-1050"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1051"><a href="#cb8-1051"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1052"><a href="#cb8-1052"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1053"><a href="#cb8-1053"></a>    <span class="kw">)</span></span>
<span id="cb8-1054"><a href="#cb8-1054"></a>  <span class="kw">)</span></span>
<span id="cb8-1055"><a href="#cb8-1055"></a>  <span class="kw">(</span><span class="ex">13</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1056"><a href="#cb8-1056"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1057"><a href="#cb8-1057"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1058"><a href="#cb8-1058"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1059"><a href="#cb8-1059"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1060"><a href="#cb8-1060"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1061"><a href="#cb8-1061"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1062"><a href="#cb8-1062"></a>      <span class="kw">)</span></span>
<span id="cb8-1063"><a href="#cb8-1063"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1064"><a href="#cb8-1064"></a>    <span class="kw">)</span></span>
<span id="cb8-1065"><a href="#cb8-1065"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1066"><a href="#cb8-1066"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1067"><a href="#cb8-1067"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1068"><a href="#cb8-1068"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1069"><a href="#cb8-1069"></a>    <span class="kw">)</span></span>
<span id="cb8-1070"><a href="#cb8-1070"></a>  <span class="kw">)</span></span>
<span id="cb8-1071"><a href="#cb8-1071"></a>  <span class="kw">(</span><span class="ex">14</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1072"><a href="#cb8-1072"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1073"><a href="#cb8-1073"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1074"><a href="#cb8-1074"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1075"><a href="#cb8-1075"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1076"><a href="#cb8-1076"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1077"><a href="#cb8-1077"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1078"><a href="#cb8-1078"></a>      <span class="kw">)</span></span>
<span id="cb8-1079"><a href="#cb8-1079"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1080"><a href="#cb8-1080"></a>    <span class="kw">)</span></span>
<span id="cb8-1081"><a href="#cb8-1081"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1082"><a href="#cb8-1082"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1083"><a href="#cb8-1083"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1084"><a href="#cb8-1084"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1085"><a href="#cb8-1085"></a>    <span class="kw">)</span></span>
<span id="cb8-1086"><a href="#cb8-1086"></a>  <span class="kw">)</span></span>
<span id="cb8-1087"><a href="#cb8-1087"></a>  <span class="kw">(</span><span class="ex">15</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1088"><a href="#cb8-1088"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1089"><a href="#cb8-1089"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1090"><a href="#cb8-1090"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1091"><a href="#cb8-1091"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1092"><a href="#cb8-1092"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1093"><a href="#cb8-1093"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1094"><a href="#cb8-1094"></a>      <span class="kw">)</span></span>
<span id="cb8-1095"><a href="#cb8-1095"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1096"><a href="#cb8-1096"></a>    <span class="kw">)</span></span>
<span id="cb8-1097"><a href="#cb8-1097"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1098"><a href="#cb8-1098"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1099"><a href="#cb8-1099"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1100"><a href="#cb8-1100"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1101"><a href="#cb8-1101"></a>    <span class="kw">)</span></span>
<span id="cb8-1102"><a href="#cb8-1102"></a>  <span class="kw">)</span></span>
<span id="cb8-1103"><a href="#cb8-1103"></a>  <span class="kw">(</span><span class="ex">16</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1104"><a href="#cb8-1104"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1105"><a href="#cb8-1105"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1106"><a href="#cb8-1106"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1107"><a href="#cb8-1107"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1108"><a href="#cb8-1108"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1109"><a href="#cb8-1109"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1110"><a href="#cb8-1110"></a>      <span class="kw">)</span></span>
<span id="cb8-1111"><a href="#cb8-1111"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1112"><a href="#cb8-1112"></a>    <span class="kw">)</span></span>
<span id="cb8-1113"><a href="#cb8-1113"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1114"><a href="#cb8-1114"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1115"><a href="#cb8-1115"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1116"><a href="#cb8-1116"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1117"><a href="#cb8-1117"></a>    <span class="kw">)</span></span>
<span id="cb8-1118"><a href="#cb8-1118"></a>  <span class="kw">)</span></span>
<span id="cb8-1119"><a href="#cb8-1119"></a>  <span class="kw">(</span><span class="ex">17</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1120"><a href="#cb8-1120"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1121"><a href="#cb8-1121"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1122"><a href="#cb8-1122"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1123"><a href="#cb8-1123"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1124"><a href="#cb8-1124"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1125"><a href="#cb8-1125"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1126"><a href="#cb8-1126"></a>      <span class="kw">)</span></span>
<span id="cb8-1127"><a href="#cb8-1127"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1128"><a href="#cb8-1128"></a>    <span class="kw">)</span></span>
<span id="cb8-1129"><a href="#cb8-1129"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1130"><a href="#cb8-1130"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1131"><a href="#cb8-1131"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1132"><a href="#cb8-1132"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1133"><a href="#cb8-1133"></a>    <span class="kw">)</span></span>
<span id="cb8-1134"><a href="#cb8-1134"></a>  <span class="kw">)</span></span>
<span id="cb8-1135"><a href="#cb8-1135"></a>  <span class="kw">(</span><span class="ex">18</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1136"><a href="#cb8-1136"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1137"><a href="#cb8-1137"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1138"><a href="#cb8-1138"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1139"><a href="#cb8-1139"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1140"><a href="#cb8-1140"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1141"><a href="#cb8-1141"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1142"><a href="#cb8-1142"></a>      <span class="kw">)</span></span>
<span id="cb8-1143"><a href="#cb8-1143"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1144"><a href="#cb8-1144"></a>    <span class="kw">)</span></span>
<span id="cb8-1145"><a href="#cb8-1145"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1146"><a href="#cb8-1146"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1147"><a href="#cb8-1147"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1148"><a href="#cb8-1148"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1149"><a href="#cb8-1149"></a>    <span class="kw">)</span></span>
<span id="cb8-1150"><a href="#cb8-1150"></a>  <span class="kw">)</span></span>
<span id="cb8-1151"><a href="#cb8-1151"></a>  <span class="kw">(</span><span class="ex">19</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1152"><a href="#cb8-1152"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1153"><a href="#cb8-1153"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1154"><a href="#cb8-1154"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1155"><a href="#cb8-1155"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1156"><a href="#cb8-1156"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1157"><a href="#cb8-1157"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1158"><a href="#cb8-1158"></a>      <span class="kw">)</span></span>
<span id="cb8-1159"><a href="#cb8-1159"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1160"><a href="#cb8-1160"></a>    <span class="kw">)</span></span>
<span id="cb8-1161"><a href="#cb8-1161"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1162"><a href="#cb8-1162"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1163"><a href="#cb8-1163"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1164"><a href="#cb8-1164"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1165"><a href="#cb8-1165"></a>    <span class="kw">)</span></span>
<span id="cb8-1166"><a href="#cb8-1166"></a>  <span class="kw">)</span></span>
<span id="cb8-1167"><a href="#cb8-1167"></a>  <span class="kw">(</span><span class="ex">20</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1168"><a href="#cb8-1168"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1169"><a href="#cb8-1169"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1170"><a href="#cb8-1170"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1171"><a href="#cb8-1171"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1172"><a href="#cb8-1172"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1173"><a href="#cb8-1173"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1174"><a href="#cb8-1174"></a>      <span class="kw">)</span></span>
<span id="cb8-1175"><a href="#cb8-1175"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1176"><a href="#cb8-1176"></a>    <span class="kw">)</span></span>
<span id="cb8-1177"><a href="#cb8-1177"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1178"><a href="#cb8-1178"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1179"><a href="#cb8-1179"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1180"><a href="#cb8-1180"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1181"><a href="#cb8-1181"></a>    <span class="kw">)</span></span>
<span id="cb8-1182"><a href="#cb8-1182"></a>  <span class="kw">)</span></span>
<span id="cb8-1183"><a href="#cb8-1183"></a>  <span class="kw">(</span><span class="ex">21</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1184"><a href="#cb8-1184"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1185"><a href="#cb8-1185"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1186"><a href="#cb8-1186"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1187"><a href="#cb8-1187"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1188"><a href="#cb8-1188"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1189"><a href="#cb8-1189"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1190"><a href="#cb8-1190"></a>      <span class="kw">)</span></span>
<span id="cb8-1191"><a href="#cb8-1191"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1192"><a href="#cb8-1192"></a>    <span class="kw">)</span></span>
<span id="cb8-1193"><a href="#cb8-1193"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1194"><a href="#cb8-1194"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1195"><a href="#cb8-1195"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1196"><a href="#cb8-1196"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1197"><a href="#cb8-1197"></a>    <span class="kw">)</span></span>
<span id="cb8-1198"><a href="#cb8-1198"></a>  <span class="kw">)</span></span>
<span id="cb8-1199"><a href="#cb8-1199"></a>  <span class="kw">(</span><span class="ex">22</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1200"><a href="#cb8-1200"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1201"><a href="#cb8-1201"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1202"><a href="#cb8-1202"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1203"><a href="#cb8-1203"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1204"><a href="#cb8-1204"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1205"><a href="#cb8-1205"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1206"><a href="#cb8-1206"></a>      <span class="kw">)</span></span>
<span id="cb8-1207"><a href="#cb8-1207"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1208"><a href="#cb8-1208"></a>    <span class="kw">)</span></span>
<span id="cb8-1209"><a href="#cb8-1209"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1210"><a href="#cb8-1210"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1211"><a href="#cb8-1211"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1212"><a href="#cb8-1212"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1213"><a href="#cb8-1213"></a>    <span class="kw">)</span></span>
<span id="cb8-1214"><a href="#cb8-1214"></a>  <span class="kw">)</span></span>
<span id="cb8-1215"><a href="#cb8-1215"></a>  <span class="kw">(</span><span class="ex">23</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1216"><a href="#cb8-1216"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1217"><a href="#cb8-1217"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1218"><a href="#cb8-1218"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1219"><a href="#cb8-1219"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1220"><a href="#cb8-1220"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1221"><a href="#cb8-1221"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1222"><a href="#cb8-1222"></a>      <span class="kw">)</span></span>
<span id="cb8-1223"><a href="#cb8-1223"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1224"><a href="#cb8-1224"></a>    <span class="kw">)</span></span>
<span id="cb8-1225"><a href="#cb8-1225"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1226"><a href="#cb8-1226"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1227"><a href="#cb8-1227"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1228"><a href="#cb8-1228"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1229"><a href="#cb8-1229"></a>    <span class="kw">)</span></span>
<span id="cb8-1230"><a href="#cb8-1230"></a>  <span class="kw">)</span></span>
<span id="cb8-1231"><a href="#cb8-1231"></a>  <span class="kw">(</span><span class="ex">24</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1232"><a href="#cb8-1232"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1233"><a href="#cb8-1233"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1234"><a href="#cb8-1234"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1235"><a href="#cb8-1235"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1236"><a href="#cb8-1236"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1237"><a href="#cb8-1237"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1238"><a href="#cb8-1238"></a>      <span class="kw">)</span></span>
<span id="cb8-1239"><a href="#cb8-1239"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1240"><a href="#cb8-1240"></a>    <span class="kw">)</span></span>
<span id="cb8-1241"><a href="#cb8-1241"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1242"><a href="#cb8-1242"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1243"><a href="#cb8-1243"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1244"><a href="#cb8-1244"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1245"><a href="#cb8-1245"></a>    <span class="kw">)</span></span>
<span id="cb8-1246"><a href="#cb8-1246"></a>  <span class="kw">)</span></span>
<span id="cb8-1247"><a href="#cb8-1247"></a>  <span class="kw">(</span><span class="ex">25</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1248"><a href="#cb8-1248"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1249"><a href="#cb8-1249"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1250"><a href="#cb8-1250"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1251"><a href="#cb8-1251"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1252"><a href="#cb8-1252"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1253"><a href="#cb8-1253"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1254"><a href="#cb8-1254"></a>      <span class="kw">)</span></span>
<span id="cb8-1255"><a href="#cb8-1255"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1256"><a href="#cb8-1256"></a>    <span class="kw">)</span></span>
<span id="cb8-1257"><a href="#cb8-1257"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1258"><a href="#cb8-1258"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1259"><a href="#cb8-1259"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1260"><a href="#cb8-1260"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1261"><a href="#cb8-1261"></a>    <span class="kw">)</span></span>
<span id="cb8-1262"><a href="#cb8-1262"></a>  <span class="kw">)</span></span>
<span id="cb8-1263"><a href="#cb8-1263"></a>  <span class="kw">(</span><span class="ex">26</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1264"><a href="#cb8-1264"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1265"><a href="#cb8-1265"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1266"><a href="#cb8-1266"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1267"><a href="#cb8-1267"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1268"><a href="#cb8-1268"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1269"><a href="#cb8-1269"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1270"><a href="#cb8-1270"></a>      <span class="kw">)</span></span>
<span id="cb8-1271"><a href="#cb8-1271"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1272"><a href="#cb8-1272"></a>    <span class="kw">)</span></span>
<span id="cb8-1273"><a href="#cb8-1273"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1274"><a href="#cb8-1274"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1275"><a href="#cb8-1275"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1276"><a href="#cb8-1276"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1277"><a href="#cb8-1277"></a>    <span class="kw">)</span></span>
<span id="cb8-1278"><a href="#cb8-1278"></a>  <span class="kw">)</span></span>
<span id="cb8-1279"><a href="#cb8-1279"></a>  <span class="kw">(</span><span class="ex">27</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1280"><a href="#cb8-1280"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1281"><a href="#cb8-1281"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1282"><a href="#cb8-1282"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1283"><a href="#cb8-1283"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1284"><a href="#cb8-1284"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1285"><a href="#cb8-1285"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1286"><a href="#cb8-1286"></a>      <span class="kw">)</span></span>
<span id="cb8-1287"><a href="#cb8-1287"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1288"><a href="#cb8-1288"></a>    <span class="kw">)</span></span>
<span id="cb8-1289"><a href="#cb8-1289"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1290"><a href="#cb8-1290"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1291"><a href="#cb8-1291"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1292"><a href="#cb8-1292"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1293"><a href="#cb8-1293"></a>    <span class="kw">)</span></span>
<span id="cb8-1294"><a href="#cb8-1294"></a>  <span class="kw">)</span></span>
<span id="cb8-1295"><a href="#cb8-1295"></a>  <span class="kw">(</span><span class="ex">28</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1296"><a href="#cb8-1296"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1297"><a href="#cb8-1297"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1298"><a href="#cb8-1298"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1299"><a href="#cb8-1299"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1300"><a href="#cb8-1300"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1301"><a href="#cb8-1301"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1302"><a href="#cb8-1302"></a>      <span class="kw">)</span></span>
<span id="cb8-1303"><a href="#cb8-1303"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1304"><a href="#cb8-1304"></a>    <span class="kw">)</span></span>
<span id="cb8-1305"><a href="#cb8-1305"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1306"><a href="#cb8-1306"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1307"><a href="#cb8-1307"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1308"><a href="#cb8-1308"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1309"><a href="#cb8-1309"></a>    <span class="kw">)</span></span>
<span id="cb8-1310"><a href="#cb8-1310"></a>  <span class="kw">)</span></span>
<span id="cb8-1311"><a href="#cb8-1311"></a>  <span class="kw">(</span><span class="ex">29</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1312"><a href="#cb8-1312"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1313"><a href="#cb8-1313"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1314"><a href="#cb8-1314"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1315"><a href="#cb8-1315"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1316"><a href="#cb8-1316"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1317"><a href="#cb8-1317"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1318"><a href="#cb8-1318"></a>      <span class="kw">)</span></span>
<span id="cb8-1319"><a href="#cb8-1319"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1320"><a href="#cb8-1320"></a>    <span class="kw">)</span></span>
<span id="cb8-1321"><a href="#cb8-1321"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1322"><a href="#cb8-1322"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1323"><a href="#cb8-1323"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1324"><a href="#cb8-1324"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1325"><a href="#cb8-1325"></a>    <span class="kw">)</span></span>
<span id="cb8-1326"><a href="#cb8-1326"></a>  <span class="kw">)</span></span>
<span id="cb8-1327"><a href="#cb8-1327"></a>  <span class="kw">(</span><span class="ex">30</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1328"><a href="#cb8-1328"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1329"><a href="#cb8-1329"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1330"><a href="#cb8-1330"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1331"><a href="#cb8-1331"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1332"><a href="#cb8-1332"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1333"><a href="#cb8-1333"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1334"><a href="#cb8-1334"></a>      <span class="kw">)</span></span>
<span id="cb8-1335"><a href="#cb8-1335"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1336"><a href="#cb8-1336"></a>    <span class="kw">)</span></span>
<span id="cb8-1337"><a href="#cb8-1337"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1338"><a href="#cb8-1338"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1339"><a href="#cb8-1339"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1340"><a href="#cb8-1340"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1341"><a href="#cb8-1341"></a>    <span class="kw">)</span></span>
<span id="cb8-1342"><a href="#cb8-1342"></a>  <span class="kw">)</span></span>
<span id="cb8-1343"><a href="#cb8-1343"></a>  <span class="kw">(</span><span class="ex">31</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1344"><a href="#cb8-1344"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1345"><a href="#cb8-1345"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1346"><a href="#cb8-1346"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1347"><a href="#cb8-1347"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1348"><a href="#cb8-1348"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1349"><a href="#cb8-1349"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1350"><a href="#cb8-1350"></a>      <span class="kw">)</span></span>
<span id="cb8-1351"><a href="#cb8-1351"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1352"><a href="#cb8-1352"></a>    <span class="kw">)</span></span>
<span id="cb8-1353"><a href="#cb8-1353"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1354"><a href="#cb8-1354"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1355"><a href="#cb8-1355"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1356"><a href="#cb8-1356"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1357"><a href="#cb8-1357"></a>    <span class="kw">)</span></span>
<span id="cb8-1358"><a href="#cb8-1358"></a>  <span class="kw">)</span></span>
<span id="cb8-1359"><a href="#cb8-1359"></a>  <span class="kw">(</span><span class="ex">32</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1360"><a href="#cb8-1360"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1361"><a href="#cb8-1361"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1362"><a href="#cb8-1362"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1363"><a href="#cb8-1363"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1364"><a href="#cb8-1364"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1365"><a href="#cb8-1365"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1366"><a href="#cb8-1366"></a>      <span class="kw">)</span></span>
<span id="cb8-1367"><a href="#cb8-1367"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1368"><a href="#cb8-1368"></a>    <span class="kw">)</span></span>
<span id="cb8-1369"><a href="#cb8-1369"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1370"><a href="#cb8-1370"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1371"><a href="#cb8-1371"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1372"><a href="#cb8-1372"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1373"><a href="#cb8-1373"></a>    <span class="kw">)</span></span>
<span id="cb8-1374"><a href="#cb8-1374"></a>  <span class="kw">)</span></span>
<span id="cb8-1375"><a href="#cb8-1375"></a>  <span class="kw">(</span><span class="ex">33</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1376"><a href="#cb8-1376"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1377"><a href="#cb8-1377"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1378"><a href="#cb8-1378"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1379"><a href="#cb8-1379"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1380"><a href="#cb8-1380"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1381"><a href="#cb8-1381"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1382"><a href="#cb8-1382"></a>      <span class="kw">)</span></span>
<span id="cb8-1383"><a href="#cb8-1383"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1384"><a href="#cb8-1384"></a>    <span class="kw">)</span></span>
<span id="cb8-1385"><a href="#cb8-1385"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1386"><a href="#cb8-1386"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1387"><a href="#cb8-1387"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1388"><a href="#cb8-1388"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1389"><a href="#cb8-1389"></a>    <span class="kw">)</span></span>
<span id="cb8-1390"><a href="#cb8-1390"></a>  <span class="kw">)</span></span>
<span id="cb8-1391"><a href="#cb8-1391"></a>  <span class="kw">(</span><span class="ex">34</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1392"><a href="#cb8-1392"></a>  <span class="kw">(</span><span class="ex">35</span><span class="kw">)</span><span class="bu">:</span> LMHeadPipe<span class="er">(</span></span>
<span id="cb8-1393"><a href="#cb8-1393"></a>    <span class="kw">(</span><span class="ex">lm_head</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1394"><a href="#cb8-1394"></a>  <span class="kw">)</span></span>
<span id="cb8-1395"><a href="#cb8-1395"></a><span class="ex">Loading</span> checkpoint shards:   0%<span class="kw">|</span>          <span class="kw">|</span> <span class="ex">0/7</span> [00:00<span class="op">&lt;</span><span class="pp">?</span>, <span class="pp">?</span>it/s]</span>
<span id="cb8-1396"><a href="#cb8-1396"></a><span class="ex">Loading</span> checkpoint shards:   0%<span class="kw">|</span>          <span class="kw">|</span> <span class="ex">0/7</span> [00:00<span class="op">&lt;</span><span class="pp">?</span>, <span class="pp">?</span>it/s]stage=0 layers=37</span>
<span id="cb8-1397"><a href="#cb8-1397"></a>     <span class="ex">0:</span> _to_float16</span>
<span id="cb8-1398"><a href="#cb8-1398"></a>     <span class="ex">1:</span> EmbeddingPipe</span>
<span id="cb8-1399"><a href="#cb8-1399"></a>     <span class="ex">2:</span> ParallelTransformerLayerPipe</span>
<span id="cb8-1400"><a href="#cb8-1400"></a>     <span class="ex">3:</span> ParallelTransformerLayerPipe</span>
<span id="cb8-1401"><a href="#cb8-1401"></a>     <span class="ex">4:</span> ParallelTransformerLayerPipe</span>
<span id="cb8-1402"><a href="#cb8-1402"></a>     <span class="ex">5:</span> ParallelTransformerLayerPipe</span>
<span id="cb8-1403"><a href="#cb8-1403"></a>     <span class="ex">6:</span> ParallelTransformerLayerPipe</span>
<span id="cb8-1404"><a href="#cb8-1404"></a>     <span class="ex">7:</span> ParallelTransformerLayerPipe</span>
<span id="cb8-1405"><a href="#cb8-1405"></a>     <span class="ex">8:</span> ParallelTransformerLayerPipe</span>
<span id="cb8-1406"><a href="#cb8-1406"></a>     <span class="ex">9:</span> ParallelTransformerLayerPipe</span>
<span id="cb8-1407"><a href="#cb8-1407"></a>    <span class="ex">10:</span> ParallelTransformerLayerPipe</span>
<span id="cb8-1408"><a href="#cb8-1408"></a>    <span class="ex">11:</span> ParallelTransformerLayerPipe</span>
<span id="cb8-1409"><a href="#cb8-1409"></a>    <span class="ex">12:</span> ParallelTransformerLayerPipe</span>
<span id="cb8-1410"><a href="#cb8-1410"></a>    <span class="ex">13:</span> ParallelTransformerLayerPipe</span>
<span id="cb8-1411"><a href="#cb8-1411"></a>    <span class="ex">14:</span> ParallelTransformerLayerPipe</span>
<span id="cb8-1412"><a href="#cb8-1412"></a>    <span class="ex">15:</span> ParallelTransformerLayerPipe</span>
<span id="cb8-1413"><a href="#cb8-1413"></a>    <span class="ex">16:</span> ParallelTransformerLayerPipe</span>
<span id="cb8-1414"><a href="#cb8-1414"></a>    <span class="ex">17:</span> ParallelTransformerLayerPipe</span>
<span id="cb8-1415"><a href="#cb8-1415"></a>    <span class="ex">18:</span> ParallelTransformerLayerPipe</span>
<span id="cb8-1416"><a href="#cb8-1416"></a>    <span class="ex">19:</span> ParallelTransformerLayerPipe</span>
<span id="cb8-1417"><a href="#cb8-1417"></a>    <span class="ex">20:</span> ParallelTransformerLayerPipe</span>
<span id="cb8-1418"><a href="#cb8-1418"></a>    <span class="ex">21:</span> ParallelTransformerLayerPipe</span>
<span id="cb8-1419"><a href="#cb8-1419"></a>    <span class="ex">22:</span> ParallelTransformerLayerPipe</span>
<span id="cb8-1420"><a href="#cb8-1420"></a>    <span class="ex">23:</span> ParallelTransformerLayerPipe</span>
<span id="cb8-1421"><a href="#cb8-1421"></a>    <span class="ex">24:</span> ParallelTransformerLayerPipe</span>
<span id="cb8-1422"><a href="#cb8-1422"></a>    <span class="ex">25:</span> ParallelTransformerLayerPipe</span>
<span id="cb8-1423"><a href="#cb8-1423"></a>    <span class="ex">26:</span> ParallelTransformerLayerPipe</span>
<span id="cb8-1424"><a href="#cb8-1424"></a>    <span class="ex">27:</span> ParallelTransformerLayerPipe</span>
<span id="cb8-1425"><a href="#cb8-1425"></a>    <span class="ex">28:</span> ParallelTransformerLayerPipe</span>
<span id="cb8-1426"><a href="#cb8-1426"></a>    <span class="ex">29:</span> ParallelTransformerLayerPipe</span>
<span id="cb8-1427"><a href="#cb8-1427"></a>    <span class="ex">30:</span> ParallelTransformerLayerPipe</span>
<span id="cb8-1428"><a href="#cb8-1428"></a>    <span class="ex">31:</span> ParallelTransformerLayerPipe</span>
<span id="cb8-1429"><a href="#cb8-1429"></a>    <span class="ex">32:</span> ParallelTransformerLayerPipe</span>
<span id="cb8-1430"><a href="#cb8-1430"></a>    <span class="ex">33:</span> ParallelTransformerLayerPipe</span>
<span id="cb8-1431"><a href="#cb8-1431"></a>    <span class="ex">34:</span> RMSNorm</span>
<span id="cb8-1432"><a href="#cb8-1432"></a>    <span class="ex">35:</span> LMHeadPipe</span>
<span id="cb8-1433"><a href="#cb8-1433"></a>    <span class="ex">36:</span> float16_to_fp32</span>
<span id="cb8-1434"><a href="#cb8-1434"></a>  <span class="ex">loss:</span> loss_func</span>
<span id="cb8-1435"><a href="#cb8-1435"></a><span class="ex">[2024-10-16</span> 15:39:38,077] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">utils.py:781:see_memory_usage</span><span class="pp">]</span> After Building Model</span>
<span id="cb8-1436"><a href="#cb8-1436"></a><span class="ex">[2024-10-16</span> 15:39:38,077] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">utils.py:782:see_memory_usage</span><span class="pp">]</span> MA 14.96 GB         Max_MA 14.96 GB         CA 14.96 GB         Max_CA 15 GB</span>
<span id="cb8-1437"><a href="#cb8-1437"></a><span class="ex">[2024-10-16</span> 15:39:38,077] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">utils.py:789:see_memory_usage</span><span class="pp">]</span> CPU Virtual Memory:  used = 105.4 GB, percent = 9.3%</span>
<span id="cb8-1438"><a href="#cb8-1438"></a><span class="ex">0</span> GPTModelPipe<span class="er">(</span></span>
<span id="cb8-1439"><a href="#cb8-1439"></a>  <span class="kw">(</span><span class="ex">tied_modules</span><span class="kw">)</span><span class="bu">:</span> ModuleDict<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1440"><a href="#cb8-1440"></a>  <span class="kw">(</span><span class="ex">1</span><span class="kw">)</span><span class="bu">:</span> EmbeddingPipe<span class="er">(</span></span>
<span id="cb8-1441"><a href="#cb8-1441"></a>    <span class="kw">(</span><span class="ex">word_embeddings</span><span class="kw">)</span><span class="bu">:</span> VocabParallelEmbedding<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1442"><a href="#cb8-1442"></a>    <span class="kw">(</span><span class="ex">embedding_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1443"><a href="#cb8-1443"></a>  <span class="kw">)</span></span>
<span id="cb8-1444"><a href="#cb8-1444"></a>  <span class="kw">(</span><span class="ex">2</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1445"><a href="#cb8-1445"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1446"><a href="#cb8-1446"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1447"><a href="#cb8-1447"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1448"><a href="#cb8-1448"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1449"><a href="#cb8-1449"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1450"><a href="#cb8-1450"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1451"><a href="#cb8-1451"></a>      <span class="kw">)</span></span>
<span id="cb8-1452"><a href="#cb8-1452"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1453"><a href="#cb8-1453"></a>    <span class="kw">)</span></span>
<span id="cb8-1454"><a href="#cb8-1454"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1455"><a href="#cb8-1455"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1456"><a href="#cb8-1456"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1457"><a href="#cb8-1457"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1458"><a href="#cb8-1458"></a>    <span class="kw">)</span></span>
<span id="cb8-1459"><a href="#cb8-1459"></a>  <span class="kw">)</span></span>
<span id="cb8-1460"><a href="#cb8-1460"></a>  <span class="kw">(</span><span class="ex">3</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1461"><a href="#cb8-1461"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1462"><a href="#cb8-1462"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1463"><a href="#cb8-1463"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1464"><a href="#cb8-1464"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1465"><a href="#cb8-1465"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1466"><a href="#cb8-1466"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1467"><a href="#cb8-1467"></a>      <span class="kw">)</span></span>
<span id="cb8-1468"><a href="#cb8-1468"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1469"><a href="#cb8-1469"></a>    <span class="kw">)</span></span>
<span id="cb8-1470"><a href="#cb8-1470"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1471"><a href="#cb8-1471"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1472"><a href="#cb8-1472"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1473"><a href="#cb8-1473"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1474"><a href="#cb8-1474"></a>    <span class="kw">)</span></span>
<span id="cb8-1475"><a href="#cb8-1475"></a>  <span class="kw">)</span></span>
<span id="cb8-1476"><a href="#cb8-1476"></a>  <span class="kw">(</span><span class="ex">4</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1477"><a href="#cb8-1477"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1478"><a href="#cb8-1478"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1479"><a href="#cb8-1479"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1480"><a href="#cb8-1480"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1481"><a href="#cb8-1481"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1482"><a href="#cb8-1482"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1483"><a href="#cb8-1483"></a>      <span class="kw">)</span></span>
<span id="cb8-1484"><a href="#cb8-1484"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1485"><a href="#cb8-1485"></a>    <span class="kw">)</span></span>
<span id="cb8-1486"><a href="#cb8-1486"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1487"><a href="#cb8-1487"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1488"><a href="#cb8-1488"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1489"><a href="#cb8-1489"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1490"><a href="#cb8-1490"></a>    <span class="kw">)</span></span>
<span id="cb8-1491"><a href="#cb8-1491"></a>  <span class="kw">)</span></span>
<span id="cb8-1492"><a href="#cb8-1492"></a>  <span class="kw">(</span><span class="ex">5</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1493"><a href="#cb8-1493"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1494"><a href="#cb8-1494"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1495"><a href="#cb8-1495"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1496"><a href="#cb8-1496"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1497"><a href="#cb8-1497"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1498"><a href="#cb8-1498"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1499"><a href="#cb8-1499"></a>      <span class="kw">)</span></span>
<span id="cb8-1500"><a href="#cb8-1500"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1501"><a href="#cb8-1501"></a>    <span class="kw">)</span></span>
<span id="cb8-1502"><a href="#cb8-1502"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1503"><a href="#cb8-1503"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1504"><a href="#cb8-1504"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1505"><a href="#cb8-1505"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1506"><a href="#cb8-1506"></a>    <span class="kw">)</span></span>
<span id="cb8-1507"><a href="#cb8-1507"></a>  <span class="kw">)</span></span>
<span id="cb8-1508"><a href="#cb8-1508"></a>  <span class="kw">(</span><span class="ex">6</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1509"><a href="#cb8-1509"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1510"><a href="#cb8-1510"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1511"><a href="#cb8-1511"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1512"><a href="#cb8-1512"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1513"><a href="#cb8-1513"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1514"><a href="#cb8-1514"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1515"><a href="#cb8-1515"></a>      <span class="kw">)</span></span>
<span id="cb8-1516"><a href="#cb8-1516"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1517"><a href="#cb8-1517"></a>    <span class="kw">)</span></span>
<span id="cb8-1518"><a href="#cb8-1518"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1519"><a href="#cb8-1519"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1520"><a href="#cb8-1520"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1521"><a href="#cb8-1521"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1522"><a href="#cb8-1522"></a>    <span class="kw">)</span></span>
<span id="cb8-1523"><a href="#cb8-1523"></a>  <span class="kw">)</span></span>
<span id="cb8-1524"><a href="#cb8-1524"></a>  <span class="kw">(</span><span class="ex">7</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1525"><a href="#cb8-1525"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1526"><a href="#cb8-1526"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1527"><a href="#cb8-1527"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1528"><a href="#cb8-1528"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1529"><a href="#cb8-1529"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1530"><a href="#cb8-1530"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1531"><a href="#cb8-1531"></a>      <span class="kw">)</span></span>
<span id="cb8-1532"><a href="#cb8-1532"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1533"><a href="#cb8-1533"></a>    <span class="kw">)</span></span>
<span id="cb8-1534"><a href="#cb8-1534"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1535"><a href="#cb8-1535"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1536"><a href="#cb8-1536"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1537"><a href="#cb8-1537"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1538"><a href="#cb8-1538"></a>    <span class="kw">)</span></span>
<span id="cb8-1539"><a href="#cb8-1539"></a>  <span class="kw">)</span></span>
<span id="cb8-1540"><a href="#cb8-1540"></a>  <span class="kw">(</span><span class="ex">8</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1541"><a href="#cb8-1541"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1542"><a href="#cb8-1542"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1543"><a href="#cb8-1543"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1544"><a href="#cb8-1544"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1545"><a href="#cb8-1545"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1546"><a href="#cb8-1546"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1547"><a href="#cb8-1547"></a>      <span class="kw">)</span></span>
<span id="cb8-1548"><a href="#cb8-1548"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1549"><a href="#cb8-1549"></a>    <span class="kw">)</span></span>
<span id="cb8-1550"><a href="#cb8-1550"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1551"><a href="#cb8-1551"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1552"><a href="#cb8-1552"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1553"><a href="#cb8-1553"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1554"><a href="#cb8-1554"></a>    <span class="kw">)</span></span>
<span id="cb8-1555"><a href="#cb8-1555"></a>  <span class="kw">)</span></span>
<span id="cb8-1556"><a href="#cb8-1556"></a>  <span class="kw">(</span><span class="ex">9</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1557"><a href="#cb8-1557"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1558"><a href="#cb8-1558"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1559"><a href="#cb8-1559"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1560"><a href="#cb8-1560"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1561"><a href="#cb8-1561"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1562"><a href="#cb8-1562"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1563"><a href="#cb8-1563"></a>      <span class="kw">)</span></span>
<span id="cb8-1564"><a href="#cb8-1564"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1565"><a href="#cb8-1565"></a>    <span class="kw">)</span></span>
<span id="cb8-1566"><a href="#cb8-1566"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1567"><a href="#cb8-1567"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1568"><a href="#cb8-1568"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1569"><a href="#cb8-1569"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1570"><a href="#cb8-1570"></a>    <span class="kw">)</span></span>
<span id="cb8-1571"><a href="#cb8-1571"></a>  <span class="kw">)</span></span>
<span id="cb8-1572"><a href="#cb8-1572"></a>  <span class="kw">(</span><span class="ex">10</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1573"><a href="#cb8-1573"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1574"><a href="#cb8-1574"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1575"><a href="#cb8-1575"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1576"><a href="#cb8-1576"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1577"><a href="#cb8-1577"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1578"><a href="#cb8-1578"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1579"><a href="#cb8-1579"></a>      <span class="kw">)</span></span>
<span id="cb8-1580"><a href="#cb8-1580"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1581"><a href="#cb8-1581"></a>    <span class="kw">)</span></span>
<span id="cb8-1582"><a href="#cb8-1582"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1583"><a href="#cb8-1583"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1584"><a href="#cb8-1584"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1585"><a href="#cb8-1585"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1586"><a href="#cb8-1586"></a>    <span class="kw">)</span></span>
<span id="cb8-1587"><a href="#cb8-1587"></a>  <span class="kw">)</span></span>
<span id="cb8-1588"><a href="#cb8-1588"></a>  <span class="kw">(</span><span class="ex">11</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1589"><a href="#cb8-1589"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1590"><a href="#cb8-1590"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1591"><a href="#cb8-1591"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1592"><a href="#cb8-1592"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1593"><a href="#cb8-1593"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1594"><a href="#cb8-1594"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1595"><a href="#cb8-1595"></a>      <span class="kw">)</span></span>
<span id="cb8-1596"><a href="#cb8-1596"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1597"><a href="#cb8-1597"></a>    <span class="kw">)</span></span>
<span id="cb8-1598"><a href="#cb8-1598"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1599"><a href="#cb8-1599"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1600"><a href="#cb8-1600"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1601"><a href="#cb8-1601"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1602"><a href="#cb8-1602"></a>    <span class="kw">)</span></span>
<span id="cb8-1603"><a href="#cb8-1603"></a>  <span class="kw">)</span></span>
<span id="cb8-1604"><a href="#cb8-1604"></a>  <span class="kw">(</span><span class="ex">12</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1605"><a href="#cb8-1605"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1606"><a href="#cb8-1606"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1607"><a href="#cb8-1607"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1608"><a href="#cb8-1608"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1609"><a href="#cb8-1609"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1610"><a href="#cb8-1610"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1611"><a href="#cb8-1611"></a>      <span class="kw">)</span></span>
<span id="cb8-1612"><a href="#cb8-1612"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1613"><a href="#cb8-1613"></a>    <span class="kw">)</span></span>
<span id="cb8-1614"><a href="#cb8-1614"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1615"><a href="#cb8-1615"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1616"><a href="#cb8-1616"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1617"><a href="#cb8-1617"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1618"><a href="#cb8-1618"></a>    <span class="kw">)</span></span>
<span id="cb8-1619"><a href="#cb8-1619"></a>  <span class="kw">)</span></span>
<span id="cb8-1620"><a href="#cb8-1620"></a>  <span class="kw">(</span><span class="ex">13</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1621"><a href="#cb8-1621"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1622"><a href="#cb8-1622"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1623"><a href="#cb8-1623"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1624"><a href="#cb8-1624"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1625"><a href="#cb8-1625"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1626"><a href="#cb8-1626"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1627"><a href="#cb8-1627"></a>      <span class="kw">)</span></span>
<span id="cb8-1628"><a href="#cb8-1628"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1629"><a href="#cb8-1629"></a>    <span class="kw">)</span></span>
<span id="cb8-1630"><a href="#cb8-1630"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1631"><a href="#cb8-1631"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1632"><a href="#cb8-1632"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1633"><a href="#cb8-1633"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1634"><a href="#cb8-1634"></a>    <span class="kw">)</span></span>
<span id="cb8-1635"><a href="#cb8-1635"></a>  <span class="kw">)</span></span>
<span id="cb8-1636"><a href="#cb8-1636"></a>  <span class="kw">(</span><span class="ex">14</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1637"><a href="#cb8-1637"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1638"><a href="#cb8-1638"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1639"><a href="#cb8-1639"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1640"><a href="#cb8-1640"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1641"><a href="#cb8-1641"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1642"><a href="#cb8-1642"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1643"><a href="#cb8-1643"></a>      <span class="kw">)</span></span>
<span id="cb8-1644"><a href="#cb8-1644"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1645"><a href="#cb8-1645"></a>    <span class="kw">)</span></span>
<span id="cb8-1646"><a href="#cb8-1646"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1647"><a href="#cb8-1647"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1648"><a href="#cb8-1648"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1649"><a href="#cb8-1649"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1650"><a href="#cb8-1650"></a>    <span class="kw">)</span></span>
<span id="cb8-1651"><a href="#cb8-1651"></a>  <span class="kw">)</span></span>
<span id="cb8-1652"><a href="#cb8-1652"></a>  <span class="kw">(</span><span class="ex">15</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1653"><a href="#cb8-1653"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1654"><a href="#cb8-1654"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1655"><a href="#cb8-1655"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1656"><a href="#cb8-1656"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1657"><a href="#cb8-1657"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1658"><a href="#cb8-1658"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1659"><a href="#cb8-1659"></a>      <span class="kw">)</span></span>
<span id="cb8-1660"><a href="#cb8-1660"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1661"><a href="#cb8-1661"></a>    <span class="kw">)</span></span>
<span id="cb8-1662"><a href="#cb8-1662"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1663"><a href="#cb8-1663"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1664"><a href="#cb8-1664"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1665"><a href="#cb8-1665"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1666"><a href="#cb8-1666"></a>    <span class="kw">)</span></span>
<span id="cb8-1667"><a href="#cb8-1667"></a>  <span class="kw">)</span></span>
<span id="cb8-1668"><a href="#cb8-1668"></a>  <span class="kw">(</span><span class="ex">16</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1669"><a href="#cb8-1669"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1670"><a href="#cb8-1670"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1671"><a href="#cb8-1671"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1672"><a href="#cb8-1672"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1673"><a href="#cb8-1673"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1674"><a href="#cb8-1674"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1675"><a href="#cb8-1675"></a>      <span class="kw">)</span></span>
<span id="cb8-1676"><a href="#cb8-1676"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1677"><a href="#cb8-1677"></a>    <span class="kw">)</span></span>
<span id="cb8-1678"><a href="#cb8-1678"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1679"><a href="#cb8-1679"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1680"><a href="#cb8-1680"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1681"><a href="#cb8-1681"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1682"><a href="#cb8-1682"></a>    <span class="kw">)</span></span>
<span id="cb8-1683"><a href="#cb8-1683"></a>  <span class="kw">)</span></span>
<span id="cb8-1684"><a href="#cb8-1684"></a>  <span class="kw">(</span><span class="ex">17</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1685"><a href="#cb8-1685"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1686"><a href="#cb8-1686"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1687"><a href="#cb8-1687"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1688"><a href="#cb8-1688"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1689"><a href="#cb8-1689"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1690"><a href="#cb8-1690"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1691"><a href="#cb8-1691"></a>      <span class="kw">)</span></span>
<span id="cb8-1692"><a href="#cb8-1692"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1693"><a href="#cb8-1693"></a>    <span class="kw">)</span></span>
<span id="cb8-1694"><a href="#cb8-1694"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1695"><a href="#cb8-1695"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1696"><a href="#cb8-1696"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1697"><a href="#cb8-1697"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1698"><a href="#cb8-1698"></a>    <span class="kw">)</span></span>
<span id="cb8-1699"><a href="#cb8-1699"></a>  <span class="kw">)</span></span>
<span id="cb8-1700"><a href="#cb8-1700"></a>  <span class="kw">(</span><span class="ex">18</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1701"><a href="#cb8-1701"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1702"><a href="#cb8-1702"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1703"><a href="#cb8-1703"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1704"><a href="#cb8-1704"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1705"><a href="#cb8-1705"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1706"><a href="#cb8-1706"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1707"><a href="#cb8-1707"></a>      <span class="kw">)</span></span>
<span id="cb8-1708"><a href="#cb8-1708"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1709"><a href="#cb8-1709"></a>    <span class="kw">)</span></span>
<span id="cb8-1710"><a href="#cb8-1710"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1711"><a href="#cb8-1711"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1712"><a href="#cb8-1712"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1713"><a href="#cb8-1713"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1714"><a href="#cb8-1714"></a>    <span class="kw">)</span></span>
<span id="cb8-1715"><a href="#cb8-1715"></a>  <span class="kw">)</span></span>
<span id="cb8-1716"><a href="#cb8-1716"></a>  <span class="kw">(</span><span class="ex">19</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1717"><a href="#cb8-1717"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1718"><a href="#cb8-1718"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1719"><a href="#cb8-1719"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1720"><a href="#cb8-1720"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1721"><a href="#cb8-1721"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1722"><a href="#cb8-1722"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1723"><a href="#cb8-1723"></a>      <span class="kw">)</span></span>
<span id="cb8-1724"><a href="#cb8-1724"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1725"><a href="#cb8-1725"></a>    <span class="kw">)</span></span>
<span id="cb8-1726"><a href="#cb8-1726"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1727"><a href="#cb8-1727"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1728"><a href="#cb8-1728"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1729"><a href="#cb8-1729"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1730"><a href="#cb8-1730"></a>    <span class="kw">)</span></span>
<span id="cb8-1731"><a href="#cb8-1731"></a>  <span class="kw">)</span></span>
<span id="cb8-1732"><a href="#cb8-1732"></a>  <span class="kw">(</span><span class="ex">20</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1733"><a href="#cb8-1733"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1734"><a href="#cb8-1734"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1735"><a href="#cb8-1735"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1736"><a href="#cb8-1736"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1737"><a href="#cb8-1737"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1738"><a href="#cb8-1738"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1739"><a href="#cb8-1739"></a>      <span class="kw">)</span></span>
<span id="cb8-1740"><a href="#cb8-1740"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1741"><a href="#cb8-1741"></a>    <span class="kw">)</span></span>
<span id="cb8-1742"><a href="#cb8-1742"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1743"><a href="#cb8-1743"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1744"><a href="#cb8-1744"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1745"><a href="#cb8-1745"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1746"><a href="#cb8-1746"></a>    <span class="kw">)</span></span>
<span id="cb8-1747"><a href="#cb8-1747"></a>  <span class="kw">)</span></span>
<span id="cb8-1748"><a href="#cb8-1748"></a>  <span class="kw">(</span><span class="ex">21</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1749"><a href="#cb8-1749"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1750"><a href="#cb8-1750"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1751"><a href="#cb8-1751"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1752"><a href="#cb8-1752"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1753"><a href="#cb8-1753"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1754"><a href="#cb8-1754"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1755"><a href="#cb8-1755"></a>      <span class="kw">)</span></span>
<span id="cb8-1756"><a href="#cb8-1756"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1757"><a href="#cb8-1757"></a>    <span class="kw">)</span></span>
<span id="cb8-1758"><a href="#cb8-1758"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1759"><a href="#cb8-1759"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1760"><a href="#cb8-1760"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1761"><a href="#cb8-1761"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1762"><a href="#cb8-1762"></a>    <span class="kw">)</span></span>
<span id="cb8-1763"><a href="#cb8-1763"></a>  <span class="kw">)</span></span>
<span id="cb8-1764"><a href="#cb8-1764"></a>  <span class="kw">(</span><span class="ex">22</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1765"><a href="#cb8-1765"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1766"><a href="#cb8-1766"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1767"><a href="#cb8-1767"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1768"><a href="#cb8-1768"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1769"><a href="#cb8-1769"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1770"><a href="#cb8-1770"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1771"><a href="#cb8-1771"></a>      <span class="kw">)</span></span>
<span id="cb8-1772"><a href="#cb8-1772"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1773"><a href="#cb8-1773"></a>    <span class="kw">)</span></span>
<span id="cb8-1774"><a href="#cb8-1774"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1775"><a href="#cb8-1775"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1776"><a href="#cb8-1776"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1777"><a href="#cb8-1777"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1778"><a href="#cb8-1778"></a>    <span class="kw">)</span></span>
<span id="cb8-1779"><a href="#cb8-1779"></a>  <span class="kw">)</span></span>
<span id="cb8-1780"><a href="#cb8-1780"></a>  <span class="kw">(</span><span class="ex">23</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1781"><a href="#cb8-1781"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1782"><a href="#cb8-1782"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1783"><a href="#cb8-1783"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1784"><a href="#cb8-1784"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1785"><a href="#cb8-1785"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1786"><a href="#cb8-1786"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1787"><a href="#cb8-1787"></a>      <span class="kw">)</span></span>
<span id="cb8-1788"><a href="#cb8-1788"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1789"><a href="#cb8-1789"></a>    <span class="kw">)</span></span>
<span id="cb8-1790"><a href="#cb8-1790"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1791"><a href="#cb8-1791"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1792"><a href="#cb8-1792"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1793"><a href="#cb8-1793"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1794"><a href="#cb8-1794"></a>    <span class="kw">)</span></span>
<span id="cb8-1795"><a href="#cb8-1795"></a>  <span class="kw">)</span></span>
<span id="cb8-1796"><a href="#cb8-1796"></a>  <span class="kw">(</span><span class="ex">24</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1797"><a href="#cb8-1797"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1798"><a href="#cb8-1798"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1799"><a href="#cb8-1799"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1800"><a href="#cb8-1800"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1801"><a href="#cb8-1801"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1802"><a href="#cb8-1802"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1803"><a href="#cb8-1803"></a>      <span class="kw">)</span></span>
<span id="cb8-1804"><a href="#cb8-1804"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1805"><a href="#cb8-1805"></a>    <span class="kw">)</span></span>
<span id="cb8-1806"><a href="#cb8-1806"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1807"><a href="#cb8-1807"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1808"><a href="#cb8-1808"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1809"><a href="#cb8-1809"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1810"><a href="#cb8-1810"></a>    <span class="kw">)</span></span>
<span id="cb8-1811"><a href="#cb8-1811"></a>  <span class="kw">)</span></span>
<span id="cb8-1812"><a href="#cb8-1812"></a>  <span class="kw">(</span><span class="ex">25</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1813"><a href="#cb8-1813"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1814"><a href="#cb8-1814"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1815"><a href="#cb8-1815"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1816"><a href="#cb8-1816"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1817"><a href="#cb8-1817"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1818"><a href="#cb8-1818"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1819"><a href="#cb8-1819"></a>      <span class="kw">)</span></span>
<span id="cb8-1820"><a href="#cb8-1820"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1821"><a href="#cb8-1821"></a>    <span class="kw">)</span></span>
<span id="cb8-1822"><a href="#cb8-1822"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1823"><a href="#cb8-1823"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1824"><a href="#cb8-1824"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1825"><a href="#cb8-1825"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1826"><a href="#cb8-1826"></a>    <span class="kw">)</span></span>
<span id="cb8-1827"><a href="#cb8-1827"></a>  <span class="kw">)</span></span>
<span id="cb8-1828"><a href="#cb8-1828"></a>  <span class="kw">(</span><span class="ex">26</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1829"><a href="#cb8-1829"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1830"><a href="#cb8-1830"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1831"><a href="#cb8-1831"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1832"><a href="#cb8-1832"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1833"><a href="#cb8-1833"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1834"><a href="#cb8-1834"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1835"><a href="#cb8-1835"></a>      <span class="kw">)</span></span>
<span id="cb8-1836"><a href="#cb8-1836"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1837"><a href="#cb8-1837"></a>    <span class="kw">)</span></span>
<span id="cb8-1838"><a href="#cb8-1838"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1839"><a href="#cb8-1839"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1840"><a href="#cb8-1840"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1841"><a href="#cb8-1841"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1842"><a href="#cb8-1842"></a>    <span class="kw">)</span></span>
<span id="cb8-1843"><a href="#cb8-1843"></a>  <span class="kw">)</span></span>
<span id="cb8-1844"><a href="#cb8-1844"></a>  <span class="kw">(</span><span class="ex">27</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1845"><a href="#cb8-1845"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1846"><a href="#cb8-1846"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1847"><a href="#cb8-1847"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1848"><a href="#cb8-1848"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1849"><a href="#cb8-1849"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1850"><a href="#cb8-1850"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1851"><a href="#cb8-1851"></a>      <span class="kw">)</span></span>
<span id="cb8-1852"><a href="#cb8-1852"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1853"><a href="#cb8-1853"></a>    <span class="kw">)</span></span>
<span id="cb8-1854"><a href="#cb8-1854"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1855"><a href="#cb8-1855"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1856"><a href="#cb8-1856"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1857"><a href="#cb8-1857"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1858"><a href="#cb8-1858"></a>    <span class="kw">)</span></span>
<span id="cb8-1859"><a href="#cb8-1859"></a>  <span class="kw">)</span></span>
<span id="cb8-1860"><a href="#cb8-1860"></a>  <span class="kw">(</span><span class="ex">28</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1861"><a href="#cb8-1861"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1862"><a href="#cb8-1862"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1863"><a href="#cb8-1863"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1864"><a href="#cb8-1864"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1865"><a href="#cb8-1865"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1866"><a href="#cb8-1866"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1867"><a href="#cb8-1867"></a>      <span class="kw">)</span></span>
<span id="cb8-1868"><a href="#cb8-1868"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1869"><a href="#cb8-1869"></a>    <span class="kw">)</span></span>
<span id="cb8-1870"><a href="#cb8-1870"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1871"><a href="#cb8-1871"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1872"><a href="#cb8-1872"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1873"><a href="#cb8-1873"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1874"><a href="#cb8-1874"></a>    <span class="kw">)</span></span>
<span id="cb8-1875"><a href="#cb8-1875"></a>  <span class="kw">)</span></span>
<span id="cb8-1876"><a href="#cb8-1876"></a>  <span class="kw">(</span><span class="ex">29</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1877"><a href="#cb8-1877"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1878"><a href="#cb8-1878"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1879"><a href="#cb8-1879"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1880"><a href="#cb8-1880"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1881"><a href="#cb8-1881"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1882"><a href="#cb8-1882"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1883"><a href="#cb8-1883"></a>      <span class="kw">)</span></span>
<span id="cb8-1884"><a href="#cb8-1884"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1885"><a href="#cb8-1885"></a>    <span class="kw">)</span></span>
<span id="cb8-1886"><a href="#cb8-1886"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1887"><a href="#cb8-1887"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1888"><a href="#cb8-1888"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1889"><a href="#cb8-1889"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1890"><a href="#cb8-1890"></a>    <span class="kw">)</span></span>
<span id="cb8-1891"><a href="#cb8-1891"></a>  <span class="kw">)</span></span>
<span id="cb8-1892"><a href="#cb8-1892"></a>  <span class="kw">(</span><span class="ex">30</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1893"><a href="#cb8-1893"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1894"><a href="#cb8-1894"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1895"><a href="#cb8-1895"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1896"><a href="#cb8-1896"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1897"><a href="#cb8-1897"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1898"><a href="#cb8-1898"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1899"><a href="#cb8-1899"></a>      <span class="kw">)</span></span>
<span id="cb8-1900"><a href="#cb8-1900"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1901"><a href="#cb8-1901"></a>    <span class="kw">)</span></span>
<span id="cb8-1902"><a href="#cb8-1902"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1903"><a href="#cb8-1903"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1904"><a href="#cb8-1904"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1905"><a href="#cb8-1905"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1906"><a href="#cb8-1906"></a>    <span class="kw">)</span></span>
<span id="cb8-1907"><a href="#cb8-1907"></a>  <span class="kw">)</span></span>
<span id="cb8-1908"><a href="#cb8-1908"></a>  <span class="kw">(</span><span class="ex">31</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1909"><a href="#cb8-1909"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1910"><a href="#cb8-1910"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1911"><a href="#cb8-1911"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1912"><a href="#cb8-1912"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1913"><a href="#cb8-1913"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1914"><a href="#cb8-1914"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1915"><a href="#cb8-1915"></a>      <span class="kw">)</span></span>
<span id="cb8-1916"><a href="#cb8-1916"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1917"><a href="#cb8-1917"></a>    <span class="kw">)</span></span>
<span id="cb8-1918"><a href="#cb8-1918"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1919"><a href="#cb8-1919"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1920"><a href="#cb8-1920"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1921"><a href="#cb8-1921"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1922"><a href="#cb8-1922"></a>    <span class="kw">)</span></span>
<span id="cb8-1923"><a href="#cb8-1923"></a>  <span class="kw">)</span></span>
<span id="cb8-1924"><a href="#cb8-1924"></a>  <span class="kw">(</span><span class="ex">32</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1925"><a href="#cb8-1925"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1926"><a href="#cb8-1926"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1927"><a href="#cb8-1927"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1928"><a href="#cb8-1928"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1929"><a href="#cb8-1929"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1930"><a href="#cb8-1930"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1931"><a href="#cb8-1931"></a>      <span class="kw">)</span></span>
<span id="cb8-1932"><a href="#cb8-1932"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1933"><a href="#cb8-1933"></a>    <span class="kw">)</span></span>
<span id="cb8-1934"><a href="#cb8-1934"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1935"><a href="#cb8-1935"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1936"><a href="#cb8-1936"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1937"><a href="#cb8-1937"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1938"><a href="#cb8-1938"></a>    <span class="kw">)</span></span>
<span id="cb8-1939"><a href="#cb8-1939"></a>  <span class="kw">)</span></span>
<span id="cb8-1940"><a href="#cb8-1940"></a>  <span class="kw">(</span><span class="ex">33</span><span class="kw">)</span><span class="bu">:</span> ParallelTransformerLayerPipe<span class="er">(</span></span>
<span id="cb8-1941"><a href="#cb8-1941"></a>    <span class="kw">(</span><span class="ex">input_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1942"><a href="#cb8-1942"></a>    <span class="kw">(</span><span class="ex">self_attention</span><span class="kw">)</span><span class="bu">:</span> ParallelAttention<span class="er">(</span></span>
<span id="cb8-1943"><a href="#cb8-1943"></a>      <span class="kw">(</span><span class="ex">query_key_value</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1944"><a href="#cb8-1944"></a>      <span class="kw">(</span><span class="ex">core_attention</span><span class="kw">)</span><span class="bu">:</span> CoreAttention<span class="er">(</span></span>
<span id="cb8-1945"><a href="#cb8-1945"></a>        <span class="kw">(</span><span class="ex">scale_mask_softmax</span><span class="kw">)</span><span class="bu">:</span> FusedScaleMaskSoftmax<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1946"><a href="#cb8-1946"></a>        <span class="kw">(</span><span class="ex">attention_dropout</span><span class="kw">)</span><span class="bu">:</span> Dropout<span class="er">(</span><span class="va">p</span><span class="op">=</span>0.0, <span class="va">inplace</span><span class="op">=</span>False<span class="kw">)</span></span>
<span id="cb8-1947"><a href="#cb8-1947"></a>      <span class="kw">)</span></span>
<span id="cb8-1948"><a href="#cb8-1948"></a>      <span class="kw">(</span><span class="ex">dense</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1949"><a href="#cb8-1949"></a>    <span class="kw">)</span></span>
<span id="cb8-1950"><a href="#cb8-1950"></a>    <span class="kw">(</span><span class="ex">post_attention_layernorm</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1951"><a href="#cb8-1951"></a>    <span class="kw">(</span><span class="ex">mlp</span><span class="kw">)</span><span class="bu">:</span> ParallelMLP<span class="er">(</span></span>
<span id="cb8-1952"><a href="#cb8-1952"></a>      <span class="kw">(</span><span class="ex">dense_h_to_4h</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1953"><a href="#cb8-1953"></a>      <span class="kw">(</span><span class="ex">dense_4h_to_h</span><span class="kw">)</span><span class="bu">:</span> RowParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1954"><a href="#cb8-1954"></a>    <span class="kw">)</span></span>
<span id="cb8-1955"><a href="#cb8-1955"></a>  <span class="kw">)</span></span>
<span id="cb8-1956"><a href="#cb8-1956"></a>  <span class="kw">(</span><span class="ex">34</span><span class="kw">)</span><span class="bu">:</span> RMSNorm<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1957"><a href="#cb8-1957"></a>  <span class="kw">(</span><span class="ex">35</span><span class="kw">)</span><span class="bu">:</span> LMHeadPipe<span class="er">(</span></span>
<span id="cb8-1958"><a href="#cb8-1958"></a>    <span class="kw">(</span><span class="ex">lm_head</span><span class="kw">)</span><span class="bu">:</span> ColumnParallelLinear<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-1959"><a href="#cb8-1959"></a>  <span class="kw">)</span></span>
<span id="cb8-1960"><a href="#cb8-1960"></a><span class="kw">)</span></span>
<span id="cb8-1961"><a href="#cb8-1961"></a><span class="ex">Loading</span> checkpoint shards:   0%<span class="kw">|</span>          <span class="kw">|</span> <span class="ex">0/7</span> [00:00<span class="op">&lt;</span><span class="pp">?</span>, <span class="pp">?</span>it/s]2024-10-16 15:39:39.334375: W external/local_tsl/tsl/lib/monitoring/collection_registry.cc:81] Trying to register 2 metrics with the same name: /tensorflow/core/bfc_allocator_delay. The old value will be erased inorder to register a new one. Please check if you link the metric more than once, or if the name is already used by other metrics.</span>
<span id="cb8-1962"><a href="#cb8-1962"></a><span class="ex">2024-10-16</span> 15:39:39.334561: W external/local_tsl/tsl/lib/monitoring/collection_registry.cc:81] Trying to register 2 metrics with the same name: /xla/service/gpu/compiled_programs_count. The old value will be erased in order to register a new one. Please check if you link the metric more than once, or if the name is already used by other metrics.</span>
<span id="cb8-1963"><a href="#cb8-1963"></a><span class="ex">2024-10-16</span> 15:39:39.335711: W external/local_tsl/tsl/lib/monitoring/collection_registry.cc:81] Trying to register 2 metrics with the same name: /jax/pjrt/pjrt_executable_executions. The old value will be erased in order to register a new one. Please check if you link the metric more than once, or if the name is already used by other metrics.</span>
<span id="cb8-1964"><a href="#cb8-1964"></a><span class="ex">2024-10-16</span> 15:39:39.335722: W external/local_tsl/tsl/lib/monitoring/collection_registry.cc:81] Trying to register 2 metrics with the same name: /jax/pjrt/pjrt_executable_execution_time_usecs. The old value will be erased in order to register a new one. Please check if you linkthe metric more than once, or if the name is already used by other metrics.</span>
<span id="cb8-1965"><a href="#cb8-1965"></a><span class="ex">2024-10-16</span> 15:39:39.587872: I itex/core/wrapper/itex_gpu_wrapper.cc:38] Intel Extension for Tensorflow<span class="pp">*</span> GPU backend is loaded.</span>
<span id="cb8-1966"><a href="#cb8-1966"></a><span class="ex">2024-10-16</span> 15:39:39.631109: I itex/core/devices/gpu/itex_gpu_runtime.cc:130] Selected platform: Intel<span class="er">(</span><span class="ex">R</span><span class="kw">)</span> <span class="ex">Level-Zero</span></span>
<span id="cb8-1967"><a href="#cb8-1967"></a><span class="ex">2024-10-16</span> 15:39:39.631594: I itex/core/devices/gpu/itex_gpu_runtime.cc:155] number of sub-devices is zero, expose root device.</span>
<span id="cb8-1968"><a href="#cb8-1968"></a><span class="ex">2024-10-16</span> 15:39:39.631598: I itex/core/devices/gpu/itex_gpu_runtime.cc:155] number of sub-devices is zero, expose root device.</span>
<span id="cb8-1969"><a href="#cb8-1969"></a><span class="ex">2024-10-16</span> 15:39:39.631600: I itex/core/devices/gpu/itex_gpu_runtime.cc:155] number of sub-devices is zero, expose root device.</span>
<span id="cb8-1970"><a href="#cb8-1970"></a><span class="ex">2024-10-16</span> 15:39:39.631602: I itex/core/devices/gpu/itex_gpu_runtime.cc:155] number of sub-devices is zero, expose root device.</span>
<span id="cb8-1971"><a href="#cb8-1971"></a><span class="ex">2024-10-16</span> 15:39:39.631604: I itex/core/devices/gpu/itex_gpu_runtime.cc:155] number of sub-devices is zero, expose root device.</span>
<span id="cb8-1972"><a href="#cb8-1972"></a><span class="ex">2024-10-16</span> 15:39:39.631607: I itex/core/devices/gpu/itex_gpu_runtime.cc:155] number of sub-devices is zero, expose root device.</span>
<span id="cb8-1973"><a href="#cb8-1973"></a><span class="ex">2024-10-16</span> 15:39:39.631609: I itex/core/devices/gpu/itex_gpu_runtime.cc:155] number of sub-devices is zero, expose root device.</span>
<span id="cb8-1974"><a href="#cb8-1974"></a><span class="ex">2024-10-16</span> 15:39:39.631611: I itex/core/devices/gpu/itex_gpu_runtime.cc:155] number of sub-devices is zero, expose root device.</span>
<span id="cb8-1975"><a href="#cb8-1975"></a><span class="ex">2024-10-16</span> 15:39:39.631613: I itex/core/devices/gpu/itex_gpu_runtime.cc:155] number of sub-devices is zero, expose root device.</span>
<span id="cb8-1976"><a href="#cb8-1976"></a><span class="ex">2024-10-16</span> 15:39:39.631615: I itex/core/devices/gpu/itex_gpu_runtime.cc:155] number of sub-devices is zero, expose root device.</span>
<span id="cb8-1977"><a href="#cb8-1977"></a><span class="ex">2024-10-16</span> 15:39:39.631617: I itex/core/devices/gpu/itex_gpu_runtime.cc:155] number of sub-devices is zero, expose root device.</span>
<span id="cb8-1978"><a href="#cb8-1978"></a><span class="ex">2024-10-16</span> 15:39:39.631619: I itex/core/devices/gpu/itex_gpu_runtime.cc:155] number of sub-devices is zero, expose root device.</span>
<span id="cb8-1979"><a href="#cb8-1979"></a><span class="op">&gt;</span> setting <span class="ex">tensorboard</span> ...</span>
<span id="cb8-1980"><a href="#cb8-1980"></a><span class="ex">WARNING:</span> WANDB writing requested but no legit wandb project or experiment name provided, therefore no WANDB logs will be written according to random generated project or experiment name.</span>
<span id="cb8-1981"><a href="#cb8-1981"></a><span class="op">&gt;</span>fused <span class="ex">kernel</span> is only supported in cuda, skip loading fused kernel</span>
<span id="cb8-1982"><a href="#cb8-1982"></a><span class="ex">[2024-10-16</span> 15:39:39,835] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb8-1983"><a href="#cb8-1983"></a><span class="ex">Loading</span> checkpoint shards: 100%<span class="kw">|</span><span class="ex">██████████</span><span class="kw">|</span> <span class="ex">7/7</span> [03:35<span class="op">&lt;</span>00:00, 30.78s/it]</span>
<span id="cb8-1984"><a href="#cb8-1984"></a><span class="ex">[2024-10-16</span> 15:43:13.654241]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:108</span><span class="pp">]</span> <span class="at">-</span> <span class="at">----------------------------hf</span> weight list----------------------------</span>
<span id="cb8-1985"><a href="#cb8-1985"></a><span class="ex">[2024-10-16</span> 15:43:13.705041]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.embed_tokens.weight</span>
<span id="cb8-1986"><a href="#cb8-1986"></a><span class="ex">[2024-10-16</span> 15:43:13.708051]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.0.self_attn.q_proj.weight</span>
<span id="cb8-1987"><a href="#cb8-1987"></a><span class="ex">[2024-10-16</span> 15:43:13.710201]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.0.self_attn.k_proj.weight</span>
<span id="cb8-1988"><a href="#cb8-1988"></a><span class="ex">[2024-10-16</span> 15:43:13.711855]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.0.self_attn.v_proj.weight</span>
<span id="cb8-1989"><a href="#cb8-1989"></a><span class="ex">[2024-10-16</span> 15:43:13.714676]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.0.self_attn.o_proj.weight</span>
<span id="cb8-1990"><a href="#cb8-1990"></a><span class="ex">[2024-10-16</span> 15:43:13.721608]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.0.mlp.gate_proj.weight</span>
<span id="cb8-1991"><a href="#cb8-1991"></a><span class="ex">[2024-10-16</span> 15:43:13.728527]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.0.mlp.up_proj.weight</span>
<span id="cb8-1992"><a href="#cb8-1992"></a><span class="ex">[2024-10-16</span> 15:43:13.735447]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.0.mlp.down_proj.weight</span>
<span id="cb8-1993"><a href="#cb8-1993"></a><span class="ex">[2024-10-16</span> 15:43:13.736224]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.0.input_layernorm.weight</span>
<span id="cb8-1994"><a href="#cb8-1994"></a><span class="ex">[2024-10-16</span> 15:43:13.736764]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.0.post_attention_layernorm.weight</span>
<span id="cb8-1995"><a href="#cb8-1995"></a><span class="ex">[2024-10-16</span> 15:43:13.739392]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.1.self_attn.q_proj.weight</span>
<span id="cb8-1996"><a href="#cb8-1996"></a><span class="ex">[2024-10-16</span> 15:43:13.741013]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.1.self_attn.k_proj.weight</span>
<span id="cb8-1997"><a href="#cb8-1997"></a><span class="ex">[2024-10-16</span> 15:43:13.742600]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.1.self_attn.v_proj.weight</span>
<span id="cb8-1998"><a href="#cb8-1998"></a><span class="ex">[2024-10-16</span> 15:43:13.745359]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.1.self_attn.o_proj.weight</span>
<span id="cb8-1999"><a href="#cb8-1999"></a><span class="ex">[2024-10-16</span> 15:43:13.752286]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.1.mlp.gate_proj.weight</span>
<span id="cb8-2000"><a href="#cb8-2000"></a><span class="ex">[2024-10-16</span> 15:43:13.759192]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.1.mlp.up_proj.weight</span>
<span id="cb8-2001"><a href="#cb8-2001"></a><span class="ex">[2024-10-16</span> 15:43:13.766054]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.1.mlp.down_proj.weight</span>
<span id="cb8-2002"><a href="#cb8-2002"></a><span class="ex">[2024-10-16</span> 15:43:13.766785]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.1.input_layernorm.weight</span>
<span id="cb8-2003"><a href="#cb8-2003"></a><span class="ex">[2024-10-16</span> 15:43:13.767321]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.1.post_attention_layernorm.weight</span>
<span id="cb8-2004"><a href="#cb8-2004"></a><span class="ex">[2024-10-16</span> 15:43:13.769938]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.2.self_attn.q_proj.weight</span>
<span id="cb8-2005"><a href="#cb8-2005"></a><span class="ex">[2024-10-16</span> 15:43:13.771536]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.2.self_attn.k_proj.weight</span>
<span id="cb8-2006"><a href="#cb8-2006"></a><span class="ex">[2024-10-16</span> 15:43:13.773107]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.2.self_attn.v_proj.weight</span>
<span id="cb8-2007"><a href="#cb8-2007"></a><span class="ex">[2024-10-16</span> 15:43:13.775861]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.2.self_attn.o_proj.weight</span>
<span id="cb8-2008"><a href="#cb8-2008"></a><span class="ex">[2024-10-16</span> 15:43:13.782733]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.2.mlp.gate_proj.weight</span>
<span id="cb8-2009"><a href="#cb8-2009"></a><span class="ex">[2024-10-16</span> 15:43:13.789559]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.2.mlp.up_proj.weight</span>
<span id="cb8-2010"><a href="#cb8-2010"></a><span class="ex">[2024-10-16</span> 15:43:13.796385]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.2.mlp.down_proj.weight</span>
<span id="cb8-2011"><a href="#cb8-2011"></a><span class="ex">[2024-10-16</span> 15:43:13.797080]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.2.input_layernorm.weight</span>
<span id="cb8-2012"><a href="#cb8-2012"></a><span class="ex">[2024-10-16</span> 15:43:13.797626]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.2.post_attention_layernorm.weight</span>
<span id="cb8-2013"><a href="#cb8-2013"></a><span class="ex">[2024-10-16</span> 15:43:13.800331]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.3.self_attn.q_proj.weight</span>
<span id="cb8-2014"><a href="#cb8-2014"></a><span class="ex">[2024-10-16</span> 15:43:13.801911]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.3.self_attn.k_proj.weight</span>
<span id="cb8-2015"><a href="#cb8-2015"></a><span class="ex">[2024-10-16</span> 15:43:13.803453]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.3.self_attn.v_proj.weight</span>
<span id="cb8-2016"><a href="#cb8-2016"></a><span class="ex">[2024-10-16</span> 15:43:13.806474]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.3.self_attn.o_proj.weight</span>
<span id="cb8-2017"><a href="#cb8-2017"></a><span class="ex">Loading</span> checkpoint shards: 100%<span class="kw">|</span><span class="ex">██████████</span><span class="kw">|</span> <span class="ex">7/7</span> [03:35<span class="op">&lt;</span>00:00, 30.84s/it]</span>
<span id="cb8-2018"><a href="#cb8-2018"></a><span class="ex">[2024-10-16</span> 15:43:13.813328]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.3.mlp.gate_proj.weight</span>
<span id="cb8-2019"><a href="#cb8-2019"></a><span class="ex">[2024-10-16</span> 15:43:13.820151]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.3.mlp.up_proj.weight</span>
<span id="cb8-2020"><a href="#cb8-2020"></a><span class="ex">[2024-10-16</span> 15:43:13.826971]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.3.mlp.down_proj.weight</span>
<span id="cb8-2021"><a href="#cb8-2021"></a><span class="ex">[2024-10-16</span> 15:43:13.827640]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.3.input_layernorm.weight</span>
<span id="cb8-2022"><a href="#cb8-2022"></a><span class="ex">[2024-10-16</span> 15:43:13.828185]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.3.post_attention_layernorm.weight</span>
<span id="cb8-2023"><a href="#cb8-2023"></a><span class="ex">[2024-10-16</span> 15:43:13.831062]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.4.self_attn.q_proj.weight</span>
<span id="cb8-2024"><a href="#cb8-2024"></a><span class="ex">[2024-10-16</span> 15:43:13.832590]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.4.self_attn.k_proj.weight</span>
<span id="cb8-2025"><a href="#cb8-2025"></a><span class="ex">[2024-10-16</span> 15:43:13.834171]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.4.self_attn.v_proj.weight</span>
<span id="cb8-2026"><a href="#cb8-2026"></a><span class="ex">[2024-10-16</span> 15:43:13.837147]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.4.self_attn.o_proj.weight</span>
<span id="cb8-2027"><a href="#cb8-2027"></a><span class="ex">[2024-10-16</span> 15:43:13.843970]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.4.mlp.gate_proj.weight</span>
<span id="cb8-2028"><a href="#cb8-2028"></a><span class="ex">[2024-10-16</span> 15:43:13.850793]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.4.mlp.up_proj.weight</span>
<span id="cb8-2029"><a href="#cb8-2029"></a><span class="ex">[2024-10-16</span> 15:43:13.857596]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.4.mlp.down_proj.weight</span>
<span id="cb8-2030"><a href="#cb8-2030"></a><span class="ex">[2024-10-16</span> 15:43:13.858284]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.4.input_layernorm.weight</span>
<span id="cb8-2031"><a href="#cb8-2031"></a><span class="ex">[2024-10-16</span> 15:43:13.858810]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.4.post_attention_layernorm.weight</span>
<span id="cb8-2032"><a href="#cb8-2032"></a><span class="ex">[2024-10-16</span> 15:43:13.861661]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.5.self_attn.q_proj.weight</span>
<span id="cb8-2033"><a href="#cb8-2033"></a><span class="ex">[2024-10-16</span> 15:43:13.863209]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.5.self_attn.k_proj.weight</span>
<span id="cb8-2034"><a href="#cb8-2034"></a><span class="ex">[2024-10-16</span> 15:43:13.864753]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.5.self_attn.v_proj.weight</span>
<span id="cb8-2035"><a href="#cb8-2035"></a><span class="ex">[2024-10-16</span> 15:43:13.867739]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.5.self_attn.o_proj.weight</span>
<span id="cb8-2036"><a href="#cb8-2036"></a><span class="ex">[2024-10-16</span> 15:43:13.874537]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.5.mlp.gate_proj.weight</span>
<span id="cb8-2037"><a href="#cb8-2037"></a><span class="ex">[2024-10-16</span> 15:43:13.881318]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.5.mlp.up_proj.weight</span>
<span id="cb8-2038"><a href="#cb8-2038"></a><span class="ex">[2024-10-16</span> 15:43:13.888097]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.5.mlp.down_proj.weight</span>
<span id="cb8-2039"><a href="#cb8-2039"></a><span class="ex">[2024-10-16</span> 15:43:13.888767]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.5.input_layernorm.weight</span>
<span id="cb8-2040"><a href="#cb8-2040"></a><span class="ex">[2024-10-16</span> 15:43:13.889295]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.5.post_attention_layernorm.weight</span>
<span id="cb8-2041"><a href="#cb8-2041"></a><span class="ex">[2024-10-16</span> 15:43:13.892151]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.6.self_attn.q_proj.weight</span>
<span id="cb8-2042"><a href="#cb8-2042"></a><span class="ex">[2024-10-16</span> 15:43:13.893641]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.6.self_attn.k_proj.weight</span>
<span id="cb8-2043"><a href="#cb8-2043"></a><span class="ex">[2024-10-16</span> 15:43:13.895211]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.6.self_attn.v_proj.weight</span>
<span id="cb8-2044"><a href="#cb8-2044"></a><span class="ex">[2024-10-16</span> 15:43:13.898154]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.6.self_attn.o_proj.weight</span>
<span id="cb8-2045"><a href="#cb8-2045"></a><span class="ex">[2024-10-16</span> 15:43:13.904976]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.6.mlp.gate_proj.weight</span>
<span id="cb8-2046"><a href="#cb8-2046"></a><span class="ex">[2024-10-16</span> 15:43:13.911767]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.6.mlp.up_proj.weight</span>
<span id="cb8-2047"><a href="#cb8-2047"></a><span class="ex">[2024-10-16</span> 15:43:13.918536]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.6.mlp.down_proj.weight</span>
<span id="cb8-2048"><a href="#cb8-2048"></a><span class="ex">[2024-10-16</span> 15:43:13.919201]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.6.input_layernorm.weight</span>
<span id="cb8-2049"><a href="#cb8-2049"></a><span class="ex">[2024-10-16</span> 15:43:13.919795]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.6.post_attention_layernorm.weight</span>
<span id="cb8-2050"><a href="#cb8-2050"></a><span class="ex">[2024-10-16</span> 15:43:13.922646]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.7.self_attn.q_proj.weight</span>
<span id="cb8-2051"><a href="#cb8-2051"></a><span class="ex">[2024-10-16</span> 15:43:13.924200]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.7.self_attn.k_proj.weight</span>
<span id="cb8-2052"><a href="#cb8-2052"></a><span class="ex">[2024-10-16</span> 15:43:13.925687]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.7.self_attn.v_proj.weight</span>
<span id="cb8-2053"><a href="#cb8-2053"></a><span class="ex">[2024-10-16</span> 15:43:13.928651]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.7.self_attn.o_proj.weight</span>
<span id="cb8-2054"><a href="#cb8-2054"></a><span class="ex">[2024-10-16</span> 15:43:13.935432]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.7.mlp.gate_proj.weight</span>
<span id="cb8-2055"><a href="#cb8-2055"></a><span class="ex">[2024-10-16</span> 15:43:13.942178]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.7.mlp.up_proj.weight</span>
<span id="cb8-2056"><a href="#cb8-2056"></a><span class="ex">[2024-10-16</span> 15:43:13.948916]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.7.mlp.down_proj.weight</span>
<span id="cb8-2057"><a href="#cb8-2057"></a><span class="ex">[2024-10-16</span> 15:43:13.949547]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.7.input_layernorm.weight</span>
<span id="cb8-2058"><a href="#cb8-2058"></a><span class="ex">[2024-10-16</span> 15:43:13.950059]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.7.post_attention_layernorm.weight</span>
<span id="cb8-2059"><a href="#cb8-2059"></a><span class="ex">[2024-10-16</span> 15:43:13.952902]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.8.self_attn.q_proj.weight</span>
<span id="cb8-2060"><a href="#cb8-2060"></a><span class="ex">[2024-10-16</span> 15:43:13.954461]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.8.self_attn.k_proj.weight</span>
<span id="cb8-2061"><a href="#cb8-2061"></a><span class="ex">[2024-10-16</span> 15:43:13.955985]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.8.self_attn.v_proj.weight</span>
<span id="cb8-2062"><a href="#cb8-2062"></a><span class="ex">[2024-10-16</span> 15:43:13.958931]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.8.self_attn.o_proj.weight</span>
<span id="cb8-2063"><a href="#cb8-2063"></a><span class="ex">[2024-10-16</span> 15:43:13.965709]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.8.mlp.gate_proj.weight</span>
<span id="cb8-2064"><a href="#cb8-2064"></a><span class="ex">[2024-10-16</span> 15:43:13.972481]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.8.mlp.up_proj.weight</span>
<span id="cb8-2065"><a href="#cb8-2065"></a><span class="ex">[2024-10-16</span> 15:43:13.979242]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.8.mlp.down_proj.weight</span>
<span id="cb8-2066"><a href="#cb8-2066"></a><span class="ex">[2024-10-16</span> 15:43:13.979876]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.8.input_layernorm.weight</span>
<span id="cb8-2067"><a href="#cb8-2067"></a><span class="ex">[2024-10-16</span> 15:43:13.980381]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.8.post_attention_layernorm.weight</span>
<span id="cb8-2068"><a href="#cb8-2068"></a><span class="ex">[2024-10-16</span> 15:43:13.983353]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.9.self_attn.q_proj.weight</span>
<span id="cb8-2069"><a href="#cb8-2069"></a><span class="ex">[2024-10-16</span> 15:43:13.984910]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.9.self_attn.k_proj.weight</span>
<span id="cb8-2070"><a href="#cb8-2070"></a><span class="ex">[2024-10-16</span> 15:43:13.986401]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.9.self_attn.v_proj.weight</span>
<span id="cb8-2071"><a href="#cb8-2071"></a><span class="ex">[2024-10-16</span> 15:43:13.989279]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.9.self_attn.o_proj.weight</span>
<span id="cb8-2072"><a href="#cb8-2072"></a><span class="ex">[2024-10-16</span> 15:43:13.996056]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.9.mlp.gate_proj.weight</span>
<span id="cb8-2073"><a href="#cb8-2073"></a><span class="ex">[2024-10-16</span> 15:43:14.002856]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.9.mlp.up_proj.weight</span>
<span id="cb8-2074"><a href="#cb8-2074"></a><span class="ex">[2024-10-16</span> 15:43:14.009601]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.9.mlp.down_proj.weight</span>
<span id="cb8-2075"><a href="#cb8-2075"></a><span class="ex">[2024-10-16</span> 15:43:14.010234]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.9.input_layernorm.weight</span>
<span id="cb8-2076"><a href="#cb8-2076"></a><span class="ex">[2024-10-16</span> 15:43:14.010742]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.9.post_attention_layernorm.weight</span>
<span id="cb8-2077"><a href="#cb8-2077"></a><span class="ex">[2024-10-16</span> 15:43:14.013552]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.10.self_attn.q_proj.weight</span>
<span id="cb8-2078"><a href="#cb8-2078"></a><span class="ex">[2024-10-16</span> 15:43:14.015117]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.10.self_attn.k_proj.weight</span>
<span id="cb8-2079"><a href="#cb8-2079"></a><span class="ex">[2024-10-16</span> 15:43:14.016607]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.10.self_attn.v_proj.weight</span>
<span id="cb8-2080"><a href="#cb8-2080"></a><span class="ex">[2024-10-16</span> 15:43:14.019542]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.10.self_attn.o_proj.weight</span>
<span id="cb8-2081"><a href="#cb8-2081"></a><span class="ex">[2024-10-16</span> 15:43:14.026297]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.10.mlp.gate_proj.weight</span>
<span id="cb8-2082"><a href="#cb8-2082"></a><span class="ex">[2024-10-16</span> 15:43:14.033038]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.10.mlp.up_proj.weight</span>
<span id="cb8-2083"><a href="#cb8-2083"></a><span class="ex">[2024-10-16</span> 15:43:14.039752]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.10.mlp.down_proj.weight</span>
<span id="cb8-2084"><a href="#cb8-2084"></a><span class="ex">[2024-10-16</span> 15:43:14.040455]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.10.input_layernorm.weight</span>
<span id="cb8-2085"><a href="#cb8-2085"></a><span class="ex">[2024-10-16</span> 15:43:14.040966]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.10.post_attention_layernorm.weight</span>
<span id="cb8-2086"><a href="#cb8-2086"></a><span class="ex">[2024-10-16</span> 15:43:14.043760]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.11.self_attn.q_proj.weight</span>
<span id="cb8-2087"><a href="#cb8-2087"></a><span class="ex">[2024-10-16</span> 15:43:14.045346]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.11.self_attn.k_proj.weight</span>
<span id="cb8-2088"><a href="#cb8-2088"></a><span class="ex">[2024-10-16</span> 15:43:14.046849]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.11.self_attn.v_proj.weight</span>
<span id="cb8-2089"><a href="#cb8-2089"></a><span class="ex">[2024-10-16</span> 15:43:14.049725]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.11.self_attn.o_proj.weight</span>
<span id="cb8-2090"><a href="#cb8-2090"></a><span class="ex">[2024-10-16</span> 15:43:14.056435]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.11.mlp.gate_proj.weight</span>
<span id="cb8-2091"><a href="#cb8-2091"></a><span class="ex">[2024-10-16</span> 15:43:14.063163]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.11.mlp.up_proj.weight</span>
<span id="cb8-2092"><a href="#cb8-2092"></a><span class="ex">[2024-10-16</span> 15:43:14.069898]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.11.mlp.down_proj.weight</span>
<span id="cb8-2093"><a href="#cb8-2093"></a><span class="ex">[2024-10-16</span> 15:43:14.070561]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.11.input_layernorm.weight</span>
<span id="cb8-2094"><a href="#cb8-2094"></a><span class="ex">[2024-10-16</span> 15:43:14.071084]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.11.post_attention_layernorm.weight</span>
<span id="cb8-2095"><a href="#cb8-2095"></a><span class="ex">[2024-10-16</span> 15:43:14.073869]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.12.self_attn.q_proj.weight</span>
<span id="cb8-2096"><a href="#cb8-2096"></a><span class="ex">[2024-10-16</span> 15:43:14.075363]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.12.self_attn.k_proj.weight</span>
<span id="cb8-2097"><a href="#cb8-2097"></a><span class="ex">[2024-10-16</span> 15:43:14.076918]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.12.self_attn.v_proj.weight</span>
<span id="cb8-2098"><a href="#cb8-2098"></a><span class="ex">[2024-10-16</span> 15:43:14.079777]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.12.self_attn.o_proj.weight</span>
<span id="cb8-2099"><a href="#cb8-2099"></a><span class="ex">[2024-10-16</span> 15:43:14.086497]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.12.mlp.gate_proj.weight</span>
<span id="cb8-2100"><a href="#cb8-2100"></a><span class="ex">[2024-10-16</span> 15:43:14.093183]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.12.mlp.up_proj.weight</span>
<span id="cb8-2101"><a href="#cb8-2101"></a><span class="ex">[2024-10-16</span> 15:43:14.099869]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.12.mlp.down_proj.weight</span>
<span id="cb8-2102"><a href="#cb8-2102"></a><span class="ex">[2024-10-16</span> 15:43:14.100523]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.12.input_layernorm.weight</span>
<span id="cb8-2103"><a href="#cb8-2103"></a><span class="ex">[2024-10-16</span> 15:43:14.101038]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.12.post_attention_layernorm.weight</span>
<span id="cb8-2104"><a href="#cb8-2104"></a><span class="ex">[2024-10-16</span> 15:43:14.103823]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.13.self_attn.q_proj.weight</span>
<span id="cb8-2105"><a href="#cb8-2105"></a><span class="ex">[2024-10-16</span> 15:43:14.105335]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.13.self_attn.k_proj.weight</span>
<span id="cb8-2106"><a href="#cb8-2106"></a><span class="ex">[2024-10-16</span> 15:43:14.106828]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.13.self_attn.v_proj.weight</span>
<span id="cb8-2107"><a href="#cb8-2107"></a><span class="ex">[2024-10-16</span> 15:43:14.109698]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.13.self_attn.o_proj.weight</span>
<span id="cb8-2108"><a href="#cb8-2108"></a><span class="ex">[2024-10-16</span> 15:43:14.116395]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.13.mlp.gate_proj.weight</span>
<span id="cb8-2109"><a href="#cb8-2109"></a><span class="ex">[2024-10-16</span> 15:43:14.123086]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.13.mlp.up_proj.weight</span>
<span id="cb8-2110"><a href="#cb8-2110"></a><span class="ex">[2024-10-16</span> 15:43:14.129807]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.13.mlp.down_proj.weight</span>
<span id="cb8-2111"><a href="#cb8-2111"></a><span class="ex">[2024-10-16</span> 15:43:14.130474]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.13.input_layernorm.weight</span>
<span id="cb8-2112"><a href="#cb8-2112"></a><span class="ex">[2024-10-16</span> 15:43:14.130997]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.13.post_attention_layernorm.weight</span>
<span id="cb8-2113"><a href="#cb8-2113"></a><span class="ex">[2024-10-16</span> 15:43:14.133762]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.14.self_attn.q_proj.weight</span>
<span id="cb8-2114"><a href="#cb8-2114"></a><span class="ex">[2024-10-16</span> 15:43:14.135290]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.14.self_attn.k_proj.weight</span>
<span id="cb8-2115"><a href="#cb8-2115"></a><span class="ex">[2024-10-16</span> 15:43:14.136791]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.14.self_attn.v_proj.weight</span>
<span id="cb8-2116"><a href="#cb8-2116"></a><span class="ex">[2024-10-16</span> 15:43:14.139860]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.14.self_attn.o_proj.weight</span>
<span id="cb8-2117"><a href="#cb8-2117"></a><span class="ex">[2024-10-16</span> 15:43:14.146560]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.14.mlp.gate_proj.weight</span>
<span id="cb8-2118"><a href="#cb8-2118"></a><span class="ex">[2024-10-16</span> 15:43:14.153229]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.14.mlp.up_proj.weight</span>
<span id="cb8-2119"><a href="#cb8-2119"></a><span class="ex">[2024-10-16</span> 15:43:14.160012]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.14.mlp.down_proj.weight</span>
<span id="cb8-2120"><a href="#cb8-2120"></a><span class="ex">[2024-10-16</span> 15:43:14.160681]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.14.input_layernorm.weight</span>
<span id="cb8-2121"><a href="#cb8-2121"></a><span class="ex">[2024-10-16</span> 15:43:14.161212]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.14.post_attention_layernorm.weight</span>
<span id="cb8-2122"><a href="#cb8-2122"></a><span class="ex">[2024-10-16</span> 15:43:14.164011]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.15.self_attn.q_proj.weight</span>
<span id="cb8-2123"><a href="#cb8-2123"></a><span class="ex">[2024-10-16</span> 15:43:14.165550]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.15.self_attn.k_proj.weight</span>
<span id="cb8-2124"><a href="#cb8-2124"></a><span class="ex">[2024-10-16</span> 15:43:14.167029]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.15.self_attn.v_proj.weight</span>
<span id="cb8-2125"><a href="#cb8-2125"></a><span class="ex">[2024-10-16</span> 15:43:14.169860]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.15.self_attn.o_proj.weight</span>
<span id="cb8-2126"><a href="#cb8-2126"></a><span class="ex">[2024-10-16</span> 15:43:14.176522]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.15.mlp.gate_proj.weight</span>
<span id="cb8-2127"><a href="#cb8-2127"></a><span class="ex">[2024-10-16</span> 15:43:14.183206]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.15.mlp.up_proj.weight</span>
<span id="cb8-2128"><a href="#cb8-2128"></a><span class="ex">[2024-10-16</span> 15:43:14.189866]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.15.mlp.down_proj.weight</span>
<span id="cb8-2129"><a href="#cb8-2129"></a><span class="ex">[2024-10-16</span> 15:43:14.190530]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.15.input_layernorm.weight</span>
<span id="cb8-2130"><a href="#cb8-2130"></a><span class="ex">[2024-10-16</span> 15:43:14.191065]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.15.post_attention_layernorm.weight</span>
<span id="cb8-2131"><a href="#cb8-2131"></a><span class="ex">[2024-10-16</span> 15:43:14.193838]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.16.self_attn.q_proj.weight</span>
<span id="cb8-2132"><a href="#cb8-2132"></a><span class="ex">[2024-10-16</span> 15:43:14.195331]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.16.self_attn.k_proj.weight</span>
<span id="cb8-2133"><a href="#cb8-2133"></a><span class="ex">[2024-10-16</span> 15:43:14.196892]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.16.self_attn.v_proj.weight</span>
<span id="cb8-2134"><a href="#cb8-2134"></a><span class="ex">[2024-10-16</span> 15:43:14.199748]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.16.self_attn.o_proj.weight</span>
<span id="cb8-2135"><a href="#cb8-2135"></a><span class="ex">[2024-10-16</span> 15:43:14.206446]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.16.mlp.gate_proj.weight</span>
<span id="cb8-2136"><a href="#cb8-2136"></a><span class="ex">[2024-10-16</span> 15:43:14.213109]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.16.mlp.up_proj.weight</span>
<span id="cb8-2137"><a href="#cb8-2137"></a><span class="ex">[2024-10-16</span> 15:43:14.219783]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.16.mlp.down_proj.weight</span>
<span id="cb8-2138"><a href="#cb8-2138"></a><span class="ex">[2024-10-16</span> 15:43:14.220402]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.16.input_layernorm.weight</span>
<span id="cb8-2139"><a href="#cb8-2139"></a><span class="ex">[2024-10-16</span> 15:43:14.220932]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.16.post_attention_layernorm.weight</span>
<span id="cb8-2140"><a href="#cb8-2140"></a><span class="ex">[2024-10-16</span> 15:43:14.223676]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.17.self_attn.q_proj.weight</span>
<span id="cb8-2141"><a href="#cb8-2141"></a><span class="ex">[2024-10-16</span> 15:43:14.225232]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.17.self_attn.k_proj.weight</span>
<span id="cb8-2142"><a href="#cb8-2142"></a><span class="ex">[2024-10-16</span> 15:43:14.226737]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.17.self_attn.v_proj.weight</span>
<span id="cb8-2143"><a href="#cb8-2143"></a><span class="ex">[2024-10-16</span> 15:43:14.229543]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.17.self_attn.o_proj.weight</span>
<span id="cb8-2144"><a href="#cb8-2144"></a><span class="ex">[2024-10-16</span> 15:43:14.236240]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.17.mlp.gate_proj.weight</span>
<span id="cb8-2145"><a href="#cb8-2145"></a><span class="ex">[2024-10-16</span> 15:43:14.242898]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.17.mlp.up_proj.weight</span>
<span id="cb8-2146"><a href="#cb8-2146"></a><span class="ex">[2024-10-16</span> 15:43:14.249590]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.17.mlp.down_proj.weight</span>
<span id="cb8-2147"><a href="#cb8-2147"></a><span class="ex">[2024-10-16</span> 15:43:14.250235]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.17.input_layernorm.weight</span>
<span id="cb8-2148"><a href="#cb8-2148"></a><span class="ex">[2024-10-16</span> 15:43:14.250747]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.17.post_attention_layernorm.weight</span>
<span id="cb8-2149"><a href="#cb8-2149"></a><span class="ex">[2024-10-16</span> 15:43:14.253465]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.18.self_attn.q_proj.weight</span>
<span id="cb8-2150"><a href="#cb8-2150"></a><span class="ex">[2024-10-16</span> 15:43:14.255062]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.18.self_attn.k_proj.weight</span>
<span id="cb8-2151"><a href="#cb8-2151"></a><span class="ex">[2024-10-16</span> 15:43:14.256546]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.18.self_attn.v_proj.weight</span>
<span id="cb8-2152"><a href="#cb8-2152"></a><span class="ex">[2024-10-16</span> 15:43:14.259362]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.18.self_attn.o_proj.weight</span>
<span id="cb8-2153"><a href="#cb8-2153"></a><span class="ex">[2024-10-16</span> 15:43:14.266006]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.18.mlp.gate_proj.weight</span>
<span id="cb8-2154"><a href="#cb8-2154"></a><span class="ex">[2024-10-16</span> 15:43:14.272677]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.18.mlp.up_proj.weight</span>
<span id="cb8-2155"><a href="#cb8-2155"></a><span class="ex">[2024-10-16</span> 15:43:14.279406]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.18.mlp.down_proj.weight</span>
<span id="cb8-2156"><a href="#cb8-2156"></a><span class="ex">[2024-10-16</span> 15:43:14.280055]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.18.input_layernorm.weight</span>
<span id="cb8-2157"><a href="#cb8-2157"></a><span class="ex">[2024-10-16</span> 15:43:14.280566]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.18.post_attention_layernorm.weight</span>
<span id="cb8-2158"><a href="#cb8-2158"></a><span class="ex">[2024-10-16</span> 15:43:14.283295]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.19.self_attn.q_proj.weight</span>
<span id="cb8-2159"><a href="#cb8-2159"></a><span class="ex">[2024-10-16</span> 15:43:14.284803]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.19.self_attn.k_proj.weight</span>
<span id="cb8-2160"><a href="#cb8-2160"></a><span class="ex">[2024-10-16</span> 15:43:14.286350]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.19.self_attn.v_proj.weight</span>
<span id="cb8-2161"><a href="#cb8-2161"></a><span class="ex">[2024-10-16</span> 15:43:14.289142]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.19.self_attn.o_proj.weight</span>
<span id="cb8-2162"><a href="#cb8-2162"></a><span class="ex">[2024-10-16</span> 15:43:14.295818]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.19.mlp.gate_proj.weight</span>
<span id="cb8-2163"><a href="#cb8-2163"></a><span class="ex">[2024-10-16</span> 15:43:14.302488]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.19.mlp.up_proj.weight</span>
<span id="cb8-2164"><a href="#cb8-2164"></a><span class="ex">[2024-10-16</span> 15:43:14.309098]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.19.mlp.down_proj.weight</span>
<span id="cb8-2165"><a href="#cb8-2165"></a><span class="ex">[2024-10-16</span> 15:43:14.309731]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.19.input_layernorm.weight</span>
<span id="cb8-2166"><a href="#cb8-2166"></a><span class="ex">[2024-10-16</span> 15:43:14.310234]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.19.post_attention_layernorm.weight</span>
<span id="cb8-2167"><a href="#cb8-2167"></a><span class="ex">[2024-10-16</span> 15:43:14.312927]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.20.self_attn.q_proj.weight</span>
<span id="cb8-2168"><a href="#cb8-2168"></a><span class="ex">[2024-10-16</span> 15:43:14.314505]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.20.self_attn.k_proj.weight</span>
<span id="cb8-2169"><a href="#cb8-2169"></a><span class="ex">[2024-10-16</span> 15:43:14.315992]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.20.self_attn.v_proj.weight</span>
<span id="cb8-2170"><a href="#cb8-2170"></a><span class="ex">[2024-10-16</span> 15:43:14.318788]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.20.self_attn.o_proj.weight</span>
<span id="cb8-2171"><a href="#cb8-2171"></a><span class="ex">[2024-10-16</span> 15:43:14.325390]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.20.mlp.gate_proj.weight</span>
<span id="cb8-2172"><a href="#cb8-2172"></a><span class="ex">[2024-10-16</span> 15:43:14.332020]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.20.mlp.up_proj.weight</span>
<span id="cb8-2173"><a href="#cb8-2173"></a><span class="ex">[2024-10-16</span> 15:43:14.338682]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.20.mlp.down_proj.weight</span>
<span id="cb8-2174"><a href="#cb8-2174"></a><span class="ex">[2024-10-16</span> 15:43:14.339334]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.20.input_layernorm.weight</span>
<span id="cb8-2175"><a href="#cb8-2175"></a><span class="ex">[2024-10-16</span> 15:43:14.339845]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.20.post_attention_layernorm.weight</span>
<span id="cb8-2176"><a href="#cb8-2176"></a><span class="ex">[2024-10-16</span> 15:43:14.342562]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.21.self_attn.q_proj.weight</span>
<span id="cb8-2177"><a href="#cb8-2177"></a><span class="ex">[2024-10-16</span> 15:43:14.344113]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.21.self_attn.k_proj.weight</span>
<span id="cb8-2178"><a href="#cb8-2178"></a><span class="ex">[2024-10-16</span> 15:43:14.345593]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.21.self_attn.v_proj.weight</span>
<span id="cb8-2179"><a href="#cb8-2179"></a><span class="ex">[2024-10-16</span> 15:43:14.348370]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.21.self_attn.o_proj.weight</span>
<span id="cb8-2180"><a href="#cb8-2180"></a><span class="ex">[2024-10-16</span> 15:43:14.355167]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.21.mlp.gate_proj.weight</span>
<span id="cb8-2181"><a href="#cb8-2181"></a><span class="ex">[2024-10-16</span> 15:43:14.361823]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.21.mlp.up_proj.weight</span>
<span id="cb8-2182"><a href="#cb8-2182"></a><span class="ex">[2024-10-16</span> 15:43:14.368428]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.21.mlp.down_proj.weight</span>
<span id="cb8-2183"><a href="#cb8-2183"></a><span class="ex">[2024-10-16</span> 15:43:14.369055]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.21.input_layernorm.weight</span>
<span id="cb8-2184"><a href="#cb8-2184"></a><span class="ex">[2024-10-16</span> 15:43:14.369558]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.21.post_attention_layernorm.weight</span>
<span id="cb8-2185"><a href="#cb8-2185"></a><span class="ex">[2024-10-16</span> 15:43:14.372269]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.22.self_attn.q_proj.weight</span>
<span id="cb8-2186"><a href="#cb8-2186"></a><span class="ex">[2024-10-16</span> 15:43:14.373830]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.22.self_attn.k_proj.weight</span>
<span id="cb8-2187"><a href="#cb8-2187"></a><span class="ex">[2024-10-16</span> 15:43:14.375316]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.22.self_attn.v_proj.weight</span>
<span id="cb8-2188"><a href="#cb8-2188"></a><span class="ex">[2024-10-16</span> 15:43:14.378084]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.22.self_attn.o_proj.weight</span>
<span id="cb8-2189"><a href="#cb8-2189"></a><span class="ex">[2024-10-16</span> 15:43:14.384700]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.22.mlp.gate_proj.weight</span>
<span id="cb8-2190"><a href="#cb8-2190"></a><span class="ex">[2024-10-16</span> 15:43:14.391366]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.22.mlp.up_proj.weight</span>
<span id="cb8-2191"><a href="#cb8-2191"></a><span class="ex">[2024-10-16</span> 15:43:14.398053]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.22.mlp.down_proj.weight</span>
<span id="cb8-2192"><a href="#cb8-2192"></a><span class="ex">[2024-10-16</span> 15:43:14.398695]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.22.input_layernorm.weight</span>
<span id="cb8-2193"><a href="#cb8-2193"></a><span class="ex">[2024-10-16</span> 15:43:14.399206]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.22.post_attention_layernorm.weight</span>
<span id="cb8-2194"><a href="#cb8-2194"></a><span class="ex">[2024-10-16</span> 15:43:14.401929]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.23.self_attn.q_proj.weight</span>
<span id="cb8-2195"><a href="#cb8-2195"></a><span class="ex">[2024-10-16</span> 15:43:14.403487]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.23.self_attn.k_proj.weight</span>
<span id="cb8-2196"><a href="#cb8-2196"></a><span class="ex">[2024-10-16</span> 15:43:14.404961]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.23.self_attn.v_proj.weight</span>
<span id="cb8-2197"><a href="#cb8-2197"></a><span class="ex">[2024-10-16</span> 15:43:14.407720]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.23.self_attn.o_proj.weight</span>
<span id="cb8-2198"><a href="#cb8-2198"></a><span class="ex">[2024-10-16</span> 15:43:14.414356]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.23.mlp.gate_proj.weight</span>
<span id="cb8-2199"><a href="#cb8-2199"></a><span class="ex">[2024-10-16</span> 15:43:14.421000]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.23.mlp.up_proj.weight</span>
<span id="cb8-2200"><a href="#cb8-2200"></a><span class="ex">[2024-10-16</span> 15:43:14.427610]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.23.mlp.down_proj.weight</span>
<span id="cb8-2201"><a href="#cb8-2201"></a><span class="ex">[2024-10-16</span> 15:43:14.428269]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.23.input_layernorm.weight</span>
<span id="cb8-2202"><a href="#cb8-2202"></a><span class="ex">[2024-10-16</span> 15:43:14.428783]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.23.post_attention_layernorm.weight</span>
<span id="cb8-2203"><a href="#cb8-2203"></a><span class="ex">[2024-10-16</span> 15:43:14.431461]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.24.self_attn.q_proj.weight</span>
<span id="cb8-2204"><a href="#cb8-2204"></a><span class="ex">[2024-10-16</span> 15:43:14.432995]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.24.self_attn.k_proj.weight</span>
<span id="cb8-2205"><a href="#cb8-2205"></a><span class="ex">[2024-10-16</span> 15:43:14.434507]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.24.self_attn.v_proj.weight</span>
<span id="cb8-2206"><a href="#cb8-2206"></a><span class="ex">[2024-10-16</span> 15:43:14.437268]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.24.self_attn.o_proj.weight</span>
<span id="cb8-2207"><a href="#cb8-2207"></a><span class="ex">[2024-10-16</span> 15:43:14.443850]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.24.mlp.gate_proj.weight</span>
<span id="cb8-2208"><a href="#cb8-2208"></a><span class="ex">[2024-10-16</span> 15:43:14.450632]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.24.mlp.up_proj.weight</span>
<span id="cb8-2209"><a href="#cb8-2209"></a><span class="ex">[2024-10-16</span> 15:43:14.457242]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.24.mlp.down_proj.weight</span>
<span id="cb8-2210"><a href="#cb8-2210"></a><span class="ex">[2024-10-16</span> 15:43:14.457890]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.24.input_layernorm.weight</span>
<span id="cb8-2211"><a href="#cb8-2211"></a><span class="ex">[2024-10-16</span> 15:43:14.458409]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.24.post_attention_layernorm.weight</span>
<span id="cb8-2212"><a href="#cb8-2212"></a><span class="ex">[2024-10-16</span> 15:43:14.461063]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.25.self_attn.q_proj.weight</span>
<span id="cb8-2213"><a href="#cb8-2213"></a><span class="ex">[2024-10-16</span> 15:43:14.462620]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.25.self_attn.k_proj.weight</span>
<span id="cb8-2214"><a href="#cb8-2214"></a><span class="ex">[2024-10-16</span> 15:43:14.464102]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.25.self_attn.v_proj.weight</span>
<span id="cb8-2215"><a href="#cb8-2215"></a><span class="ex">[2024-10-16</span> 15:43:14.466871]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.25.self_attn.o_proj.weight</span>
<span id="cb8-2216"><a href="#cb8-2216"></a><span class="ex">[2024-10-16</span> 15:43:14.473435]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.25.mlp.gate_proj.weight</span>
<span id="cb8-2217"><a href="#cb8-2217"></a><span class="ex">[2024-10-16</span> 15:43:14.480017]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.25.mlp.up_proj.weight</span>
<span id="cb8-2218"><a href="#cb8-2218"></a><span class="ex">[2024-10-16</span> 15:43:14.486605]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.25.mlp.down_proj.weight</span>
<span id="cb8-2219"><a href="#cb8-2219"></a><span class="ex">[2024-10-16</span> 15:43:14.487227]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.25.input_layernorm.weight</span>
<span id="cb8-2220"><a href="#cb8-2220"></a><span class="ex">[2024-10-16</span> 15:43:14.487743]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.25.post_attention_layernorm.weight</span>
<span id="cb8-2221"><a href="#cb8-2221"></a><span class="ex">[2024-10-16</span> 15:43:14.490427]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.26.self_attn.q_proj.weight</span>
<span id="cb8-2222"><a href="#cb8-2222"></a><span class="ex">[2024-10-16</span> 15:43:14.491946]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.26.self_attn.k_proj.weight</span>
<span id="cb8-2223"><a href="#cb8-2223"></a><span class="ex">[2024-10-16</span> 15:43:14.493433]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.26.self_attn.v_proj.weight</span>
<span id="cb8-2224"><a href="#cb8-2224"></a><span class="ex">[2024-10-16</span> 15:43:14.496192]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.26.self_attn.o_proj.weight</span>
<span id="cb8-2225"><a href="#cb8-2225"></a><span class="ex">[2024-10-16</span> 15:43:14.502792]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.26.mlp.gate_proj.weight</span>
<span id="cb8-2226"><a href="#cb8-2226"></a><span class="ex">[2024-10-16</span> 15:43:14.509329]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.26.mlp.up_proj.weight</span>
<span id="cb8-2227"><a href="#cb8-2227"></a><span class="ex">[2024-10-16</span> 15:43:14.515980]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.26.mlp.down_proj.weight</span>
<span id="cb8-2228"><a href="#cb8-2228"></a><span class="ex">[2024-10-16</span> 15:43:14.516659]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.26.input_layernorm.weight</span>
<span id="cb8-2229"><a href="#cb8-2229"></a><span class="ex">[2024-10-16</span> 15:43:14.517200]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.26.post_attention_layernorm.weight</span>
<span id="cb8-2230"><a href="#cb8-2230"></a><span class="ex">[2024-10-16</span> 15:43:14.519874]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.27.self_attn.q_proj.weight</span>
<span id="cb8-2231"><a href="#cb8-2231"></a><span class="ex">[2024-10-16</span> 15:43:14.521415]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.27.self_attn.k_proj.weight</span>
<span id="cb8-2232"><a href="#cb8-2232"></a><span class="ex">[2024-10-16</span> 15:43:14.522879]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.27.self_attn.v_proj.weight</span>
<span id="cb8-2233"><a href="#cb8-2233"></a><span class="ex">[2024-10-16</span> 15:43:14.525620]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.27.self_attn.o_proj.weight</span>
<span id="cb8-2234"><a href="#cb8-2234"></a><span class="ex">[2024-10-16</span> 15:43:14.532202]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.27.mlp.gate_proj.weight</span>
<span id="cb8-2235"><a href="#cb8-2235"></a><span class="ex">[2024-10-16</span> 15:43:14.538768]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.27.mlp.up_proj.weight</span>
<span id="cb8-2236"><a href="#cb8-2236"></a><span class="ex">[2024-10-16</span> 15:43:14.545303]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.27.mlp.down_proj.weight</span>
<span id="cb8-2237"><a href="#cb8-2237"></a><span class="ex">[2024-10-16</span> 15:43:14.545921]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.27.input_layernorm.weight</span>
<span id="cb8-2238"><a href="#cb8-2238"></a><span class="ex">[2024-10-16</span> 15:43:14.546440]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.27.post_attention_layernorm.weight</span>
<span id="cb8-2239"><a href="#cb8-2239"></a><span class="ex">[2024-10-16</span> 15:43:14.549101]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.28.self_attn.q_proj.weight</span>
<span id="cb8-2240"><a href="#cb8-2240"></a><span class="ex">[2024-10-16</span> 15:43:14.550596]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.28.self_attn.k_proj.weight</span>
<span id="cb8-2241"><a href="#cb8-2241"></a><span class="ex">[2024-10-16</span> 15:43:14.552114]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.28.self_attn.v_proj.weight</span>
<span id="cb8-2242"><a href="#cb8-2242"></a><span class="ex">[2024-10-16</span> 15:43:14.554821]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.28.self_attn.o_proj.weight</span>
<span id="cb8-2243"><a href="#cb8-2243"></a><span class="ex">[2024-10-16</span> 15:43:14.561373]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.28.mlp.gate_proj.weight</span>
<span id="cb8-2244"><a href="#cb8-2244"></a><span class="ex">[2024-10-16</span> 15:43:14.567945]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.28.mlp.up_proj.weight</span>
<span id="cb8-2245"><a href="#cb8-2245"></a><span class="ex">[2024-10-16</span> 15:43:14.574713]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.28.mlp.down_proj.weight</span>
<span id="cb8-2246"><a href="#cb8-2246"></a><span class="ex">[2024-10-16</span> 15:43:14.575333]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.28.input_layernorm.weight</span>
<span id="cb8-2247"><a href="#cb8-2247"></a><span class="ex">[2024-10-16</span> 15:43:14.575833]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.28.post_attention_layernorm.weight</span>
<span id="cb8-2248"><a href="#cb8-2248"></a><span class="ex">[2024-10-16</span> 15:43:14.578469]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.29.self_attn.q_proj.weight</span>
<span id="cb8-2249"><a href="#cb8-2249"></a><span class="ex">[2024-10-16</span> 15:43:14.580020]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.29.self_attn.k_proj.weight</span>
<span id="cb8-2250"><a href="#cb8-2250"></a><span class="ex">[2024-10-16</span> 15:43:14.581498]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.29.self_attn.v_proj.weight</span>
<span id="cb8-2251"><a href="#cb8-2251"></a><span class="ex">[2024-10-16</span> 15:43:14.584217]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.29.self_attn.o_proj.weight</span>
<span id="cb8-2252"><a href="#cb8-2252"></a><span class="ex">[2024-10-16</span> 15:43:14.590850]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.29.mlp.gate_proj.weight</span>
<span id="cb8-2253"><a href="#cb8-2253"></a><span class="ex">[2024-10-16</span> 15:43:14.597375]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.29.mlp.up_proj.weight</span>
<span id="cb8-2254"><a href="#cb8-2254"></a><span class="ex">[2024-10-16</span> 15:43:14.603899]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.29.mlp.down_proj.weight</span>
<span id="cb8-2255"><a href="#cb8-2255"></a><span class="ex">[2024-10-16</span> 15:43:14.604548]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.29.input_layernorm.weight</span>
<span id="cb8-2256"><a href="#cb8-2256"></a><span class="ex">[2024-10-16</span> 15:43:14.605067]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.29.post_attention_layernorm.weight</span>
<span id="cb8-2257"><a href="#cb8-2257"></a><span class="ex">[2024-10-16</span> 15:43:14.607694]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.30.self_attn.q_proj.weight</span>
<span id="cb8-2258"><a href="#cb8-2258"></a><span class="ex">[2024-10-16</span> 15:43:14.609232]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.30.self_attn.k_proj.weight</span>
<span id="cb8-2259"><a href="#cb8-2259"></a><span class="ex">[2024-10-16</span> 15:43:14.610733]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.30.self_attn.v_proj.weight</span>
<span id="cb8-2260"><a href="#cb8-2260"></a><span class="ex">[2024-10-16</span> 15:43:14.613448]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.30.self_attn.o_proj.weight</span>
<span id="cb8-2261"><a href="#cb8-2261"></a><span class="ex">[2024-10-16</span> 15:43:14.619991]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.30.mlp.gate_proj.weight</span>
<span id="cb8-2262"><a href="#cb8-2262"></a><span class="ex">[2024-10-16</span> 15:43:14.626559]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.30.mlp.up_proj.weight</span>
<span id="cb8-2263"><a href="#cb8-2263"></a><span class="ex">[2024-10-16</span> 15:43:14.633070]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.30.mlp.down_proj.weight</span>
<span id="cb8-2264"><a href="#cb8-2264"></a><span class="ex">[2024-10-16</span> 15:43:14.633733]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.30.input_layernorm.weight</span>
<span id="cb8-2265"><a href="#cb8-2265"></a><span class="ex">[2024-10-16</span> 15:43:14.634259]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.30.post_attention_layernorm.weight</span>
<span id="cb8-2266"><a href="#cb8-2266"></a><span class="ex">[2024-10-16</span> 15:43:14.636877]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.31.self_attn.q_proj.weight</span>
<span id="cb8-2267"><a href="#cb8-2267"></a><span class="ex">[2024-10-16</span> 15:43:14.638487]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.31.self_attn.k_proj.weight</span>
<span id="cb8-2268"><a href="#cb8-2268"></a><span class="ex">[2024-10-16</span> 15:43:14.639954]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.31.self_attn.v_proj.weight</span>
<span id="cb8-2269"><a href="#cb8-2269"></a><span class="ex">[2024-10-16</span> 15:43:14.642672]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.31.self_attn.o_proj.weight</span>
<span id="cb8-2270"><a href="#cb8-2270"></a><span class="ex">[2024-10-16</span> 15:43:14.649190]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.31.mlp.gate_proj.weight</span>
<span id="cb8-2271"><a href="#cb8-2271"></a><span class="ex">[2024-10-16</span> 15:43:14.655715]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.31.mlp.up_proj.weight</span>
<span id="cb8-2272"><a href="#cb8-2272"></a><span class="ex">[2024-10-16</span> 15:43:14.662234]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.31.mlp.down_proj.weight</span>
<span id="cb8-2273"><a href="#cb8-2273"></a><span class="ex">[2024-10-16</span> 15:43:14.662858]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.31.input_layernorm.weight</span>
<span id="cb8-2274"><a href="#cb8-2274"></a><span class="ex">[2024-10-16</span> 15:43:14.663389]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.layers.31.post_attention_layernorm.weight</span>
<span id="cb8-2275"><a href="#cb8-2275"></a><span class="ex">[2024-10-16</span> 15:43:14.663915]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> model.norm.weight</span>
<span id="cb8-2276"><a href="#cb8-2276"></a><span class="ex">[2024-10-16</span> 15:43:14,693] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb8-2277"><a href="#cb8-2277"></a><span class="ex">[2024-10-16</span> 15:43:14.713565]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:113</span><span class="pp">]</span> <span class="at">-</span> lm_head.weight</span>
<span id="cb8-2278"><a href="#cb8-2278"></a><span class="ex">[2024-10-16</span> 15:43:14.714574]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:504</span><span class="pp">]</span> <span class="at">-</span> before deepspeed init</span>
<span id="cb8-2279"><a href="#cb8-2279"></a><span class="ex">[2024-10-16</span> 15:43:14,715] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">logging.py:129:log_dist</span><span class="pp">]</span> [Rank 0] DeepSpeed info: version=0.15.3+unknown, git-hash=unknown, git-branch=unknown</span>
<span id="cb8-2280"><a href="#cb8-2280"></a><span class="ex">[2024-10-16</span> 15:43:14,715] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb8-2281"><a href="#cb8-2281"></a><span class="ex">Loading</span> checkpoint shards: 100%<span class="kw">|</span><span class="ex">██████████</span><span class="kw">|</span> <span class="ex">7/7</span> [03:57<span class="op">&lt;</span>00:00, 33.97s/it]</span>
<span id="cb8-2282"><a href="#cb8-2282"></a><span class="ex">[2024-10-16</span> 15:43:36,676] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb8-2283"><a href="#cb8-2283"></a><span class="ex">Loading</span> checkpoint shards: 100%<span class="kw">|</span><span class="ex">██████████</span><span class="kw">|</span> <span class="ex">7/7</span> [04:00<span class="op">&lt;</span>00:00, 34.36s/it]</span>
<span id="cb8-2284"><a href="#cb8-2284"></a><span class="ex">Loading</span> checkpoint shards: 100%<span class="kw">|</span><span class="ex">██████████</span><span class="kw">|</span> <span class="ex">7/7</span> [04:00<span class="op">&lt;</span>00:00, 34.38s/it]</span>
<span id="cb8-2285"><a href="#cb8-2285"></a><span class="ex">Loading</span> checkpoint shards: 100%<span class="kw">|</span><span class="ex">██████████</span><span class="kw">|</span> <span class="ex">7/7</span> [04:00<span class="op">&lt;</span>00:00, 34.40s/it]</span>
<span id="cb8-2286"><a href="#cb8-2286"></a><span class="ex">Loading</span> checkpoint shards: 100%<span class="kw">|</span><span class="ex">██████████</span><span class="kw">|</span> <span class="ex">7/7</span> [04:00<span class="op">&lt;</span>00:00, 34.39s/it]</span>
<span id="cb8-2287"><a href="#cb8-2287"></a><span class="ex">Loading</span> checkpoint shards: 100%<span class="kw">|</span><span class="ex">██████████</span><span class="kw">|</span> <span class="ex">7/7</span> [04:00<span class="op">&lt;</span>00:00, 34.41s/it]</span>
<span id="cb8-2288"><a href="#cb8-2288"></a><span class="ex">Loading</span> checkpoint shards: 100%<span class="kw">|</span><span class="ex">██████████</span><span class="kw">|</span> <span class="ex">7/7</span> [04:01<span class="op">&lt;</span>00:00, 34.43s/it]</span>
<span id="cb8-2289"><a href="#cb8-2289"></a><span class="ex">Loading</span> checkpoint shards: 100%<span class="kw">|</span><span class="ex">██████████</span><span class="kw">|</span> <span class="ex">7/7</span> [04:00<span class="op">&lt;</span>00:00, 34.40s/it]</span>
<span id="cb8-2290"><a href="#cb8-2290"></a><span class="ex">Loading</span> checkpoint shards: 100%<span class="kw">|</span><span class="ex">██████████</span><span class="kw">|</span> <span class="ex">7/7</span> [04:00<span class="op">&lt;</span>00:00, 34.41s/it]</span>
<span id="cb8-2291"><a href="#cb8-2291"></a><span class="ex">Loading</span> checkpoint shards: 100%<span class="kw">|</span><span class="ex">██████████</span><span class="kw">|</span> <span class="ex">7/7</span> [04:00<span class="op">&lt;</span>00:00, 34.41s/it]</span>
<span id="cb8-2292"><a href="#cb8-2292"></a><span class="ex">Loading</span> checkpoint shards: 100%<span class="kw">|</span><span class="ex">██████████</span><span class="kw">|</span> <span class="ex">7/7</span> [04:01<span class="op">&lt;</span>00:00, 34.44s/it]</span>
<span id="cb8-2293"><a href="#cb8-2293"></a><span class="ex">Loading</span> checkpoint shards: 100%<span class="kw">|</span><span class="ex">██████████</span><span class="kw">|</span> <span class="ex">7/7</span> [04:01<span class="op">&lt;</span>00:00, 34.44s/it]</span>
<span id="cb8-2294"><a href="#cb8-2294"></a><span class="ex">Loading</span> checkpoint shards: 100%<span class="kw">|</span><span class="ex">██████████</span><span class="kw">|</span> <span class="ex">7/7</span> [04:01<span class="op">&lt;</span>00:00, 34.46s/it]</span>
<span id="cb8-2295"><a href="#cb8-2295"></a><span class="ex">Loading</span> checkpoint shards: 100%<span class="kw">|</span><span class="ex">██████████</span><span class="kw">|</span> <span class="ex">7/7</span> [04:01<span class="op">&lt;</span>00:00, 34.44s/it]</span>
<span id="cb8-2296"><a href="#cb8-2296"></a><span class="ex">Loading</span> checkpoint shards: 100%<span class="kw">|</span><span class="ex">██████████</span><span class="kw">|</span> <span class="ex">7/7</span> [04:01<span class="op">&lt;</span>00:00, 34.43s/it]</span>
<span id="cb8-2297"><a href="#cb8-2297"></a><span class="ex">Loading</span> checkpoint shards: 100%<span class="kw">|</span><span class="ex">██████████</span><span class="kw">|</span> <span class="ex">7/7</span> [04:01<span class="op">&lt;</span>00:00, 34.47s/it]</span>
<span id="cb8-2298"><a href="#cb8-2298"></a><span class="ex">Loading</span> checkpoint shards: 100%<span class="kw">|</span><span class="ex">██████████</span><span class="kw">|</span> <span class="ex">7/7</span> [04:01<span class="op">&lt;</span>00:00, 34.44s/it]</span>
<span id="cb8-2299"><a href="#cb8-2299"></a><span class="ex">Loading</span> checkpoint shards: 100%<span class="kw">|</span><span class="ex">██████████</span><span class="kw">|</span> <span class="ex">7/7</span> [04:01<span class="op">&lt;</span>00:00, 34.47s/it]</span>
<span id="cb8-2300"><a href="#cb8-2300"></a><span class="ex">Loading</span> checkpoint shards: 100%<span class="kw">|</span><span class="ex">██████████</span><span class="kw">|</span> <span class="ex">7/7</span> [04:01<span class="op">&lt;</span>00:00, 34.44s/it]</span>
<span id="cb8-2301"><a href="#cb8-2301"></a><span class="ex">Loading</span> checkpoint shards: 100%<span class="kw">|</span><span class="ex">██████████</span><span class="kw">|</span> <span class="ex">7/7</span> [03:58<span class="op">&lt;</span>00:00, 34.12s/it]</span>
<span id="cb8-2302"><a href="#cb8-2302"></a><span class="ex">Loading</span> checkpoint shards: 100%<span class="kw">|</span><span class="ex">██████████</span><span class="kw">|</span> <span class="ex">7/7</span> [04:01<span class="op">&lt;</span>00:00, 34.47s/it]</span>
<span id="cb8-2303"><a href="#cb8-2303"></a><span class="ex">Loading</span> checkpoint shards: 100%<span class="kw">|</span><span class="ex">██████████</span><span class="kw">|</span> <span class="ex">7/7</span> [04:01<span class="op">&lt;</span>00:00, 34.51s/it]</span>
<span id="cb8-2304"><a href="#cb8-2304"></a><span class="ex">[2024-10-16</span> 15:43:39,455] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb8-2305"><a href="#cb8-2305"></a><span class="ex">[2024-10-16</span> 15:43:39,666] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb8-2306"><a href="#cb8-2306"></a><span class="ex">[2024-10-16</span> 15:43:39,739] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb8-2307"><a href="#cb8-2307"></a><span class="ex">[2024-10-16</span> 15:43:39,801] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb8-2308"><a href="#cb8-2308"></a><span class="ex">[2024-10-16</span> 15:43:39,842] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb8-2309"><a href="#cb8-2309"></a><span class="ex">[2024-10-16</span> 15:43:39,860] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb8-2310"><a href="#cb8-2310"></a><span class="ex">[2024-10-16</span> 15:43:39,862] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb8-2311"><a href="#cb8-2311"></a><span class="ex">[2024-10-16</span> 15:43:39,881] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb8-2312"><a href="#cb8-2312"></a><span class="ex">[2024-10-16</span> 15:43:39,920] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb8-2313"><a href="#cb8-2313"></a><span class="ex">[2024-10-16</span> 15:43:40,138] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb8-2314"><a href="#cb8-2314"></a><span class="ex">[2024-10-16</span> 15:43:40,153] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb8-2315"><a href="#cb8-2315"></a><span class="ex">[2024-10-16</span> 15:43:40,174] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb8-2316"><a href="#cb8-2316"></a><span class="ex">[2024-10-16</span> 15:43:40,175] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb8-2317"><a href="#cb8-2317"></a><span class="ex">[2024-10-16</span> 15:43:40,205] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb8-2318"><a href="#cb8-2318"></a><span class="ex">[2024-10-16</span> 15:43:40,212] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb8-2319"><a href="#cb8-2319"></a><span class="ex">[2024-10-16</span> 15:43:40,224] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb8-2320"><a href="#cb8-2320"></a><span class="ex">[2024-10-16</span> 15:43:40,251] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb8-2321"><a href="#cb8-2321"></a><span class="ex">[2024-10-16</span> 15:43:40,255] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb8-2322"><a href="#cb8-2322"></a><span class="ex">[2024-10-16</span> 15:43:40,256] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb8-2323"><a href="#cb8-2323"></a><span class="ex">[2024-10-16</span> 15:43:40,417] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb8-2324"><a href="#cb8-2324"></a><span class="ex">[2024-10-16</span> 15:43:40,538] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:733:__init__</span><span class="pp">]</span> Config mesh_device None world_size = 24</span>
<span id="cb8-2325"><a href="#cb8-2325"></a><span class="ex">[2024-10-16</span> 15:43:56,920] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">logging.py:129:log_dist</span><span class="pp">]</span> [Rank 0] DeepSpeed Flops Profiler Enabled: False</span>
<span id="cb8-2326"><a href="#cb8-2326"></a><span class="ex">[2024-10-16</span> 15:43:56,921] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">logging.py:129:log_dist</span><span class="pp">]</span> [Rank 0] Creating BF16 optimizer</span>
<span id="cb8-2327"><a href="#cb8-2327"></a><span class="ex">[2024-10-16</span> 15:43:57,118] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">utils.py:781:see_memory_usage</span><span class="pp">]</span> begin bf16_optimizer</span>
<span id="cb8-2328"><a href="#cb8-2328"></a><span class="ex">[2024-10-16</span> 15:43:57,118] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">utils.py:782:see_memory_usage</span><span class="pp">]</span> MA 14.96 GB         Max_MA 14.96 GB         CA 14.96 GB         Max_CA 15 GB</span>
<span id="cb8-2329"><a href="#cb8-2329"></a><span class="ex">[2024-10-16</span> 15:43:57,118] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">utils.py:789:see_memory_usage</span><span class="pp">]</span> CPU Virtual Memory:  used = 490.74 GB, percent = 43.3%</span>
<span id="cb8-2330"><a href="#cb8-2330"></a><span class="ex">[2024-10-16</span> 15:43:57,290] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">utils.py:781:see_memory_usage</span><span class="pp">]</span> end bf16_ optimizer</span>
<span id="cb8-2331"><a href="#cb8-2331"></a><span class="ex">[2024-10-16</span> 15:43:57,291] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">utils.py:782:see_memory_usage</span><span class="pp">]</span> MA 14.96 GB         Max_MA 14.96 GB         CA 14.96 GB         Max_CA 15 GB</span>
<span id="cb8-2332"><a href="#cb8-2332"></a><span class="ex">[2024-10-16</span> 15:43:57,291] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">utils.py:789:see_memory_usage</span><span class="pp">]</span> CPU Virtual Memory:  used = 490.74 GB, percent = 43.3%</span>
<span id="cb8-2333"><a href="#cb8-2333"></a><span class="ex">[2024-10-16</span> 15:43:57,291] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:999:print</span><span class="pp">]</span> DeepSpeedEngine configuration:</span>
<span id="cb8-2334"><a href="#cb8-2334"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   activation_checkpointing_config  {</span>
<span id="cb8-2335"><a href="#cb8-2335"></a>    <span class="st">"partition_activations"</span><span class="ex">:</span> false,</span>
<span id="cb8-2336"><a href="#cb8-2336"></a>    <span class="st">"contiguous_memory_optimization"</span><span class="ex">:</span> false,</span>
<span id="cb8-2337"><a href="#cb8-2337"></a>    <span class="st">"cpu_checkpointing"</span><span class="ex">:</span> false,</span>
<span id="cb8-2338"><a href="#cb8-2338"></a>    <span class="st">"number_checkpoints"</span><span class="ex">:</span> null,</span>
<span id="cb8-2339"><a href="#cb8-2339"></a>    <span class="st">"synchronize_checkpoint_boundary"</span><span class="ex">:</span> false,</span>
<span id="cb8-2340"><a href="#cb8-2340"></a>    <span class="st">"profile"</span><span class="ex">:</span> false</span>
<span id="cb8-2341"><a href="#cb8-2341"></a><span class="er">}</span></span>
<span id="cb8-2342"><a href="#cb8-2342"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   aio_config ................... {<span class="st">'block_size'</span>: 1048576, <span class="st">'queue_depth'</span>: 8, <span class="st">'thread_count'</span>: 1, <span class="st">'single_submit'</span>: False, <span class="st">'overlap_events'</span>: True, <span class="st">'use_gds'</span>: False}</span>
<span id="cb8-2343"><a href="#cb8-2343"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   amp_enabled .................. False</span>
<span id="cb8-2344"><a href="#cb8-2344"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   amp_params ................... False</span>
<span id="cb8-2345"><a href="#cb8-2345"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   autotuning_config ............ {</span>
<span id="cb8-2346"><a href="#cb8-2346"></a>    <span class="st">"enabled"</span><span class="ex">:</span> false,</span>
<span id="cb8-2347"><a href="#cb8-2347"></a>    <span class="st">"start_step"</span><span class="ex">:</span> null,</span>
<span id="cb8-2348"><a href="#cb8-2348"></a>    <span class="st">"end_step"</span><span class="ex">:</span> null,</span>
<span id="cb8-2349"><a href="#cb8-2349"></a>    <span class="st">"metric_path"</span><span class="ex">:</span> null,</span>
<span id="cb8-2350"><a href="#cb8-2350"></a>    <span class="st">"arg_mappings"</span><span class="ex">:</span> null,</span>
<span id="cb8-2351"><a href="#cb8-2351"></a>    <span class="st">"metric"</span><span class="ex">:</span> <span class="st">"throughput"</span>,</span>
<span id="cb8-2352"><a href="#cb8-2352"></a>    <span class="st">"model_info"</span><span class="ex">:</span> null,</span>
<span id="cb8-2353"><a href="#cb8-2353"></a>    <span class="st">"results_dir"</span><span class="ex">:</span> <span class="st">"autotuning_results"</span>,</span>
<span id="cb8-2354"><a href="#cb8-2354"></a>    <span class="st">"exps_dir"</span><span class="ex">:</span> <span class="st">"autotuning_exps"</span>,</span>
<span id="cb8-2355"><a href="#cb8-2355"></a>    <span class="st">"overwrite"</span><span class="ex">:</span> true,</span>
<span id="cb8-2356"><a href="#cb8-2356"></a>    <span class="st">"fast"</span><span class="ex">:</span> true,</span>
<span id="cb8-2357"><a href="#cb8-2357"></a>    <span class="st">"start_profile_step"</span><span class="ex">:</span> 3,</span>
<span id="cb8-2358"><a href="#cb8-2358"></a>    <span class="st">"end_profile_step"</span><span class="ex">:</span> 5,</span>
<span id="cb8-2359"><a href="#cb8-2359"></a>    <span class="st">"tuner_type"</span><span class="ex">:</span> <span class="st">"gridsearch"</span>,</span>
<span id="cb8-2360"><a href="#cb8-2360"></a>    <span class="st">"tuner_early_stopping"</span><span class="ex">:</span> 5,</span>
<span id="cb8-2361"><a href="#cb8-2361"></a>    <span class="st">"tuner_num_trials"</span><span class="ex">:</span> 50,</span>
<span id="cb8-2362"><a href="#cb8-2362"></a>    <span class="st">"model_info_path"</span><span class="ex">:</span> null,</span>
<span id="cb8-2363"><a href="#cb8-2363"></a>    <span class="st">"mp_size"</span><span class="ex">:</span> 1,</span>
<span id="cb8-2364"><a href="#cb8-2364"></a>    <span class="st">"max_train_batch_size"</span><span class="ex">:</span> null,</span>
<span id="cb8-2365"><a href="#cb8-2365"></a>    <span class="st">"min_train_batch_size"</span><span class="ex">:</span> 1,</span>
<span id="cb8-2366"><a href="#cb8-2366"></a>    <span class="st">"max_train_micro_batch_size_per_gpu"</span><span class="ex">:</span> 1.024000e+03,</span>
<span id="cb8-2367"><a href="#cb8-2367"></a>    <span class="st">"min_train_micro_batch_size_per_gpu"</span><span class="ex">:</span> 1,</span>
<span id="cb8-2368"><a href="#cb8-2368"></a>    <span class="st">"num_tuning_micro_batch_sizes"</span><span class="ex">:</span> 3</span>
<span id="cb8-2369"><a href="#cb8-2369"></a><span class="er">}</span></span>
<span id="cb8-2370"><a href="#cb8-2370"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   bfloat16_enabled ............. True</span>
<span id="cb8-2371"><a href="#cb8-2371"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   bfloat16_immediate_grad_update  False</span>
<span id="cb8-2372"><a href="#cb8-2372"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   checkpoint_parallel_write_pipeline  False</span>
<span id="cb8-2373"><a href="#cb8-2373"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   checkpoint_tag_validation_enabled  True</span>
<span id="cb8-2374"><a href="#cb8-2374"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   checkpoint_tag_validation_fail  False</span>
<span id="cb8-2375"><a href="#cb8-2375"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   comms_config ................. <span class="op">&lt;</span>deepspeed.comm.config.DeepSpeedCommsConfig object at 0x145ea0815900<span class="op">&gt;</span></span>
<span id="cb8-2376"><a href="#cb8-2376"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   communication_data_type ...... None</span>
<span id="cb8-2377"><a href="#cb8-2377"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   compression_config ........... {<span class="st">'weight_quantization'</span>: {<span class="st">'shared_parameters'</span>: {<span class="st">'enabled'</span>: False, <span class="st">'quantizer_kernel'</span>: False, <span class="st">'schedule_offset'</span>: 0, <span class="st">'quantize_groups'</span>: 1, <span class="st">'quantize_verbose'</span>: False, <span class="st">'quantization_type'</span>: <span class="st">'symmetric'</span>, <span class="st">'quantize_weight_in_forward'</span>: False, <span class="st">'rounding'</span>: <span class="st">'nearest'</span>, <span class="st">'fp16_mixed_quantize'</span>: False, <span class="st">'quantize_change_ratio'</span>: 0.001}, <span class="st">'different_groups'</span>: {}}, <span class="st">'activation_quantization'</span>: {<span class="st">'shared_parameters'</span>: {<span class="st">'enabled'</span>: False, <span class="st">'quantization_type'</span>: <span class="st">'symmetric'</span>, <span class="st">'range_calibration'</span>: <span class="st">'dynamic'</span>, <span class="st">'schedule_offset'</span>: 1000}, <span class="st">'different_groups'</span>: {}}, <span class="st">'sparse_pruning'</span>: {<span class="st">'shared_parameters'</span>: {<span class="st">'enabled'</span>: False, <span class="st">'method'</span>: <span class="st">'l1'</span>, <span class="st">'schedule_offset'</span>: 1000}, <span class="st">'different_groups'</span>: {}}, <span class="st">'row_pruning'</span>: {<span class="st">'shared_parameters'</span>: {<span class="st">'enabled'</span>: False, <span class="st">'method'</span>: <span class="st">'l1'</span>, <span class="st">'schedule_offset'</span>: 1000}, <span class="st">'different_groups'</span>: {}}, <span class="st">'head_pruning'</span>: {<span class="st">'shared_parameters'</span>: {<span class="st">'enabled'</span>: False, <span class="st">'method'</span>: <span class="st">'topk'</span>, <span class="st">'schedule_offset'</span>: 1000}, <span class="st">'different_groups'</span>: {}}, <span class="st">'channel_pruning'</span>: {<span class="st">'shared_parameters'</span>: {<span class="st">'enabled'</span>: False, <span class="st">'method'</span>: <span class="st">'l1'</span>, <span class="st">'schedule_offset'</span>: 1000}, <span class="st">'different_groups'</span>: {}}, <span class="st">'layer_reduction'</span>: {<span class="st">'enabled'</span>: False}}</span>
<span id="cb8-2378"><a href="#cb8-2378"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   curriculum_enabled_legacy .... False</span>
<span id="cb8-2379"><a href="#cb8-2379"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   curriculum_params_legacy ..... False</span>
<span id="cb8-2380"><a href="#cb8-2380"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   data_efficiency_config ....... {<span class="st">'enabled'</span>: False, <span class="st">'seed'</span>: 1234, <span class="st">'data_sampling'</span>: {<span class="st">'enabled'</span>: False, <span class="st">'num_epochs'</span>: 1000, <span class="st">'num_workers'</span>: 0, <span class="st">'curriculum_learning'</span>: {<span class="st">'enabled'</span>: False}}, <span class="st">'data_routing'</span>: {<span class="st">'enabled'</span>: False, <span class="st">'random_ltd'</span>: {<span class="st">'enabled'</span>: False, <span class="st">'layer_token_lr_schedule'</span>: {<span class="st">'enabled'</span>: False}}}}</span>
<span id="cb8-2381"><a href="#cb8-2381"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   data_efficiency_enabled ...... False</span>
<span id="cb8-2382"><a href="#cb8-2382"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   dataloader_drop_last ......... False</span>
<span id="cb8-2383"><a href="#cb8-2383"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   disable_allgather ............ False</span>
<span id="cb8-2384"><a href="#cb8-2384"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   dump_state ................... False</span>
<span id="cb8-2385"><a href="#cb8-2385"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   dynamic_loss_scale_args ...... None</span>
<span id="cb8-2386"><a href="#cb8-2386"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   eigenvalue_enabled ........... False</span>
<span id="cb8-2387"><a href="#cb8-2387"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   eigenvalue_gas_boundary_resolution  1</span>
<span id="cb8-2388"><a href="#cb8-2388"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   eigenvalue_layer_name ........ bert.encoder.layer</span>
<span id="cb8-2389"><a href="#cb8-2389"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   eigenvalue_layer_num ......... 0</span>
<span id="cb8-2390"><a href="#cb8-2390"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   eigenvalue_max_iter .......... 100</span>
<span id="cb8-2391"><a href="#cb8-2391"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   eigenvalue_stability ......... 1e-06</span>
<span id="cb8-2392"><a href="#cb8-2392"></a><span class="ex">[2024-10-16</span> 15:43:57,292] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   eigenvalue_tol ............... 0.01</span>
<span id="cb8-2393"><a href="#cb8-2393"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   eigenvalue_verbose ........... False</span>
<span id="cb8-2394"><a href="#cb8-2394"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   elasticity_enabled ........... False</span>
<span id="cb8-2395"><a href="#cb8-2395"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   flops_profiler_config ........ {</span>
<span id="cb8-2396"><a href="#cb8-2396"></a>    <span class="st">"enabled"</span><span class="ex">:</span> false,</span>
<span id="cb8-2397"><a href="#cb8-2397"></a>    <span class="st">"recompute_fwd_factor"</span><span class="ex">:</span> 0.0,</span>
<span id="cb8-2398"><a href="#cb8-2398"></a>    <span class="st">"profile_step"</span><span class="ex">:</span> 1,</span>
<span id="cb8-2399"><a href="#cb8-2399"></a>    <span class="st">"module_depth"</span><span class="ex">:</span> <span class="at">-1,</span></span>
<span id="cb8-2400"><a href="#cb8-2400"></a>    <span class="st">"top_modules"</span><span class="ex">:</span> 1,</span>
<span id="cb8-2401"><a href="#cb8-2401"></a>    <span class="st">"detailed"</span><span class="ex">:</span> true,</span>
<span id="cb8-2402"><a href="#cb8-2402"></a>    <span class="st">"output_file"</span><span class="ex">:</span> null</span>
<span id="cb8-2403"><a href="#cb8-2403"></a><span class="er">}</span></span>
<span id="cb8-2404"><a href="#cb8-2404"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   fp16_auto_cast ............... None</span>
<span id="cb8-2405"><a href="#cb8-2405"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   fp16_enabled ................. False</span>
<span id="cb8-2406"><a href="#cb8-2406"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   fp16_master_weights_and_gradients  False</span>
<span id="cb8-2407"><a href="#cb8-2407"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   global_rank .................. 0</span>
<span id="cb8-2408"><a href="#cb8-2408"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   grad_accum_dtype ............. None</span>
<span id="cb8-2409"><a href="#cb8-2409"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   gradient_accumulation_steps .. 1</span>
<span id="cb8-2410"><a href="#cb8-2410"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   gradient_clipping ............ 0.0</span>
<span id="cb8-2411"><a href="#cb8-2411"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   gradient_predivide_factor .... 1.0</span>
<span id="cb8-2412"><a href="#cb8-2412"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   graph_harvesting ............. False</span>
<span id="cb8-2413"><a href="#cb8-2413"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8</span>
<span id="cb8-2414"><a href="#cb8-2414"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   initial_dynamic_scale ........ 1</span>
<span id="cb8-2415"><a href="#cb8-2415"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   load_universal_checkpoint .... False</span>
<span id="cb8-2416"><a href="#cb8-2416"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   loss_scale ................... 1.0</span>
<span id="cb8-2417"><a href="#cb8-2417"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   memory_breakdown ............. False</span>
<span id="cb8-2418"><a href="#cb8-2418"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   mics_hierarchial_params_gather  False</span>
<span id="cb8-2419"><a href="#cb8-2419"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   mics_shard_size .............. <span class="at">-1</span></span>
<span id="cb8-2420"><a href="#cb8-2420"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   monitor_config ............... tensorboard=TensorBoardConfig<span class="er">(</span><span class="va">enabled</span><span class="op">=</span>False, <span class="va">output_path</span><span class="op">=</span><span class="st">''</span>, <span class="va">job_name</span><span class="op">=</span><span class="st">'DeepSpeedJobName'</span><span class="kw">)</span> <span class="va">comet</span><span class="op">=</span>CometConfig<span class="kw">(</span><span class="va">enabled</span><span class="op">=</span>False, <span class="va">samples_log_interval</span><span class="op">=</span>100, <span class="va">project</span><span class="op">=</span>None, <span class="va">workspace</span><span class="op">=</span>None, <span class="va">api_key</span><span class="op">=</span>None, <span class="va">experiment_name</span><span class="op">=</span>None, <span class="va">experiment_key</span><span class="op">=</span>None, <span class="va">online</span><span class="op">=</span>None, <span class="va">mode</span><span class="op">=</span>None<span class="kw">)</span> <span class="va">wandb</span><span class="op">=</span>WandbConfig<span class="kw">(</span><span class="va">enabled</span><span class="op">=</span>False, <span class="va">group</span><span class="op">=</span>None, <span class="va">team</span><span class="op">=</span>None, <span class="va">project</span><span class="op">=</span><span class="st">'deepspeed'</span><span class="kw">)</span> <span class="va">csv_monitor</span><span class="op">=</span>CSVConfig<span class="kw">(</span><span class="va">enabled</span><span class="op">=</span>False, <span class="va">output_path</span><span class="op">=</span><span class="st">''</span>, <span class="va">job_name</span><span class="op">=</span><span class="st">'DeepSpeedJobName'</span><span class="kw">)</span></span>
<span id="cb8-2421"><a href="#cb8-2421"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   nebula_config ................ {</span>
<span id="cb8-2422"><a href="#cb8-2422"></a>    <span class="st">"enabled"</span><span class="ex">:</span> false,</span>
<span id="cb8-2423"><a href="#cb8-2423"></a>    <span class="st">"persistent_storage_path"</span><span class="ex">:</span> null,</span>
<span id="cb8-2424"><a href="#cb8-2424"></a>    <span class="st">"persistent_time_interval"</span><span class="ex">:</span> 100,</span>
<span id="cb8-2425"><a href="#cb8-2425"></a>    <span class="st">"num_of_version_in_retention"</span><span class="ex">:</span> 2,</span>
<span id="cb8-2426"><a href="#cb8-2426"></a>    <span class="st">"enable_nebula_load"</span><span class="ex">:</span> true,</span>
<span id="cb8-2427"><a href="#cb8-2427"></a>    <span class="st">"load_path"</span><span class="ex">:</span> null</span>
<span id="cb8-2428"><a href="#cb8-2428"></a><span class="er">}</span></span>
<span id="cb8-2429"><a href="#cb8-2429"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   optimizer_legacy_fusion ...... False</span>
<span id="cb8-2430"><a href="#cb8-2430"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   optimizer_name ............... None</span>
<span id="cb8-2431"><a href="#cb8-2431"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   optimizer_params ............. None</span>
<span id="cb8-2432"><a href="#cb8-2432"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   pipeline ..................... {<span class="st">'stages'</span>: <span class="st">'auto'</span>, <span class="st">'partition'</span>: <span class="st">'best'</span>, <span class="st">'seed_layers'</span>: False, <span class="st">'activation_checkpoint_interval'</span>: 0, <span class="st">'pipe_partitioned'</span>: True, <span class="st">'grad_partitioned'</span>: True}</span>
<span id="cb8-2433"><a href="#cb8-2433"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   pld_enabled .................. False</span>
<span id="cb8-2434"><a href="#cb8-2434"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   pld_params ................... False</span>
<span id="cb8-2435"><a href="#cb8-2435"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   prescale_gradients ........... False</span>
<span id="cb8-2436"><a href="#cb8-2436"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   scheduler_name ............... None</span>
<span id="cb8-2437"><a href="#cb8-2437"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   scheduler_params ............. None</span>
<span id="cb8-2438"><a href="#cb8-2438"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   seq_parallel_communication_data_type  torch.float32</span>
<span id="cb8-2439"><a href="#cb8-2439"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   sparse_attention ............. None</span>
<span id="cb8-2440"><a href="#cb8-2440"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   sparse_gradients_enabled ..... False</span>
<span id="cb8-2441"><a href="#cb8-2441"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   steps_per_print .............. 100</span>
<span id="cb8-2442"><a href="#cb8-2442"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   timers_config ................ enabled=True synchronized=True</span>
<span id="cb8-2443"><a href="#cb8-2443"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   train_batch_size ............. 24</span>
<span id="cb8-2444"><a href="#cb8-2444"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   train_micro_batch_size_per_gpu  1</span>
<span id="cb8-2445"><a href="#cb8-2445"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   use_data_before_expert_parallel_  False</span>
<span id="cb8-2446"><a href="#cb8-2446"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   use_node_local_storage ....... False</span>
<span id="cb8-2447"><a href="#cb8-2447"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   wall_clock_breakdown ......... False</span>
<span id="cb8-2448"><a href="#cb8-2448"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   weight_quantization_config ... None</span>
<span id="cb8-2449"><a href="#cb8-2449"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   world_size ................... 24</span>
<span id="cb8-2450"><a href="#cb8-2450"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   zero_allow_untested_optimizer  False</span>
<span id="cb8-2451"><a href="#cb8-2451"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True</span>
<span id="cb8-2452"><a href="#cb8-2452"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   zero_enabled ................. False</span>
<span id="cb8-2453"><a href="#cb8-2453"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   zero_force_ds_cpu_optimizer .. True</span>
<span id="cb8-2454"><a href="#cb8-2454"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:1003:print</span><span class="pp">]</span>   zero_optimization_stage ...... 0</span>
<span id="cb8-2455"><a href="#cb8-2455"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">config.py:989:print_user_config</span><span class="pp">]</span>   json = {</span>
<span id="cb8-2456"><a href="#cb8-2456"></a>    <span class="st">"train_batch_size"</span><span class="ex">:</span> 24,</span>
<span id="cb8-2457"><a href="#cb8-2457"></a>    <span class="st">"train_micro_batch_size_per_gpu"</span><span class="ex">:</span> 1,</span>
<span id="cb8-2458"><a href="#cb8-2458"></a>    <span class="st">"steps_per_print"</span><span class="ex">:</span> 100,</span>
<span id="cb8-2459"><a href="#cb8-2459"></a>    <span class="st">"zero_optimization"</span><span class="ex">:</span> {</span>
<span id="cb8-2460"><a href="#cb8-2460"></a>        <span class="st">"stage"</span><span class="ex">:</span> 0</span>
<span id="cb8-2461"><a href="#cb8-2461"></a>    <span class="er">}</span><span class="ex">,</span></span>
<span id="cb8-2462"><a href="#cb8-2462"></a>    <span class="st">"bf16"</span><span class="ex">:</span> {</span>
<span id="cb8-2463"><a href="#cb8-2463"></a>        <span class="st">"enabled"</span><span class="ex">:</span> true</span>
<span id="cb8-2464"><a href="#cb8-2464"></a>    <span class="er">}</span></span>
<span id="cb8-2465"><a href="#cb8-2465"></a><span class="er">}</span></span>
<span id="cb8-2466"><a href="#cb8-2466"></a><span class="ex">[2024-10-16</span> 15:43:57,293] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:105:__init__</span><span class="pp">]</span> CONFIG: micro_batches=1 micro_batch_size=1</span>
<span id="cb8-2467"><a href="#cb8-2467"></a><span class="ex">[2024-10-16</span> 15:43:57,294] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:146:__init__</span><span class="pp">]</span> is_pipe_partitioned= False is_grad_partitioned= False</span>
<span id="cb8-2468"><a href="#cb8-2468"></a><span class="ex">[2024-10-16</span> 15:43:57,298] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:165:__init__</span><span class="pp">]</span> RANK=0 STAGE=0 LAYERS=37 [0, 37<span class="kw">)</span> <span class="va">STAGE_PARAMS</span><span class="op">=</span>8032358400 <span class="kw">(</span><span class="ex">8032.358M</span><span class="kw">)</span> <span class="va">TOTAL_PARAMS</span><span class="op">=</span>8032358400 <span class="kw">(</span><span class="ex">8032.358M</span><span class="kw">)</span> <span class="va">UNIQUE_PARAMS</span><span class="op">=</span>8032358400 <span class="kw">(</span><span class="ex">8032.358M</span><span class="kw">)</span></span>
<span id="cb8-2469"><a href="#cb8-2469"></a><span class="ex">[2024-10-16</span> 15:43:57.298751]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:511</span><span class="pp">]</span> <span class="at">-</span> after deepspeed init</span>
<span id="cb8-2470"><a href="#cb8-2470"></a><span class="ex">[2024-10-16</span> 15:43:57.299527]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:162</span><span class="pp">]</span> <span class="at">-</span> hf_w.shape<span class="pp">[</span><span class="ss">0</span><span class="pp">]</span>=128512</span>
<span id="cb8-2471"><a href="#cb8-2471"></a><span class="ex">[2024-10-16</span> 15:43:57.299951]<span class="pp">[</span><span class="ss">INFO</span><span class="pp">][</span><span class="ss">hf2megads_weight_converter.py:163</span><span class="pp">]</span> <span class="at">-</span> self.token_vocab=128000</span>
<span id="cb8-2472"><a href="#cb8-2472"></a><span class="ex">[2024-10-16</span> 15:43:57,373] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:146:__init__</span><span class="pp">]</span> is_pipe_partitioned= False is_grad_partitioned= False</span>
<span id="cb8-2473"><a href="#cb8-2473"></a><span class="ex">[2024-10-16</span> 15:43:57,410] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:146:__init__</span><span class="pp">]</span> is_pipe_partitioned= False is_grad_partitioned= False</span>
<span id="cb8-2474"><a href="#cb8-2474"></a><span class="ex">[2024-10-16</span> 15:43:57,525] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:146:__init__</span><span class="pp">]</span> is_pipe_partitioned= False is_grad_partitioned= False</span>
<span id="cb8-2475"><a href="#cb8-2475"></a>                                     <span class="ex">[--hidden-size</span> HIDDEN_SIZE]</span>
<span id="cb8-2476"><a href="#cb8-2476"></a><span class="ex">!!!</span> ATTENTION !!!</span>
<span id="cb8-2477"><a href="#cb8-2477"></a>                                     <span class="ex">[--num-attention-heads</span> NUM_ATTENTION_HEADS]</span>
<span id="cb8-2478"><a href="#cb8-2478"></a><span class="ex">Type</span> <span class="st">'up'</span> to get to the frame that called dist.breakpoint<span class="er">(</span><span class="va">rank</span><span class="op">=</span>0<span class="kw">)</span></span>
<span id="cb8-2479"><a href="#cb8-2479"></a>                                     <span class="ex">[--kv-channels</span> KV_CHANNELS]</span>
<span id="cb8-2480"><a href="#cb8-2480"></a><span class="op">&gt;</span> /opt/aurora/24.180.0/frameworks/aurora_nre_models_frameworks-2024.2.1_u1/lib/python3.10/site-packages/torch/distributed/__init__.py<span class="kw">(</span><span class="ex">89</span><span class="kw">)</span><span class="fu">breakpoint()</span></span>
<span id="cb8-2481"><a href="#cb8-2481"></a><span class="ex">-</span><span class="op">&gt;</span> barrier<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-2482"><a href="#cb8-2482"></a><span class="kw">(</span><span class="ex">Pdb</span><span class="kw">)</span> <span class="ex">[2024-10-16</span> 15:43:57,531] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:146:__init__</span><span class="pp">]</span> is_pipe_partitioned= False is_grad_partitioned= False</span>
<span id="cb8-2483"><a href="#cb8-2483"></a><span class="ex">[2024-10-16</span> 15:43:57,539] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:146:__init__</span><span class="pp">]</span> is_pipe_partitioned= False is_grad_partitioned= False</span>
<span id="cb8-2484"><a href="#cb8-2484"></a><span class="ex">[2024-10-16</span> 15:43:57,613] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:146:__init__</span><span class="pp">]</span> is_pipe_partitioned= False is_grad_partitioned= False</span>
<span id="cb8-2485"><a href="#cb8-2485"></a><span class="ex">[2024-10-16</span> 15:43:57,677] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:146:__init__</span><span class="pp">]</span> is_pipe_partitioned= False is_grad_partitioned= False</span>
<span id="cb8-2486"><a href="#cb8-2486"></a><span class="ex">[2024-10-16</span> 15:43:57,689] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:146:__init__</span><span class="pp">]</span> is_pipe_partitioned= False is_grad_partitioned= False</span>
<span id="cb8-2487"><a href="#cb8-2487"></a><span class="ex">[2024-10-16</span> 15:43:57,689] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:146:__init__</span><span class="pp">]</span> is_pipe_partitioned= False is_grad_partitioned= False</span>
<span id="cb8-2488"><a href="#cb8-2488"></a><span class="ex">[2024-10-16</span> 15:43:57,694] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:146:__init__</span><span class="pp">]</span> is_pipe_partitioned= False is_grad_partitioned= False</span>
<span id="cb8-2489"><a href="#cb8-2489"></a><span class="ex">[2024-10-16</span> 15:43:57,694] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:146:__init__</span><span class="pp">]</span> is_pipe_partitioned= False is_grad_partitioned= False</span>
<span id="cb8-2490"><a href="#cb8-2490"></a><span class="ex">[2024-10-16</span> 15:43:57,733] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:146:__init__</span><span class="pp">]</span> is_pipe_partitioned= False is_grad_partitioned= False</span>
<span id="cb8-2491"><a href="#cb8-2491"></a><span class="ex">[2024-10-16</span> 15:43:57,768] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:146:__init__</span><span class="pp">]</span> is_pipe_partitioned= False is_grad_partitioned= False</span>
<span id="cb8-2492"><a href="#cb8-2492"></a><span class="ex">[2024-10-16</span> 15:43:57,769] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:146:__init__</span><span class="pp">]</span> is_pipe_partitioned= False is_grad_partitioned= False</span>
<span id="cb8-2493"><a href="#cb8-2493"></a><span class="ex">[2024-10-16</span> 15:43:57,823] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:146:__init__</span><span class="pp">]</span> is_pipe_partitioned= False is_grad_partitioned= False</span>
<span id="cb8-2494"><a href="#cb8-2494"></a><span class="ex">[2024-10-16</span> 15:43:57,857] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:146:__init__</span><span class="pp">]</span> is_pipe_partitioned= False is_grad_partitioned= False</span>
<span id="cb8-2495"><a href="#cb8-2495"></a><span class="ex">[2024-10-16</span> 15:43:57,869] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:146:__init__</span><span class="pp">]</span> is_pipe_partitioned= False is_grad_partitioned= False</span>
<span id="cb8-2496"><a href="#cb8-2496"></a><span class="ex">[2024-10-16</span> 15:43:57,876] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:146:__init__</span><span class="pp">]</span> is_pipe_partitioned= False is_grad_partitioned= False</span>
<span id="cb8-2497"><a href="#cb8-2497"></a><span class="ex">[2024-10-16</span> 15:43:57,876] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:146:__init__</span><span class="pp">]</span> is_pipe_partitioned= False is_grad_partitioned= False</span>
<span id="cb8-2498"><a href="#cb8-2498"></a><span class="ex">[2024-10-16</span> 15:43:58,057] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:146:__init__</span><span class="pp">]</span> is_pipe_partitioned= False is_grad_partitioned= False</span>
<span id="cb8-2499"><a href="#cb8-2499"></a><span class="ex">[2024-10-16</span> 15:43:58,058] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:146:__init__</span><span class="pp">]</span> is_pipe_partitioned= False is_grad_partitioned= False</span>
<span id="cb8-2500"><a href="#cb8-2500"></a><span class="ex">[2024-10-16</span> 15:43:58,102] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:146:__init__</span><span class="pp">]</span> is_pipe_partitioned= False is_grad_partitioned= False</span>
<span id="cb8-2501"><a href="#cb8-2501"></a><span class="ex">[2024-10-16</span> 15:43:58,102] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">engine.py:146:__init__</span><span class="pp">]</span> is_pipe_partitioned= False is_grad_partitioned= False</span>
<span id="cb8-2502"><a href="#cb8-2502"></a>                                     <span class="ex">[--ffn-hidden-size</span> FFN_HIDDEN_SIZE]</span>
<span id="cb8-2503"><a href="#cb8-2503"></a><span class="kw">(</span><span class="ex">Pdb</span><span class="kw">)</span> <span class="ex">l</span></span>
<span id="cb8-2504"><a href="#cb8-2504"></a> <span class="ex">84</span>                pdb.message<span class="er">(</span></span>
<span id="cb8-2505"><a href="#cb8-2505"></a> <span class="ex">85</span>                    <span class="st">"\n!!! ATTENTION !!!\n\n"</span></span>
<span id="cb8-2506"><a href="#cb8-2506"></a> <span class="ex">86</span>                    f<span class="st">"Type 'up' to get to the frame that called dist.breakpoint(rank={rank})\n"</span></span>
<span id="cb8-2507"><a href="#cb8-2507"></a> <span class="ex">87</span>                <span class="kw">)</span></span>
<span id="cb8-2508"><a href="#cb8-2508"></a> <span class="ex">88</span>                pdb.set_trace<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-2509"><a href="#cb8-2509"></a> <span class="ex">89</span>  <span class="at">-</span><span class="op">&gt;</span>        barrier<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-2510"><a href="#cb8-2510"></a> <span class="ex">90</span>     </span>
<span id="cb8-2511"><a href="#cb8-2511"></a> <span class="ex">91</span>        if sys.platform != <span class="st">"win32"</span>:</span>
<span id="cb8-2512"><a href="#cb8-2512"></a> <span class="ex">92</span>            from torch._C._distributed_c10d import <span class="er">(</span></span>
<span id="cb8-2513"><a href="#cb8-2513"></a> <span class="ex">93</span>                HashStore,</span>
<span id="cb8-2514"><a href="#cb8-2514"></a> <span class="ex">94</span>                _round_robin_process_groups,</span>
<span id="cb8-2515"><a href="#cb8-2515"></a><span class="kw">(</span><span class="ex">Pdb</span><span class="kw">)</span> <span class="ex">ll</span></span>
<span id="cb8-2516"><a href="#cb8-2516"></a> <span class="ex">74</span>        def breakpoint<span class="er">(</span><span class="ex">rank:</span> int = 0<span class="kw">)</span><span class="bu">:</span></span>
<span id="cb8-2517"><a href="#cb8-2517"></a> <span class="ex">75</span>            <span class="st">"""</span></span>
<span id="cb8-2518"><a href="#cb8-2518"></a><span class="st"> 76            Set a breakpoint, but only on a single rank.  All other ranks will wait for you to be</span></span>
<span id="cb8-2519"><a href="#cb8-2519"></a><span class="st"> 77            done with the breakpoint before continuing.</span></span>
<span id="cb8-2520"><a href="#cb8-2520"></a><span class="st"> 78     </span></span>
<span id="cb8-2521"><a href="#cb8-2521"></a><span class="st"> 79            Args:</span></span>
<span id="cb8-2522"><a href="#cb8-2522"></a><span class="st"> 80                rank (int): Which rank to break on.  Default: </span><span class="kw">``</span><span class="st">0</span><span class="kw">``</span></span>
<span id="cb8-2523"><a href="#cb8-2523"></a><span class="st"> 81            """</span></span>
<span id="cb8-2524"><a href="#cb8-2524"></a> <span class="ex">82</span>            if get_rank<span class="er">(</span><span class="kw">)</span> <span class="ex">==</span> rank:</span>
<span id="cb8-2525"><a href="#cb8-2525"></a> <span class="ex">83</span>                pdb = _DistributedPdb<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-2526"><a href="#cb8-2526"></a> <span class="ex">84</span>                pdb.message<span class="er">(</span></span>
<span id="cb8-2527"><a href="#cb8-2527"></a> <span class="ex">85</span>                    <span class="st">"\n!!! ATTENTION !!!\n\n"</span></span>
<span id="cb8-2528"><a href="#cb8-2528"></a> <span class="ex">86</span>                    f<span class="st">"Type 'up' to get to the frame that called dist.breakpoint(rank={rank})\n"</span></span>
<span id="cb8-2529"><a href="#cb8-2529"></a> <span class="ex">87</span>                <span class="kw">)</span></span>
<span id="cb8-2530"><a href="#cb8-2530"></a> <span class="ex">88</span>                pdb.set_trace<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-2531"><a href="#cb8-2531"></a> <span class="ex">89</span>  <span class="at">-</span><span class="op">&gt;</span>        barrier<span class="er">(</span><span class="kw">)</span></span>
<span id="cb8-2532"><a href="#cb8-2532"></a><span class="kw">(</span><span class="ex">Pdb</span><span class="kw">)</span> <span class="ex">up</span></span>
<span id="cb8-2533"><a href="#cb8-2533"></a><span class="op">&gt;</span> /lus/flare/projects/Aurora_deployment/foremans/projects/argonne-lcf/Megatron-DeepSpeed/tools/hf2megads_weight_converter.py<span class="kw">(</span><span class="ex">224</span><span class="kw">)</span><span class="fu">_qkv_refactor()</span></span>
<span id="cb8-2534"><a href="#cb8-2534"></a><span class="ex">-</span><span class="op">&gt;</span> torch.distributed.breakpoint<span class="er">(</span><span class="ex">0</span><span class="kw">)</span></span>
<span id="cb8-2535"><a href="#cb8-2535"></a><span class="kw">(</span><span class="ex">Pdb</span><span class="kw">)</span> <span class="ex">ll</span></span>
<span id="cb8-2536"><a href="#cb8-2536"></a><span class="ex">194</span>        def _qkv_refactor<span class="er">(</span><span class="ex">self,</span> pname, p, hf_layer<span class="kw">)</span><span class="bu">:</span></span>
<span id="cb8-2537"><a href="#cb8-2537"></a><span class="ex">195</span>            hf_wq_name = f<span class="st">"model.layers.{hf_layer}.self_attn.q_proj.weight"</span></span>
<span id="cb8-2538"><a href="#cb8-2538"></a><span class="ex">196</span>            hf_wk_name = f<span class="st">"model.layers.{hf_layer}.self_attn.k_proj.weight"</span></span>
<span id="cb8-2539"><a href="#cb8-2539"></a><span class="ex">197</span>            hf_wv_name = f<span class="st">"model.layers.{hf_layer}.self_attn.v_proj.weight"</span></span>
<span id="cb8-2540"><a href="#cb8-2540"></a><span class="ex">198</span>            wq = self.hf_model<span class="pp">[</span><span class="ss">hf_wq_name</span><span class="pp">]</span></span>
<span id="cb8-2541"><a href="#cb8-2541"></a><span class="ex">199</span>            wk = self.hf_model<span class="pp">[</span><span class="ss">hf_wk_name</span><span class="pp">]</span></span>
<span id="cb8-2542"><a href="#cb8-2542"></a><span class="ex">200</span>            wv = self.hf_model<span class="pp">[</span><span class="ss">hf_wv_name</span><span class="pp">]</span></span>
<span id="cb8-2543"><a href="#cb8-2543"></a><span class="ex">201</span>     </span>
<span id="cb8-2544"><a href="#cb8-2544"></a><span class="ex">202</span>            hidden_size = wq.shape<span class="pp">[</span><span class="ss">0</span><span class="pp">]</span></span>
<span id="cb8-2545"><a href="#cb8-2545"></a><span class="ex">203</span>            per_partition_size, start_index, end_index = compute_partition_range<span class="er">(</span></span>
<span id="cb8-2546"><a href="#cb8-2546"></a><span class="ex">204</span>                hidden_size, self.tp_rank, self.tp_size<span class="kw">)</span></span>
<span id="cb8-2547"><a href="#cb8-2547"></a><span class="ex">205</span>            hidden_size_per_attention_head = divide<span class="er">(</span><span class="ex">hidden_size,</span></span>
<span id="cb8-2548"><a href="#cb8-2548"></a><span class="ex">206</span>                                                    self.config.num_attention_heads<span class="kw">)</span></span>
<span id="cb8-2549"><a href="#cb8-2549"></a><span class="ex">207</span>            num_attention_heads_per_partition = divide<span class="er">(</span><span class="ex">self.config.num_attention_heads,</span></span>
<span id="cb8-2550"><a href="#cb8-2550"></a><span class="ex">208</span>                                                       self.tp_size<span class="kw">)</span></span>
<span id="cb8-2551"><a href="#cb8-2551"></a><span class="ex">209</span>     </span>
<span id="cb8-2552"><a href="#cb8-2552"></a><span class="ex">210</span>            new_w = torch.zeros<span class="er">(</span><span class="kw">(</span><span class="ex">per_partition_size</span> <span class="pp">*</span> 3, wq.shape<span class="pp">[</span><span class="ss">1</span><span class="pp">]</span><span class="kw">)</span><span class="ex">,</span> dtype=wq.dtype<span class="kw">)</span></span>
<span id="cb8-2553"><a href="#cb8-2553"></a><span class="ex">211</span>     </span>
<span id="cb8-2554"><a href="#cb8-2554"></a><span class="ex">212</span>            for i in range<span class="er">(</span><span class="ex">num_attention_heads_per_partition</span><span class="kw">)</span><span class="bu">:</span></span>
<span id="cb8-2555"><a href="#cb8-2555"></a><span class="ex">213</span>                try:</span>
<span id="cb8-2556"><a href="#cb8-2556"></a><span class="ex">214</span>                    current_index = start_index + i <span class="pp">*</span> hidden_size_per_attention_head</span>
<span id="cb8-2557"><a href="#cb8-2557"></a><span class="ex">215</span>                    next_index = current_index + hidden_size_per_attention_head</span>
<span id="cb8-2558"><a href="#cb8-2558"></a><span class="ex">216</span>                    new_w_index = i <span class="pp">*</span> <span class="er">(</span><span class="ex">3</span> <span class="pp">*</span> hidden_size_per_attention_head<span class="kw">)</span></span>
<span id="cb8-2559"><a href="#cb8-2559"></a><span class="ex">217</span>                    new_w[new_w_index: new_w_index + <span class="er">(</span><span class="ex">3</span> <span class="pp">*</span> hidden_size_per_attention_head<span class="kw">)</span><span class="ex">,</span> :] = <span class="dt">\</span></span>
<span id="cb8-2560"><a href="#cb8-2560"></a>218                        torch.cat<span class="er">(</span><span class="bu">[</span></span>
<span id="cb8-2561"><a href="#cb8-2561"></a>219                            wq[current_index: next_index, <span class="er">:],</span></span>
<span id="cb8-2562"><a href="#cb8-2562"></a><span class="ex">220</span>                            wk[current_index: next_index, :],</span>
<span id="cb8-2563"><a href="#cb8-2563"></a><span class="ex">221</span>                            wv[current_index: next_index, :]</span>
<span id="cb8-2564"><a href="#cb8-2564"></a><span class="ex">222</span>                        ], dim=0<span class="kw">)</span></span>
<span id="cb8-2565"><a href="#cb8-2565"></a><span class="ex">223</span>                except Exception:</span>
<span id="cb8-2566"><a href="#cb8-2566"></a><span class="ex">224</span>  <span class="at">-</span><span class="op">&gt;</span>                torch.distributed.breakpoint<span class="er">(</span><span class="ex">0</span><span class="kw">)</span></span>
<span id="cb8-2567"><a href="#cb8-2567"></a><span class="ex">225</span>            self.record_mapping_info<span class="er">(</span></span>
<span id="cb8-2568"><a href="#cb8-2568"></a><span class="ex">226</span>                f<span class="st">"mega-ds:{pname,p.data.shape}&lt;--hf{hf_wq_name,hf_wk_name,hf_wv_name,}  cat q,k,v [{current_index}:{next_index},:]  of q,k,v{wq.shape}"</span></span>
<span id="cb8-2569"><a href="#cb8-2569"></a><span class="ex">227</span>            <span class="kw">)</span></span>
<span id="cb8-2570"><a href="#cb8-2570"></a><span class="ex">228</span>            return new_w</span>
<span id="cb8-2571"><a href="#cb8-2571"></a><span class="kw">(</span><span class="ex">Pdb</span><span class="kw">)</span> <span class="ex">current_index</span></span>
<span id="cb8-2572"><a href="#cb8-2572"></a><span class="ex">1024</span></span>
<span id="cb8-2573"><a href="#cb8-2573"></a><span class="kw">(</span><span class="ex">Pdb</span><span class="kw">)</span> <span class="ex">next_index</span></span>
<span id="cb8-2574"><a href="#cb8-2574"></a><span class="ex">1152</span></span>
<span id="cb8-2575"><a href="#cb8-2575"></a><span class="kw">(</span><span class="ex">Pdb</span><span class="kw">)</span> <span class="ex">new_w_index</span></span>
<span id="cb8-2576"><a href="#cb8-2576"></a><span class="ex">3072</span></span>
<span id="cb8-2577"><a href="#cb8-2577"></a><span class="kw">(</span><span class="ex">Pdb</span><span class="kw">)</span> <span class="ex">new_w.shape</span></span>
<span id="cb8-2578"><a href="#cb8-2578"></a><span class="ex">torch.Size</span><span class="er">(</span><span class="ex">[12288,</span> 4096]<span class="kw">)</span></span>
<span id="cb8-2579"><a href="#cb8-2579"></a><span class="kw">(</span><span class="ex">Pdb</span><span class="kw">)</span> <span class="ex">wq</span></span>
<span id="cb8-2580"><a href="#cb8-2580"></a><span class="ex">tensor</span><span class="er">(</span><span class="kw">[[</span> 0.0053, -0.0291, -0.0058,  <span class="er">...,</span>  <span class="ex">0.0095,</span> <span class="at">-0.0420,</span> <span class="at">-0.0272],</span></span>
<span id="cb8-2581"><a href="#cb8-2581"></a>        <span class="ex">[-0.0142,</span> <span class="at">-0.0679,</span> <span class="at">-0.0049,</span>  ..., <span class="at">-0.0142,</span> <span class="at">-0.0498,</span>  0.0192],</span>
<span id="cb8-2582"><a href="#cb8-2582"></a>        <span class="ex">[-0.0162,</span> <span class="at">-0.0393,</span> <span class="at">-0.0026,</span>  ...,  0.0115, <span class="at">-0.0126,</span>  0.0071],</span>
<span id="cb8-2583"><a href="#cb8-2583"></a>        <span class="ex">...,</span></span>
<span id="cb8-2584"><a href="#cb8-2584"></a>        <span class="ex">[-0.0039,</span> <span class="at">-0.0393,</span>  0.0806,  ...,  0.0061, <span class="at">-0.0013,</span>  0.0023],</span>
<span id="cb8-2585"><a href="#cb8-2585"></a>        <span class="ex">[-0.0035,</span> <span class="at">-0.0101,</span>  0.0459,  ...,  0.0049, <span class="at">-0.0011,</span>  0.0011],</span>
<span id="cb8-2586"><a href="#cb8-2586"></a>        <span class="ex">[-0.0018,</span> <span class="at">-0.0153,</span>  0.0347,  ...,  0.0110,  0.0004,  0.0044]],</span>
<span id="cb8-2587"><a href="#cb8-2587"></a>       <span class="va">dtype</span><span class="op">=</span>torch.bfloat16, <span class="va">grad_fn</span><span class="op">=&lt;</span>CloneBackward0<span class="op">&gt;</span><span class="kw">)</span></span>
<span id="cb8-2588"><a href="#cb8-2588"></a><span class="kw">(</span><span class="ex">Pdb</span><span class="kw">)</span> <span class="ex">wq.shape</span></span>
<span id="cb8-2589"><a href="#cb8-2589"></a><span class="ex">torch.Size</span><span class="er">(</span><span class="ex">[4096,</span> 4096]<span class="kw">)</span></span>
<span id="cb8-2590"><a href="#cb8-2590"></a><span class="kw">(</span><span class="ex">Pdb</span><span class="kw">)</span> <span class="ex">wk.shape</span></span>
<span id="cb8-2591"><a href="#cb8-2591"></a><span class="ex">torch.Size</span><span class="er">(</span><span class="ex">[1024,</span> 4096]<span class="kw">)</span></span>
<span id="cb8-2592"><a href="#cb8-2592"></a><span class="kw">(</span><span class="ex">Pdb</span><span class="kw">)</span> <span class="ex">wv.shape</span></span>
<span id="cb8-2593"><a href="#cb8-2593"></a><span class="ex">torch.Size</span><span class="er">(</span><span class="ex">[1024,</span> 4096]<span class="kw">)</span></span>
<span id="cb8-2594"><a href="#cb8-2594"></a><span class="kw">(</span><span class="ex">Pdb</span><span class="kw">)</span> <span class="ex">hidden_size</span></span>
<span id="cb8-2595"><a href="#cb8-2595"></a><span class="ex">4096</span></span>
<span id="cb8-2596"><a href="#cb8-2596"></a><span class="kw">(</span><span class="ex">Pdb</span><span class="kw">)</span> <span class="ex">per_partition_size</span></span>
<span id="cb8-2597"><a href="#cb8-2597"></a><span class="ex">4096</span></span>
<span id="cb8-2598"><a href="#cb8-2598"></a><span class="kw">(</span><span class="ex">Pdb</span><span class="kw">)</span> <span class="ex">num_attention_heads_per_partition</span></span>
<span id="cb8-2599"><a href="#cb8-2599"></a><span class="ex">32</span></span>
<span id="cb8-2600"><a href="#cb8-2600"></a><span class="kw">(</span><span class="ex">Pdb</span><span class="kw">)</span> <span class="ex">new_w.shape</span></span>
<span id="cb8-2601"><a href="#cb8-2601"></a><span class="ex">torch.Size</span><span class="er">(</span><span class="ex">[12288,</span> 4096]<span class="kw">)</span></span>
<span id="cb8-2602"><a href="#cb8-2602"></a><span class="kw">(</span><span class="ex">Pdb</span><span class="kw">)</span> <span class="ex">new_w.shape</span></span>
<span id="cb8-2603"><a href="#cb8-2603"></a><span class="ex">torch.Size</span><span class="er">(</span><span class="ex">[12288,</span> 4096]<span class="kw">)</span></span>
<span id="cb8-2604"><a href="#cb8-2604"></a><span class="kw">(</span><span class="ex">Pdb</span><span class="kw">)</span></span>
<span id="cb8-2605"><a href="#cb8-2605"></a><span class="kw">```</span></span>
<span id="cb8-2606"><a href="#cb8-2606"></a></span>
<span id="cb8-2607"><a href="#cb8-2607"></a><span class="op">&lt;</span>/details<span class="op">&gt;</span></span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/saforem2/personal_site/blob/main/posts/AuroraGPT/checkpoints/index.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/saforem2/personal_site/edit/main/posts/AuroraGPT/checkpoints/index.qmd" class="toc-action"><i class="bi empty"></i>Edit this page</a></li><li><a href="https://github.com/saforem2/personal_site/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div></div></footer></body></html>