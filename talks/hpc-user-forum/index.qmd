---
# title: "[Training LLMs at Scale](https://samforeman.me/talks/llms-at-scale/slides.html)"
title: "AuroraGPT"
# navbar: default
# sidebar: false
cookie-consent: true
location: "[HPC User Forum Fall '24](https://www.hpcuserforum.com/hpc-user-forum-fall-2024/)"
date: 2024-09-04
# fig-cap-location: top
# date-modified: last-modified
number-sections: false
bibliography: ../../references.bib
editor:
  render-on-save: true
  freeze: auto
twitter-card:
  image: ./assets/thumbnail.png
  site: "saforem2"
  creator: "saforem2"
  title: "AuroraGPT @ HPC User Forum"
  description: "Overview of AuroraGPT at ALCF"
  card-style: summary
open-graph:
  title: "AuroraGPT @ HPC User Forum"
  description: "Overview of AuroraGPT at ALCF"
  image: ./assets/thumbnail.png
citation:
   author: Sam Foreman
   type: speech
   url: https://samforeman.me/talks/hpc-user-forum/slides
# toc-expand: true
format:
  html:
    shift-heading-level-by: 1
  revealjs:
    footer: "[[samforeman.me/talks/hpc-user-forum/slides](https://samforeman.me/talks/hpc-user-forum/slides)]{.small}"
    slides-url: https://samforeman.me/talks/hpc-user-forum/slides.html
    template-partials:
      - title-slide.html
    title-slide-attributes:
      data-background-iframe: https://emilhvitfeldt.github.io/quarto-iframe-examples/center-of-universe/index.html
      data-background-size: contain
      data-background-color: dark
      background-color: dark
---

# AuroraGPT @ ALCF {background-color="white"}

::: {#fig-llms-at-anl}

[![](https://github.com/Hannibal046/Awesome-LLM/raw/main/resources/image8.gif)](https://github.com/Hannibal046/Awesome-LLM/raw/main/resources/)

Large Language Models (LLMs) have taken the world by storm

:::

## üìÑ Overview {background-color="white"}

- **AuroraGPT**: _General purpose scientific foundation model_
    - Model Sizes: {7B, 70B, ~ 1T}

- Safe, with a focus on:
    - trustworthiness, safety, robustness, privacy, machine ethics

- Multi-lingual: {üá∫üá∏, üáØüáµ[^riken], üá´üá∑, üá©üá™, üá™üá∏, üáÆüáπ}
    - Institutional Partnerships through TPC[^tpc]

- Multi-modal:

  :::: {.flex-container style="text-align: left; width: 100%; justify-content: flex-start;"}
  ::: {.col1}
    - scientific text
    - images
    - tables
    - equations
    - proofs
  :::

  ::: {.col2 style="margin-left: 10%;"}
    - structured data
    - sequences
    - time-series
    - graphs
    - fields
  :::

  ::::

[^tpc]:|
    [New international consortium for generative AI models for science](https://www.anl.gov/article/new-international-consortium-formed-to-create-trustworthy-and-reliable-generative-ai-models-for)

[^riken]:|
    [Argonne and RIKEN sign a MOU in support of AI for science](https://www.anl.gov/article/argonne-and-riken-sign-a-memorandum-of-understanding-in-support-of-ai-for-science)

## üîç Details {background-color="white"}

- Llama Style Architecture:
    - [{{< iconify ph github-logo-duotone >}} `argonne-lcf/Megatron-DeepSpeed`](https://github.com/argonne-lcf/Megatron-DeepSpeed)
    <!-- - FLASH attention.  Focus on Llama2 for Year 1 (7B and 70B targets) -->
- Performant training implementations of AuroraGPT architecture on
    - {Aurora, Polaris, Cerebras, SambaNova}
- Workflow to capture:
    - loss curves
    - snapshots, checkpoints
    - scaling / performance data
- Training runs for AuroraGPT-7B [(ongoing)]{.dim-text}
    - Baseline (Dolma) @ {Aurora, Polaris} [(twins for debugging)]{.dim-text}
    - Baseline + Science @ Aurora
- Trained raw models üìÆ delivered to post-pretraining team
    - AuroraGPT-7B-A, AuroraGPT-7B-P, AuroraGPT-7B-S  
      (A=`Aurora`, P=`Polaris`, S=`Science`)

## ü§î Why? {background-color="white"}

- **For Science!**
- Data-{sets, pipelines} for {preparing, aggregating, parsing, analyzing} scientific data
- Infrastructure to {train, eval, deploy} LLMs for science
    - Comparative analysis across: {models, tasks, languages, contexts, ...}
- Augment text data from the web with:
    - full text papers
    - structured scientific data[^tough]
- Safety-driven, publicly-visible, open-source approach:
    - Distribution of research grade artifacts (models, checkpoints, etc.)
    - International collaborations on AGI for science

[^tough]: Can be much more difficult than text (or even image) data

## üë• Teams {auto-animate=true background-color="white"}

::: {.flex-container}

::: {.col1}
- **Planning**
- **Data Prep**
    - Accumulate 20+ T tokens of high-quality scientific text and structured data
- [**Models / Training**[^me]]{style="background: oklch(from #ff1a8f calc(l * 1.15) c h / 0.1); border: 1px solid #ff1a8f; border-radius: 0.25px;"}
    - Train (entirely from scratch) a series of models on publicly available data
- **Evaluation**
    - Skills, trustworthiness, safety, robustness, privacy, machine ethics

[^me]: Co-led by: Venkat Vishwanath, Sam Foreman
:::

::: {.col2}
- **Post-Training**
    - Fine-tuning, alignment
- **Inference**
    - Model serving, API development / public-facing web services
- **Distribution**
    - Licensing, generating and distributing artifacts for public consumption
- **Communication**
:::

:::

<!-- ## Data Prep -->
<!---->
<!-- ::: {.callout-note collapse=false icon=false title="üéØ Goal" style="width:100%!important;"} -->
<!---->
<!-- Accumulate 20+ **T tokens** of high-quality scientific text and structured data with: -->
<!---->
<!-- - strong quality control -->
<!-- - de-duplication -->
<!-- - provenance -->
<!-- - subsetting -->
<!---->
<!-- ::: -->

## ü§ñ Aurora[^aurora]: System Overview {background-color="white"}

:::: {.flex-container}

::: {.col1 style="width:30%;"}

- 166 Racks
- 10,624 Nodes
- 21,248 CPUs
- 63,744 GPUs
- 84,992 NICs
- 8 PB HBM
- 10 PB DDR5c

:::

::: {.col2 style="width:70%;"}

![](./assets/aurora.png)

:::

::::

[^aurora]:|
    [The Computer That Will Change Everything ‚Äì Chicago Magazine](https://www.chicagomag.com/chicago-magazine/february-2023/the-computer-that-will-change-everything/)

# ü¶ô LLMs {background-color="white"}

## ü¶ú Model Training {background-color="white"}

:::: {.flex-container style="text-align: left; width: 100%; justify-content: center;"}

::: {.col1 width="49%" style="background: oklch(from #03BD00 calc(l * 1.15) c h / 0.1); border: 1px solid #03BD00; border-radius: 0.25em; padding: 3pt 8pt; margin-right: 1%"}

‚úÖ [**Goals**]{style="color: #03BD00;"}

- Want training runs _at scale_ to be:
    - Efficient
    - Stable
    - Reproducible

- This requires:
    - Robust data pipelines / file IO
    - Effectively overlapping compute with communication
    - Stability across {network, filesystem, machine}

- For larger models:
    - Multi-dimensional parallelism strategies

:::

::: {.col2 width="49%" style="background: oklch(from #E90102 calc(l * 1.15) c h / 0.1); border: 1px solid #E90102; border-radius: 0.25em; padding: 3pt 8pt; margin-left: 1%;"}

‚ùå [**Difficulties**]{style="color: #E90102;"}

- _Looong time_ to train

- Stability issues
    - Failures are **expensive** [(and unavoidable)]{.dim-text}
    - stragglers common at scale

- Individual jobs are:
    - **fragile**
    - only as good as the worst rank
    - one hang or bad worker can crash job
    - network / filesystem / other-user(s) dependent

:::

::::

## üöÄ Training at Scale

::: {.flex-container}

::: {.col1}
- 3D Parallelism
- Highly optimized GPU kernels
- Network performance
- Cost / benefits of different collective communication algorithms
    - depend on optimized / efficient implementations
:::

::: {.col2}
- Large batch training
- Second order optimizers
- State space models
- Sub-quadratic attention (?)
:::

:::

## ‚ôªÔ∏è  Life Cycle of the LLM {background-color="white"}

::: {.panel-tabset}

### üìù Pre-training {auto-animate=true background-color="white"}

::: {#fig-pretrain-one style="text-align: left;"}

![](https://jalammar.github.io/images/gpt3/03-gpt3-training-step-back-prop.gif)

**Pre-training**: Virtually all of the compute used during pretraining phase

:::

### üéÄ Fine-Tuning {auto-animate=true background-color="white"}

::: {#fig-pretrain-two style="text-align: left;"}

[![](https://jalammar.github.io/images/gpt3/10-gpt3-fine-tuning.gif)](http://jalammar.github.io/illustrated-transformer/){width="90%"}

**Fine-tuning**: Fine-tuning actually updates the model's weights to make the model better at a certain task.

:::

:::

# üîó Links  {background-color="white"}

::: {.flex-container}

::: {.col1}
- üè° [samforeman.me](https://samforeman.me):
  - ü¶ú [Talks](https://samforeman.me/talks/):
    - [HPC User Forum](https://samforeman.me/talks/hpc-user-forum/) \[[slides](https://samforeman.me/talks/hpc-user-forum/slides.html)\]
- See my other slides on:
    - [LLMs from Scratch](https://saforem2.github.io/llm-workshop-talk)
    - [Creating Small(\~ish) LLMs](https://saforem2.github.io/LLM-tutorial)
    - [Parallel Training Techniques](https://saforem2.github.io/parallel-training-slides) for additional details
    - [LLMs on Polaris](https://samforeman.me/talks/llms-on-polaris/#/title-slide)
    - [Training LLMs at Scale](https://samforeman.me/talks/llms-at-scale/)
:::

<!-- ## üìì References {background-color="white"} -->

::: {.col2}
- [üèéÔ∏è `Megatron-DeepSpeed`](https://github.com/argonne-lcf/Megatron-DeepSpeed)  
    [For the largest of large language models.]{.dim-text}
- [üçã `saforem2/ezpz`](https://github.com/saforem2/ezpz)  
    [Distributed training, ezpz.]{.dim-text}
- üëÄ See also:
  - [PyTorch Distributed Overview](https://pytorch.org/tutorials/beginner/dist_overview.html)
  - [Distributed Data Parallel ‚Äî PyTorch master documentation](https://pytorch.org/docs/master/notes/ddp.html)
  - [ü§ó Efficient Training on Multiple GPUs](https://huggingface.co/docs/transformers/en/perf_train_gpu_many)
  - [Getting Started - DeepSpeed](https://www.deepspeed.ai/getting-started/)

:::

:::

## ‚ù§Ô∏è  Thank you! {background-color="white"}

- Organizers
- Feel free to reach out!

    <split even>

    [<i class="fas fa-home"></i>](https://samforeman.me)
    [<i class="far fa-paper-plane"></i>](mailto:///foremans@anl.gov)
    [<i class="fab fa-twitter"></i>](https://www.twitter.com/saforem2)

    </split>

::: {.callout-note icon=false title="üôè Acknowledgements" collapse="false"}

This research used resources of the Argonne Leadership Computing Facility,
which is a DOE Office of Science User Facility supported under Contract
DE-AC02-06CH11357.

:::

## üìó Bibliography {background-color="white"}

- Refs:
    - @wei2022emergentabilitieslargelanguage
    - Animations from [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)

::: {#refs}
:::

# üç≠ Extras {background-color="white"}

## üçé Training LLMs {.smaller background-color="white"}

:::: {.flex-container style="align-items: flex-end;"}

::: {.col1 style="width:33%;"}

::: {#fig-it-hungers}

![](https://github.com/saforem2/llm-lunch-talk/blob/main/docs/assets/it_hungers.jpeg?raw=true)

It's hungry!
:::

:::

::: {.col2 style="width:60%;"}

::: {#fig-evolution}

![](https://github.com/Mooler0410/LLMsPracticalGuide/raw/main/imgs/survey-gif-test.gif)

Visualization from @yang2023harnessing

:::

:::

::::

## üöÇ Loooooooooong Sequence Lengths {.smaller background-color="#1c1c1c"}

::: {.flex-container style="text-align: center; align-items: center;"}

![](../../assets/anl.svg){style="width:48%;"}

[{{< iconify ph arrows-left-right >}}]{.dim-text style="font-size: 2.0em; padding-left: 15pt;"}

![](../../assets/deepspeed-logo-transparent.svg){style="width: 60%"}

:::

<!-- - Working with [{{< fa brands microsoft >}} Microsoft -->
<!-- DeepSpeed](https://github.com/microsoft/DeepSpeed) team to enable longer -->
<!-- sequence lengths (context windows) for LLMs -->

::: {#fig-long-seq}

::: {.flex-container}

![25B](https://raw.githubusercontent.com/saforem2/scaling4science/main/assets/25B.svg)

![33B](https://raw.githubusercontent.com/saforem2/scaling4science/main/assets/33B.svg)

:::

Maximum (achievable) `SEQ_LEN` for both `25B` and `33B` models (See: @song2023ds4sci)

:::

::: aside
[{{< fa brands github >}} `scaling4science`](https://github.com/saforem2/scaling4science)  
[{{< fa brands github >}} `Megatron-DS-Benchmarking`](https://github.com/saforem2/Megatron-DS-Benchmarking)
:::

## üíæ Evaluating Checkpoints

```python
from typing import Optional
import os
from pathlib import Path

from transformers import LlamaForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7B-hf")

def load_model(ckpt_dir) -> LlamaForCausalLM:
    return LlamaForCausalLM.from_pretrained(ckpt_dir)

def eval_model(model, max_length: int, prompt: str) -> str:
    return (
        tokenizer.batch_decode(
            model.generate(
                **tokenizer(prompt, return_tensors="pt"),
                 max_length=max_length,
            ),
            clean_up_tokenization_spaces=True,
            skip_special_tokens=True,
        )[0]
    )

def load_and_eval_model_from_checkpoint(
        step: int,
        max_length: int = 64,
        prompt: Optional[str] = None,
        ckpt_root: Optional[os.PathLike | Path | str] = None,
) -> str:
    print(f"Loading model from checkpoint at global step: {step}")
    prompt = "What is it like in there?" if prompt is None else prompt
    ckpt_root = Path("checkpoints") if ckpt_root is None else Path(ckpt_root)
    ckpt_dir = ckpt_root.joinpath(f"global_step{step}")
    return (
        eval_model(
            model=load_model(ckpt_dir.as_posix())
            max_length=max_length,
            prompt=prompt,
        )
    )
```

## üöÄ Model Evaluations

::: {.panel-tabset}

### 7000

Tokens: 88B

```python
>>> print(load_checkpoint(7000))
Loading model from checkpoint at global step: 7000
"What is it like in there?"
"""
I'm not sure if it's a good idea to use a different name for the same thing,
but I'm sure it's a good idea to use a different name for the same thing.
I'm not sure if it's a good idea to use a different name for the same thing,
but I'm sure it's a good idea to use a different name for the same thing.
I'm not sure if it's a good idea to use a different name for the same thing,
but I'm sure it
"""
```

### 12000

Tokens: 150B

```python
>>> print(load_checkpoint(12000))
Loading model from checkpoint at global step: 12000
"What is it like in there?"
"""
What is it like in there?
The people are very friendly and helpful.
What is it like in there?
The people are very friendly and helpful.
What is it like in there?
The people are very friendly and helpful.
What is it like in there?
The people are very friendly and helpful.
What is it like in there?
The people are very friendly and helpful.
What is it like in there?
"""
```

### 17000

Tokens: 215B

```python
>>> print(load_checkpoint(17000))
Loading model from checkpoint at global step: 17000
"What is it like in there?"
"""
I‚Äôm not sure what to expect. I‚Äôm not sure what to expect from the people I‚Äôm
with. I‚Äôm not sure what to expect from the people I‚Äôm with. I‚Äôm not sure what
to expect from the people I‚Äôm with. I‚Äôm not sure what to expect from the people
I‚Äôm with.
I‚Äôm not sure what to expect from the people I‚Äôm with.
I‚Äôm not sure what to expect from the people I‚Äôm with.
I‚Äôm not sure what to expect from the people
"""
```

### 22000

Tokens: 277B

```python
>>> print(load_checkpoint(22000))
Loading model from checkpoint at global step: 22000
"What is it like in there?"
"""
I‚Äôm a 20 year old guy from the UK. I‚Äôm a student at the University of
Manchester, studying Computer Science. I‚Äôm a big fan of the band, The Beatles,
and I‚Äôm a huge fan of the movie, The Wizard of Oz. I‚Äôm a huge fan of the band,
The Beatles, and I‚Äôm a huge fan of the movie, The Wizard of Oz.
I‚Äôm a big fan of the band, The Beatles, and I‚Äôm a huge fan of the movie
"""
```

### 32000

Tokens: 400B

```python
>>> print(load_checkpoint(32000))
Loading model from checkpoint at global step: 32000
"What is it like in there?"
"""
I've been to the US and I've been to Canada.
In the US, it's a lot like the US.
In Canada, it's a lot like the US.
In the US, it's a lot like the US.
In Canada, it's a lot like the US.
In the US, it's a lot like the US.
In Canada, it's a lot like the US.
In the US, it's
"""
```

### 40000

Tokens: 503B

```python
>>> print(load_checkpoint(40000))
Loading model from checkpoint at global step: 40000
"What is it like in there?"
"""
The first thing you notice when you enter the room is the size. It‚Äôs huge. It‚Äôs
like a football field. It‚Äôs a lot of space.
The second thing you notice is the light. It‚Äôs bright. It‚Äôs bright.
The third thing you notice is the sound. It‚Äôs loud. It‚Äôs loud.
The fourth thing you notice is the smell. It‚Äôs a lot of smells. It‚Äôs a lot of smells.
The fifth thing you notice is the temperature. It‚Äôs hot.
"""
```

:::

