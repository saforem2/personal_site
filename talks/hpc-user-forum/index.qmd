---
# title: "[Training LLMs at Scale](https://samforeman.me/talks/llms-at-scale/slides.html)"
title: "AuroraGPT"
# navbar: default
# sidebar: false
cookie-consent: true
location: "[HPC User Forum Fall '24](https://www.hpcuserforum.com/hpc-user-forum-fall-2024/)"
date: 2024-09-04
# fig-cap-location: top
# date-modified: last-modified
number-sections: false
bibliography: ../../references.bib
editor:
  render-on-save: true
  freeze: auto
twitter-card:
  image: ./assets/thumbnail.png
  site: "saforem2"
  creator: "saforem2"
  title: "AuroraGPT @ HPC User Forum"
  description: "Overview of AuroraGPT at ALCF"
  card-style: summary
open-graph:
  title: "AuroraGPT @ HPC User Forum"
  description: "Overview of AuroraGPT at ALCF"
  image: ./assets/thumbnail.png
citation:
   author: Sam Foreman
   type: speech
   url: https://samforeman.me/talks/hpc-user-forum/slides
# toc-expand: true
format:
  revealjs:
    footer: "[[samforeman.me/talks/hpc-user-forum/slides](https://samforeman.me/talks/hpc-user-forum/slides)]{.small}"
    slides-url: https://samforeman.me/talks/hpc-user-forum/slides.html
    template-partials:
      - title-slide.html
    title-slide-attributes:
      data-background-iframe: https://emilhvitfeldt.github.io/quarto-iframe-examples/center-of-universe/index.html
      data-background-size: contain
      # data-background-color: dark
      # background-color: dark
    shift-heading-level-by: -1
  html:
    shift-heading-level-by: -1
  gfm:
        output-file: "AuroraGPT.md"
---

## ğŸ“„ Overview {.smaller background-color="white"}

::: {.flex-container}

::: {.col1}
::: {.blue-card style="margin-bottom: 0.5em;"}
**AuroraGPT**: _General purpose scientific foundation model_
:::

- Model sizes:
    - {`7B`, `70B`, `1T`}

- {{< iconify fa github >}} [`Megatron-DeepSpeed`](https://github.com/argonne-lcf/Megatron-DeepSpeed)
    - âœ… {Intel, NVIDIA, AMD}

:::

::: {.col2 style="width:75%;"}
[![](https://github.com/Hannibal046/Awesome-LLM/raw/main/resources/image8.gif)](https://github.com/Hannibal046/Awesome-LLM/raw/main/resources/)
:::

:::

- Trained on:

  :::: {.flex-container style="flex-direction:row; justify-content: space-around;"}

  ::: {.flex-container style="flex-direction:column;"}

   ğŸ‡ºğŸ‡¸English  
   ğŸ‡¯ğŸ‡µæ—¥æœ¬èª[^riken]  
   ğŸ‡«ğŸ‡·French  
   ğŸ‡©ğŸ‡ªDeutsch  
   ğŸ‡ªğŸ‡¸EspaÃ±ol[^bsc]  
   ğŸ‡®ğŸ‡¹Italian  

  :::

  ::: {.flex-container style="flex-direction:column;"}

  ğŸ§ª scientific text  
  ğŸ–¼ï¸ images  
  ğŸ“Š tables  
  â• equations  
  ğŸ“– proofs

  :::

  ::: {.flex-container style="flex-direction:column;"}

  ğŸ“† structured data  
  â›“ï¸ sequences  
  â° time-series  
  ğŸ•¸ï¸ graphs  
  ğŸŒ€ fields

  :::

  ::::

[^riken]:|
    [Argonne and RIKEN sign a MOU in support of AI for science](https://www.anl.gov/article/argonne-and-riken-sign-a-memorandum-of-understanding-in-support-of-ai-for-science)

[^bsc]:|
    Collaborations with Barcelona Supercomputing Center

### ğŸ¯ Project Goals {background-color="white"}

::: {#fig-project-goals}

![](./assets/AuroraGPT.svg)

Overview of AuroraGPT Project
:::

### ğŸ‘¥ Teams {auto-animate=true background-color="white"}

::: {.flex-container}

::: {.col1}
- **Planning**
- **Data Prep**
    - Accumulate 20+ T tokens of high-quality scientific text and structured data
- [**Models / Training**]{style="background: oklch(from #ff1a8f calc(l * 1.15) c h / 0.1); border: 1px solid #ff1a8f; border-radius: 0.25px;"}[^me]
    - Train (entirely from scratch) a series of models on publicly available data
- **Evaluation**
    - Skills, trustworthiness, safety, robustness, privacy, machine ethics

[^me]: Co-led by: Venkat Vishwanath, Sam Foreman
:::

::: {.col2}
- **Post-Training**
    - Fine-tuning, alignment
- **Inference**
    - Model serving, API development / public-facing web services
- **Distribution**
    - Licensing, generating and distributing artifacts for public consumption
- **Communication**
:::

:::

### ğŸ¤– Aurora[^aurora] {background-color="white"}

:::: {.flex-container}

::: {.col1 style="width:30%;"}

- 166 Racks
- 10,624 Nodes
- 21,248 CPUs
- 63,744 GPUs
- 84,992 NICs
- 8 PB HBM
- 10 PB DDR5c

:::

::: {.col2 style="width:70%;"}

![](./assets/aurora.png)

:::

::::

[^aurora]:|
    [The Computer That Will Change Everything â€“ Chicago Magazine](https://www.chicagomag.com/chicago-magazine/february-2023/the-computer-that-will-change-everything/)

## ğŸ¦™ LLMs {background-color="white"}

### ğŸ¦œ Model Training {background-color="white"}

:::: {.flex-container style="text-align: left; width: 100%; justify-content: center; line-height: 1em;"}

::: {.col1 width="49%" style="background: oklch(from #03BD00 calc(l * 1.15) c h / 0.1); border: 1px solid #03BD00; border-radius: 0.25em; padding: 3pt 8pt; margin-right: 1%"}

âœ… [**Goals**]{style="color: #03BD00;"}

- Want training runs _at scale_ to be:
    - Efficient
    - Stable
    - Reproducible

- This requires:
    - Robust data pipelines / file IO
    - Effectively overlapping compute with communication
    - Stability across {network, filesystem, machine}

- For larger models:
    - Multi-dimensional parallelism strategies

:::

::: {.col2 width="49%" style="background: oklch(from #E90102 calc(l * 1.15) c h / 0.1); border: 1px solid #E90102; border-radius: 0.25em; padding: 3pt 8pt; margin-left: 1%;"}

âŒ [**Difficulties**]{style="color: #E90102;"}

- _Looong time_ to train

- Stability issues
    - Failures are **expensive** [(and unavoidable)]{.dim-text}
    - stragglers common at scale

- Individual jobs are:
    - **fragile**
    - only as good as the worst rank
    - one hang or bad worker can crash job
    - network / filesystem / other-user(s) dependent

:::

::::

### ğŸ“Š Data Pipeline {background-color="white"}

::: {#fig-data-pipeline}

::: {.flex-container}

![Time spent building `BlendableDataset`](./assets/blendable.svg)

![Time spent building `GPTDataset`](./assets/gpt.svg)

:::

Complete re-write [(parallel, async)]{.dim-text} of original data input pipeline gives _significant_ improvements

:::

### ğŸš€ Training at Scale {background-color="white"}

::: {.flex-container}

::: {.col1}
- 3D Parallelism
- Highly optimized GPU kernels
- Network performance
- Cost / benefits of different collective communication algorithms
    - depend on optimized / efficient implementations
:::

::: {.col2}
- Large batch training
- Second order optimizers
- State space models
- Sub-quadratic attention (?)
:::

:::

### â™»ï¸  Life Cycle of the LLM {background-color="white"}

::: {.panel-tabset style="text-align:center"}

#### ğŸ“ Pre-training {background-color="white"}


::: {#fig-pretraining style="width:90%; text-align: center; margin-left: auto; margin-right: auto;"}

![](https://jalammar.github.io/images/gpt3/03-gpt3-training-step-back-prop.gif)

**Pre-training**: Virtually all of the compute used during pretraining phase
:::

#### ğŸ€ Fine-Tuning {background-color="white"}

::: {#fig-fine-tuning style="width:90%; text-align: center; margin-left: auto; margin-right: auto;"}

![](https://jalammar.github.io/images/gpt3/10-gpt3-fine-tuning.gif)

**Fine-tuning**: Fine-tuning actually updates the model's weights to make the model better at a certain task.
:::

:::

## ğŸ”— Links  {background-color="white"}

::: {.flex-container}

::: {.col1}
- ğŸ¡ [samforeman.me](https://samforeman.me):
  - ğŸ¦œ [Talks](https://samforeman.me/talks/):
    - [HPC User Forum](https://samforeman.me/talks/hpc-user-forum/) \[[slides](https://samforeman.me/talks/hpc-user-forum/slides.html)\]
- See my other slides on:
    - [LLMs from Scratch](https://saforem2.github.io/llm-workshop-talk)
    - [Creating Small(\~ish) LLMs](https://saforem2.github.io/LLM-tutorial)
    - [Parallel Training Techniques](https://saforem2.github.io/parallel-training-slides) for additional details
    - [LLMs on Polaris](https://samforeman.me/talks/llms-on-polaris/#/title-slide)
    - [Training LLMs at Scale](https://samforeman.me/talks/llms-at-scale/)
:::

<!-- ## ğŸ““ References {background-color="white"} -->

::: {.col2}
- [ğŸï¸ `Megatron-DeepSpeed`](https://github.com/argonne-lcf/Megatron-DeepSpeed)  
    [For the largest of large language models.]{.dim-text}
- [ğŸ‹ `saforem2/ezpz`](https://github.com/saforem2/ezpz)  
    [Distributed training, ezpz.]{.dim-text}
- ğŸ‘€ See also:
    - [New international consortium for generative AI models for science](https://www.anl.gov/article/new-international-consortium-formed-to-create-trustworthy-and-reliable-generative-ai-models-for)
    - [PyTorch Distributed Overview](https://pytorch.org/tutorials/beginner/dist_overview.html)
    - [Distributed Data Parallel â€” PyTorch master documentation](https://pytorch.org/docs/master/notes/ddp.html)
    - [ğŸ¤— Efficient Training on Multiple GPUs](https://huggingface.co/docs/transformers/en/perf_train_gpu_many)
    - [Getting Started - DeepSpeed](https://www.deepspeed.ai/getting-started/)

:::

:::

### â¤ï¸  Thank you! {background-color="white"}

- Organizers
- Feel free to reach out!

    <split even>

    [<i class="fas fa-home"></i>](https://samforeman.me)
    [<i class="far fa-paper-plane"></i>](mailto:///foremans@anl.gov)
    [<i class="fab fa-twitter"></i>](https://www.twitter.com/saforem2)

    </split>

::: {.callout-note icon=false title="ğŸ™ Acknowledgements" collapse="false"}

This research used resources of the Argonne Leadership Computing Facility,
which is a DOE Office of Science User Facility supported under Contract
DE-AC02-06CH11357.

:::

### ğŸ“— Bibliography {background-color="white"}

- Refs:
    - @wei2022emergentabilitieslargelanguage
    - Animations from [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)

::: {#refs}
:::

## ğŸ Extras {background-color="white"}

### ğŸ” Details {.smaller background-color="white"}

- Llama Style Architecture:
    - [{{< iconify ph github-logo-duotone >}} `argonne-lcf/Megatron-DeepSpeed`](https://github.com/argonne-lcf/Megatron-DeepSpeed)
- Performant training implementations of AuroraGPT architecture on
    - {Aurora, Polaris, Cerebras, SambaNova}
- Workflow to capture:
    - loss curves
    - snapshots, checkpoints
    - scaling / performance data
- Training runs for AuroraGPT-7B [(ongoing)]{.dim-text}
    - Baseline (Dolma) @ {Aurora, Polaris} [(twins for debugging)]{.dim-text}
    - Baseline + Science @ Aurora
- Trained raw models ğŸ“® delivered to post-pretraining team
    - AuroraGPT-7B-A, AuroraGPT-7B-P, AuroraGPT-7B-S  
      (A=`Aurora`, P=`Polaris`, S=`Science`)

### ğŸ¤” Why? {.smaller background-color="white"}

- **For Science!**
- Data-{sets, pipelines} for {preparing, aggregating, parsing, analyzing} scientific data
- Infrastructure to {train, eval, deploy} LLMs for science
    - Comparative analysis across: {models, tasks, languages, contexts, ...}
- Augment text data from the web with:
    - full text papers
    - structured scientific data[^tough]
- Safety-driven, publicly-visible, open-source approach:
    - Distribution of research grade artifacts (models, checkpoints, etc.)
    - International collaborations on AGI for science

[^tough]: Can be much more difficult than text (or even image) data


### ğŸ Training LLMs {.smaller background-color="white"}

:::: {.flex-container style="align-items: flex-end;"}

::: {.col1 style="width:33%;"}

::: {#fig-it-hungers}

![](https://github.com/saforem2/llm-lunch-talk/blob/main/docs/assets/it_hungers.jpeg?raw=true)

It's hungry!
:::

:::

::: {.col2 style="width:60%;"}

::: {#fig-evolution}

![](https://github.com/Mooler0410/LLMsPracticalGuide/raw/main/imgs/survey-gif-test.gif)

Visualization from @yang2023harnessing

:::

:::

::::

### ğŸš‚ Loooooooooong Sequence Lengths {.smaller background-color="#1c1c1c"}

::: {.flex-container style="text-align: center; align-items: center;"}

![](../../assets/anl.svg){style="width:48%;"}

[{{< iconify ph arrows-left-right >}}]{.dim-text style="font-size: 2.0em; padding-left: 15pt;"}

![](../../assets/deepspeed-logo-transparent.svg){style="width: 60%"}

:::

<!-- - Working with [{{< fa brands microsoft >}} Microsoft -->
<!-- DeepSpeed](https://github.com/microsoft/DeepSpeed) team to enable longer -->
<!-- sequence lengths (context windows) for LLMs -->

::: {#fig-long-seq}

::: {.flex-container}

![25B](https://raw.githubusercontent.com/saforem2/scaling4science/main/assets/25B.svg)

![33B](https://raw.githubusercontent.com/saforem2/scaling4science/main/assets/33B.svg)

:::

Maximum (achievable) `SEQ_LEN` for both `25B` and `33B` models (See: @song2023ds4sci)

:::

::: aside
[{{< fa brands github >}} `scaling4science`](https://github.com/saforem2/scaling4science)  
[{{< fa brands github >}} `Megatron-DS-Benchmarking`](https://github.com/saforem2/Megatron-DS-Benchmarking)
:::

### ğŸ’¾ Evaluating Checkpoints {background-color="white"}

```python
from typing import Optional
import os
from pathlib import Path

from transformers import LlamaForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7B-hf")

def load_model(ckpt_dir) -> LlamaForCausalLM:
    return LlamaForCausalLM.from_pretrained(ckpt_dir)

def eval_model(model, max_length: int, prompt: str) -> str:
    return (
        tokenizer.batch_decode(
            model.generate(
                **tokenizer(prompt, return_tensors="pt"),
                 max_length=max_length,
            ),
            clean_up_tokenization_spaces=True,
            skip_special_tokens=True,
        )[0]
    )

def load_and_eval_model_from_checkpoint(
        step: int,
        max_length: int = 64,
        prompt: Optional[str] = None,
        ckpt_root: Optional[os.PathLike | Path | str] = None,
) -> str:
    print(f"Loading model from checkpoint at global step: {step}")
    prompt = "What is it like in there?" if prompt is None else prompt
    ckpt_root = Path("checkpoints") if ckpt_root is None else Path(ckpt_root)
    ckpt_dir = ckpt_root.joinpath(f"global_step{step}")
    return (
        eval_model(
            model=load_model(ckpt_dir.as_posix())
            max_length=max_length,
            prompt=prompt,
        )
    )
```

### ğŸš€ Model Evaluations {background-color="white"}

::: {.panel-tabset}

#### 7000

Tokens: 88B

```python
>>> print(load_checkpoint(7000))
Loading model from checkpoint at global step: 7000
"What is it like in there?"
"""
I'm not sure if it's a good idea to use a different name for the same thing,
but I'm sure it's a good idea to use a different name for the same thing.
I'm not sure if it's a good idea to use a different name for the same thing,
but I'm sure it's a good idea to use a different name for the same thing.
I'm not sure if it's a good idea to use a different name for the same thing,
but I'm sure it
"""
```

#### 12000

Tokens: 150B

```python
>>> print(load_checkpoint(12000))
Loading model from checkpoint at global step: 12000
"What is it like in there?"
"""
What is it like in there?
The people are very friendly and helpful.
What is it like in there?
The people are very friendly and helpful.
What is it like in there?
The people are very friendly and helpful.
What is it like in there?
The people are very friendly and helpful.
What is it like in there?
The people are very friendly and helpful.
What is it like in there?
"""
```

#### 17000

Tokens: 215B

```python
>>> print(load_checkpoint(17000))
Loading model from checkpoint at global step: 17000
"What is it like in there?"
"""
Iâ€™m not sure what to expect. Iâ€™m not sure what to expect from the people Iâ€™m
with. Iâ€™m not sure what to expect from the people Iâ€™m with. Iâ€™m not sure what
to expect from the people Iâ€™m with. Iâ€™m not sure what to expect from the people
Iâ€™m with.
Iâ€™m not sure what to expect from the people Iâ€™m with.
Iâ€™m not sure what to expect from the people Iâ€™m with.
Iâ€™m not sure what to expect from the people
"""
```

#### 22000

Tokens: 277B

```python
>>> print(load_checkpoint(22000))
Loading model from checkpoint at global step: 22000
"What is it like in there?"
"""
Iâ€™m a 20 year old guy from the UK. Iâ€™m a student at the University of
Manchester, studying Computer Science. Iâ€™m a big fan of the band, The Beatles,
and Iâ€™m a huge fan of the movie, The Wizard of Oz. Iâ€™m a huge fan of the band,
The Beatles, and Iâ€™m a huge fan of the movie, The Wizard of Oz.
Iâ€™m a big fan of the band, The Beatles, and Iâ€™m a huge fan of the movie
"""
```

#### 32000

Tokens: 400B

```python
>>> print(load_checkpoint(32000))
Loading model from checkpoint at global step: 32000
"What is it like in there?"
"""
I've been to the US and I've been to Canada.
In the US, it's a lot like the US.
In Canada, it's a lot like the US.
In the US, it's a lot like the US.
In Canada, it's a lot like the US.
In the US, it's a lot like the US.
In Canada, it's a lot like the US.
In the US, it's
"""
```

#### 40000

Tokens: 503B

```python
>>> print(load_checkpoint(40000))
Loading model from checkpoint at global step: 40000
"What is it like in there?"
"""
The first thing you notice when you enter the room is the size. Itâ€™s huge. Itâ€™s
like a football field. Itâ€™s a lot of space.
The second thing you notice is the light. Itâ€™s bright. Itâ€™s bright.
The third thing you notice is the sound. Itâ€™s loud. Itâ€™s loud.
The fourth thing you notice is the smell. Itâ€™s a lot of smells. Itâ€™s a lot of smells.
The fifth thing you notice is the temperature. Itâ€™s hot.
"""
```

:::
