---
title: "Training LLMs at Scale"
toc: false
date: today
date-modified: last-modified
author-title: ""
affiliation-title: ""
published-title: ""
modified-title: ""
title-block-categories: false
number-sections: false
bibliography: ../../references.bib
appendix-cite-as: display
# keep-md: false
editor:
  render-on-save: true
twitter-card:
  image: assets/thumbnail.png
  # image: "https://raw.githubusercontent.com/saforem2/personal_site/main/talks/llms-on-polaris/assets/thumbnail.png"
  site: "saforem2"
  creator: "@saforem2"
  title: "Training LLMs at Scale"
  description: "Training LLMs at Scale"
  card-style: summary
open-graph:
  title: "Training LLMs at Scale"
  description: "Training LLMs at Scale"
  image: assets/thumbnail.png
  # image: "/Users/samforeman/Desktop/ScreenShot-2024-08-10-121608.png"
author:
  name: Sam Foreman
  url: https://samforeman.me
  orcid: 0000-0002-9981-0876
  email: foremans@anl.gov
  affiliation: Argonne National Laboratory
  affiliation-url: https://alcf.anl.gov/about/people/sam-foreman
citation:
   author: Sam Foreman
   type: speech
   # genre: "Presentation at the 2023 International Symposium on Lattice Field Theory"
   # container-title: https://indico.fnal.gov/event/57249/contributions/271305/
   # title: "MLMC: Machine Learning Monte Carlo for Lattice Gauge Theory"
   url: https://samforeman.me/talks/llms-at-scale
   # abstract: |
   #   We present a trainable framework for efficiently generating gauge
   #   configurations, and discuss ongoing work in this direction. In particular, we
   #   consider the problem of sampling configurations from a 4D ùëÜùëà(3) lattice gauge
   #   theory, and consider a generalized leapfrog integrator in the molecular
   #   dynamics update that can be trained to improve sampling efficiency.
citations-hover: true
footnotes-hover: true
# filters:
#   - roughnotation
format:
  revealjs:
    # reference-location: section
    navigation-mode: linear
    # output-file: "index.revealjs.html"
    code-line-numbers: true
    code-link: false
    code-copy: false
    # syntax-definitions:
    #   - ./docs/python.xml
    scrollable: true
    title-block-style: none
    slide-number: c
    title-slide-style: default
    chalkboard:
      buttons: false
    auto-animate: true
    touch: true
    link-external-newwindow: true
    pause: false
    footnotes-hover: true
    citations-hover: true
    preview-links: auto
    controls-tutorial: true
    controls: false
    logo: "https://raw.githubusercontent.com/saforem2/llm-lunch-talk/main/docs/assets/anl.svg"
    history: false
    highlight-style: "atom-one"
    # theme: [css/dark.scss]
    # callout-style: default
    css:
      # - ../../css/text.css
      # - ../../css/default.css
      - ../../css/custom.css
      # - ../../css/reveal/_callouts.scss
      # - ../../css/reveal/light/default.css
      # - ../../css/reveal/light/callouts.css
    theme:
      # - ../../css/reveal/light/light.scss
      # - white
      - ../../css/reveal/reveal.scss
      - ../../css/common.scss
      - ../../css/light.scss
      - ../../css/syntax-light.scss
      - ../../css/callout-cards.scss
    # css:
    #   - ./theme/light/default.css
    #   - ./theme/light/reset.css
    # theme:
    #   - default
    #   - ./theme/light/common.scss
    #   - ./theme/light/light.scss
    #   - ./theme/light/syntax-light.scss
    #   - ./theme/light/reveal.scss
    # css:
    #   - ./theme/dark/default.css
    #   - ./theme/dark/callouts.css
    # theme: dark
    self-contained: false
    embed-resources: false
    self-contained-math: false
    center: true
    # background-color: #ffffff
    default-image-extension: svg
    code-overflow: scroll
    html-math-method: katex
    fig-align: center
    mermaid:
      theme: neutral
  # gfm:
  #   output-file: "llms-at-scale.md"
---

# {.centeredslide background-color="#FFFFFF" background-iframe="https://emilhvitfeldt.github.io/quarto-iframe-examples/colored-particles/index.html" loading="lazy" background-color="#FFFFFF"}

::: {style="background-color: rgba(245,245,245, 0.875); border-radius: 10px; text-align:center; padding: 0px; padding-left: 1.5em; padding-right: 1.5em; max-width: min-content; min-width: max-content; margin-left: auto; margin-right: auto; padding-top: 0.2em; padding-bottom: 0.2em; line-height: 1.5em!important;"}
<!-- ::: {style="text-align:center; padding: 0px; padding-left: 1.5em; padding-right: 1.5em; max-width: min-content; min-width: max-content; margin-left: auto; margin-right: auto; padding-top: 0.2em; padding-bottom: 0.2em; line-height: 1.5em!important; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.5);"} -->
<!-- ::: {style="text-align:center; padding: 0px; padding-left: 1.5em; padding-right: 1.5em; max-width: min-content; min-width: max-content; margin-left: auto; margin-right: auto; padding-top: 0.2em; padding-bottom: 0.2em; line-height: 1.5em!important;  text-shadow: -1px -1px 1px rgba(255, 255, 255, 0.2), 1px 1px 1px rgba(0, 0, 0, 0.6);"} -->
[Training LLMs]{style="color:#333333; font-size:1.75em; font-weight: 600; padding-bottom: 0.25rem;"}  
[(at Scale)]{style="color:#444444; font-size:1.5em; font-weight: 500; padding-bottom: 1rem;"}  

<!-- ::: -->
<!-- [<br>&nbsp;]{style="padding-bottom: 0.5rem;"}   -->
<!-- <br>&nbsp; -->
<!-- [<br>&nbsp;]{style="padding-bottom: 0.5rem;"}   -->
<!-- " text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.5);"} -->
<!-- ::: {style="background-color: rgba(245,245,245, 0.875); border-radius: 10px; text-align:center; padding: 0px; padding-left: 1.5em; padding-right: 1.5em; max-width: min-content; min-width: max-content; margin-left: auto; margin-right: auto; padding-top: 0.2em; padding-bottom: 0.2em; line-height: 1.5em!important;"} -->
[[üè°](https://samforeman.me) Sam Foreman]{style="color:#555555"}  
[@ [ATPESC 2024](https://extremecomputingtraining.anl.gov/agenda-2024/)]{.dim-text style="font-size: 0.75em"}  
[üóìÔ∏è 2024-08-09]{.dim-text style="font-size: 0.66em;"}  
:::

&nbsp;<br>  

<!-- ::: {style="background-color: none; opacity:0.75; border-radius: 10px; text-align:center; padding: 0px; padding-left: 1.5em; padding-right: 1.5em; max-width: min-content; min-width: max-content; margin-left: auto; margin-right: auto; padding-top: 0.2em; padding-bottom: 0.2em; line-height: 1.5em!important;"} -->
<!-- [[samforeman.me/talks/llms-at-scale](https://samforeman.me/talks/llms-at-scale)]{.dim-text style="font-size:0.6em;"}   -->
<!-- ::: -->

::: footer
::: {style="display: flex; flex-direction: row; align-items: center; text-align: center!important; justify-content: center;"}
[[[{{< iconify ph house-line-duotone >}}]{.icon style="background-color:rgba(0,0,0,0.0);!important"}](https://samforeman.me)]{.icon style="font-size:1.5rem; padding-right: 0pt;"}
[[{{< iconify ph github-logo-duotone >}}](https://github.com/saforem2)]{.icon style="font-size:1.5rem; padding-left: 0pt; padding-right: 0pt"}
[[{{< iconify ph twitter-logo-duotone >}}](https://www.twitter.com/saforem2)]{.icon style="font-size:1.5rem; padding-left: 0pt; padding-right: 0pt"}
[[{{< iconify ph envelope-open-duotone >}}](mailto:///foremans@anl.gov)]{.icon style="font-size:1.5rem; padding-left: 0pt; padding-right: 0pt"}
[[{{< iconify ph graduation-cap-duotone >}}](https://scholar.google.com/citations?user=vV_1zDwAAAAJ&hl=en)]{.icon style="font-size:1.5rem; padding-left: 0pt; padding-right: 0pt"}
[[{{< iconify ph spotify-logo-duotone >}}](https://open.spotify.com/user/saforem2)]{.icon style="font-size:1.5rem; padding-left: 0pt; padding-right: 0pt"}
[[{{< iconify ph lastfm-logo-duotone >}}](https://www.last.fm/user/saforem2)]{.icon style="font-size:1.5rem; padding-left: 0pt; padding-right: 0pt"}
[[{{< iconify ph linkedin-logo-duotone >}}](https://linkedin.com/in/saforem2)]{.icon style="font-size:1.5rem; padding-left: 0pt; padding-right: 0pt"}
:::
[[samforeman.me/talks/llms-at-scale]{style="color:#137af5!important; background:oklch(from #137af5 calc(l * 1.15) c h / 0.085); border:0px solid #137af5!important; border-radius: 0.25rem; text-shadow: 1px 1px rgba(0, 0, 0, 0.25);"}](https://samforeman.me/talks/llms-at-scale)  
:::

{{< include partials/_about.qmd >}}

{{< include partials/_parallelism.qmd >}}

{{< include partials/_collectives.qmd >}}

{{< include partials/_llms.qmd >}}

{{< include partials/_ezpz.qmd >}}

{{< include partials/_wordplay.qmd >}}

{{< include partials/_acknowledgements.qmd >}}

{{< include partials/_extras.qmd >}}

# References

- üîó See also:
  - [PyTorch Distributed Overview](https://pytorch.org/tutorials/beginner/dist_overview.html)
  - [Distributed Data Parallel ‚Äî PyTorch master documentation](https://pytorch.org/docs/master/notes/ddp.html)
  - [ü§ó Efficient Training on Multiple GPUs](https://huggingface.co/docs/transformers/en/perf_train_gpu_many)
  - [Getting Started - DeepSpeed](https://www.deepspeed.ai/getting-started/)

- See my slides on:
  - [Parallel Training Techniques](https://saforem2.github.io/parallel-training-slides) for additional details
  - [{{< fa brands github >}} `saforem2/llm-lunch-talk`](https://github.com/Hannibal046/Awesome-LLM) [(slides)](https://saforem2.github.io/llm-lunch-talk)

## Bibliography

::: {#refs}
:::

