---
title: "Training LLMs at Scale"
location: '[ATPESC 2024](https://extremecomputingtraining.anl.gov/agenda-2024/)'
date: 2024-08-09
number-sections: false
bibliography: ../../references.bib
image: assets/thumbnail.png
editor:
  render-on-save: true
twitter-card:
  image: assets/thumbnail.png
  site: "saforem2"
  creator: "saforem2"
  title: "Training LLMs at Scale"
  description: "Training LLMs at Scale"
  card-style: summary
open-graph:
  title: "Training LLMs at Scale"
  description: "Training LLMs at Scale"
  image: assets/thumbnail.png
citation:
   author: Sam Foreman
   type: speech
   url: https://samforeman.me/talks/llms-at-scale
format:
  html:
    # reference-location: section
    # toc-location: right
    page-layout: full
    # grid:
    #   body-width: 800px
  revealjs:
    slides-url: https://samforeman.me/talks/llms-at-scale/slides.html
    template-partials:
      - ./title-slide.html
      # - ./title-fancy/title-slide.html
      # - ./title_slide_template.html
      # - ../../title-slide.html
    title-slide-attributes:
      data-background-iframe: https://emilhvitfeldt.github.io/quarto-iframe-examples/colored-particles/index.html
      data-background-size: contain
      data-background-color: white
      background-color: white
    mermaid:
      theme: neutral
    scrollable: true
    background-color: white
    output-file: "slides.html"
    navigation-mode: linear
    # title-block-style: none
    slide-number: c
    # title-slide-style: default
    chalkboard:
      buttons: false
    auto-animate: true
    # reference-location: section
    touch: true
    pause: false
    footnotes-hover: true
    citations-hover: true
    preview-links: true
    controls-tutorial: true
    controls: false
    logo: "/assets/anl.svg"
    history: false
    highlight-style: "atom-one"
    center: true
    default-image-extension: svg
    code-overflow: scroll
    html-math-method: katex
    fig-align: center
    css:
      # - css/default.css
      - ../../css/custom.css
    theme:
      # - light:
      - white
      - ../../css/title-slide-template.scss
      - ../../css/reveal/reveal.scss
      - ../../css/common.scss
      - ../../css/light.scss
      - ../../css/syntax-light.scss
      - ../../css/callout-cards.scss
      # - dark:
      #   - black
      #   - ./title-fancy/title-slide-template.scss
      #   - ../../css/reveal/reveal.scss
      #   - ../../css/common.scss
      #   - ../../css/dark.scss
      #   - ../../css/syntax-dark.scss
      #   - ../../css/callout-cards.scss
    # theme: [title-fancy/title-slide-template.scss]
    callout-style: simple
    # css: [css/default.css, css/callouts.css]
---

## üîó Links  {background-color="white"}

- üè° [samforeman.me](https://samforeman.me):

  - ü¶ú [Talks](https://samforeman.me/talks/):
    - [Training LLMs at Scale](https://samforeman.me/talks/llms-at-scale/) \[[slides](https://samforeman.me/talks/llms-at-scale/slides.html)\]

  - üì¶ [Repos](https://github.com/saforem2/):
    - [üçã `saforem2/ezpz`](https://github.com/saforem2/ezpz)  
      [Train your model across any number of arbitrary devices, ezpz.]{.dim-text}
    - [üí¨ `saforem2/wordplay`](https://github.com/saforem2/wordplay)  
      [Playing with words.]{.dim-text}
    - [üèéÔ∏è `argonne-lcf/Megatron-DeepSpeed`](https://github.com/argonne-lcf/Megatron-DeepSpeed)  
      [For only the largest of large language models.]{.dim-text}
    <!-- - \[[slides](https://samforeman.me/talks/llms-at-scale/slides.html)\] -->

{{< include partials/_about.qmd >}}

{{< include partials/_parallelism.qmd >}}

{{< include partials/_collectives.qmd >}}

{{< include partials/_llms.qmd >}}

{{< include partials/_ezpz.qmd >}}

{{< include partials/_wordplay.qmd >}}

{{< include partials/_acknowledgements.qmd >}}

{{< include partials/_extras.qmd >}}

# References {background-color="white"}

- üîó See also:
  - [PyTorch Distributed Overview](https://pytorch.org/tutorials/beginner/dist_overview.html)
  - [Distributed Data Parallel ‚Äî PyTorch master documentation](https://pytorch.org/docs/master/notes/ddp.html)
  - [ü§ó Efficient Training on Multiple GPUs](https://huggingface.co/docs/transformers/en/perf_train_gpu_many)
  - [Getting Started - DeepSpeed](https://www.deepspeed.ai/getting-started/)

- See my slides on:
  - [Parallel Training Techniques](https://saforem2.github.io/parallel-training-slides) for additional details
  - [{{< fa brands github >}} `saforem2/llm-lunch-talk`](https://github.com/Hannibal046/Awesome-LLM) [(slides)](https://saforem2.github.io/llm-lunch-talk)

## Bibliography {background-color="white"}

::: {#refs}
:::
