::: {.flex-container style="width: 100%; justify-content: space-between; align-items: flex-start;"}

::: {.column}

- [Computational scientist](https://alcf.anl.gov/about/people/sam-foreman) @ Argonne National Laboratory
    - AI / ML [Group](https://www.alcf.anl.gov/about/people/group/506) @ [ALCF](https://alcf.anl.gov)
- Working on:
    - üß™ {AI, HPC} for [science](https://scholar.google.com/citations?view_op=list_works&hl=en&hl=en&user=vV_1zDwAAAAJ)
    - üöÄ [training large models](https://samforeman.me/talks/AuroraGPT/alcf-hpc-workshop-2024/slides.html) on [supercomputers](https://www.alcf.anl.gov/aurora)

<!--[^mprot]: [MProt-DPO: Breaking the ExaFLOPS Barrier for Multimodal Protein Design Workflows with Direct Preference Optimization](https://dl.acm.org/doi/abs/10.1109/SC41406.2024.00013)-->
<!--[^lqcd]: [Deep Learning Hamiltonian Monte Carlo](https://arxiv.org/abs/2105.03418)-->

:::


::: {.column}

::: {.callout-tip icon=false aria-title="Recent Talks" title='üé§ [Recent Talks]{.dim-text style="font-size:1.0em!important;"}' collapse="false" style="text-align: left!important; width: 100%; background-color:rgba(131,131,131,0.05)!important; opacity:100%;"}
[\[[here](talks/index.qmd)\] ( \+ how I [make them](./posts/dope-slides/index.qmd)! )]{.dim-text style="font-size:1em;"}
:::

<!-- ::: {.callout-tip icon=false aria-title="Bumpin' that" title='<img src="/assets/spotify-green.svg" loading="lazy" style="height:1.25rem; background-color: oklch(from #1CD760 calc(l * 1.15) c h / 0.11)!important; opacity: 100% width: -webkit-fill-available!important;margin-top: 0.5rem;">[Bumpin that]{style="color:#1CD760;"}' collapse="true" style='background-color: oklch(from #1CD760 calc(l * 1.15) c h / 0.11)!important; opacity: 100% width: -webkit-fill-available!important;margin-top: 0.5rem;'} -->
<!-- ::: {.callout-tip icon=false aria-title="Bumpin' that" title='[![](/assets/spotify-green.svg){.inline-icon style="height:1.25rem;vertical-align:text-bottom;"} Bumpin' that]{style="color:#1CD760;"}' collapse="true" style='background-color: oklch(from #1CD760 calc(l * 1.15) c h / 0.11)!important; opacity: 100% width: -webkit-fill-available!important;margin-top: 0.5rem;'} -->

::: {.callout-tip icon=false aria-title="Now Playing" title='[![](/assets/spotify-green.svg){.inline-icon style="height:1.25rem;vertical-align:text-bottom;"} Now Playing]{style="color:#1CD760;"}' collapse="true" style='width:100%; border: none!important; border: 1px solid rgba(28, 215, 96, 0.05);background-color: oklch(from #1CD760 calc(l * 1.15) c h / 0.11)!important; opacity: 100%;'}

<!-- <a href="https://open.spotify.com/user/saforem2" target="_blank"><img loading="lazy" src="https://spotify-github-profile.kittinanx.com/api/view?uid=saforem2&cover_image=true&theme=novatorem&show_offline=false&background_color=none&interchange=true" alt="Now Playing" /></a> -->
<a href="https://open.spotify.com/user/saforem2" target="_blank"><img loading="lazy" src="https://spotify-github-profile.kittinanx.com/api/view?uid=saforem2&cover_image=true&theme=natemoo-re&show_offline=false&background_color=none&interchange=true" alt="Now Playing" /></a>

::: {.callout-tip collapse="true" icon=false aria-title="last.fm" title='[[![](https://api.iconify.design/logos:lastfm.svg?color=%23888888)](https://last.fm/user/saforem2)]{style="color:#D41109;"}' collapse="true" style='border: none!important; border: 1px solid rgba(212, 17, 9, 0.0)!important; background-color: oklch(from #D41109 calc(l * 1.15) c h / 0.11)!important; opacity: 100% width: 100%!important;'}

{{< include about/_lastfm.qmd >}}

:::
:::

:::

:::

<!---->
<!--     - <details closed><summary>üìç How I got here</summary> -->
<!---->
<!--       [NOTE: Update the **NEW** text below !!]: # -->
<!---->
<!--       My [current research](https://saforem2.github.io/l2hmc-qcd) focuses on -->
<!--       using deep generative modeling to help build better sampling algorithms -->
<!--       in lattice gauge theory. In particular, I'm interested in building gauge -->
<!--       equivariant neural network architectures and using inductive priors to -->
<!--       incorporate physical symmetries into machine learning models. -->
<!---->
<!--       I received my PhD in Physics from the University of Iowa in 2019 and my thesis -->
<!--       was on [Learning Better Physics: A Machine Learning Approach to Lattice Gauge -->
<!--       Theory](https://iro.uiowa.edu/esploro/outputs/doctoral/Learning-better-physics-a-machine-learning/9983776792002771). -->
<!---->
<!--       Prior to this, I completed two bachelors degrees (Engineering Physics and -->
<!--       Applied Mathematics, 2015) at The University of Illinois at -->
<!--       Urbana-Champaign. My undergraduate dissertation was titled [Energy -->
<!--       Storage in Quantum -->
<!--       Resonators](https://aip.scitation.org/doi/10.1063/1.5009698) and was -->
<!--       supervised by Professor [Alfred -->
<!--       H√ºbler](https://en.wikipedia.org/wiki/Alfred_H%C3%BCbler) within the -->
<!--       Center for Complex Systems Research at UIUC[^patent]. -->
<!---->
<!--       [^patent]: And resulted in a [patent](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=vV_1zDwAAAAJ&pagesize=80&citation_for_view=vV_1zDwAAAAJ:SeFeTyx0c_EC) !! -->
<!---->
<!--     </details> -->
<!---->
<!--     - <details closed><summary>ü§î what I work on</summary> -->
<!---->
<!--       As a member of the [AI / ML Group](https://www.alcf.anl.gov/about/people/group/506) at -->
<!--       [ALCF](https://alcf.anl.gov), I work on: -->
<!---->
<!--       ::: {.flex-container} -->
<!---->
<!--       ::: {.flex-container} -->
<!---->
<!--       - [AI + Science](https://github.com/saforem2/) -->
<!---->
<!--         - [Building better sampling methods for Lattice QCD](https://github.com/saforem2/l2hmc-qcd) -->
<!---->
<!--         - [GenSLM: Genome-Scale Language Models](https://www.biorxiv.org/content/10.1101/2022.10.10.511571v2) -->
<!---->
<!--         - [Foundation models for long term climate forecasting](https://saforem2.github.io/climate-analysis) -->
<!---->
<!--       ::: -->
<!---->
<!--       ::: {.flex-container} -->
<!---->
<!--       - [Scaling Large Language Models](https://github.com/saforem2/Megatron-DS-Benchmarking) -->
<!---->
<!--       - [Optimizing distributed training across thousands of GPUs](https://github.com/argonne-lcf/mlprof) -->
<!---->
<!--       - Building new parallelism techniques for efficient scaling -->
<!---->
<!--       - Generative modeling (esp. for physical systems) -->
<!---->
<!--       ::: -->
<!---->
<!--       ::: -->
<!---->
<!--     </details> -->
<!---->
<!-- </details> -->

<!-- ::: {.callout-tip icon=false aria-title="Now Playing" title='[[![](assets/lastfm.svg)]{style="display:inline;height:1em!important;"} Now Playing:]{style="color:#D51007; font-size:1.1em;"}' collapse="true" style='width:100%; border: none!important; border: 1px solid rgba(213, 16, 7, 1.0, 0.5)!important; opacity: 100%;'} -->

<!-- ::: {.callout-tip icon=false aria-title="Now Playing" title='üéµ Now Playing' collapse="true" style='width:100%; border: none!important; border: 1px solid rgba(131, 131, 131, 0.05)!important;background-color: rgba(0,0,0,0.0)!important; opacity: 100%;'} -->
<!-- I did my undergrad at the University of Illinois at Urbana-Champaign -->
<!-- (UIUC^[ILL]), where I got bachelors degrees in Engineering Physics and -->
<!-- Applied Mathematics. -->
<!-- I decided on Physics for grad school (the math GRE was brutal üòÇ) and attended -->
<!-- the University of Iowa planning -->

<!---->
<!-- \begin{table}[!ht] -->
<!--     \centering -->
<!--     \begin{tabular}{r|l} -->
<!--         \texttt{dtype} & \texttt{bf16} \\ \hline -->
<!--         \texttt{optimizer} & \texttt{adamw} \\ \hline -->
<!--         \texttt{beta1} & 0.9 \\ \hline -->
<!--         \texttt{beta2} & 0.95 \\ \hline -->
<!--         \texttt{eps} & 0.00001 \\ \hline -->
<!--         \texttt{wdecay} & 0.1 \\ \hline -->
<!--         \texttt{seq} & 4096 \\ \hline -->
<!--         \texttt{num\_layers} & 32 \\ \hline -->
<!--         \texttt{hidden\_size} & 4096 \\ \hline -->
<!--         \texttt{aten\_heads} & 32 \\ \hline -->
<!--         \texttt{micro\_batch} & 4 \\ \hline -->
<!--         \texttt{global\_batch} & 3072 \\ \hline -->
<!--         \texttt{lr} & 0.000\\texttt{3,24} \\ \hline -->
<!--         \texttt{activation} & swigu \\ \hline -->
<!--         \texttt{normalization} & rmsnorm \\ \hline -->
<!--         \texttt{num\_key\_val\_heads} & 8 \\ \hline -->
<!--         \texttt{ffn\_hidden\_size} & 11008 \\ \hline -->
<!--         \texttt{zero\_stage} & 1 \\ \hline -->
<!--         \texttt{activation\_checkpointing} & true \\ \hline -->
<!--         \texttt{flash} & False \\ \hline -->
<!--     \end{tabular} -->
<!-- \end{table} -->
